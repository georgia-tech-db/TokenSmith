
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Chunk Visualization</title>
    <link rel="icon" type="image/svg+xml" href="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdCb3g9IjAgMCAxMDAgMTAwIiB3aWR0aD0iMTAwIiBoZWlnaHQ9IjEwMCI+PHRleHQgeD0iNTAiIHk9IjU1IiBmb250LXNpemU9IjkwIiB0ZXh0LWFuY2hvcj0ibWlkZGxlIiBkb21pbmFudC1iYXNlbGluZT0ibWlkZGxlIj7wn6abPC90ZXh0Pjwvc3ZnPg==">
    <style>
        body { font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif, "Apple Color Emoji", "Segoe UI Emoji", "Segoe UI Symbol"; line-height: 1.6; padding: 0; margin: 0; background-color: #F0F2F5; color: #333333; display: flex; flex-direction: column; min-height: 100vh; }
        .content-box { max-width: 900px; width: 100%; margin: 30px auto; padding: 30px 20px 20px 20px; background-color: #FFFFFF; border-radius: 8px; box-shadow: 0 2px 10px rgba(0,0,0,0.1); box-sizing: border-box; }
        .text-display { white-space: pre-wrap; word-wrap: break-word; font-family: "Consolas", "Monaco", "Courier New", monospace; font-size: 0.95em; padding: 0; }
        .text-display span[style*="background-color"] { border-radius: 3px; padding: 0.1em 0; cursor: help; }
        .text-display br { display: block; content: ""; margin-top: 0.6em; }
        footer { text-align: center; margin-top: auto; padding: 15px 0; font-size: 0.8em; color: #888; border-top: 1px solid #eee; background-color: #f0f2f5; width: 100%; }
        footer a { color: #666; text-decoration: none; }
        footer a:hover { text-decoration: underline; }
        footer .heart { color: #d63384; display: inline-block; }
    </style>
</head>
<body>
    
<div class="content-box">
    <div class="text-display"><span style="background-color: #FFADAD;" title="Chunk 0 | Start: 0 | End: 20000 | Tokens: 2976">For Evaluation Only.<br>dddddd<br>For Evaluation Only.<br>Copyright (c) by Foxit Software Company, 2004<br>Edited by Foxit PDF Editor<br>For Evaluation Only.<br>Copyright (c) by Foxit Software Company, 2004<br>Edited by Foxit PDF Editor<br>Computer <br>Science<br>Volume 1<br>Silberschatz−Korth−Sudarshan  •  Database System Concepts, Fourth Edition  <br>Front Matter <br>1<br>Preface <br>1<br>1. Introduction <br>11<br>Text <br>11<br>I. Data Models <br>35<br>Introduction <br>35<br>2. Entity−Relationship Model <br>36<br>3. Relational Model <br>87<br>II. Relational Databases <br>140<br>Introduction <br>140<br>4. SQL <br>141<br>5. Other Relational Languages <br>194<br>6. Integrity and Security <br>229<br>7. Relational−Database Design <br>260<br>III. Object−Based Databases and XML <br>307<br>Introduction <br>307<br>8. Object−Oriented Databases <br>308<br>9. Object−Relational Databases <br>337<br>10. XML <br>363<br>IV. Data Storage and Querying <br>393<br>Introduction <br>393<br>11. Storage and File Structure <br>394<br>12. Indexing and Hashing <br>446<br>13. Query Processing <br>494<br>14. Query Optimization <br>529<br>V. Transaction Management <br>563<br>Introduction <br>563<br>15. Transactions <br>564<br>16. Concurrency Control <br>590<br>17. Recovery System <br>637<br>iii<br>VI. Database System Architecture <br>679<br>Introduction <br>679<br>18. Database System Architecture <br>680<br>19. Distributed Databases <br>705<br>20. Parallel Databases <br>750<br>VII. Other Topics <br>773<br>Introduction <br>773<br>21. Application Development and Administration <br>774<br>22. Advanced Querying and Information Retrieval <br>810<br>23. Advanced Data Types and New Applications <br>856<br>24. Advanced Transaction Processing <br>884<br>iv<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>Front Matter<br>Preface<br>1<br>© The McGraw−Hill <br>Companies, 2001<br>Preface<br>Database management has evolved from a specialized computer application to a<br>central component of a modern computing environment, and, as a result, knowl-<br>edge about database systems has become an essential part of an education in com-<br>puter science. In this text, we present the fundamental concepts of database manage-<br>ment. These concepts include aspects of database design, database languages, and<br>database-system implementation.<br>This text is intended for a ﬁrst course in databases at the junior or senior under-<br>graduate, or ﬁrst-year graduate, level. In addition to basic material for a ﬁrst course,<br>the text contains advanced material that can be used for course supplements, or as<br>introductory material for an advanced course.<br>We assume only a familiarity with basic data structures, computer organization,<br>and a high-level programming language such as Java, C, or Pascal. We present con-<br>cepts as intuitive descriptions, many of which are based on our running example of<br>a bank enterprise. Important theoretical results are covered, but formal proofs are<br>omitted. The bibliographical notes contain pointers to research papers in which re-<br>sults were ﬁrst presented and proved, as well as references to material for further<br>reading. In place of proofs, ﬁgures and examples are used to suggest why a result is<br>true.<br>The fundamental concepts and algorithms covered in the book are often based<br>on those used in existing commercial or experimental database systems. Our aim is<br>to present these concepts and algorithms in a general setting that is not tied to one<br>particular database system. Details of particular commercial database systems are<br>discussed in Part 8, “Case Studies.”<br>In this fourth edition of Database System Concepts, we have retained the overall style<br>of the ﬁrst three editions, while addressing the evolution of database management.<br>Several new chapters have been added to cover new technologies. Every chapter has<br>been edited, and most have been modiﬁed extensively. We shall describe the changes<br>in detail shortly.<br>xv<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>Front Matter<br>Preface<br>2<br>© The McGraw−Hill <br>Companies, 2001<br>xvi<br>Preface<br>Organization<br>The text is organized in eight major parts, plus three appendices:<br>• Overview (Chapter 1). Chapter 1 provides a general overview of the nature<br>and purpose of database systems. We explain how the concept of a database<br>system has developed, what the common features of database systems are,<br>what a database system does for the user, and how a database system inter-<br>faces with operating systems. We also introduce an example database applica-<br>tion: a banking enterprise consisting of multiple bank branches. This example<br>is used as a running example throughout the book. This chapter is motiva-<br>tional, historical, and explanatory in nature.<br>• Data models (Chapters 2 and 3). Chapter 2 presents the entity-relationship<br>model. This model provides a high-level view of the issues in database design,<br>and of the problems that we encounter in capturing the semantics of realistic<br>applications within the constraints of a data model. Chapter 3 focuses on the<br>relational data model, covering the relevant relational algebra and relational<br>calculus.<br>• Relational databases (Chapters 4 through 7). Chapter 4 focuses on the most<br>inﬂuential of the user-oriented relational languages: SQL. Chapter 5 covers<br>two other relational languages, QBE and Datalog. These two chapters describe<br>data manipulation: queries, updates, insertions, and deletions. Algorithms<br>and design issues are deferred to later chapters. Thus, these chapters are suit-<br>able for introductory courses or those individuals who want to learn the basics<br>of database systems, without getting into the details of the internal algorithms<br>and structure.<br>Chapter 6 presents constraints from the standpoint of database integrity<br>and security; Chapter 7 shows how constraints can be used in the design of<br>a relational database. Referential integrity; mechanisms for integrity mainte-<br>nance, such as triggers and assertions; and authorization mechanisms are pre-<br>sented in Chapter 6. The theme of this chapter is the protection of the database<br>from accidental and intentional damage.<br>Chapter 7 introduces the theory of relational database design. The theory<br>of functional dependencies and normalization is covered, with emphasis on<br>the motivation and intuitive understanding of each normal form. The overall<br>process of database design is also described in detail.<br>• Object-based databases and XML (Chapters 8 through 10). Chapter 8 covers<br>object-oriented databases. It introduces the concepts of object-oriented pro-<br>gramming, and shows how these concepts form the basis for a data model.<br>No prior knowledge of object-oriented languages is assumed. Chapter 9 cov-<br>ers object-relational databases, and shows how the SQL:1999 standard extends<br>the relational data model to include object-oriented features, such as inheri-<br>tance, complex types, and object identity.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>Front Matter<br>Preface<br>3<br>© The McGraw−Hill <br>Companies, 2001<br>Preface<br>xvii<br>Chapter 10 covers the XML standard for data representation, which is see-<br>ing increasing use in data communication and in the storage of complex data<br>types. The chapter also describes query languages for XML.<br>• Data storage and querying (Chapters 11 through 14). Chapter 11 deals with<br>disk, ﬁle, and ﬁle-system structure, and with the mapping of relational and<br>object data to a ﬁle system. A variety of data-access techniques are presented<br>in Chapter 12, including hashing, B+-tree indices, and grid ﬁle indices. Chap-<br>ters 13 and 14 address query-evaluation algorithms, and query optimization<br>based on equivalence-preserving query transformations.<br>These chapters provide an understanding of the internals of the storage and<br>retrieval components of a database.<br>• Transaction management (Chapters 15 through 17). Chapter 15 focuses on<br>the fundamentals of a transaction-processing system, including transaction<br>atomicity, consistency, isolation, and durability, as well as the notion of serial-<br>izability.<br>Chapter 16 focuses on concurrency control and presents several techniques<br>for ensuring serializability, including locking, timestamping, and optimistic<br>(validation) techniques. The chapter also covers deadlock issues. Chapter 17<br>covers the primary techniques for ensuring correct transaction execution de-<br>spite system crashes and disk failures. These techniques include logs, shadow<br>pages, checkpoints, and database dumps.<br>• Database system architecture (Chapters 18 through 20). Chapter 18 covers<br>computer-system architecture, and describes the inﬂuence of the underlying<br>computer system on the database system. We discuss centralized systems,<br>client–server systems, parallel and distributed architectures, and network<br>types in this chapter. Chapter 19 covers distributed database systems, revis-<br>iting the issues of database design, transaction management, and query eval-<br>uation and optimization, in the context of distributed databases. The chap-<br>ter also covers issues of system availability during failures and describes the<br>LDAP directory system.<br>Chapter 20, on parallel databases explores a variety of parallelization tech-<br>niques, including I/O parallelism, interquery and intraquery parallelism, and<br>interoperation and intraoperation parallelism. The chapter also describes<br>parallel-system design.<br>• Other topics (Chapters 21 through 24). Chapter 21 covers database appli-<br>cation development and administration. Topics include database interfaces,<br>particularly Web interfaces, performance tuning, performance benchmarks,<br>standardization, and database issues in e-commerce. Chapter 22 covers query-<br>ing techniques, including decision support systems, and information retrieval.<br>Topics covered in the area of decision support include online analytical pro-<br>cessing (OLAP) techniques, SQL:1999 support for OLAP, data mining, and data<br>warehousing. The chapter also describes information retrieval techniques for<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>Front Matter<br>Preface<br>4<br>© The McGraw−Hill <br>Companies, 2001<br>xviii<br>Preface<br>querying textual data, including hyperlink-based techniques used in Web<br>search engines.<br>Chapter 23 covers advanced data types and new applications, including<br>temporal data, spatial and geographic data, multimedia data, and issues in the<br>management of mobile and personal databases. Finally, Chapter 24 deals with<br>advanced transaction processing. We discuss transaction-processing monitors,<br>high-performance transaction systems, real-time transaction systems, and<br>transactional workﬂows.<br>• Case studies (Chapters 25 through 27). In this part we present case studies of<br>three leading commercial database systems, including Oracle, IBM DB2, and<br>Microsoft SQL Server. These chapters outline unique features of each of these<br>products, and describe their internal structure. They provide a wealth of in-<br>teresting information about the respective products, and help you see how the<br>various implementation techniques described in earlier parts are used in real<br>systems. They also cover several interesting practical aspects in the design of<br>real systems.<br>• Online appendices. Although most new database applications use either the<br>relational model or the object-oriented model, the network and hierarchical<br>data models are still in use. For the beneﬁt of readers who wish to learn about<br>these data models, we provide appendices describing the network and hier-<br>archical data models, in Appendices A and B respectively; the appendices are<br>available only online (http://www.bell-labs.com/topic/books/db-book).<br>Appendix C describes advanced relational database design, including the<br>theory of multivalued dependencies, join dependencies, and the project-join<br>and domain-key normal forms. This appendix is for the beneﬁt of individuals<br>who wish to cover the theory of relational database design in more detail, and<br>instructors who wish to do so in their courses. This appendix, too, is available<br>only online, on the Web page of the book.<br>The Fourth Edition<br>The production of this fourth edition has been guided by the many comments and<br>suggestions we received concerning the earlier editions, by our own observations<br>while teaching at IIT Bombay, and by our analysis of the directions in which database<br>technology is evolving.<br>Our basic procedure was to rewrite the material in each chapter, bringing the older<br>material up to date, adding discussions on recent developments in database technol-<br>ogy, and improving descriptions of topics that students found difﬁcult to understand.<br>Each chapter now has a list of review terms, which can help you review key topics<br>covered in the chapter. We have also added a tools section at the end of most chap-<br>ters, which provide information on software tools related to the topic of the chapter.<br>We have also added new exercises, and updated references.<br>We have added a new chapter covering XML, and three case study chapters cov-<br>ering the leading commercial database systems, including Oracle, IBM DB2, and Mi-<br>crosoft SQL Server.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>Front Matter<br>Preface<br>5<br>© The McGraw−Hill <br>Companies, 2001<br>Preface<br>xix<br>We have organized the chapters into several parts, and reorganized the contents<br>of several chapters. For the beneﬁt of those readers familiar with the third edition,<br>we explain the main changes here:<br>• Entity-relationship model. We have improved our coverage of the entity-<br>relationship (E-R) model. More examples have been added, and some changed,<br>to give better intuition to the reader. A summary of alternative E-R notations<br>has been added, along with a new section on UML.<br>• Relational databases. Our coverage of SQL in Chapter 4 now references the<br>SQL:1999 standard, which was approved after publication of the third edition.<br>SQL coverage has been signiﬁcantly expanded to include the with clause, ex-<br>panded coverage of embedded SQL, and coverage of ODBC and JDBC whose<br>usage has increased greatly in the past few years. Coverage of Quel has been<br>dropped from Chapter 5, since it is no longer in wide use. Coverage of QBE<br>has been revised to remove some ambiguities and to add coverage of the QBE<br>version used in the Microsoft Access database.<br>Chapter 6 now covers integrity constraints and security. Coverage of se-<br>curity has been moved to Chapter 6 from its third-edition position of Chap-<br>ter 19. Chapter 6 also covers triggers. Chapter 7 covers relational-database<br>design and normal forms. Discussion of functional dependencies has been<br>moved into Chapter 7 from its third-edition position of Chapter 6. Chapter<br>7 has been signiﬁcantly rewritten, providing several short-cut algorithms for<br>dealing with functional dependencies and extended coverage of the overall<br>database design process. Axioms for multivalued dependency inference, PJNF<br>and DKNF, have been moved into an appendix.<br>• Object-based databases. Coverage of object orientation in Chapter 8 has been<br>improved, and the discussion of ODMG updated. Object-relational coverage in<br>Chapter 9 has been updated, and in particular the SQL:1999 standard replaces<br>the extended SQL used in the third edition.<br>• XML. Chapter 10, covering XML, is a new chapter in the fourth edition.<br>• Storage, indexing, and query processing. Coverage of storage and ﬁle struc-<br>tures, in Chapter 11, has been updated; this chapter was Chapter 10 in the<br>third edition. Many characteristics of disk drives and other storage mecha-<br>nisms have changed greatly in the past few years, and our coverage has been<br>correspondingly updated. Coverage of RAID has been updated to reﬂect tech-<br>nology trends. Coverage of data dictionaries (catalogs) has been extended.<br>Chapter 12, on indexing, now includes coverage of bitmap indices; this<br>chapter was Chapter 11 in the third edition. The B+-tree insertion algorithm<br>has been simpliﬁed, and pseudocode has been provided for search. Parti-<br>tioned hashing has been dropped, since it is not in signiﬁcant use.<br>Our treatment of query processing has been reorganized, with the earlier<br>chapter (Chapter 12 in the third edition) split into two chapters, one on query<br>processing (Chapter 13) and another on query optimization (Chapter 14). All<br>details regarding cost estimation and query optimization have been moved<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>Front Matter<br>Preface<br>6<br>© The McGraw−Hill <br>Companies, 2001<br>xx<br>Preface<br>to Chapter 14, allowing Chapter 13 to concentrate on query processing algo-<br>rithms. We have dropped several detailed (and tedious) formulae for calcu-<br>lating the exact number of I/O operations for different operations. Chapter 14<br>now has pseudocode for optimization algorithms, and new sections on opti-<br>mization of nested subqueries and on materialized views.<br>• Transaction processing. Chapter 15, which provides an introduction to trans-<br>actions, has been updated; this chapter was numbered Chapter 13 in the third<br>edition. Tests for view serializability have been dropped.<br>Chapter 16, on concurrency control, includes a new section on implemen-<br>tation of lock managers, and a section on weak levels of consistency, which<br>was in Chapter 20 of the third edition. Concurrency control of index structures<br>has been expanded, providing details of the crabbing protocol, which is a sim-<br>pler alternative to the B-link protocol, and next-key locking to avoid the phan-<br>tom problem. Chapter 17, on recovery, now includes coverage of the ARIES<br>recovery algorithm. This chapter also covers remote backup systems for pro-<br>viding high availability despite failures, an increasingly important feature in<br>“24 × 7” applications.<br>As in the third edition, instructors can choose between just introducing<br>transaction-processing concepts (by covering only Chapter 15), or offering de-<br>tailed coverage (based on Chapters 15 through 17).<br>• Database system architectures. Chapter 18, which provides an overview of<br>database system architectures, has been updated to cover current technology;<br>this was Chapter 16 in the third edition. The order of the parallel database<br>chapter and the distributed database chapters has been ﬂipped. While the cov-<br>erage of parallel database query processing techniques in Chapter 20<br>(which was Chapter 16 in the third edition) is mainly of interest to those who<br>wish to learn about database internals, distributed databases, now covered in<br>Chapter 19, is a topic that is more fundamental; it is one that anyone dealing<br>with databases should be familiar with.<br>Chapter 19 on distributed databases has been signiﬁcantly rewritten, to re-<br>duce the emphasis on naming and transparency and to increase coverage of<br>operation during failures, including concurrency control techniques to pro-<br>vide high availability. Coverage of three-phase commit protocol has been ab-<br>breviated, as has distributed detection of global deadlocks, since neither is<br>used much in practice. Coverage of query processing issues in heterogeneous<br>databases has been moved up from Chapter 20 of the third edition. There is<br>a new section on directory systems, in particular LDAP, since these are quite<br>widely used as a mechanism for making information available in a distributed<br>setting.<br>• Other topics. Although we have modiﬁed and updated the entire text, we<br>concentrated our presentation of material pertaining to ongoing database re-<br>search and new database applications in four new chapters, from Chapter 21<br>to Chapter 24.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>Front Matter<br>Preface<br>7<br>© The McGraw−Hill <br>Companies, 2001<br>Preface<br>xxi<br>Chapter 21 is new in the fourth edition and covers application develop-<br>ment and administration. The description of how to build Web interfaces to<br>databases, including servlets and other mechanisms for server-side scripting,<br>is new. The section on performance tuning, which was earlier in Chapter 19,<br>has new material on the famous 5-minute rule and the 1-minute rule, as well<br>as some new examples. Coverage of materialized view selection is also new.<br>Coverage of benchmarks and standards has been updated. There is a new sec-<br>tion on e-commerce, focusing on database issues in e-commerce, and a new<br>section on dealing with legacy systems.<br>Chapter 22, which covers advanced querying and inform</span><br><br><span style="background-color: #FFD6A5;" title="Chunk 1 | Start: 20002 | End: 40002 | Tokens: 3128">ation retrieval,<br>includes new material on OLAP, particulary on SQL:1999 extensions for data<br>analysis. Coverage of data warehousing and data mining has also been ex-<br>tended greatly. Coverage of information retrieval has been signiﬁcantly ex-<br>tended, particulary in the area of Web searching. Earlier versions of this ma-<br>terial were in Chapter 21 of the third edition.<br>Chapter 23, which covers advanced data types and new applications, has<br>material on temporal data, spatial data, multimedia data, and mobile data-<br>bases. This material is an updated version of material that was in Chapter 21<br>of the third edition. Chapter 24, which covers advanced transaction process-<br>ing, contains updated versions of sections on TP monitors, workﬂow systems,<br>main-memory and real-time databases, long-duration transactions, and trans-<br>action management in multidatabases, which appeared in Chapter 20 of the<br>third edition.<br>• Case studies. The case studies covering Oracle, IBM DB2 and Microsoft SQL<br>Server are new to the fourth edition. These chapters outline unique features<br>of each of these products, and describe their internal structure.<br>Instructor’s Note<br>The book contains both basic and advanced material, which might not be covered in<br>a single semester. We have marked several sections as advanced, using the symbol<br>“∗∗”. These sections may be omitted if so desired, without a loss of continuity.<br>It is possible to design courses by using various subsets of the chapters. We outline<br>some of the possibilities here:<br>• Chapter 5 can be omitted if students will not be using QBE or Datalog as part<br>of the course.<br>• If object orientation is to be covered in a separate advanced course, Chapters<br>8 and 9, and Section 11.9, can be omitted. Alternatively, they could constitute<br>the foundation of an advanced course in object databases.<br>• Chapter 10 (XML) and Chapter 14 (query optimization) can be omitted from<br>an introductory course.<br>• Both our coverage of transaction processing (Chapters 15 through 17) and our<br>coverage of database-system architecture (Chapters 18 through 20) consist of<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>Front Matter<br>Preface<br>8<br>© The McGraw−Hill <br>Companies, 2001<br>xxii<br>Preface<br>an overview chapter (Chapters 15 and 18, respectively), followed by chap-<br>ters with details. You might choose to use Chapters 15 and 18, while omitting<br>Chapters 16, 17, 19, and 20, if you defer these latter chapters to an advanced<br>course.<br>• Chapters 21 through 24 are suitable for an advanced course or for self-study<br>by students, although Section 21.1 may be covered in a ﬁrst database course.<br>Model course syllabi, based on the text, can be found on the Web home page of the<br>book (see the following section).<br>Web Page and Teaching Supplements<br>A Web home page for the book is available at the URL:<br>http://www.bell-labs.com/topic/books/db-book<br>The Web page contains:<br>• Slides covering all the chapters of the book<br>• Answers to selected exercises<br>• The three appendices<br>• An up-to-date errata list<br>• Supplementary material contributed by users of the book<br>A complete solution manual will be made available only to faculty. For more infor-<br>mation about how to get a copy of the solution manual, please send electronic mail to<br>customer.service@mcgraw-hill.com. In the United States, you may call 800-338-3987.<br>The McGraw-Hill Web page for this book is<br>http://www.mhhe.com/silberschatz<br>Contacting Us and Other Users<br>We provide a mailing list through which users of our book can communicate among<br>themselves and with us. If you wish to be on the list, please send a message to<br>db-book@research.bell-labs.com, include your name, afﬁliation, title, and electronic<br>mail address.<br>We have endeavored to eliminate typos, bugs, and the like from the text. But, as in<br>new releases of software, bugs probably remain; an up-to-date errata list is accessible<br>from the book’s home page. We would appreciate it if you would notify us of any<br>errors or omissions in the book that are not on the current list of errata.<br>We would be glad to receive suggestions on improvements to the books. We also<br>welcome any contributions to the book Web page that could be of use to other read-<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>Front Matter<br>Preface<br>9<br>© The McGraw−Hill <br>Companies, 2001<br>Preface<br>xxiii<br>ers, such as programming exercises, project suggestions, online labs and tutorials,<br>and teaching tips.<br>E-mail should be addressed to db-book@research.bell-labs.com. Any other cor-<br>respondence should be sent to Avi Silberschatz, Bell Laboratories, Room 2T-310, 600<br>Mountain Avenue, Murray Hill, NJ 07974, USA.<br>Acknowledgments<br>This edition has beneﬁted from the many useful comments provided to us by the<br>numerous students who have used the third edition. In addition, many people have<br>written or spoken to us about the book, and have offered suggestions and comments.<br>Although we cannot mention all these people here, we especially thank the following:<br>• Phil Bernhard, Florida Institute of Technology; Eitan M. Gurari, The Ohio State<br>University; Irwin Levinstein, Old Dominion University; Ling Liu, Georgia In-<br>stitute of Technology; Ami Motro, George Mason University; Bhagirath Nara-<br>hari, Meral Ozsoyoglu, Case Western Reserve University; and Odinaldo Ro-<br>driguez, King’s College London; who served as reviewers of the book and<br>whose comments helped us greatly in formulating this fourth edition.<br>• Soumen Chakrabarti, Sharad Mehrotra, Krithi Ramamritham, Mike Reiter,<br>Sunita Sarawagi, N. L. Sarda, and Dilys Thomas, for extensive and invaluable<br>feedback on several chapters of the book.<br>• Phil Bohannon, for writing the ﬁrst draft of Chapter 10 describing XML.<br>• Hakan Jakobsson (Oracle), Sriram Padmanabhan (IBM), and C´esar Galindo-<br>Legaria, Goetz Graefe, Jos´e A. Blakeley, Kalen Delaney, Michael Rys, Michael<br>Zwilling, Sameet Agarwal, Thomas Casey (all of Microsoft) for writing the<br>appendices describing the Oracle, IBM DB2, and Microsoft SQL Server database<br>systems.<br>• Yuri Breitbart, for help with the distributed database chapter; Mike Reiter, for<br>help with the security sections; and Jim Melton, for clariﬁcations on SQL:1999.<br>• Marilyn Turnamian and Nandprasad Joshi, whose excellent secretarial assis-<br>tance was essential for timely completion of this fourth edition.<br>The publisher was Betsy Jones. The senior developmental editor was Kelley<br>Butcher. The project manager was Jill Peter. The executive marketing manager was<br>John Wannemacher. The cover illustrator was Paul Tumbaugh while the cover de-<br>signer was JoAnne Schopler. The freelance copyeditor was George Watson. The free-<br>lance proofreader was Marie Zartman. The supplement producer was Jodi Banowetz.<br>The designer was Rick Noel. The freelance indexer was Tobiah Waldron.<br>This edition is based on the three previous editions, so we thank once again the<br>many people who helped us with the ﬁrst three editions, including R. B. Abhyankar,<br>Don Batory, Haran Boral, Paul Bourgeois, Robert Brazile, Michael Carey, J. Edwards,<br>Christos Faloutsos, Homma Farian, Alan Fekete, Shashi Gadia, Jim Gray, Le Gruen-<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>Front Matter<br>Preface<br>10<br>© The McGraw−Hill <br>Companies, 2001<br>xxiv<br>Preface<br>wald, Ron Hitchens, Yannis Ioannidis, Hyoung-Joo Kim, Won Kim, Henry Korth (fa-<br>ther of Henry F.), Carol Kroll, Gary Lindstrom, Dave Maier, Keith Marzullo, Fletcher<br>Mattox, Alberto Mendelzon, Hector Garcia-Molina, Ami Motro, Anil Nigam, Cyril<br>Orji, Bruce Porter, Jim Peterson, K. V. Raghavan, Mark Roth, Marek Rusinkiewicz,<br>S. Seshadri, Shashi Shekhar, Amit Sheth, Nandit Soparkar, Greg Speegle, and Mari-<br>anne Winslett. Lyn Dupr´e copyedited the third edition and Sara Strandtman edited<br>the text of the third edition. Greg Speegle, Dawn Bezviner, and K. V. Raghavan helped<br>us to prepare the instructor’s manual for earlier editions. The new cover is an evo-<br>lution of the covers of the ﬁrst three editions; Marilyn Turnamian created an early<br>draft of the cover design for this edition. The idea of using ships as part of the cover<br>concept was originally suggested to us by Bruce Stephan.<br>Finally, Sudarshan would like to acknowledge his wife, Sita, for her love and sup-<br>port, two-year old son Madhur for his love, and mother, Indira, for her support. Hank<br>would like to acknowledge his wife, Joan, and his children, Abby and Joe, for their<br>love and understanding. Avi would like to acknowledge his wife Haya, and his son,<br>Aaron, for their patience and support during the revision of this book.<br>A. S.<br>H. F. K.<br>S. S.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>1. Introduction<br>Text<br>11<br>© The McGraw−Hill <br>Companies, 2001<br>C<br>H<br>A<br>P<br>T<br>E<br>R<br>1<br>Introduction<br>A database-management system (DBMS) is a collection of interrelated data and a<br>set of programs to access those data. The collection of data, usually referred to as the<br>database, contains information relevant to an enterprise. The primary goal of a DBMS<br>is to provide a way to store and retrieve database information that is both convenient<br>and efﬁcient.<br>Database systems are designed to manage large bodies of information. Manage-<br>ment of data involves both deﬁning structures for storage of information and pro-<br>viding mechanisms for the manipulation of information. In addition, the database<br>system must ensure the safety of the information stored, despite system crashes or<br>attempts at unauthorized access. If data are to be shared among several users, the<br>system must avoid possible anomalous results.<br>Because information is so important in most organizations, computer scientists<br>have developed a large body of concepts and techniques for managing data. These<br>concepts and technique form the focus of this book. This chapter brieﬂy introduces<br>the principles of database systems.<br>1.1<br>Database System Applications<br>Databases are widely used. Here are some representative applications:<br>• Banking: For customer information, accounts, and loans, and banking transac-<br>tions.<br>• Airlines: For reservations and schedule information. Airlines were among the<br>ﬁrst to use databases in a geographically distributed manner—terminals sit-<br>uated around the world accessed the central database system through phone<br>lines and other data networks.<br>• Universities: For student information, course registrations, and grades.<br>1<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>1. Introduction<br>Text<br>12<br>© The McGraw−Hill <br>Companies, 2001<br>2<br>Chapter 1<br>Introduction<br>• Credit card transactions: For purchases on credit cards and generation of month-<br>ly statements.<br>• Telecommunication: For keeping records of calls made, generating monthly bills,<br>maintaining balances on prepaid calling cards, and storing information about<br>the communication networks.<br>• Finance: For storing information about holdings, sales, and purchases of ﬁnan-<br>cial instruments such as stocks and bonds.<br>• Sales: For customer, product, and purchase information.<br>• Manufacturing: For management of supply chain and for tracking production<br>of items in factories, inventories of items in warehouses/stores, and orders for<br>items.<br>• Human resources: For information about employees, salaries, payroll taxes and<br>beneﬁts, and for generation of paychecks.<br>As the list illustrates, databases form an essential part of almost all enterprises today.<br>Over the course of the last four decades of the twentieth century, use of databases<br>grew in all enterprises. In the early days, very few people interacted directly with<br>database systems, although without realizing it they interacted with databases in-<br>directly—through printed reports such as credit card statements, or through agents<br>such as bank tellers and airline reservation agents. Then automated teller machines<br>came along and let users interact directly with databases. Phone interfaces to com-<br>puters (interactive voice response systems) also allowed users to deal directly with<br>databases—a caller could dial a number, and press phone keys to enter information<br>or to select alternative options, to ﬁnd ﬂight arrival/departure times, for example, or<br>to register for courses in a university.<br>The internet revolution of the late 1990s sharply increased direct user access to<br>databases. Organizations converted many of their phone interfaces to databases into<br>Web interfaces, and made a variety of services and information available online. For<br>instance, when you access an online bookstore and browse a book or music collec-<br>tion, you are accessing data stored in a database. When you enter an order online,<br>your order is stored in a database. When you access a bank Web site and retrieve<br>your bank balance and transaction information, the information is retrieved from the<br>bank’s database system. When you access a Web site, information about you may be<br>retrieved from a database, to select which advertisements should be shown to you.<br>Furthermore, data about your Web accesses may be stored in a database.<br>Thus, although user interfaces hide details of access to a database, and most people<br>are not even aware they are dealing with a database, accessing databases forms an<br>essential part of almost everyone’s life today.<br>The importance of database systems can be judged in another way—today, data-<br>base system vendors like Oracle are among the largest software companies in the<br>world, and database systems form an important part of the product line of more<br>diversiﬁed companies like Microsoft and IBM.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>1. Introduction<br>Text<br>13<br>© The McGraw−Hill <br>Companies, 2001<br>1.2<br>Database Systems versus File Systems<br>3<br>1.2<br>Database Systems versus File Systems<br>Consider part of a savings-bank enterprise that keeps information about all cus-<br>tomers and savings accounts. One way to keep the information on a computer is<br>to store it in operating system ﬁles. To allow users to manipulate the information, the<br>system has a number of application programs that manipulate the ﬁles, including<br>• A program to debit or credit an account<br>• A program to add a new account<br>• A program to ﬁnd the balance of an account<br>• A program to generate monthly statements<br>System programmers wrote these application programs to meet the needs of the<br>bank.<br>New application programs are added to the system as the need arises. For exam-<br>ple, suppose that the savings bank decides to offer checking accounts. As a result,<br>the bank creates new permanent ﬁles that contain information about all the checking<br>accounts maintained in the bank, and it may have to write new application programs<br>to deal with situations that do not arise in savings accounts, such as overdrafts. Thus,<br>as time goes by, the system acquires more ﬁles and more application programs.<br>This typical ﬁle-processing system is supported by a conventional operating sys-<br>tem. The system stores permanent records in various ﬁles, and it needs different<br>application programs to extract records from, and add records to, the appropriate<br>ﬁles. Before database management systems (DBMSs) came along, organizations usu-<br>ally stored information in such systems.<br>Keeping organizational information in a ﬁle-processing system has a number of<br>major disadvantages:<br>• Data redundancy and inconsistency. Since different programmers create the<br>ﬁles and application programs over a long period, the various ﬁles are likely<br>to have different formats and the programs may be written in several pro-<br>gramming languages. Moreover, the same information may be duplicated in<br>several places (ﬁles). For example, the address and telephone number of a par-<br>ticular customer may appear in a ﬁle that consists of savings-account records<br>and in a ﬁle that consists of checking-account records. This redundancy leads<br>to higher storage and access cost. In addition, it may lead to data inconsis-<br>tency; that is, the various copies of the same data may no longer agree. For<br>example, a changed customer address may be reﬂected in savings-account<br>records but not elsewhere in the system.<br>• Difﬁculty in accessing data. Suppose that one of the bank ofﬁcers needs to<br>ﬁnd out the names of all customers who live within a particular postal-code<br>area. The ofﬁcer asks the data-processing department to generate such a list.<br>Because the designers of the original system did not anticipate this request,<br>there is no application program on hand to meet it. There is, however, an ap-<br>plication program to generate the list of all customers. The bank ofﬁcer has<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>1. Introduction<br>Text<br>14<br>© The McGraw−Hill <br>Companies, 2001<br>4<br>Chapter 1<br>Introduction<br>now two choices: either obtain the list of all customers and extract the needed<br>information manually or ask a system programmer to write the necessary<br>application program. Both alternatives are obviously unsatisfactory. Suppose<br>that such a program is written, and that, several days later, the same ofﬁcer<br>needs to trim that list to include only those customers who have an account<br>balance of $10,000 or more. As expected, a program to generate such a list does<br>not exist. Again, the ofﬁcer has the preceding two options, neither of which is<br>satisfactory.<br>The point here is that conventional ﬁle-processing environments do not al-<br>low needed data to be retrieved in a convenient and efﬁcient manner. More<br>responsive data-retrieval systems are required for general use.<br>• Data isolation. Because data are scattered in various ﬁles, and ﬁles may be in<br>different formats, writing new application programs to retrieve the appropri-<br>ate data is difﬁcult.<br>• Integrity problems. The data values stored in the database must satisfy cer-<br>tain types of consistency constraints. For example, the balance of a bank ac-<br>count may never fall below a prescribed amount (say, $25). Developers enforce<br>these constraints in the system by adding appropriate code in the various ap-<br>plication programs. However, when new constraints are added, it is difﬁcult<br>to change the programs to enforce them. The problem is compounded when<br>constraints involve several data items from different ﬁles.<br>• Atomicity problems. A computer system, like any other mechanical or elec-<br>trical device, is subject to failure. In many applications, it is crucial that, if a<br>failure occurs, the data be restored to the consistent state that existed prior to<br>the failure. Consider a program to transfer $50 from account A to account B.<br>If a system failure occurs during the execution of the program, it is possible<br>that the $50 was removed from account A but was not credited to account B,<br>resulting in an inconsistent database state. Clearly, it is essential to database<br>consistency that either both the credit and debit occur, or that neither occur.<br>That is, the funds transfer must be atomic—it must happen in its entirety or<br>not at all. It is difﬁcult to ensure atomicity in a conventional ﬁle-processing<br>system.<br>• Concurrent-access anomalies. For the sake of overall performance of the sys-<br>tem and faster response, many systems allow multiple users to update the<br>data simultaneously. In such an environment, interaction of concurrent up-<br>dates may result in inconsistent data. Consider bank account A, containing<br>$500. If two customers withdraw funds (say $50 and $100 respectively) from<br>account A at about the same time, the result of the concurrent executions may<br>leave the account in an incorrect (or inconsistent) state. Suppose that the pro-<br>grams executing on behalf of each withdrawal read the old balance, reduce<br>that value by the amount being withdrawn, and write the result back. If the<br>two programs run concurrently, they may both read the value $500, and write<br>back $450 and $400, respectively. Depending on which one writes the value<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>1. Introduction<br>Text<br>15<br>© The McGraw−Hill <br>Companies, 2001<br>1.3<br>View of Data<br>5<br>last, the account may contain either $450 or $400, rather than the correct value<br>of $350. To guard against this possibili</span><br><br><span style="background-color: #FDFFB6;" title="Chunk 2 | Start: 40004 | End: 60004 | Tokens: 3117">ty, the system must maintain some form<br>of supervision. But supervision is difﬁcult to provide because data may be<br>accessed by many different application programs that have not been coordi-<br>nated previously.<br>• Security problems. Not every user of the database system should be able to<br>access all the data. For example, in a banking system, payroll personnel need<br>to see only that part of the database that has information about the various<br>bank employees. They do not need access to information about customer ac-<br>counts. But, since application programs are added to the system in an ad hoc<br>manner, enforcing such security constraints is difﬁcult.<br>These difﬁculties, among others, prompted the development of database systems.<br>In what follows, we shall see the concepts and algorithms that enable database sys-<br>tems to solve the problems with ﬁle-processing systems. In most of this book, we<br>use a bank enterprise as a running example of a typical data-processing application<br>found in a corporation.<br>1.3<br>View of Data<br>A database system is a collection of interrelated ﬁles and a set of programs that allow<br>users to access and modify these ﬁles. A major purpose of a database system is to<br>provide users with an abstract view of the data. That is, the system hides certain<br>details of how the data are stored and maintained.<br>1.3.1<br>Data Abstraction<br>For the system to be usable, it must retrieve data efﬁciently. The need for efﬁciency<br>has led designers to use complex data structures to represent data in the database.<br>Since many database-systems users are not computer trained, developers hide the<br>complexity from users through several levels of abstraction, to simplify users’ inter-<br>actions with the system:<br>• Physical level. The lowest level of abstraction describes how the data are actu-<br>ally stored. The physical level describes complex low-level data structures in<br>detail.<br>• Logical level. The next-higher level of abstraction describes what data are<br>stored in the database, and what relationships exist among those data. The<br>logical level thus describes the entire database in terms of a small number<br>of relatively simple structures. Although implementation of the simple struc-<br>tures at the logical level may involve complex physical-level structures, the<br>user of the logical level does not need to be aware of this complexity. Database<br>administrators, who must decide what information to keep in the database,<br>use the logical level of abstraction.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>1. Introduction<br>Text<br>16<br>© The McGraw−Hill <br>Companies, 2001<br>6<br>Chapter 1<br>Introduction<br>• View level. The highest level of abstraction describes only part of the entire<br>database. Even though the logical level uses simpler structures, complexity<br>remains because of the variety of information stored in a large database. Many<br>users of the database system do not need all this information; instead, they<br>need to access only a part of the database. The view level of abstraction exists<br>to simplify their interaction with the system. The system may provide many<br>views for the same database.<br>Figure 1.1 shows the relationship among the three levels of abstraction.<br>An analogy to the concept of data types in programming languages may clarify<br>the distinction among levels of abstraction. Most high-level programming languages<br>support the notion of a record type. For example, in a Pascal-like language, we may<br>declare a record as follows:<br>type customer = record<br>customer-id : string;<br>customer-name : string;<br>customer-street : string;<br>customer-city : string;<br>end;<br>This code deﬁnes a new record type called customer with four ﬁelds. Each ﬁeld has<br>a name and a type associated with it. A banking enterprise may have several such<br>record types, including<br>• account, with ﬁelds account-number and balance<br>• employee, with ﬁelds employee-name and salary<br>At the physical level, a customer, account, or employee record can be described as a<br>block of consecutive storage locations (for example, words or bytes). The language<br>view 1<br>view 2<br>logical<br>level<br>physical<br>level<br>view n<br>…<br>view level<br>Figure 1.1<br>The three levels of data abstraction.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>1. Introduction<br>Text<br>17<br>© The McGraw−Hill <br>Companies, 2001<br>1.4<br>Data Models<br>7<br>compiler hides this level of detail from programmers. Similarly, the database system<br>hides many of the lowest-level storage details from database programmers. Database<br>administrators, on the other hand, may be aware of certain details of the physical<br>organization of the data.<br>At the logical level, each such record is described by a type deﬁnition, as in the<br>previous code segment, and the interrelationship of these record types is deﬁned as<br>well. Programmers using a programming language work at this level of abstraction.<br>Similarly, database administrators usually work at this level of abstraction.<br>Finally, at the view level, computer users see a set of application programs that<br>hide details of the data types. Similarly, at the view level, several views of the database<br>are deﬁned, and database users see these views. In addition to hiding details of the<br>logical level of the database, the views also provide a security mechanism to prevent<br>users from accessing certain parts of the database. For example, tellers in a bank see<br>only that part of the database that has information on customer accounts; they cannot<br>access information about salaries of employees.<br>1.3.2<br>Instances and Schemas<br>Databases change over time as information is inserted and deleted. The collection of<br>information stored in the database at a particular moment is called an instance of the<br>database. The overall design of the database is called the database schema. Schemas<br>are changed infrequently, if at all.<br>The concept of database schemas and instances can be understood by analogy to<br>a program written in a programming language. A database schema corresponds to<br>the variable declarations (along with associated type deﬁnitions) in a program. Each<br>variable has a particular value at a given instant. The values of the variables in a<br>program at a point in time correspond to an instance of a database schema.<br>Database systems have several schemas, partitioned according to the levels of ab-<br>straction. The physical schema describes the database design at the physical level,<br>while the logical schema describes the database design at the logical level. A database<br>may also have several schemas at the view level, sometimes called subschemas, that<br>describe different views of the database.<br>Of these, the logical schema is by far the most important, in terms of its effect on<br>application programs, since programmers construct applications by using the logical<br>schema. The physical schema is hidden beneath the logical schema, and can usually<br>be changed easily without affecting application programs. Application programs are<br>said to exhibit physical data independence if they do not depend on the physical<br>schema, and thus need not be rewritten if the physical schema changes.<br>We study languages for describing schemas, after introducing the notion of data<br>models in the next section.<br>1.4<br>Data Models<br>Underlying the structure of a database is the data model: a collection of conceptual<br>tools for describing data, data relationships, data semantics, and consistency con-<br>straints. To illustrate the concept of a data model, we outline two data models in this<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>1. Introduction<br>Text<br>18<br>© The McGraw−Hill <br>Companies, 2001<br>8<br>Chapter 1<br>Introduction<br>section: the entity-relationship model and the relational model. Both provide a way<br>to describe the design of a database at the logical level.<br>1.4.1<br>The Entity-Relationship Model<br>The entity-relationship (E-R) data model is based on a perception of a real world that<br>consists of a collection of basic objects, called entities, and of relationships among these<br>objects. An entity is a “thing” or “object” in the real world that is distinguishable<br>from other objects. For example, each person is an entity, and bank accounts can be<br>considered as entities.<br>Entities are described in a database by a set of attributes. For example, the at-<br>tributes account-number and balance may describe one particular account in a bank,<br>and they form attributes of the account entity set. Similarly, attributes customer-name,<br>customer-street address and customer-city may describe a customer entity.<br>An extra attribute customer-id is used to uniquely identify customers (since it may<br>be possible to have two customers with the same name, street address, and city).<br>A unique customer identiﬁer must be assigned to each customer. In the United States,<br>many enterprises use the social-security number of a person (a unique number the<br>U.S. government assigns to every person in the United States) as a customer<br>identiﬁer.<br>A relationship is an association among several entities. For example, a depositor<br>relationship associates a customer with each account that she has. The set of all enti-<br>ties of the same type and the set of all relationships of the same type are termed an<br>entity set and relationship set, respectively.<br>The overall logical structure (schema) of a database can be expressed graphically<br>by an E-R diagram, which is built up from the following components:<br>• Rectangles, which represent entity sets<br>• Ellipses, which represent attributes<br>• Diamonds, which represent relationships among entity sets<br>• Lines, which link attributes to entity sets and entity sets to relationships<br>Each component is labeled with the entity or relationship that it represents.<br>As an illustration, consider part of a database banking system consisting of<br>customers and of the accounts that these customers have. Figure 1.2 shows the cor-<br>responding E-R diagram. The E-R diagram indicates that there are two entity sets,<br>customer and account, with attributes as outlined earlier. The diagram also shows a<br>relationship depositor between customer and account.<br>In addition to entities and relationships, the E-R model represents certain con-<br>straints to which the contents of a database must conform. One important constraint<br>is mapping cardinalities, which express the number of entities to which another en-<br>tity can be associated via a relationship set. For example, if each account must belong<br>to only one customer, the E-R model can express that constraint.<br>The entity-relationship model is widely used in database design, and Chapter 2<br>explores it in detail.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>1. Introduction<br>Text<br>19<br>© The McGraw−Hill <br>Companies, 2001<br>1.4<br>Data Models<br>9<br>customer-name<br>customer-street<br>customer-id<br>customer-city<br>customer<br>balance<br>account<br>depositor<br>account-number<br>Figure 1.2<br>A sample E-R diagram.<br>1.4.2<br>Relational Model<br>The relational model uses a collection of tables to represent both data and the rela-<br>tionships among those data. Each table has multiple columns, and each column has<br>a unique name. Figure 1.3 presents a sample relational database comprising three ta-<br>bles: One shows details of bank customers, the second shows accounts, and the third<br>shows which accounts belong to which customers.<br>The ﬁrst table, the customer table, shows, for example, that the customer identiﬁed<br>by customer-id 192-83-7465 is named Johnson and lives at 12 Alma St. in Palo Alto.<br>The second table, account, shows, for example, that account A-101 has a balance of<br>$500, and A-201 has a balance of $900.<br>The third table shows which accounts belong to which customers. For example,<br>account number A-101 belongs to the customer whose customer-id is 192-83-7465,<br>namely Johnson, and customers 192-83-7465 (Johnson) and 019-28-3746 (Smith) share<br>account number A-201 (they may share a business venture).<br>The relational model is an example of a record-based model. Record-based mod-<br>els are so named because the database is structured in ﬁxed-format records of several<br>types. Each table contains records of a particular type. Each record type deﬁnes a<br>ﬁxed number of ﬁelds, or attributes. The columns of the table correspond to the at-<br>tributes of the record type.<br>It is not hard to see how tables may be stored in ﬁles. For instance, a special<br>character (such as a comma) may be used to delimit the different attributes of a<br>record, and another special character (such as a newline character) may be used to<br>delimit records. The relational model hides such low-level implementation details<br>from database developers and users.<br>The relational data model is the most widely used data model, and a vast majority<br>of current database systems are based on the relational model. Chapters 3 through 7<br>cover the relational model in detail.<br>The relational model is at a lower level of abstraction than the E-R model. Database<br>designs are often carried out in the E-R model, and then translated to the relational<br>model; Chapter 2 describes the translation process. For example, it is easy to see that<br>the tables customer and account correspond to the entity sets of the same name, while<br>the table depositor corresponds to the relationship set depositor.<br>We also note that it is possible to create schemas in the relational model that have<br>problems such as unnecessarily duplicated information. For example, suppose we<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>1. Introduction<br>Text<br>20<br>© The McGraw−Hill <br>Companies, 2001<br>10<br>Chapter 1<br>Introduction<br>customer-id<br>customer-name<br>customer-street<br>customer-city<br>192-83-7465<br>Johnson<br>12 Alma St.<br>Palo Alto<br>019-28-3746<br>Smith<br>4 North St.<br>Rye<br>677-89-9011<br>Hayes<br>3 Main St.<br>Harrison<br>182-73-6091<br>Turner<br>123 Putnam Ave.<br>Stamford<br>321-12-3123<br>Jones<br>100 Main St.<br>Harrison<br>336-66-9999<br>Lindsay<br>175 Park Ave.<br>Pittsfield<br>019-28-3746<br>Smith<br>72 North St.<br>Rye<br>(a) The customer table<br>account-number<br>balance<br>A-101<br>500<br>A-215<br>700<br>A-102<br>400<br>A-305<br>350<br>A-201<br>900<br>A-217<br>750<br>A-222<br>700<br>(b) The account table<br>customer-id<br>account-number<br>192-83-7465<br>A-101<br>192-83-7465<br>A-201<br>019-28-3746<br>A-215<br>677-89-9011<br>A-102<br>182-73-6091<br>A-305<br>321-12-3123<br>A-217<br>336-66-9999<br>A-222<br>019-28-3746<br>A-201<br>(c) The depositor table<br>Figure 1.3<br>A sample relational database.<br>store account-number as an attribute of the customer record. Then, to represent the fact<br>that accounts A-101 and A-201 both belong to customer Johnson (with customer-id<br>192-83-7465), we would need to store two rows in the customer table. The values for<br>customer-name, customer-street, and customer-city for Johnson would get unneces-<br>sarily duplicated in the two rows. In Chapter 7, we shall study how to distinguish<br>good schema designs from bad schema designs.<br>1.4.3<br>Other Data Models<br>The object-oriented data model is another data model that has seen increasing atten-<br>tion. The object-oriented model can be seen as extending the E-R model with notions<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>1. Introduction<br>Text<br>21<br>© The McGraw−Hill <br>Companies, 2001<br>1.5<br>Database Languages<br>11<br>of encapsulation, methods (functions), and object identity. Chapter 8 examines the<br>object-oriented data model.<br>The object-relational data model combines features of the object-oriented data<br>model and relational data model. Chapter 9 examines it.<br>Semistructured data models permit the speciﬁcation of data where individual data<br>items of the same type may have different sets of attributes. This is in contrast with<br>the data models mentioned earlier, where every data item of a particular type must<br>have the same set of attributes. The extensible markup language (XML) is widely<br>used to represent semistructured data. Chapter 10 covers it.<br>Historically, two other data models, the network data model and the hierarchical<br>data model, preceded the relational data model. These models were tied closely to<br>the underlying implementation, and complicated the task of modeling data. As a<br>result they are little used now, except in old database code that is still in service in<br>some places. They are outlined in Appendices A and B, for interested readers.<br>1.5<br>Database Languages<br>A database system provides a data deﬁnition language to specify the database sche-<br>ma and a data manipulation language to express database queries and updates. In<br>practice, the data deﬁnition and data manipulation languages are not two separate<br>languages; instead they simply form parts of a single database language, such as the<br>widely used SQL language.<br>1.5.1<br>Data-Deﬁnition Language<br>We specify a database schema by a set of deﬁnitions expressed by a special language<br>called a data-deﬁnition language (DDL).<br>For instance, the following statement in the SQL language deﬁnes the account table:<br>create table account<br>(account-number char(10),<br>balance integer)<br>Execution of the above DDL statement creates the account table. In addition, it up-<br>dates a special set of tables called the data dictionary or data directory.<br>A data dictionary contains metadata—that is, data about data. The schema of a ta-<br>ble is an example of metadata. A database system consults the data dictionary before<br>reading or modifying actual data.<br>We specify the storage structure and access methods used by the database system<br>by a set of statements in a special type of DDL called a data storage and deﬁnition lan-<br>guage. These statements deﬁne the implementation details of the database schemas,<br>which are usually hidden from the users.<br>The data values stored in the database must satisfy certain consistency constraints.<br>For example, suppose the balance on an account should not fall below $100. The DDL<br>provides facilities to specify such constraints. The database systems check these con-<br>straints every time the database is updated.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>1. Introduction<br>Text<br>22<br>© The McGraw−Hill <br>Companies, 2001<br>12<br>Chapter 1<br>Introduction<br>1.5.2<br>Data-Manipulation Language<br>Data manipulation is<br>• The retrieval of information stored in the database<br>• The insertion of new information into the database<br>• The deletion of information from the database<br>• The modiﬁcation of information stored in the database<br>A data-manipulation language (DML) is a language that enables users to access<br>or manipulate data as organized by the appropriate data model. There are basically<br>two types:<br>• Procedural DMLs require a user to specify what data are needed and how to<br>get those data.<br>• Declarative DMLs (also referred to as nonprocedural DMLs) require a user to<br>specify what data are needed without specifying how to get those data.<br>Declarative DMLs are usually easier to learn and use than are procedural DMLs.<br>However, since a user does not have to specify how to get the data, the database<br>system has to ﬁgure out an efﬁcient means of accessing data. The DML component of<br>the SQL language is nonprocedural.<br>A query is a statement requesting the retrieval of information. The portion of a<br>DML that involves information retrieval is called a query language. Although tech-<br>nically incorrect, it is common practice to use the terms query language and data-<br>manipulation language synonymously.<br>This query in the SQL language ﬁnds the name of the customer whose customer-id<br>is 192-83-7465:<br>select customer.customer-name<br>from customer<br>where customer.customer-id = 192-83-7465<br>The query speciﬁes that those rows from the table customer where the customer-id is<br>192-83-7465 must be retrieved, and the customer-name attribute of these rows must be<br>displayed. If the query were run on the table in Figure 1.3, the name Johnson would<br>be displayed.<br>Queries may involve information from more than one table. For instance, the fol-<br>lowing query ﬁnds the balance of all accounts owned by the customer with customer-<br>id 192-83-7465.<br>select account.balance<br>from depositor, account<br>where depositor.customer-id = 192-83-7465 and<br>depositor.account-number = account.account-number<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>1. Introduction<br>Text<br>23<br>© The McGraw−Hill <br>Companies, 2001<br>1.6<br>Database Users and Administrators<br>1</span><br><br><span style="background-color: #CAFFBF;" title="Chunk 3 | Start: 60006 | End: 80006 | Tokens: 3119">3<br>If the above query were run on the tables in Figure 1.3, the system would ﬁnd that<br>the two accounts numbered A-101 and A-201 are owned by customer 192-83-7465<br>and would print out the balances of the two accounts, namely 500 and 900.<br>There are a number of database query languages in use, either commercially or<br>experimentally. We study the most widely used query language, SQL, in Chapter 4.<br>We also study some other query languages in Chapter 5.<br>The levels of abstraction that we discussed in Section 1.3 apply not only to deﬁning<br>or structuring data, but also to manipulating data. At the physical level, we must<br>deﬁne algorithms that allow efﬁcient access to data. At higher levels of abstraction,<br>we emphasize ease of use. The goal is to allow humans to interact efﬁciently with the<br>system. The query processor component of the database system (which we study in<br>Chapters 13 and 14) translates DML queries into sequences of actions at the physical<br>level of the database system.<br>1.5.3<br>Database Access from Application Programs<br>Application programs are programs that are used to interact with the database. Ap-<br>plication programs are usually written in a host language, such as Cobol, C, C++, or<br>Java. Examples in a banking system are programs that generate payroll checks, debit<br>accounts, credit accounts, or transfer funds between accounts.<br>To access the database, DML statements need to be executed from the host lan-<br>guage. There are two ways to do this:<br>• By providing an application program interface (set of procedures) that can<br>be used to send DML and DDL statements to the database, and retrieve the<br>results.<br>The Open Database Connectivity (ODBC) standard deﬁned by Microsoft<br>for use with the C language is a commonly used application program inter-<br>face standard. The Java Database Connectivity (JDBC) standard provides cor-<br>responding features to the Java language.<br>• By extending the host language syntax to embed DML calls within the host<br>language program. Usually, a special character prefaces DML calls, and a pre-<br>processor, called the DML precompiler, converts the DML statements to nor-<br>mal procedure calls in the host language.<br>1.6<br>Database Users and Administrators<br>A primary goal of a database system is to retrieve information from and store new<br>information in the database. People who work with a database can be categorized as<br>database users or database administrators.<br>1.6.1<br>Database Users and User Interfaces<br>There are four different types of database-system users, differentiated by the way<br>they expect to interact with the system. Different types of user interfaces have been<br>designed for the different types of users.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>1. Introduction<br>Text<br>24<br>© The McGraw−Hill <br>Companies, 2001<br>14<br>Chapter 1<br>Introduction<br>• Naive users are unsophisticated users who interact with the system by invok-<br>ing one of the application programs that have been written previously. For<br>example, a bank teller who needs to transfer $50 from account A to account B<br>invokes a program called transfer. This program asks the teller for the amount<br>of money to be transferred, the account from which the money is to be trans-<br>ferred, and the account to which the money is to be transferred.<br>As another example, consider a user who wishes to ﬁnd her account bal-<br>ance over the World Wide Web. Such a user may access a form, where she<br>enters her account number. An application program at the Web server then<br>retrieves the account balance, using the given account number, and passes<br>this information back to the user.<br>The typical user interface for naive users is a forms interface, where the<br>user can ﬁll in appropriate ﬁelds of the form. Naive users may also simply<br>read reports generated from the database.<br>• Application programmers are computer professionals who write application<br>programs. Application programmers can choose from many tools to develop<br>user interfaces. Rapid application development (RAD) tools are tools that en-<br>able an application programmer to construct forms and reports without writ-<br>ing a program. There are also special types of programming languages that<br>combine imperative control structures (for example, for loops, while loops<br>and if-then-else statements) with statements of the data manipulation lan-<br>guage. These languages, sometimes called fourth-generation languages, often<br>include special features to facilitate the generation of forms and the display of<br>data on the screen. Most major commercial database systems include a fourth-<br>generation language.<br>• Sophisticated users interact with the system without writing programs. In-<br>stead, they form their requests in a database query language. They submit<br>each such query to a query processor, whose function is to break down DML<br>statements into instructions that the storage manager understands. Analysts<br>who submit queries to explore data in the database fall in this category.<br>Online analytical processing (OLAP) tools simplify analysts’ tasks by let-<br>ting them view summaries of data in different ways. For instance, an analyst<br>can see total sales by region (for example, North, South, East, and West), or by<br>product, or by a combination of region and product (that is, total sales of each<br>product in each region). The tools also permit the analyst to select speciﬁc re-<br>gions, look at data in more detail (for example, sales by city within a region)<br>or look at the data in less detail (for example, aggregate products together by<br>category).<br>Another class of tools for analysts is data mining tools, which help them<br>ﬁnd certain kinds of patterns in data.<br>We study OLAP tools and data mining in Chapter 22.<br>• Specialized users are sophisticated users who write specialized database<br>applications that do not ﬁt into the traditional data-processing framework.<br>Among these applications are computer-aided design systems, knowledge-<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>1. Introduction<br>Text<br>25<br>© The McGraw−Hill <br>Companies, 2001<br>1.7<br>Transaction Management<br>15<br>base and expert systems, systems that store data with complex data types (for<br>example, graphics data and audio data), and environment-modeling systems.<br>Chapters 8 and 9 cover several of these applications.<br>1.6.2<br>Database Administrator<br>One of the main reasons for using DBMSs is to have central control of both the data<br>and the programs that access those data. A person who has such central control over<br>the system is called a database administrator (DBA). The functions of a DBA include:<br>• Schema deﬁnition. The DBA creates the original database schema by execut-<br>ing a set of data deﬁnition statements in the DDL.<br>• Storage structure and access-method deﬁnition.<br>• Schema and physical-organization modiﬁcation. The DBA carries out chang-<br>es to the schema and physical organization to reﬂect the changing needs of the<br>organization, or to alter the physical organization to improve performance.<br>• Granting of authorization for data access. By granting different types of<br>authorization, the database administrator can regulate which parts of the data-<br>base various users can access. The authorization information is kept in a<br>special system structure that the database system consults whenever some-<br>one attempts to access the data in the system.<br>• Routine maintenance. Examples of the database administrator’s routine<br>maintenance activities are:<br>  Periodically backing up the database, either onto tapes or onto remote<br>servers, to prevent loss of data in case of disasters such as ﬂooding.<br>  Ensuring that enough free disk space is available for normal operations,<br>and upgrading disk space as required.<br>  Monitoring jobs running on the database and ensuring that performance<br>is not degraded by very expensive tasks submitted by some users.<br>1.7<br>Transaction Management<br>Often, several operations on the database form a single logical unit of work. An ex-<br>ample is a funds transfer, as in Section 1.2, in which one account (say A) is debited and<br>another account (say B) is credited. Clearly, it is essential that either both the credit<br>and debit occur, or that neither occur. That is, the funds transfer must happen in its<br>entirety or not at all. This all-or-none requirement is called atomicity. In addition, it<br>is essential that the execution of the funds transfer preserve the consistency of the<br>database. That is, the value of the sum A + B must be preserved. This correctness<br>requirement is called consistency. Finally, after the successful execution of a funds<br>transfer, the new values of accounts A and B must persist, despite the possibility of<br>system failure. This persistence requirement is called durability.<br>A transaction is a collection of operations that performs a single logical function<br>in a database application. Each transaction is a unit of both atomicity and consis-<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>1. Introduction<br>Text<br>26<br>© The McGraw−Hill <br>Companies, 2001<br>16<br>Chapter 1<br>Introduction<br>tency. Thus, we require that transactions do not violate any database-consistency<br>constraints. That is, if the database was consistent when a transaction started, the<br>database must be consistent when the transaction successfully terminates. However,<br>during the execution of a transaction, it may be necessary temporarily to allow incon-<br>sistency, since either the debit of A or the credit of B must be done before the other.<br>This temporary inconsistency, although necessary, may lead to difﬁculty if a failure<br>occurs.<br>It is the programmer’s responsibility to deﬁne properly the various transactions,<br>so that each preserves the consistency of the database. For example, the transaction to<br>transfer funds from account A to account B could be deﬁned to be composed of two<br>separate programs: one that debits account A, and another that credits account B. The<br>execution of these two programs one after the other will indeed preserve consistency.<br>However, each program by itself does not transform the database from a consistent<br>state to a new consistent state. Thus, those programs are not transactions.<br>Ensuring the atomicity and durability properties is the responsibility of the data-<br>base system itself—speciﬁcally, of the transaction-management component. In the<br>absence of failures, all transactions complete successfully, and atomicity is achieved<br>easily. However, because of various types of failure, a transaction may not always<br>complete its execution successfully. If we are to ensure the atomicity property, a failed<br>transaction must have no effect on the state of the database. Thus, the database must<br>be restored to the state in which it was before the transaction in question started exe-<br>cuting. The database system must therefore perform failure recovery, that is, detect<br>system failures and restore the database to the state that existed prior to the occur-<br>rence of the failure.<br>Finally, when several transactions update the database concurrently, the consis-<br>tency of data may no longer be preserved, even though each individual transac-<br>tion is correct. It is the responsibility of the concurrency-control manager to control<br>the interaction among the concurrent transactions, to ensure the consistency of the<br>database.<br>Database systems designed for use on small personal computers may not have<br>all these features. For example, many small systems allow only one user to access<br>the database at a time. Others do not offer backup and recovery, leaving that to the<br>user. These restrictions allow for a smaller data manager, with fewer requirements for<br>physical resources—especially main memory. Although such a low-cost, low-feature<br>approach is adequate for small personal databases, it is inadequate for a medium- to<br>large-scale enterprise.<br>1.8<br>Database System Structure<br>A database system is partitioned into modules that deal with each of the responsi-<br>bilites of the overall system. The functional components of a database system can be<br>broadly divided into the storage manager and the query processor components.<br>The storage manager is important because databases typically require a large<br>amount of storage space. Corporate databases range in size from hundreds of gi-<br>gabytes to, for the largest databases, terabytes of data. A gigabyte is 1000 megabytes<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>1. Introduction<br>Text<br>27<br>© The McGraw−Hill <br>Companies, 2001<br>1.8<br>Database System Structure<br>17<br>(1 billion bytes), and a terabyte is 1 million megabytes (1 trillion bytes). Since the<br>main memory of computers cannot store this much information, the information is<br>stored on disks. Data are moved between disk storage and main memory as needed.<br>Since the movement of data to and from disk is slow relative to the speed of the cen-<br>tral processing unit, it is imperative that the database system structure the data so as<br>to minimize the need to move data between disk and main memory.<br>The query processor is important because it helps the database system simplify<br>and facilitate access to data. High-level views help to achieve this goal; with them,<br>users of the system are not be burdened unnecessarily with the physical details of the<br>implementation of the system. However, quick processing of updates and queries<br>is important. It is the job of the database system to translate updates and queries<br>written in a nonprocedural language, at the logical level, into an efﬁcient sequence of<br>operations at the physical level.<br>1.8.1<br>Storage Manager<br>A storage manager is a program module that provides the interface between the low-<br>level data stored in the database and the application programs and queries submit-<br>ted to the system. The storage manager is responsible for the interaction with the ﬁle<br>manager. The raw data are stored on the disk using the ﬁle system, which is usu-<br>ally provided by a conventional operating system. The storage manager translates<br>the various DML statements into low-level ﬁle-system commands. Thus, the storage<br>manager is responsible for storing, retrieving, and updating data in the database.<br>The storage manager components include:<br>• Authorization and integrity manager, which tests for the satisfaction of in-<br>tegrity constraints and checks the authority of users to access data.<br>• Transaction manager, which ensures that the database remains in a consistent<br>(correct) state despite system failures, and that concurrent transaction execu-<br>tions proceed without conﬂicting.<br>• File manager, which manages the allocation of space on disk storage and the<br>data structures used to represent information stored on disk.<br>• Buffer manager, which is responsible for fetching data from disk storage into<br>main memory, and deciding what data to cache in main memory. The buffer<br>manager is a critical part of the database system, since it enables the database<br>to handle data sizes that are much larger than the size of main memory.<br>The storage manager implements several data structures as part of the physical<br>system implementation:<br>• Data ﬁles, which store the database itself.<br>• Data dictionary, which stores metadata about the structure of the database, in<br>particular the schema of the database.<br>• Indices, which provide fast access to data items that hold particular values.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>1. Introduction<br>Text<br>28<br>© The McGraw−Hill <br>Companies, 2001<br>18<br>Chapter 1<br>Introduction<br>1.8.2<br>The Query Processor<br>The query processor components include<br>• DDL interpreter, which interprets DDL statements and records the deﬁnitions<br>in the data dictionary.<br>• DML compiler, which translates DML statements in a query language into an<br>evaluation plan consisting of low-level instructions that the query evaluation<br>engine understands.<br>A query can usually be translated into any of a number of alternative eval-<br>uation plans that all give the same result. The DML compiler also performs<br>query optimization, that is, it picks the lowest cost evaluation plan from amo-<br>ng the alternatives.<br>• Query evaluation engine, which executes low-level instructions generated by<br>the DML compiler.<br>Figure 1.4 shows these components and the connections among them.<br>1.9<br>Application Architectures<br>Most users of a database system today are not present at the site of the database<br>system, but connect to it through a network. We can therefore differentiate between<br>client machines, on which remote database users work, and server machines, on<br>which the database system runs.<br>Database applications are usually partitioned into two or three parts, as in Fig-<br>ure 1.5. In a two-tier architecture, the application is partitioned into a component<br>that resides at the client machine, which invokes database system functionality at the<br>server machine through query language statements. Application program interface<br>standards like ODBC and JDBC are used for interaction between the client and the<br>server.<br>In contrast, in a three-tier architecture, the client machine acts as merely a front<br>end and does not contain any direct database calls. Instead, the client end communi-<br>cates with an application server, usually through a forms interface. The application<br>server in turn communicates with a database system to access data. The business<br>logic of the application, which says what actions to carry out under what conditions,<br>is embedded in the application server, instead of being distributed across multiple<br>clients. Three-tier applications are more appropriate for large applications, and for<br>applications that run on the World Wide Web.<br>1.10<br>History of Database Systems<br>Data processing drives the growth of computers, as it has from the earliest days of<br>commercial computers. In fact, automation of data processing tasks predates com-<br>puters. Punched cards, invented by Hollerith, were used at the very beginning of the<br>twentieth century to record U.S. census data, and mechanical systems were used to<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>1. Introduction<br>Text<br>29<br>© The McGraw−Hill <br>Companies, 2001<br>1.10<br>History of Database Systems<br>19<br>naive users<br>(tellers, agents, <br>web-users) <br>query processor<br>storage manager<br>disk storage<br>indices<br>statistical data<br>data<br>data dictionary<br>application<br>programmers<br>application<br>interfaces<br>application<br>program<br>object code<br>compiler and<br>linker<br>buffer manager<br>file manager<br>authorization<br>and integrity<br> manager<br>transaction<br>manager<br>DML compiler <br>and organizer<br>query evaluation<br>engine<br>DML queries<br>DDL interpreter<br>application<br>programs<br>query<br>tools<br>administration<br>tools<br>sophisticated<br>users<br>(analysts)<br>database<br>administrator<br>use<br>write<br>use<br>use<br>Figure 1.4<br>System structure.<br>process the cards and tabulate results. Punched cards were later widely used as a<br>means of entering data into computers.<br>Techniques for data storage and processing have evolved over the years:<br>• 1950s and early 1960s: Magnetic tapes were developed for data storage. Data<br>processing tasks such as payroll were automated, with data stored on tapes.<br>Processing of data consisted of reading data from one or more tapes and<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>1. Introduction<br>Text<br>30<br>© The McGraw−Hill <br>Companies, 2001<br>20<br>Chapter 1<br>Introduction<br>user<br>application server<br>database system<br>user<br>database system<br>b.  three-tier architecture<br>a.  two-tier architecture<br>network<br>server<br>client<br>application<br>network<br>application client<br>Figure 1.5<br>Two-tier and three-tier architectures.<br>writing data to a new tape. Data could also be input from punched card decks,<br>and output to printers. For example, salary raises were processed by entering<br>the raises on punched cards and reading the punched card deck in synchro-<br>nization with a tape containing the master salary details. The records had to<br>be in the same sorted order. The salary raises would be added to the salary<br>read from the master tape, and written to a new tape; the new tape would<br>become the new master tape.<br>Tapes (and card decks) could be read only sequentially, and data sizes were<br>much larger than main memory; thus, data process</span><br><br><span style="background-color: #9BF6FF;" title="Chunk 4 | Start: 80008 | End: 100008 | Tokens: 3098">ing programs were forced<br>to process data in a particular order, by reading and merging data from tapes<br>and card decks.<br>• Late 1960s and 1970s: Widespread use of hard disks in the late 1960s changed<br>the scenario for data processing greatly, since hard disks allowed direct access<br>to data. The position of data on disk was immaterial, since any location on disk<br>could be accessed in just tens of milliseconds. Data were thus freed from the<br>tyranny of sequentiality. With disks, network and hierarchical databases could<br>be created that allowed data structures such as lists and trees to be stored on<br>disk. Programmers could construct and manipulate these data structures.<br>A landmark paper by Codd [1970] deﬁned the relational model, and non-<br>procedural ways of querying data in the relational model, and relational<br>databases were born. The simplicity of the relational model and the possibil-<br>ity of hiding implementation details completely from the programmer were<br>enticing indeed. Codd later won the prestigious Association of Computing<br>Machinery Turing Award for his work.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>1. Introduction<br>Text<br>31<br>© The McGraw−Hill <br>Companies, 2001<br>1.11<br>Summary<br>21<br>• 1980s: Although academically interesting, the relational model was not used<br>in practice initially, because of its perceived performance disadvantages; re-<br>lational databases could not match the performance of existing network and<br>hierarchical databases. That changed with System R, a groundbreaking project<br>at IBM Research that developed techniques for the construction of an efﬁcient<br>relational database system. Excellent overviews of System R are provided by<br>Astrahan et al. [1976] and Chamberlin et al. [1981]. The fully functional Sys-<br>tem R prototype led to IBM’s ﬁrst relational database product, SQL/DS. Initial<br>commercial relational database systems, such as IBM DB2, Oracle, Ingres, and<br>DEC Rdb, played a major role in advancing techniques for efﬁcient process-<br>ing of declarative queries. By the early 1980s, relational databases had become<br>competitive with network and hierarchical database systems even in the area<br>of performance. Relational databases were so easy to use that they eventually<br>replaced network/hierarchical databases; programmers using such databases<br>were forced to deal with many low-level implementation details, and had to<br>code their queries in a procedural fashion. Most importantly, they had to keep<br>efﬁciency in mind when designing their programs, which involved a lot of<br>effort. In contrast, in a relational database, almost all these low-level tasks<br>are carried out automatically by the database, leaving the programmer free to<br>work at a logical level. Since attaining dominance in the 1980s, the relational<br>model has reigned supreme among data models.<br>The 1980s also saw much research on parallel and distributed databases, as<br>well as initial work on object-oriented databases.<br>• Early 1990s: The SQL language was designed primarily for decision support<br>applications, which are query intensive, yet the mainstay of databases in the<br>1980s was transaction processing applications, which are update intensive.<br>Decision support and querying re-emerged as a major application area for<br>databases. Tools for analyzing large amounts of data saw large growths in<br>usage.<br>Many database vendors introduced parallel database products in this pe-<br>riod. Database vendors also began to add object-relational support to their<br>databases.<br>• Late 1990s: The major event was the explosive growth of the World Wide Web.<br>Databases were deployed much more extensively than ever before. Database<br>systems now had to support very high transaction processing rates, as well as<br>very high reliability and 24×7 availability (availability 24 hours a day, 7 days a<br>week, meaning no downtime for scheduled maintenance activities). Database<br>systems also had to support Web interfaces to data.<br>1.11<br>Summary<br>• A database-management system (DBMS) consists of a collection of interre-<br>lated data and a collection of programs to access that data. The data describe<br>one particular enterprise.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>1. Introduction<br>Text<br>32<br>© The McGraw−Hill <br>Companies, 2001<br>22<br>Chapter 1<br>Introduction<br>• The primary goal of a DBMS is to provide an environment that is both conve-<br>nient and efﬁcient for people to use in retrieving and storing information.<br>• Database systems are ubiquitous today, and most people interact, either di-<br>rectly or indirectly, with databases many times every day.<br>• Database systems are designed to store large bodies of information. The man-<br>agement of data involves both the deﬁnition of structures for the storage of<br>information and the provision of mechanisms for the manipulation of infor-<br>mation. In addition, the database system must provide for the safety of the<br>information stored, in the face of system crashes or attempts at unauthorized<br>access. If data are to be shared among several users, the system must avoid<br>possible anomalous results.<br>• A major purpose of a database system is to provide users with an abstract<br>view of the data. That is, the system hides certain details of how the data are<br>stored and maintained.<br>• Underlying the structure of a database is the data model: a collection of con-<br>ceptual tools for describing data, data relationships, data semantics, and data<br>constraints. The entity-relationship (E-R) data model is a widely used data<br>model, and it provides a convenient graphical representation to view data, re-<br>lationships and constraints. The relational data model is widely used to store<br>data in databases. Other data models are the object-oriented model, the object-<br>relational model, and semistructured data models.<br>• The overall design of the database is called the database schema. A database<br>schema is speciﬁed by a set of deﬁnitions that are expressed using a data-<br>deﬁnition language (DDL).<br>• A data-manipulation language (DML) is a language that enables users to ac-<br>cess or manipulate data. Nonprocedural DMLs, which require a user to specify<br>only what data are needed, without specifying exactly how to get those data,<br>are widely used today.<br>• Database users can be categorized into several classes, and each class of users<br>usually uses a different type of interface to the database.<br>• A database system has several subsystems.<br>  The transaction manager subsystem is responsible for ensuring that the<br>database remains in a consistent (correct) state despite system failures.<br>The transaction manager also ensures that concurrent transaction execu-<br>tions proceed without conﬂicting.<br>  The query processor subsystem compiles and executes DDL and DML<br>statements.<br>  The storage manager subsystem provides the interface between the low-<br>level data stored in the database and the application programs and queries<br>submitted to the system.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>1. Introduction<br>Text<br>33<br>© The McGraw−Hill <br>Companies, 2001<br>Exercises<br>23<br>• Database applications are typically broken up into a front-end part that runs at<br>client machines and a part that runs at the back end. In two-tier architectures,<br>the front-end directly communicates with a database running at the back end.<br>In three-tier architectures, the back end part is itself broken up into an appli-<br>cation server and a database server.<br>Review Terms<br>• Database<br>management<br>system<br>(DBMS)<br>• Database systems applications<br>• File systems<br>• Data inconsistency<br>• Consistency constraints<br>• Data views<br>• Data abstraction<br>• Database instance<br>• Schema<br>  Database schema<br>  Physical schema<br>  Logical schema<br>• Physical data independence<br>• Data models<br>  Entity-relationship model<br>  Relational data model<br>  Object-oriented data model<br>  Object-relational data model<br>• Database languages<br>  Data deﬁnition language<br>  Data manipulation language<br>  Query language<br>• Data dictionary<br>• Metadata<br>• Application program<br>• Database administrator (DBA)<br>• Transactions<br>• Concurrency<br>• Client and server machines<br>Exercises<br>1.1 List four signiﬁcant differences between a ﬁle-processing system and a DBMS.<br>1.2 This chapter has described several major advantages of a database system. What<br>are two disadvantages?<br>1.3 Explain the difference between physical and logical data independence.<br>1.4 List ﬁve responsibilities of a database management system. For each responsi-<br>bility, explain the problems that would arise if the responsibility were not dis-<br>charged.<br>1.5 What are ﬁve main functions of a database administrator?<br>1.6 List seven programming languages that are procedural and two that are non-<br>procedural. Which group is easier to learn and use? Explain your answer.<br>1.7 List six major steps that you would take in setting up a database for a particular<br>enterprise.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>1. Introduction<br>Text<br>34<br>© The McGraw−Hill <br>Companies, 2001<br>24<br>Chapter 1<br>Introduction<br>1.8 Consider a two-dimensional integer array of size n × m that is to be used in<br>your favorite programming language. Using the array as an example, illustrate<br>the difference (a) between the three levels of data abstraction, and (b) between<br>a schema and instances.<br>Bibliographical Notes<br>We list below general purpose books, research paper collections, and Web sites on<br>databases. Subsequent chapters provide references to material on each topic outlined<br>in this chapter.<br>Textbooks covering database systems include Abiteboul et al. [1995], Date [1995],<br>Elmasri and Navathe [2000], O’Neil and O’Neil [2000], Ramakrishnan and Gehrke<br>[2000], and Ullman [1988]. Textbook coverage of transaction processing is provided<br>by Bernstein and Newcomer [1997] and Gray and Reuter [1993].<br>Several books contain collections of research papers on database management.<br>Among these are Bancilhon and Buneman [1990], Date [1986], Date [1990], Kim [1995],<br>Zaniolo et al. [1997], and Stonebraker and Hellerstein [1998].<br>A review of accomplishments in database management and an assessment of fu-<br>ture research challenges appears in Silberschatz et al. [1990], Silberschatz et al. [1996]<br>and Bernstein et al. [1998]. The home page of the ACM Special Interest Group on<br>Management of Data (see www.acm.org/sigmod) provides a wealth of information<br>about database research. Database vendor Web sites (see the tools section below)<br>provide details about their respective products.<br>Codd [1970] is the landmark paper that introduced the relational model. Discus-<br>sions concerning the evolution of DBMSs and the development of database technol-<br>ogy are offered by Fry and Sibley [1976] and Sibley [1976].<br>Tools<br>There are a large number of commercial database systems in use today. The ma-<br>jor ones include: IBM DB2 (www.ibm.com/software/data), Oracle (www.oracle.com),<br>Microsoft SQL Server (www.microsoft.com/sql), Informix (www.informix.com), and<br>Sybase (www.sybase.com). Some of these systems are available free for personal or<br>noncommercial use, or for development, but are not free for actual deployment.<br>There are also a number of free/public domain database systems; widely used<br>ones include MySQL (www.mysql.com) and PostgresSQL (www.postgressql.org).<br>A more complete list of links to vendor Web sites and other information is avail-<br>able from the home page of this book, at www.research.bell-labs.com/topic/books/db-<br>book.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>I. Data Models<br>Introduction<br>35<br>© The McGraw−Hill <br>Companies, 2001<br>P<br>A<br>R<br>T<br>1<br>Data Models<br>A data model is a collection of conceptual tools for describing data, data relation-<br>ships, data semantics, and consistency constraints. In this part, we study two data<br>models—the entity–relationship model and the relational model.<br>The entity–relationship (E-R) model is a high-level data model. It is based on a<br>perception of a real world that consists of a collection of basic objects, called entities,<br>and of relationships among these objects.<br>The relational model is a lower-level model. It uses a collection of tables to repre-<br>sent both data and the relationships among those data. Its conceptual simplicity has<br>led to its widespread adoption; today a vast majority of database products are based<br>on the relational model. Designers often formulate database schema design by ﬁrst<br>modeling data at a high level, using the E-R model, and then translating it into the<br>the relational model.<br>We shall study other data models later in the book. The object-oriented data model,<br>for example, extends the representation of entities by adding notions of encapsula-<br>tion, methods (functions), and object identity. The object-relational data model com-<br>bines features of the object-oriented data model and the relational data model. Chap-<br>ters 8 and 9, respectively, cover these two data models.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>I. Data Models<br>2. Entity−Relationship <br>Model<br>36<br>© The McGraw−Hill <br>Companies, 2001<br>C<br>H<br>A<br>P<br>T<br>E<br>R<br>2<br>Entity-Relationship Model<br>The entity-relationship (E-R) data model perceives the real world as consisting of<br>basic objects, called entities, and relationships among these objects. It was developed<br>to facilitate database design by allowing speciﬁcation of an enterprise schema, which<br>represents the overall logical structure of a database. The E-R data model is one of sev-<br>eral semantic data models; the semantic aspect of the model lies in its representation<br>of the meaning of the data. The E-R model is very useful in mapping the meanings<br>and interactions of real-world enterprises onto a conceptual schema. Because of this<br>usefulness, many database-design tools draw on concepts from the E-R model.<br>2.1<br>Basic Concepts<br>The E-R data model employs three basic notions: entity sets, relationship sets, and<br>attributes.<br>2.1.1<br>Entity Sets<br>An entity is a “thing” or “object” in the real world that is distinguishable from all<br>other objects. For example, each person in an enterprise is an entity. An entity has a<br>set of properties, and the values for some set of properties may uniquely identify an<br>entity. For instance, a person may have a person-id property whose value uniquely<br>identiﬁes that person. Thus, the value 677-89-9011 for person-id would uniquely iden-<br>tify one particular person in the enterprise. Similarly, loans can be thought of as enti-<br>ties, and loan number L-15 at the Perryridge branch uniquely identiﬁes a loan entity.<br>An entity may be concrete, such as a person or a book, or it may be abstract, such as<br>a loan, or a holiday, or a concept.<br>An entity set is a set of entities of the same type that share the same properties, or<br>attributes. The set of all persons who are customers at a given bank, for example, can<br>be deﬁned as the entity set customer. Similarly, the entity set loan might represent the<br>27<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>I. Data Models<br>2. Entity−Relationship <br>Model<br>37<br>© The McGraw−Hill <br>Companies, 2001<br>28<br>Chapter 2<br>Entity-Relationship Model<br>set of all loans awarded by a particular bank. The individual entities that constitute a<br>set are said to be the extension of the entity set. Thus, all the individual bank customers<br>are the extension of the entity set customer.<br>Entity sets do not need to be disjoint. For example, it is possible to deﬁne the entity<br>set of all employees of a bank (employee) and the entity set of all customers of the bank<br>(customer). A person entity may be an employee entity, a customer entity, both, or neither.<br>An entity is represented by a set of attributes. Attributes are descriptive proper-<br>ties possessed by each member of an entity set. The designation of an attribute for an<br>entity set expresses that the database stores similar information concerning each en-<br>tity in the entity set; however, each entity may have its own value for each attribute.<br>Possible attributes of the customer entity set are customer-id, customer-name, customer-<br>street, and customer-city. In real life, there would be further attributes, such as street<br>number, apartment number, state, postal code, and country, but we omit them to<br>keep our examples simple. Possible attributes of the loan entity set are loan-number<br>and amount.<br>Each entity has a value for each of its attributes. For instance, a particular customer<br>entity may have the value 321-12-3123 for customer-id, the value Jones for customer-<br>name, the value Main for customer-street, and the value Harrison for customer-city.<br>The customer-id attribute is used to uniquely identify customers, since there may<br>be more than one customer with the same name, street, and city. In the United States,<br>many enterprises ﬁnd it convenient to use the social-security number of a person1<br>as an attribute whose value uniquely identiﬁes the person. In general the enterprise<br>would have to create and assign a unique identiﬁer for each customer.<br>For each attribute, there is a set of permitted values, called the domain, or value<br>set, of that attribute. The domain of attribute customer-name might be the set of all<br>text strings of a certain length. Similarly, the domain of attribute loan-number might<br>be the set of all strings of the form “L-n” where n is a positive integer.<br>A database thus includes a collection of entity sets, each of which contains any<br>number of entities of the same type. Figure 2.1 shows part of a bank database that<br>consists of two entity sets: customer and loan.<br>Formally, an attribute of an entity set is a function that maps from the entity set into<br>a domain. Since an entity set may have several attributes, each entity can be described<br>by a set of (attribute, data value) pairs, one pair for each attribute of the entity set. For<br>example, a particular customer entity may be described by the set {(customer-id, 677-<br>89-9011), (customer-name, Hayes), (customer-street, Main), (customer-city, Harrison)},<br>meaning that the entity describes a person named Hayes whose customer identiﬁer<br>is 677-89-9011 and who resides at Main Street in Harrison. We can see, at this point,<br>an integration of the abstract schema with the actual enterprise being modeled. The<br>attribute values describing an entity will constitute a signiﬁcant portion of the data<br>stored in the database.<br>An attribute, as used in the E-R model, can be characterized by the following at-<br>tribute types.<br>1.<br>In the United States, the government assigns to each person in the country a unique number, called a<br>social-security number, to identify that person uniquely. Each person is supposed to have only one social-<br>security number, and no two people are supposed to have the same social-security number.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>I. Data Models<br>2. Entity−Relationship <br>Model<br>38<br>© The McGraw−Hill <br>Companies, 2001<br>2.1<br>Basic Concepts<br>29<br>555-55-5555 Jackson    Dupont  Woodside<br>321-12-3123  Jones         Main       Harrison<br>019-28-3746 Smith        North     Rye<br>677-89-9011 Hayes       Main      Harrison<br>244-66-8800 Curry        North     Rye<br> 963-96-3963 Williams  Nassau   Princeton<br>335-57-7991 Adams      Spring    Pittsfield<br>L-17   1000<br>L-15   1500<br>L-14   1500<br>L-16   1300<br>L-23   2000<br>L-19     500<br>L-11     900<br>loan<br>customer<br>Figure 2.1<br>Entity sets customer and loan.<br>• Simple and composite attributes. In our examples thus far, the attributes have<br>been simple; that is, they are not divided into subparts. Composite attributes,<br>on the other hand, can be divided into subparts (that is, other attributes). For<br>example, an attribute name could be structured as a composite attribute con-<br>sisting of ﬁrst-name, middle-initial, and last-name. Using composite attributes in<br>a design schema is a good choice if a user will wish to refer to an entire at-<br>tribute on some occasions, and to only a component of the attribute on other<br>occasions. Suppose we were to substitute for the customer entity-set attributes<br>customer-street and customer-city the composite attribute address with the at-<br>tributes street, city, state, and zip-code.2 Composite attributes help us to gr</span><br><br><span style="background-color: #A0C4FF;" title="Chunk 5 | Start: 100010 | End: 120010 | Tokens: 3230">oup<br>together related attributes, making the modeling cleaner.<br>Note also that a composite attribute may appear as a hierarchy. In the com-<br>posite attribute address, its component attribute street can be further divided<br>into street-number, street-name, and apartment-number. Figure 2.2 depicts these<br>examples of composite attributes for the customer entity set.<br>• Single-valued and multivalued attributes. The attributes in our examples all<br>have a single value for a particular entity. For instance, the loan-number at-<br>tribute for a speciﬁc loan entity refers to only one loan number. Such attributes<br>are said to be single valued. There may be instances where an attribute has<br>a set of values for a speciﬁc entity. Consider an employee entity set with the<br>attribute phone-number. An employee may have zero, one, or several phone<br>numbers, and different employees may have different numbers of phones.<br>This type of attribute is said to be multivalued. As another example, an at-<br>2.<br>We assume the address format used in the United States, which includes a numeric postal code called<br>a zip code.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>I. Data Models<br>2. Entity−Relationship <br>Model<br>39<br>© The McGraw−Hill <br>Companies, 2001<br>30<br>Chapter 2<br>Entity-Relationship Model<br>Composite<br>Attributes<br>Component<br>Attributes <br>first-name  middle-initial  last-name<br>street  city   state   postal-code<br>street-number street-name  apartment-number<br>name<br>address<br>Figure 2.2<br>Composite attributes customer-name and customer-address.<br>tribute dependent-name of the employee entity set would be multivalued, since<br>any particular employee may have zero, one, or more dependent(s).<br>Where appropriate, upper and lower bounds may be placed on the number<br>of values in a multivalued attribute. For example, a bank may limit the num-<br>ber of phone numbers recorded for a single customer to two. Placing bounds<br>in this case expresses that the phone-number attribute of the customer entity set<br>may have between zero and two values.<br>• Derived attribute. The value for this type of attribute can be derived from<br>the values of other related attributes or entities. For instance, let us say that<br>the customer entity set has an attribute loans-held, which represents how many<br>loans a customer has from the bank. We can derive the value for this attribute<br>by counting the number of loan entities associated with that customer.<br>As another example, suppose that the customer entity set has an attribute<br>age, which indicates the customer’s age. If the customer entity set also has an<br>attribute date-of-birth, we can calculate age from date-of-birth and the current<br>date. Thus, age is a derived attribute. In this case, date-of-birth may be referred<br>to as a base attribute, or a stored attribute. The value of a derived attribute is<br>not stored, but is computed when required.<br>An attribute takes a null value when an entity does not have a value for it. The<br>null value may indicate “not applicable”—that is, that the value does not exist for the<br>entity. For example, one may have no middle name. Null can also designate that an<br>attribute value is unknown. An unknown value may be either missing (the value does<br>exist, but we do not have that information) or not known (we do not know whether or<br>not the value actually exists).<br>For instance, if the name value for a particular customer is null, we assume that<br>the value is missing, since every customer must have a name. A null value for the<br>apartment-number attribute could mean that the address does not include an apart-<br>ment number (not applicable), that an apartment number exists but we do not know<br>what it is (missing), or that we do not know whether or not an apartment number is<br>part of the customer’s address (unknown).<br>A database for a banking enterprise may include a number of different entity sets.<br>For example, in addition to keeping track of customers and loans, the bank also<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>I. Data Models<br>2. Entity−Relationship <br>Model<br>40<br>© The McGraw−Hill <br>Companies, 2001<br>2.1<br>Basic Concepts<br>31<br>provides accounts, which are represented by the entity set account with attributes<br>account-number and balance. Also, if the bank has a number of different branches, then<br>we may keep information about all the branches of the bank. Each branch entity set<br>may be described by the attributes branch-name, branch-city, and assets.<br>2.1.2<br>Relationship Sets<br>A relationship is an association among several entities. For example, we can deﬁne<br>a relationship that associates customer Hayes with loan L-15. This relationship spec-<br>iﬁes that Hayes is a customer with loan number L-15.<br>A relationship set is a set of relationships of the same type. Formally, it is a math-<br>ematical relation on n ≥2 (possibly nondistinct) entity sets. If E1, E2, . . . , En are<br>entity sets, then a relationship set R is a subset of<br>{(e1, e2, . . . , en) | e1 ∈E1, e2 ∈E2, . . . , en ∈En}<br>where (e1, e2, . . . , en) is a relationship.<br>Consider the two entity sets customer and loan in Figure 2.1. We deﬁne the rela-<br>tionship set borrower to denote the association between customers and the bank loans<br>that the customers have. Figure 2.3 depicts this association.<br>As another example, consider the two entity sets loan and branch. We can deﬁne<br>the relationship set loan-branch to denote the association between a bank loan and the<br>branch in which that loan is maintained.<br>555-55-5555 <br>Jackson      Dupont    Woodside<br>321-12-3123 <br>Jones          Main        Harrison<br>019-28-3746  <br>Smith         North       Rye<br>677-89-9011 <br>Hayes        Main        Harrison<br>244-66-8800 <br>Curry         North       Rye<br>963-96-3963  <br>Williams    Nassau    Princeton<br>335-57-7991 <br>Adams       Spring     Pittsfield<br>L-17   1000<br>L-15   1500<br>L-14   1500<br>L-16   1300<br>L-23   2000<br>L-19     500<br>L-11     900<br>loan<br>customer<br>Figure 2.3<br>Relationship set borrower.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>I. Data Models<br>2. Entity−Relationship <br>Model<br>41<br>© The McGraw−Hill <br>Companies, 2001<br>32<br>Chapter 2<br>Entity-Relationship Model<br>The association between entity sets is referred to as participation; that is, the entity<br>sets E1, E2, . . . , En participate in relationship set R. A relationship instance in an<br>E-R schema represents an association between the named entities in the real-world<br>enterprise that is being modeled. As an illustration, the individual customer entity<br>Hayes, who has customer identiﬁer 677-89-9011, and the loan entity L-15 participate<br>in a relationship instance of borrower. This relationship instance represents that, in the<br>real-world enterprise, the person called Hayes who holds customer-id 677-89-9011 has<br>taken the loan that is numbered L-15.<br>The function that an entity plays in a relationship is called that entity’s role. Since<br>entity sets participating in a relationship set are generally distinct, roles are implicit<br>and are not usually speciﬁed. However, they are useful when the meaning of a re-<br>lationship needs clariﬁcation. Such is the case when the entity sets of a relationship<br>set are not distinct; that is, the same entity set participates in a relationship set more<br>than once, in different roles. In this type of relationship set, sometimes called a re-<br>cursive relationship set, explicit role names are necessary to specify how an entity<br>participates in a relationship instance. For example, consider an entity set employee<br>that records information about all the employees of the bank. We may have a rela-<br>tionship set works-for that is modeled by ordered pairs of employee entities. The ﬁrst<br>employee of a pair takes the role of worker, whereas the second takes the role of man-<br>ager. In this way, all relationships of works-for are characterized by (worker, manager)<br>pairs; (manager, worker) pairs are excluded.<br>A relationship may also have attributes called descriptive attributes. Consider a<br>relationship set depositor with entity sets customer and account. We could associate the<br>attribute access-date to that relationship to specify the most recent date on which a<br>customer accessed an account. The depositor relationship among the entities corre-<br>sponding to customer Jones and account A-217 has the value “23 May 2001” for at-<br>tribute access-date, which means that the most recent date that Jones accessed account<br>A-217 was 23 May 2001.<br>As another example of descriptive attributes for relationships, suppose we have<br>entity sets student and course which participate in a relationship set registered-for. We<br>may wish to store a descriptive attribute for-credit with the relationship, to record<br>whether a student has taken the course for credit, or is auditing (or sitting in on) the<br>course.<br>A relationship instance in a given relationship set must be uniquely identiﬁable<br>from its participating entities, without using the descriptive attributes. To understand<br>this point, suppose we want to model all the dates when a customer accessed an<br>account. The single-valued attribute access-date can store a single access date only . We<br>cannot represent multiple access dates by multiple relationship instances between the<br>same customer and account, since the relationship instances would not be uniquely<br>identiﬁable using only the participating entities. The right way to handle this case is<br>to create a multivalued attribute access-dates, which can store all the access dates.<br>However, there can be more than one relationship set involving the same entity<br>sets. In our example the customer and loan entity sets participate in the relationship<br>set borrower. Additionally, suppose each loan must have another customer who serves<br>as a guarantor for the loan. Then the customer and loan entity sets may participate in<br>another relationship set, guarantor.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>I. Data Models<br>2. Entity−Relationship <br>Model<br>42<br>© The McGraw−Hill <br>Companies, 2001<br>2.2<br>Constraints<br>33<br>The relationship sets borrower and loan-branch provide an example of a binary rela-<br>tionship set—that is, one that involves two entity sets. Most of the relationship sets in<br>a database system are binary. Occasionally, however, relationship sets involve more<br>than two entity sets.<br>As an example, consider the entity sets employee, branch, and job. Examples of job<br>entities could include manager, teller, auditor, and so on. Job entities may have the at-<br>tributes title and level. The relationship set works-on among employee, branch, and job is<br>an example of a ternary relationship. A ternary relationship among Jones, Perryridge,<br>and manager indicates that Jones acts as a manager at the Perryridge branch. Jones<br>could also act as auditor at the Downtown branch, which would be represented by<br>another relationship. Yet another relationship could be between Smith, Downtown,<br>and teller, indicating Smith acts as a teller at the Downtown branch.<br>The number of entity sets that participate in a relationship set is also the degree of<br>the relationship set. A binary relationship set is of degree 2; a ternary relationship set<br>is of degree 3.<br>2.2<br>Constraints<br>An E-R enterprise schema may deﬁne certain constraints to which the contents of a<br>database must conform. In this section, we examine mapping cardinalities and par-<br>ticipation constraints, which are two of the most important types of constraints.<br>2.2.1<br>Mapping Cardinalities<br>Mapping cardinalities, or cardinality ratios, express the number of entities to which<br>another entity can be associated via a relationship set.<br>Mapping cardinalities are most useful in describing binary relationship sets, al-<br>though they can contribute to the description of relationship sets that involve more<br>than two entity sets. In this section, we shall concentrate on only binary relationship<br>sets.<br>For a binary relationship set R between entity sets A and B, the mapping cardinal-<br>ity must be one of the following:<br>• One to one. An entity in A is associated with at most one entity in B, and an<br>entity in B is associated with at most one entity in A. (See Figure 2.4a.)<br>• One to many. An entity in A is associated with any number (zero or more) of<br>entities in B. An entity in B, however, can be associated with at most one entity<br>in A. (See Figure 2.4b.)<br>• Many to one. An entity in A is associated with at most one entity in B. An<br>entity in B, however, can be associated with any number (zero or more) of<br>entities in A. (See Figure 2.5a.)<br>• Many to many. An entity in A is associated with any number (zero or more) of<br>entities in B, and an entity in B is associated with any number (zero or more)<br>of entities in A. (See Figure 2.5b.)<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>I. Data Models<br>2. Entity−Relationship <br>Model<br>43<br>© The McGraw−Hill <br>Companies, 2001<br>34<br>Chapter 2<br>Entity-Relationship Model<br>a1<br>a2<br>a3<br>a4<br>b1<br>b2<br>b3<br>b4<br>(a)<br>(b)<br>A                      B<br>a1<br>a2<br>a3<br>b1<br>b2<br>b3<br>b4<br>b5<br>A                      B<br>Figure 2.4<br>Mapping cardinalities. (a) One to one. (b) One to many.<br>The appropriate mapping cardinality for a particular relationship set obviously de-<br>pends on the real-world situation that the relationship set is modeling.<br>As an illustration, consider the borrower relationship set. If, in a particular bank, a<br>loan can belong to only one customer, and a customer can have several loans, then<br>the relationship set from customer to loan is one to many. If a loan can belong to several<br>customers (as can loans taken jointly by several business partners), the relationship<br>set is many to many. Figure 2.3 depicts this type of relationship.<br>2.2.2<br>Participation Constraints<br>The participation of an entity set E in a relationship set R is said to be total if every<br>entity in E participates in at least one relationship in R. If only some entities in E<br>participate in relationships in R, the participation of entity set E in relationship R is<br>said to be partial. For example, we expect every loan entity to be related to at least<br>one customer through the borrower relationship. Therefore the participation of loan in<br>a1<br>a2<br>a3<br>a4<br>b1<br>b2<br>b3<br>(a)<br>(b)<br>A                      B<br>A                       B<br>a5<br>a1<br>a2<br>a3<br>a4<br>b1<br>b2<br>b3<br>b4<br>Figure 2.5<br>Mapping cardinalities. (a) Many to one. (b) Many to many.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>I. Data Models<br>2. Entity−Relationship <br>Model<br>44<br>© The McGraw−Hill <br>Companies, 2001<br>2.3<br>Keys<br>35<br>the relationship set borrower is total. In contrast, an individual can be a bank customer<br>whether or not she has a loan with the bank. Hence, it is possible that only some of<br>the customer entities are related to the loan entity set through the borrower relationship,<br>and the participation of customer in the borrower relationship set is therefore partial.<br>2.3<br>Keys<br>We must have a way to specify how entities within a given entity set are distin-<br>guished. Conceptually, individual entities are distinct; from a database perspective,<br>however, the difference among them must be expressed in terms of their attributes.<br>Therefore, the values of the attribute values of an entity must be such that they can<br>uniquely identify the entity. In other words, no two entities in an entity set are allowed<br>to have exactly the same value for all attributes.<br>A key allows us to identify a set of attributes that sufﬁce to distinguish entities<br>from each other. Keys also help uniquely identify relationships, and thus distinguish<br>relationships from each other.<br>2.3.1<br>Entity Sets<br>A superkey is a set of one or more attributes that, taken collectively, allow us to iden-<br>tify uniquely an entity in the entity set. For example, the customer-id attribute of the<br>entity set customer is sufﬁcient to distinguish one customer entity from another. Thus,<br>customer-id is a superkey. Similarly, the combination of customer-name and customer-id<br>is a superkey for the entity set customer. The customer-name attribute of customer is not<br>a superkey, because several people might have the same name.<br>The concept of a superkey is not sufﬁcient for our purposes, since, as we saw, a<br>superkey may contain extraneous attributes. If K is a superkey, then so is any superset<br>of K. We are often interested in superkeys for which no proper subset is a superkey.<br>Such minimal superkeys are called candidate keys.<br>It is possible that several distinct sets of attributes could serve as a candidate key.<br>Suppose that a combination of customer-name and customer-street is sufﬁcient to dis-<br>tinguish among members of the customer entity set. Then, both {customer-id} and<br>{customer-name, customer-street} are candidate keys. Although the attributes customer-<br>id and customer-name together can distinguish customer entities, their combination<br>does not form a candidate key, since the attribute customer-id alone is a candidate<br>key.<br>We shall use the term primary key to denote a candidate key that is chosen by<br>the database designer as the principal means of identifying entities within an entity<br>set. A key (primary, candidate, and super) is a property of the entity set, rather than<br>of the individual entities. Any two individual entities in the set are prohibited from<br>having the same value on the key attributes at the same time. The designation of a<br>key represents a constraint in the real-world enterprise being modeled.<br>Candidate keys must be chosen with care. As we noted, the name of a person is<br>obviously not sufﬁcient, because there may be many people with the same name.<br>In the United States, the social-security number attribute of a person would be a<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>I. Data Models<br>2. Entity−Relationship <br>Model<br>45<br>© The McGraw−Hill <br>Companies, 2001<br>36<br>Chapter 2<br>Entity-Relationship Model<br>candidate key. Since non-U.S. residents usually do not have social-security numbers,<br>international enterprises must generate their own unique identiﬁers. An alternative<br>is to use some unique combination of other attributes as a key.<br>The primary key should be chosen such that its attributes are never, or very rarely,<br>changed. For instance, the address ﬁeld of a person should not be part of the primary<br>key, since it is likely to change. Social-security numbers, on the other hand, are guar-<br>anteed to never change. Unique identiﬁers generated by enterprises generally do not<br>change, except if two enterprises merge; in such a case the same identiﬁer may have<br>been issued by both enterprises, and a reallocation of identiﬁers may be required to<br>make sure they are unique.<br>2.3.2<br>Relationship Sets<br>The primary key of an entity set allows us to distinguish among the various entities of<br>the set. We need a similar mechanism to distinguish among the various relationships<br>of a relationship set.<br>Let R be a relationship set involving entity sets E1, E2, . . . , En. Let primary-key(Ei)<br>denote the set of attributes that forms the primary key for entity set Ei. Assume<br>for now that the attribute names of all primary keys are unique, and each entity set<br>participates only once in the relationship. The composition of the primary key for<br>a relationship set depends on the set of attributes associated with the relationship<br>set R.<br>If the relationship set R has no attributes associated with it, then the set of at-<br>tributes<br>primary-key(E1) ∪primary-key(E2) ∪· · · ∪primary-key(En)<br>describes an individual relationship in set R.<br>If the relationship set R has attributes a1, a2, · · · , am associated with it, then the set<br>of attributes<br>primary-key(E1) ∪primary-key(E2) ∪· · · ∪primary-key(En) ∪{a1, a2, . . . , am}<br>describes an individual relationship in set R.<br>In both of the above cases, the set of attributes<br>primary-key(E1) ∪primary-key(E2) ∪· · · ∪primary-key(En)<br>forms a superkey for the relationship set.<br>In case the attribute names of primary keys are not unique across entity sets, the<br>attributes are renamed to distinguish them; the name of the entity set combined with<br>the name of the attribute would form a unique name. In case an entity set participates<br>more than once in a relationship set (as in the works-for relationship in Sectio</span><br><br><span style="background-color: #BDB2FF;" title="Chunk 6 | Start: 120012 | End: 140012 | Tokens: 3109">n 2.1.2),<br>the role name is used instead of the name of the entity set, to form a unique attribute<br>name.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>I. Data Models<br>2. Entity−Relationship <br>Model<br>46<br>© The McGraw−Hill <br>Companies, 2001<br>2.4<br>Design Issues<br>37<br>The structure of the primary key for the relationship set depends on the map-<br>ping cardinality of the relationship set. As an illustration, consider the entity sets<br>customer and account, and the relationship set depositor, with attribute access-date, in<br>Section 2.1.2. Suppose that the relationship set is many to many. Then the primary<br>key of depositor consists of the union of the primary keys of customer and account.<br>However, if a customer can have only one account—that is, if the depositor relation-<br>ship is many to one from customer to account—then the primary key of depositor is<br>simply the primary key of customer. Similarly, if the relationship is many to one from<br>account to customer—that is, each account is owned by at most one customer—then<br>the primary key of depositor is simply the primary key of account. For one-to-one re-<br>lationships either primary key can be used.<br>For nonbinary relationships, if no cardinality constraints are present then the su-<br>perkey formed as described earlier in this section is the only candidate key, and it<br>is chosen as the primary key. The choice of the primary key is more complicated if<br>cardinality constraints are present. Since we have not discussed how to specify cardi-<br>nality constraints on nonbinary relations, we do not discuss this issue further in this<br>chapter. We consider the issue in more detail in Section 7.3.<br>2.4<br>Design Issues<br>The notions of an entity set and a relationship set are not precise, and it is possible<br>to deﬁne a set of entities and the relationships among them in a number of differ-<br>ent ways. In this section, we examine basic issues in the design of an E-R database<br>schema. Section 2.7.4 covers the design process in further detail.<br>2.4.1<br>Use of Entity Sets versus Attributes<br>Consider the entity set employee with attributes employee-name and telephone-number.<br>It can easily be argued that a telephone is an entity in its own right with attributes<br>telephone-number and location (the ofﬁce where the telephone is located). If we take<br>this point of view, we must redeﬁne the employee entity set as:<br>• The employee entity set with attribute employee-name<br>• The telephone entity set with attributes telephone-number and location<br>• The relationship set emp-telephone, which denotes the association between em-<br>ployees and the telephones that they have<br>What, then, is the main difference between these two deﬁnitions of an employee?<br>Treating a telephone as an attribute telephone-number implies that employees have<br>precisely one telephone number each. Treating a telephone as an entity telephone per-<br>mits employees to have several telephone numbers (including zero) associated with<br>them. However, we could instead easily deﬁne telephone-number as a multivalued at-<br>tribute to allow multiple telephones per employee.<br>The main difference then is that treating a telephone as an entity better models a<br>situation where one may want to keep extra information about a telephone, such as<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>I. Data Models<br>2. Entity−Relationship <br>Model<br>47<br>© The McGraw−Hill <br>Companies, 2001<br>38<br>Chapter 2<br>Entity-Relationship Model<br>its location, or its type (mobile, video phone, or plain old telephone), or who all share<br>the telephone. Thus, treating telephone as an entity is more general than treating it<br>as an attribute and is appropriate when the generality may be useful.<br>In contrast, it would not be appropriate to treat the attribute employee-name as an<br>entity; it is difﬁcult to argue that employee-name is an entity in its own right (in contrast<br>to the telephone). Thus, it is appropriate to have employee-name as an attribute of the<br>employee entity set.<br>Two natural questions thus arise: What constitutes an attribute, and what con-<br>stitutes an entity set? Unfortunately, there are no simple answers. The distinctions<br>mainly depend on the structure of the real-world enterprise being modeled, and on<br>the semantics associated with the attribute in question.<br>A common mistake is to use the primary key of an entity set as an attribute of an-<br>other entity set, instead of using a relationship. For example, it is incorrect to model<br>customer-id as an attribute of loan even if each loan had only one customer. The re-<br>lationship borrower is the correct way to represent the connection between loans and<br>customers, since it makes their connection explicit, rather than implicit via an at-<br>tribute.<br>Another related mistake that people sometimes make is to designate the primary<br>key attributes of the related entity sets as attributes of the relationship set. This should<br>not be done, since the primary key attributes are already implicit in the relationship.<br>2.4.2<br>Use of Entity Sets versus Relationship Sets<br>It is not always clear whether an object is best expressed by an entity set or a rela-<br>tionship set. In Section 2.1.1, we assumed that a bank loan is modeled as an entity.<br>An alternative is to model a loan not as an entity, but rather as a relationship between<br>customers and branches, with loan-number and amount as descriptive attributes. Each<br>loan is represented by a relationship between a customer and a branch.<br>If every loan is held by exactly one customer and is associated with exactly one<br>branch, we may ﬁnd satisfactory the design where a loan is represented as a rela-<br>tionship. However, with this design, we cannot represent conveniently a situation in<br>which several customers hold a loan jointly. To handle such a situation, we must de-<br>ﬁne a separate relationship for each holder of the joint loan. Then, we must replicate<br>the values for the descriptive attributes loan-number and amount in each such relation-<br>ship. Each such relationship must, of course, have the same value for the descriptive<br>attributes loan-number and amount.<br>Two problems arise as a result of the replication: (1) the data are stored multiple<br>times, wasting storage space, and (2) updates potentially leave the data in an incon-<br>sistent state, where the values differ in two relationships for attributes that are sup-<br>posed to have the same value. The issue of how to avoid such replication is treated<br>formally by normalization theory, discussed in Chapter 7.<br>The problem of replication of the attributes loan-number and amount is absent in<br>the original design of Section 2.1.1, because there loan is an entity set.<br>One possible guideline in determining whether to use an entity set or a relation-<br>ship set is to designate a relationship set to describe an action that occurs between<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>I. Data Models<br>2. Entity−Relationship <br>Model<br>48<br>© The McGraw−Hill <br>Companies, 2001<br>2.4<br>Design Issues<br>39<br>entities. This approach can also be useful in deciding whether certain attributes may<br>be more appropriately expressed as relationships.<br>2.4.3<br>Binary versus n-ary Relationship Sets<br>Relationships in databases are often binary. Some relationships that appear to be<br>nonbinary could actually be better represented by several binary relationships. For<br>instance, one could create a ternary relationship parent, relating a child to his/her<br>mother and father. However, such a relationship could also be represented by two<br>binary relationships, mother and father, relating a child to his/her mother and father<br>separately. Using the two relationships mother and father allows us record a child’s<br>mother, even if we are not aware of the father’s identity; a null value would be<br>required if the ternary relationship parent is used. Using binary relationship sets is<br>preferable in this case.<br>In fact, it is always possible to replace a nonbinary (n-ary, for n &gt; 2) relationship<br>set by a number of distinct binary relationship sets. For simplicity, consider the ab-<br>stract ternary (n = 3) relationship set R, relating entity sets A, B, and C. We replace<br>the relationship set R by an entity set E, and create three relationship sets:<br>• RA, relating E and A<br>• RB, relating E and B<br>• RC, relating E and C<br>If the relationship set R had any attributes, these are assigned to entity set E; further,<br>a special identifying attribute is created for E (since it must be possible to distinguish<br>different entities in an entity set on the basis of their attribute values). For each rela-<br>tionship (ai, bi, ci) in the relationship set R, we create a new entity ei in the entity set<br>E. Then, in each of the three new relationship sets, we insert a relationship as follows:<br>• (ei, ai) in RA<br>• (ei, bi) in RB<br>• (ei, ci) in RC<br>We can generalize this process in a straightforward manner to n-ary relationship<br>sets. Thus, conceptually, we can restrict the E-R model to include only binary rela-<br>tionship sets. However, this restriction is not always desirable.<br>• An identifying attribute may have to be created for the entity set created to<br>represent the relationship set. This attribute, along with the extra relationship<br>sets required, increases the complexity of the design and (as we shall see in<br>Section 2.9) overall storage requirements.<br>• A n-ary relationship set shows more clearly that several entities participate in<br>a single relationship.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>I. Data Models<br>2. Entity−Relationship <br>Model<br>49<br>© The McGraw−Hill <br>Companies, 2001<br>40<br>Chapter 2<br>Entity-Relationship Model<br>• There may not be a way to translate constraints on the ternary relationship<br>into constraints on the binary relationships. For example, consider a constraint<br>that says that R is many-to-one from A, B to C; that is, each pair of entities<br>from A and B is associated with at most one C entity. This constraint cannot<br>be expressed by using cardinality constraints on the relationship sets RA, RB,<br>and RC.<br>Consider the relationship set works-on in Section 2.1.2, relating employee, branch,<br>and job. We cannot directly split works-on into binary relationships between employee<br>and branch and between employee and job. If we did so, we would be able to record<br>that Jones is a manager and an auditor and that Jones works at Perryridge and Down-<br>town; however, we would not be able to record that Jones is a manager at Perryridge<br>and an auditor at Downtown, but is not an auditor at Perryridge or a manager at<br>Downtown.<br>The relationship set works-on can be split into binary relationships by creating a<br>new entity set as described above. However, doing so would not be very natural.<br>2.4.4<br>Placement of Relationship Attributes<br>The cardinality ratio of a relationship can affect the placement of relationship at-<br>tributes. Thus, attributes of one-to-one or one-to-many relationship sets can be as-<br>sociated with one of the participating entity sets, rather than with the relationship<br>set. For instance, let us specify that depositor is a one-to-many relationship set such<br>that one customer may have several accounts, but each account is held by only one<br>customer. In this case, the attribute access-date, which speciﬁes when the customer last<br>accessed that account, could be associated with the account entity set, as Figure 2.6 de-<br>picts; to keep the ﬁgure simple, only some of the attributes of the two entity sets are<br>shown. Since each account entity participates in a relationship with at most one in-<br>stance of customer, making this attribute designation would have the same meaning<br>A-101    24 May 1996<br>A-215     3 June 1996<br>A-102    10 June 1996<br>A-305    28 May 1996<br>A-201    17 June 1996<br>A-222    24 June 1996<br>A-217    23 May 1996<br>customer (customer-name)<br>account (account-number, access-date)<br>depositor<br>Johnson<br>Smith<br>Hayes<br>Turner<br>Jones<br>Lindsay<br>Figure 2.6<br>Access-date as attribute of the account entity set.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>I. Data Models<br>2. Entity−Relationship <br>Model<br>50<br>© The McGraw−Hill <br>Companies, 2001<br>2.4<br>Design Issues<br>41<br>as would placing access-date with the depositor relationship set. Attributes of a one-to-<br>many relationship set can be repositioned to only the entity set on the “many” side of<br>the relationship. For one-to-one relationship sets, on the other hand, the relationship<br>attribute can be associated with either one of the participating entities.<br>The design decision of where to place descriptive attributes in such cases—as a<br>relationship or entity attribute—should reﬂect the characteristics of the enterprise<br>being modeled. The designer may choose to retain access-date as an attribute of depos-<br>itor to express explicitly that an access occurs at the point of interaction between the<br>customer and account entity sets.<br>The choice of attribute placement is more clear-cut for many-to-many relationship<br>sets. Returning to our example, let us specify the perhaps more realistic case that<br>depositor is a many-to-many relationship set expressing that a customer may have<br>one or more accounts, and that an account can be held by one or more customers.<br>If we are to express the date on which a speciﬁc customer last accessed a speciﬁc<br>account, access-date must be an attribute of the depositor relationship set, rather than<br>either one of the participating entities. If access-date were an attribute of account, for<br>instance, we could not determine which customer made the most recent access to a<br>joint account. When an attribute is determined by the combination of participating<br>entity sets, rather than by either entity separately, that attribute must be associated<br>with the many-to-many relationship set. Figure 2.7 depicts the placement of access-<br>date as a relationship attribute; again, to keep the ﬁgure simple, only some of the<br>attributes of the two entity sets are shown.<br>Johnson<br>Smith<br>Hayes<br>Turner<br>Jones<br>Lindsay<br>A-101    <br>A-215  <br>A-102   <br>A-305    <br>A-201    <br>A-222    <br>A-217   <br>customer(customer-name)<br>account(account-number)<br>depositor(access-date)<br>24 May 1996<br>  3 June 1996<br>21 June 1996<br>10 June 1996<br>17 June 1996<br>28 May 1996<br>28 May 1996<br>24 June 1996<br>23 May 1996<br>Figure 2.7<br>Access-date as attribute of the depositor relationship set.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>I. Data Models<br>2. Entity−Relationship <br>Model<br>51<br>© The McGraw−Hill <br>Companies, 2001<br>42<br>Chapter 2<br>Entity-Relationship Model<br>2.5<br>Entity-Relationship Diagram<br>As we saw brieﬂy in Section 1.4, an E-R diagram can express the overall logical struc-<br>ture of a database graphically. E-R diagrams are simple and clear—qualities that may<br>well account in large part for the widespread use of the E-R model. Such a diagram<br>consists of the following major components:<br>• Rectangles, which represent entity sets<br>• Ellipses, which represent attributes<br>• Diamonds, which represent relationship sets<br>• Lines, which link attributes to entity sets and entity sets to relationship sets<br>• Double ellipses, which represent multivalued attributes<br>• Dashed ellipses, which denote derived attributes<br>• Double lines, which indicate total participation of an entity in a relation-<br>ship set<br>• Double rectangles, which represent weak entity sets (described later, in Sec-<br>tion 2.6.)<br>Consider the entity-relationship diagram in Figure 2.8, which consists of two en-<br>tity sets, customer and loan, related through a binary relationship set borrower. The at-<br>tributes associated with customer are customer-id, customer-name, customer-street, and<br>customer-city. The attributes associated with loan are loan-number and amount. In Fig-<br>ure 2.8, attributes of an entity set that are members of the primary key are underlined.<br>The relationship set borrower may be many-to-many, one-to-many, many-to-one,<br>or one-to-one. To distinguish among these types, we draw either a directed line (→)<br>or an undirected line (—) between the relationship set and the entity set in question.<br>• A directed line from the relationship set borrower to the entity set loan speci-<br>ﬁes that borrower is either a one-to-one or many-to-one relationship set, from<br>customer to loan; borrower cannot be a many-to-many or a one-to-many rela-<br>tionship set from customer to loan.<br>customer-name<br>customer-street<br>customer-id<br>customer-city<br>customer<br>loan-number<br>amount<br>loan<br>borrower<br>Figure 2.8<br>E-R diagram corresponding to customers and loans.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>I. Data Models<br>2. Entity−Relationship <br>Model<br>52<br>© The McGraw−Hill <br>Companies, 2001<br>2.5<br>Entity-Relationship Diagram<br>43<br>• An undirected line from the relationship set borrower to the entity set loan spec-<br>iﬁes that borrower is either a many-to-many or one-to-many relationship set<br>from customer to loan.<br>Returning to the E-R diagram of Figure 2.8, we see that the relationship set borrower<br>is many-to-many. If the relationship set borrower were one-to-many, from customer to<br>loan, then the line from borrower to customer would be directed, with an arrow point-<br>ing to the customer entity set (Figure 2.9a). Similarly, if the relationship set borrower<br>were many-to-one from customer to loan, then the line from borrower to loan would<br>have an arrow pointing to the loan entity set (Figure 2.9b). Finally, if the relation-<br>ship set borrower were one-to-one, then both lines from borrower would have arrows:<br>customer-name<br>customer-street<br>customer-id<br>customer-city<br>customer<br>loan-number<br>amount<br>loan<br>borrower<br>(a)<br>customer-name<br>customer-street<br>customer-id<br>customer-city<br>customer<br>loan-number<br>amount<br>loan<br>borrower<br>(b)<br>customer-name<br>customer-street<br>customer-id<br>customer-city<br>customer<br>loan-number<br>amount<br>loan<br>borrower<br>(c)<br>Figure 2.9<br>Relationships. (a) one to many. (b) many to one. (c) one-to-one.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>I. Data Models<br>2. Entity−Relationship <br>Model<br>53<br>© The McGraw−Hill <br>Companies, 2001<br>44<br>Chapter 2<br>Entity-Relationship Model<br>customer-name<br>customer-street<br>customer-id<br>customer-city<br>customer<br>balance<br>account<br>depositor<br>access-date<br>account-number<br>Figure 2.10<br>E-R diagram with an attribute attached to a relationship set.<br>one pointing to the loan entity set and one pointing to the customer entity set (Fig-<br>ure 2.9c).<br>If a relationship set has also some attributes associated with it, then we link these<br>attributes to that relationship set. For example, in Figure 2.10, we have the access-<br>date descriptive attribute attached to the relationship set depositor to specify the most<br>recent date on which a customer accessed that account.<br>Figure 2.11 shows how composite attributes can be represented in the E-R notation.<br>Here, a composite attribute name, with component attributes ﬁrst-name, middle-initial,<br>and last-name replaces the simple attribute customer-name of customer. Also, a compos-<br>ite attribute address, whose component attributes are street, city, state, and zip-code re-<br>places the attributes customer-street and customer-city of customer. The attribute street is<br>itself a composite attribute whose component attributes are street-number, street-name,<br>and apartment number.<br>Figure 2.11 also illustrates a multivalued attribute phone-number, depicted by a<br>double ellipse, and a derived attribute age, depicted by a dashed ellipse.<br>city<br>zip-code<br>street<br>state<br>name<br>customer<br>customer-id<br>middle-initial<br>last-name<br>first-name<br>street-number<br>street-name<br>apartment-number<br>address<br>phone-number<br>date-of-birth<br>age<br>Figure 2.11<br>E-R diagram with composite, multivalued, and derived attributes.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>I. Data Models<br>2. Entity−Relationship <br>Model<br>54<br>© The McGraw−Hill <br>Companies, 2001<br>2.5<br>Entity-Relationship Diagram<br>45<br>employee-id<br>employee-name<br>telephone-number<br>employee<br>works-for<br>manager<br>worker<br>Figure 2.12<br>E-R diagram with role indicators.<br>We indicate roles in E-R diagrams by labeling the lines that connect diamonds<br>to rectangles. Figure 2.12 shows the role indicators manager and worker between the<br>employee entity set and the works-for relationship set.<br>Nonbinary relationship sets</span><br><br><span style="background-color: #FFC6FF;" title="Chunk 7 | Start: 140014 | End: 160014 | Tokens: 3140"> can be speciﬁed easily in an E-R diagram. Figure 2.13<br>consists of the three entity sets employee, job, and branch, related through the relation-<br>ship set works-on.<br>We can specify some types of many-to-one relationships in the case of nonbinary<br>relationship sets. Suppose an employee can have at most one job in each branch (for<br>example, Jones cannot be a manager and an auditor at the same branch). This con-<br>straint can be speciﬁed by an arrow pointing to job on the edge from works-on.<br>We permit at most one arrow out of a relationship set, since an E-R diagram with<br>two or more arrows out of a nonbinary relationship set can be interpreted in two<br>ways. Suppose there is a relationship set R between entity sets A1, A2, . . . , An, and the<br>only arrows are on the edges to entity sets Ai+1, Ai+2, . . . , An. Then, the two possible<br>interpretations are:<br>1. A particular combination of entities from A1, A2, . . . , Ai can be associated with<br>at most one combination of entities from Ai+1, Ai+2, . . . , An. Thus, the pri-<br>mary key for the relationship R can be constructed by the union of the primary<br>keys of A1, A2, . . . , Ai.<br>branch<br>branch-city<br>branch-name<br>assets<br>employee-id<br>title<br>level<br>street<br>city<br>employee-name<br>employee<br>job<br>works-on<br>Figure 2.13<br>E-R diagram with a ternary relationship.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>I. Data Models<br>2. Entity−Relationship <br>Model<br>55<br>© The McGraw−Hill <br>Companies, 2001<br>46<br>Chapter 2<br>Entity-Relationship Model<br>borrower<br>amount<br>loan-number<br>loan<br>customer-city<br>customer-id<br>customer-name<br>customer-street<br>customer<br>Figure 2.14<br>Total participation of an entity set in a relationship set.<br>2. For each entity set Ak, i &lt; k ≤n, each combination of the entities from the<br>other entity sets can be associated with at most one entity from Ak. Each set<br>{A1, A2, . . . , Ak−1, Ak+1, . . . , An}, for i &lt; k ≤n, then forms a candidate key.<br>Each of these interpretations has been used in different books and systems. To avoid<br>confusion, we permit only one arrow out of a relationship set, in which case the two<br>interpretations are equivalent. In Chapter 7 (Section 7.3) we study the notion of func-<br>tional dependencies, which allow either of these interpretations to be speciﬁed in an<br>unambiguous manner.<br>Double lines are used in an E-R diagram to indicate that the participation of an<br>entity set in a relationship set is total; that is, each entity in the entity set occurs in at<br>least one relationship in that relationship set. For instance, consider the relationship<br>borrower between customers and loans. A double line from loan to borrower, as in<br>Figure 2.14, indicates that each loan must have at least one associated customer.<br>E-R diagrams also provide a way to indicate more complex constraints on the num-<br>ber of times each entity participates in relationships in a relationship set. An edge<br>between an entity set and a binary relationship set can have an associated minimum<br>and maximum cardinality, shown in the form l..h, where l is the minimum and h<br>the maximum cardinality. A minimum value of 1 indicates total participation of the<br>entity set in the relationship set. A maximum value of 1 indicates that the entity par-<br>ticipates in at most one relationship, while a maximum value ∗indicates no limit.<br>Note that a label 1..∗on an edge is equivalent to a double line.<br>For example, consider Figure 2.15. The edge between loan and borrower has a car-<br>dinality constraint of 1..1, meaning the minimum and the maximum cardinality are<br>both 1. That is, each loan must have exactly one associated customer. The limit 0..∗<br>on the edge from customer to borrower indicates that a customer can have zero or<br>more loans. Thus, the relationship borrower is one to many from customer to loan, and<br>further the participation of loan in borrower is total.<br>It is easy to misinterpret the 0..∗on the edge between customer and borrower, and<br>think that the relationship borrower is many to one from customer to loan—this is<br>exactly the reverse of the correct interpretation.<br>If both edges from a binary relationship have a maximum value of 1, the relation-<br>ship is one to one. If we had speciﬁed a cardinality limit of 1..∗on the edge between<br>customer and borrower, we would be saying that each customer must have at least one<br>loan.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>I. Data Models<br>2. Entity−Relationship <br>Model<br>56<br>© The McGraw−Hill <br>Companies, 2001<br>2.6<br>Weak Entity Sets<br>47<br>borrower<br>amount<br>loan-number<br>loan<br>customer-city<br>customer-street<br>customer<br>0..*<br>1..1<br>customer-name<br>customer-id<br>Figure 2.15<br>Cardinality limits on relationship sets.<br>2.6<br>Weak Entity Sets<br>An entity set may not have sufﬁcient attributes to form a primary key. Such an entity<br>set is termed a weak entity set. An entity set that has a primary key is termed a strong<br>entity set.<br>As an illustration, consider the entity set payment, which has the three attributes:<br>payment-number, payment-date, and payment-amount. Payment numbers are typically<br>sequential numbers, starting from 1, generated separately for each loan. Thus, al-<br>though each payment entity is distinct, payments for different loans may share the<br>same payment number. Thus, this entity set does not have a primary key; it is a weak<br>entity set.<br>For a weak entity set to be meaningful, it must be associated with another entity<br>set, called the identifying or owner entity set. Every weak entity must be associated<br>with an identifying entity; that is, the weak entity set is said to be existence depen-<br>dent on the identifying entity set. The identifying entity set is said to own the weak<br>entity set that it identiﬁes. The relationship associating the weak entity set with the<br>identifying entity set is called the identifying relationship. The identifying relation-<br>ship is many to one from the weak entity set to the identifying entity set, and the<br>participation of the weak entity set in the relationship is total.<br>In our example, the identifying entity set for payment is loan, and a relationship<br>loan-payment that associates payment entities with their corresponding loan entities is<br>the identifying relationship.<br>Although a weak entity set does not have a primary key, we nevertheless need a<br>means of distinguishing among all those entities in the weak entity set that depend<br>on one particular strong entity. The discriminator of a weak entity set is a set of at-<br>tributes that allows this distinction to be made. For example, the discriminator of the<br>weak entity set payment is the attribute payment-number, since, for each loan, a pay-<br>ment number uniquely identiﬁes one single payment for that loan. The discriminator<br>of a weak entity set is also called the partial key of the entity set.<br>The primary key of a weak entity set is formed by the primary key of the iden-<br>tifying entity set, plus the weak entity set’s discriminator. In the case of the entity<br>set payment, its primary key is {loan-number, payment-number}, where loan-number is<br>the primary key of the identifying entity set, namely loan, and payment-number dis-<br>tinguishes payment entities within the same loan.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>I. Data Models<br>2. Entity−Relationship <br>Model<br>57<br>© The McGraw−Hill <br>Companies, 2001<br>48<br>Chapter 2<br>Entity-Relationship Model<br>The identifying relationship set should have no descriptive attributes, since any<br>required attributes can be associated with the weak entity set (see the discussion of<br>moving relationship-set attributes to participating entity sets in Section 2.2.1).<br>A weak entity set can participate in relationships other than the identifying re-<br>lationship. For instance, the payment entity could participate in a relationship with<br>the account entity set, identifying the account from which the payment was made. A<br>weak entity set may participate as owner in an identifying relationship with another<br>weak entity set. It is also possible to have a weak entity set with more than one iden-<br>tifying entity set. A particular weak entity would then be identiﬁed by a combination<br>of entities, one from each identifying entity set. The primary key of the weak entity<br>set would consist of the union of the primary keys of the identifying entity sets, plus<br>the discriminator of the weak entity set.<br>In E-R diagrams, a doubly outlined box indicates a weak entity set, and a dou-<br>bly outlined diamond indicates the corresponding identifying relationship. In Fig-<br>ure 2.16, the weak entity set payment depends on the strong entity set loan via the<br>relationship set loan-payment.<br>The ﬁgure also illustrates the use of double lines to indicate total participation—the<br>participation of the (weak) entity set payment in the relationship loan-payment is total,<br>meaning that every payment must be related via loan-payment to some loan. Finally,<br>the arrow from loan-payment to loan indicates that each payment is for a single loan.<br>The discriminator of a weak entity set also is underlined, but with a dashed, rather<br>than a solid, line.<br>In some cases, the database designer may choose to express a weak entity set as<br>a multivalued composite attribute of the owner entity set. In our example, this alter-<br>native would require that the entity set loan have a multivalued, composite attribute<br>payment, consisting of payment-number, payment-date, and payment-amount. A weak<br>entity set may be more appropriately modeled as an attribute if it participates in only<br>the identifying relationship, and if it has few attributes. Conversely, a weak-entity-<br>set representation will more aptly model a situation where the set participates in<br>relationships other than the identifying relationship, and where the weak entity set<br>has several attributes.<br>loan-number<br>amount<br>loan<br>payment-number<br>payment-amount<br>payment<br>loan-payment<br>payment-date<br>Figure 2.16<br>E-R diagram with a weak entity set.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>I. Data Models<br>2. Entity−Relationship <br>Model<br>58<br>© The McGraw−Hill <br>Companies, 2001<br>2.7<br>Extended E-R Features<br>49<br>As another example of an entity set that can be modeled as a weak entity set,<br>consider offerings of a course at a university. The same course may be offered in<br>different semesters, and within a semester there may be several sections for the same<br>course. Thus we can create a weak entity set course-offering, existence dependent on<br>course; different offerings of the same course are identiﬁed by a semester and a section-<br>number, which form a discriminator but not a primary key.<br>2.7<br>Extended E-R Features<br>Although the basic E-R concepts can model most database features, some aspects of a<br>database may be more aptly expressed by certain extensions to the basic E-R model.<br>In this section, we discuss the extended E-R features of specialization, generalization,<br>higher- and lower-level entity sets, attribute inheritance, and aggregation.<br>2.7.1<br>Specialization<br>An entity set may include subgroupings of entities that are distinct in some way<br>from other entities in the set. For instance, a subset of entities within an entity set<br>may have attributes that are not shared by all the entities in the entity set. The E-R<br>model provides a means for representing these distinctive entity groupings.<br>Consider an entity set person, with attributes name, street, and city. A person may<br>be further classiﬁed as one of the following:<br>• customer<br>• employee<br>Each of these person types is described by a set of attributes that includes all the at-<br>tributes of entity set person plus possibly additional attributes. For example, customer<br>entities may be described further by the attribute customer-id, whereas employee enti-<br>ties may be described further by the attributes employee-id and salary. The process of<br>designating subgroupings within an entity set is called specialization. The special-<br>ization of person allows us to distinguish among persons according to whether they<br>are employees or customers.<br>As another example, suppose the bank wishes to divide accounts into two cat-<br>egories, checking account and savings account. Savings accounts need a minimum<br>balance, but the bank may set interest rates differently for different customers, offer-<br>ing better rates to favored customers. Checking accounts have a ﬁxed interest rate,<br>but offer an overdraft facility; the overdraft amount on a checking account must be<br>recorded.<br>The bank could then create two specializations of account, namely savings-account<br>and checking-account. As we saw earlier, account entities are described by the at-<br>tributes account-number and balance. The entity set savings-account would have all the<br>attributes of account and an additional attribute interest-rate. The entity set checking-<br>account would have all the attributes of account, and an additional attribute overdraft-<br>amount.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>I. Data Models<br>2. Entity−Relationship <br>Model<br>59<br>© The McGraw−Hill <br>Companies, 2001<br>50<br>Chapter 2<br>Entity-Relationship Model<br>We can apply specialization repeatedly to reﬁne a design scheme. For instance,<br>bank employees may be further classiﬁed as one of the following:<br>• ofﬁcer<br>• teller<br>• secretary<br>Each of these employee types is described by a set of attributes that includes all the<br>attributes of entity set employee plus additional attributes. For example, ofﬁcer entities<br>may be described further by the attribute ofﬁce-number, teller entities by the attributes<br>station-number and hours-per-week, and secretary entities by the attribute hours-per-<br>week. Further, secretary entities may participate in a relationship secretary-for, which<br>identiﬁes which employees are assisted by a secretary.<br>An entity set may be specialized by more than one distinguishing feature. In our<br>example, the distinguishing feature among employee entities is the job the employee<br>performs. Another, coexistent, specialization could be based on whether the person<br>is a temporary (limited-term) employee or a permanent employee, resulting in the<br>entity sets temporary-employee and permanent-employee. When more than one special-<br>ization is formed on an entity set, a particular entity may belong to multiple spe-<br>cializations. For instance, a given employee may be a temporary employee who is a<br>secretary.<br>In terms of an E-R diagram, specialization is depicted by a triangle component<br>labeled ISA, as Figure 2.17 shows. The label ISA stands for “is a” and represents, for<br>example, that a customer “is a” person. The ISA relationship may also be referred to as<br>a superclass-subclass relationship. Higher- and lower-level entity sets are depicted<br>as regular entity sets—that is, as rectangles containing the name of the entity set.<br>2.7.2<br>Generalization<br>The reﬁnement from an initial entity set into successive levels of entity subgroupings<br>represents a top-down design process in which distinctions are made explicit. The<br>design process may also proceed in a bottom-up manner, in which multiple entity<br>sets are synthesized into a higher-level entity set on the basis of common features. The<br>database designer may have ﬁrst identiﬁed a customer entity set with the attributes<br>name, street, city, and customer-id, and an employee entity set with the attributes name,<br>street, city, employee-id, and salary.<br>There are similarities between the customer entity set and the employee entity set<br>in the sense that they have several attributes in common. This commonality can be<br>expressed by generalization, which is a containment relationship that exists between<br>a higher-level entity set and one or more lower-level entity sets. In our example, person<br>is the higher-level entity set and customer and employee are lower-level entity sets.<br>Higher- and lower-level entity sets also may be designated by the terms superclass<br>and subclass, respectively. The person entity set is the superclass of the customer and<br>employee subclasses.<br>For all practical purposes, generalization is a simple inversion of specialization.<br>We will apply both processes, in combination, in the course of designing the E-R<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>I. Data Models<br>2. Entity−Relationship <br>Model<br>60<br>© The McGraw−Hill <br>Companies, 2001<br>2.7<br>Extended E-R Features<br>51<br>street<br>employee<br>customer<br>officer<br>teller<br>secretary<br>ISA<br>ISA<br>person<br>name<br>city<br>hours-worked<br>office-number<br>hours-worked<br>station-number<br>credit-rating<br>salary<br>Figure 2.17<br>Specialization and generalization.<br>schema for an enterprise. In terms of the E-R diagram itself, we do not distinguish be-<br>tween specialization and generalization. New levels of entity representation will be<br>distinguished (specialization) or synthesized (generalization) as the design schema<br>comes to express fully the database application and the user requirements of the<br>database. Differences in the two approaches may be characterized by their starting<br>point and overall goal.<br>Specialization stems from a single entity set; it emphasizes differences among enti-<br>ties within the set by creating distinct lower-level entity sets. These lower-level entity<br>sets may have attributes, or may participate in relationships, that do not apply to all<br>the entities in the higher-level entity set. Indeed, the reason a designer applies special-<br>ization is to represent such distinctive features. If customer and employee neither have<br>attributes that person entities do not have nor participate in different relationships<br>than those in which person entities participate, there would be no need to specialize<br>the person entity set.<br>Generalization proceeds from the recognition that a number of entity sets share<br>some common features (namely, they are described by the same attributes and par-<br>ticipate in the same relationship sets). On the basis of their commonalities, generaliza-<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>I. Data Models<br>2. Entity−Relationship <br>Model<br>61<br>© The McGraw−Hill <br>Companies, 2001<br>52<br>Chapter 2<br>Entity-Relationship Model<br>tion synthesizes these entity sets into a single, higher-level entity set. Generalization<br>is used to emphasize the similarities among lower-level entity sets and to hide the<br>differences; it also permits an economy of representation in that shared attributes are<br>not repeated.<br>2.7.3<br>Attribute Inheritance<br>A crucial property of the higher- and lower-level entities created by specialization<br>and generalization is attribute inheritance. The attributes of the higher-level entity<br>sets are said to be inherited by the lower-level entity sets. For example, customer and<br>employee inherit the attributes of person. Thus, customer is described by its name, street,<br>and city attributes, and additionally a customer-id attribute; employee is described by<br>its name, street, and city attributes, and additionally employee-id and salary attributes.<br>A lower-level entity set (or subclass) also inherits participation in the relationship<br>sets in which its higher-level entity (or superclass) participates. The ofﬁcer, teller, and<br>secretary entity sets can participate in the works-for relationship set, since the super-<br>class employee participates in the works-for relationship. Attribute inheritance applies<br>through all tiers of lower-level entity sets. The above entity sets can participate in any<br>relationships in which the person entity set participates.<br>Whether a given portion of an E-R model was arrived at by specialization or gen-<br>eralization, the outcome is basically the same:<br>• A higher-level entity set with attributes and relationships that apply to all of<br>its lower-level entity sets<br>• Lower-level entity sets with distinctive features that apply only within a par-<br>ticular lower-level entity set<br>In what follows, although we often refer to only generalization, the properties that<br>we discuss belong fully to both processes.<br>Figure 2.17 depicts a hierarchy of entity sets. In the ﬁgure, employee is a lower-level<br>entity set of person and a higher-level entity set of the ofﬁcer, teller, and secretary entity<br>sets. In a hierarchy, a given entity set may be involved as a lower-level ent</span><br><br><span style="background-color: #FFADAD;" title="Chunk 8 | Start: 160016 | End: 180016 | Tokens: 3129">ity set in<br>only one ISA relationship; that is, entity sets in this diagram have only single inher-<br>itance. If an entity set is a lower-level entity set in more than one ISA relationship,<br>then the entity set has multiple inheritance, and the resulting structure is said to be<br>a lattice.<br>2.7.4<br>Constraints on Generalizations<br>To model an enterprise more accurately, the database designer may choose to place<br>certain constraints on a particular generalization. One type of constraint involves<br>determining which entities can be members of a given lower-level entity set. Such<br>membership may be one of the following:<br>• Condition-deﬁned. In condition-deﬁned lower-level entity sets, membership<br>is evaluated on the basis of whether or not an entity satisﬁes an explicit con-<br>dition or predicate. For example, assume that the higher-level entity set ac-<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>I. Data Models<br>2. Entity−Relationship <br>Model<br>62<br>© The McGraw−Hill <br>Companies, 2001<br>2.7<br>Extended E-R Features<br>53<br>count has the attribute account-type. All account entities are evaluated on the<br>deﬁning account-type attribute. Only those entities that satisfy the condition<br>account-type = “savings account” are allowed to belong to the lower-level en-<br>tity set person. All entities that satisfy the condition account-type = “checking<br>account” are included in checking account. Since all the lower-level entities are<br>evaluated on the basis of the same attribute (in this case, on account-type), this<br>type of generalization is said to be attribute-deﬁned.<br>• User-deﬁned. User-deﬁned lower-level entity sets are not constrained by a<br>membership condition; rather, the database user assigns entities to a given en-<br>tity set. For instance, let us assume that, after 3 months of employment, bank<br>employees are assigned to one of four work teams. We therefore represent the<br>teams as four lower-level entity sets of the higher-level employee entity set. A<br>given employee is not assigned to a speciﬁc team entity automatically on the<br>basis of an explicit deﬁning condition. Instead, the user in charge of this de-<br>cision makes the team assignment on an individual basis. The assignment is<br>implemented by an operation that adds an entity to an entity set.<br>A second type of constraint relates to whether or not entities may belong to more<br>than one lower-level entity set within a single generalization. The lower-level entity<br>sets may be one of the following:<br>• Disjoint. A disjointness constraint requires that an entity belong to no more<br>than one lower-level entity set. In our example, an account entity can satisfy<br>only one condition for the account-type attribute; an entity can be either a sav-<br>ings account or a checking account, but cannot be both.<br>• Overlapping. In overlapping generalizations, the same entity may belong to<br>more than one lower-level entity set within a single generalization. For an<br>illustration, consider the employee work team example, and assume that cer-<br>tain managers participate in more than one work team. A given employee may<br>therefore appear in more than one of the team entity sets that are lower-level<br>entity sets of employee. Thus, the generalization is overlapping.<br>As another example, suppose generalization applied to entity sets customer<br>and employee leads to a higher-level entity set person. The generalization is<br>overlapping if an employee can also be a customer.<br>Lower-level entity overlap is the default case; a disjointness constraint must be placed<br>explicitly on a generalization (or specialization). We can note a disjointedness con-<br>straint in an E-R diagram by adding the word disjoint next to the triangle symbol.<br>A ﬁnal constraint, the completeness constraint on a generalization or specializa-<br>tion, speciﬁes whether or not an entity in the higher-level entity set must belong to at<br>least one of the lower-level entity sets within the generalization/specialization. This<br>constraint may be one of the following:<br>• Total generalization or specialization. Each higher-level entity must belong<br>to a lower-level entity set.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>I. Data Models<br>2. Entity−Relationship <br>Model<br>63<br>© The McGraw−Hill <br>Companies, 2001<br>54<br>Chapter 2<br>Entity-Relationship Model<br>• Partial generalization or specialization. Some higher-level entities may not<br>belong to any lower-level entity set.<br>Partial generalization is the default. We can specify total generalization in an E-R dia-<br>gram by using a double line to connect the box representing the higher-level entity set<br>to the triangle symbol. (This notation is similar to the notation for total participation<br>in a relationship.)<br>The account generalization is total: All account entities must be either a savings<br>account or a checking account. Because the higher-level entity set arrived at through<br>generalization is generally composed of only those entities in the lower-level entity<br>sets, the completeness constraint for a generalized higher-level entity set is usually<br>total. When the generalization is partial, a higher-level entity is not constrained to<br>appear in a lower-level entity set. The work team entity sets illustrate a partial spe-<br>cialization. Since employees are assigned to a team only after 3 months on the job,<br>some employee entities may not be members of any of the lower-level team entity sets.<br>We may characterize the team entity sets more fully as a partial, overlapping spe-<br>cialization of employee. The generalization of checking-account and savings-account into<br>account is a total, disjoint generalization. The completeness and disjointness con-<br>straints, however, do not depend on each other. Constraint patterns may also be<br>partial-disjoint and total-overlapping.<br>We can see that certain insertion and deletion requirements follow from the con-<br>straints that apply to a given generalization or specialization. For instance, when a<br>total completeness constraint is in place, an entity inserted into a higher-level en-<br>tity set must also be inserted into at least one of the lower-level entity sets. With a<br>condition-deﬁned constraint, all higher-level entities that satisfy the condition must<br>be inserted into that lower-level entity set. Finally, an entity that is deleted from a<br>higher-level entity set also is deleted from all the associated lower-level entity sets to<br>which it belongs.<br>2.7.5<br>Aggregation<br>One limitation of the E-R model is that it cannot express relationships among rela-<br>tionships. To illustrate the need for such a construct, consider the ternary relationship<br>works-on, which we saw earlier, between a employee, branch, and job (see Figure 2.13).<br>Now, suppose we want to record managers for tasks performed by an employee at a<br>branch; that is, we want to record managers for (employee, branch, job) combinations.<br>Let us assume that there is an entity set manager.<br>One alternative for representing this relationship is to create a quaternary relation-<br>ship manages between employee, branch, job, and manager. (A quaternary relationship is<br>required—a binary relationship between manager and employee would not permit us<br>to represent which (branch, job) combinations of an employee are managed by which<br>manager.) Using the basic E-R modeling constructs, we obtain the E-R diagram of<br>Figure 2.18. (We have omitted the attributes of the entity sets, for simplicity.)<br>It appears that the relationship sets works-on and manages can be combined into<br>one single relationship set. Nevertheless, we should not combine them into a single<br>relationship, since some employee, branch, job combinations many not have a manager.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>I. Data Models<br>2. Entity−Relationship <br>Model<br>64<br>© The McGraw−Hill <br>Companies, 2001<br>2.7<br>Extended E-R Features<br>55<br>employee<br>branch<br>manages<br>manager<br>job<br>works-on<br>Figure 2.18<br>E-R diagram with redundant relationships.<br>There is redundant information in the resultant ﬁgure, however, since every em-<br>ployee, branch, job combination in manages is also in works-on. If the manager were a<br>value rather than an manager entity, we could instead make manager a multivalued at-<br>tribute of the relationship works-on. But doing so makes it more difﬁcult (logically as<br>well as in execution cost) to ﬁnd, for example, employee-branch-job triples for which<br>a manager is responsible. Since the manager is a manager entity, this alternative is<br>ruled out in any case.<br>The best way to model a situation such as the one just described is to use aggrega-<br>tion. Aggregation is an abstraction through which relationships are treated as higher-<br>level entities. Thus, for our example, we regard the relationship set works-on (relating<br>the entity sets employee, branch, and job) as a higher-level entity set called works-on.<br>Such an entity set is treated in the same manner as is any other entity set. We can<br>then create a binary relationship manages between works-on and manager to represent<br>who manages what tasks. Figure 2.19 shows a notation for aggregation commonly<br>used to represent the above situation.<br>2.7.6<br>Alternative E-R Notations<br>Figure 2.20 summarizes the set of symbols we have used in E-R diagrams. There is<br>no universal standard for E-R diagram notation, and different books and E-R diagram<br>software use different notations; Figure 2.21 indicates some of the alternative nota-<br>tions that are widely used. An entity set may be represented as a box with the name<br>outside, and the attributes listed one below the other within the box. The primary<br>key attributes are indicated by listing them at the top, with a line separating them<br>from the other attributes.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>I. Data Models<br>2. Entity−Relationship <br>Model<br>65<br>© The McGraw−Hill <br>Companies, 2001<br>56<br>Chapter 2<br>Entity-Relationship Model<br>branch<br>employee<br>manages<br>manager<br>works-on<br>job<br>Figure 2.19<br>E-R diagram with aggregation.<br>Cardinality constraints can be indicated in several different ways, as Figure 2.21<br>shows. The labels ∗and 1 on the edges out of the relationship are sometimes used for<br>depicting many-to-many, one-to-one, and many-to-one relationships, as the ﬁgure<br>shows. The case of one-to-many is symmetric to many-to-one, and is not shown. In<br>another alternative notation in the ﬁgure, relationship sets are represented by lines<br>between entity sets, without diamonds; only binary relationships can be modeled<br>thus. Cardinality constraints in such a notation are shown by “crow’s foot” notation,<br>as in the ﬁgure.<br>2.8<br>Design of an E-R Database Schema<br>The E-R data model gives us much ﬂexibility in designing a database schema to<br>model a given enterprise. In this section, we consider how a database designer may<br>select from the wide range of alternatives. Among the designer’s decisions are:<br>• Whether to use an attribute or an entity set to represent an object (discussed<br>earlier in Section 2.2.1)<br>• Whether a real-world concept is expressed more accurately by an entity set or<br>by a relationship set (Section 2.2.2)<br>• Whether to use a ternary relationship or a pair of binary relationships (Sec-<br>tion 2.2.3)<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>I. Data Models<br>2. Entity−Relationship <br>Model<br>66<br>© The McGraw−Hill <br>Companies, 2001<br>2.8<br>Design of an E-R Database Schema<br>57<br>total <br>participation <br>of entity set <br>in relationship<br>A<br>R<br>E<br>many-to-many<br>relationship<br>R<br>R<br>E<br>role-<br>name<br>ISA<br>(specialization or<br>generalization) <br>many-to-one<br>relationship<br>R<br>E<br>R<br>l..h<br>cardinality <br>limits<br>discriminating <br>attribute of <br>weak entity set<br>ISA<br>ISA<br>total<br>generalization<br>ISA<br>attribute<br>multivalued<br>attribute<br>derived attribute<br>weak entity set<br>entity set<br>E<br>R<br>A<br>A<br>R<br>E<br>A<br>A<br>primary key<br>relationship set<br>identifying<br>relationship <br>set for weak <br>entity set<br>role indicator<br>one-to-one<br>relationship<br>R<br>disjoint<br>disjoint<br>generalization<br>Figure 2.20<br>Symbols used in the E-R notation.<br>• Whether to use a strong or a weak entity set (Section 2.6); a strong entity set<br>and its dependent weak entity sets may be regarded as a single “object” in the<br>database, since weak entities are existence dependent on a strong entity<br>• Whether using generalization (Section 2.7.2) is appropriate; generalization, or<br>a hierarchy of ISA relationships, contributes to modularity by allowing com-<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>I. Data Models<br>2. Entity−Relationship <br>Model<br>67<br>© The McGraw−Hill <br>Companies, 2001<br>58<br>Chapter 2<br>Entity-Relationship Model<br>R<br>R<br>many-to-many<br>relationship<br>entity set E with<br>attributes A1, A2, A3<br>and primary key A1<br>*<br>*<br>R<br>1<br>1<br>R<br>*<br>1<br>R<br>R<br>one-to-one<br>relationship<br>many-to-one<br>relationship<br>E<br>A1<br>A2<br>A3<br>Figure 2.21<br>Alternative E-R notations.<br>mon attributes of similar entity sets to be represented in one place in an E-R<br>diagram<br>• Whether using aggregation (Section 2.7.5) is appropriate; aggregation groups<br>a part of an E-R diagram into a single entity set, allowing us to treat the ag-<br>gregate entity set as a single unit without concern for the details of its internal<br>structure.<br>We shall see that the database designer needs a good understanding of the enterprise<br>being modeled to make these decisions.<br>2.8.1<br>Design Phases<br>A high-level data model serves the database designer by providing a conceptual<br>framework in which to specify, in a systematic fashion, what the data requirements<br>of the database users are, and how the database will be structured to fulﬁll these<br>requirements. The initial phase of database design, then, is to characterize fully the<br>data needs of the prospective database users. The database designer needs to interact<br>extensively with domain experts and users to carry out this task. The outcome of this<br>phase is a speciﬁcation of user requirements.<br>Next, the designer chooses a data model, and by applying the concepts of the<br>chosen data model, translates these requirements into a conceptual schema of the<br>database. The schema developed at this conceptual-design phase provides a detailed<br>overview of the enterprise. Since we have studied only the E-R model so far, we shall<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>I. Data Models<br>2. Entity−Relationship <br>Model<br>68<br>© The McGraw−Hill <br>Companies, 2001<br>2.8<br>Design of an E-R Database Schema<br>59<br>use it to develop the conceptual schema. Stated in terms of the E-R model, the schema<br>speciﬁes all entity sets, relationship sets, attributes, and mapping constraints. The de-<br>signer reviews the schema to conﬁrm that all data requirements are indeed satisﬁed<br>and are not in conﬂict with one another. She can also examine the design to remove<br>any redundant features. Her focus at this point is describing the data and their rela-<br>tionships, rather than on specifying physical storage details.<br>A fully developed conceptual schema will also indicate the functional require-<br>ments of the enterprise. In a speciﬁcation of functional requirements, users describe<br>the kinds of operations (or transactions) that will be performed on the data. Example<br>operations include modifying or updating data, searching for and retrieving speciﬁc<br>data, and deleting data. At this stage of conceptual design, the designer can review<br>the schema to ensure it meets functional requirements.<br>The process of moving from an abstract data model to the implementation of the<br>database proceeds in two ﬁnal design phases. In the logical-design phase, the de-<br>signer maps the high-level conceptual schema onto the implementation data model<br>of the database system that will be used. The designer uses the resulting system-<br>speciﬁc database schema in the subsequent physical-design phase, in which the<br>physical features of the database are speciﬁed. These features include the form of ﬁle<br>organization and the internal storage structures; they are discussed in Chapter 11.<br>In this chapter, we cover only the concepts of the E-R model as used in the concep-<br>tual-schema-design phase. We have presented a brief overview of the database-design<br>process to provide a context for the discussion of the E-R data model. Database design<br>receives a full treatment in Chapter 7.<br>In Section 2.8.2, we apply the two initial database-design phases to our banking-<br>enterprise example. We employ the E-R data model to translate user requirements<br>into a conceptual design schema that is depicted as an E-R diagram.<br>2.8.2<br>Database Design for Banking Enterprise<br>We now look at the database-design requirements of a banking enterprise in more<br>detail, and develop a more realistic, but also more complicated, design than what<br>we have seen in our earlier examples. However, we do not attempt to model every<br>aspect of the database-design for a bank; we consider only a few aspects, in order to<br>illustrate the process of database design.<br>2.8.2.1<br>Data Requirements<br>The initial speciﬁcation of user requirements may be based on interviews with the<br>database users, and on the designer’s own analysis of the enterprise. The description<br>that arises from this design phase serves as the basis for specifying the conceptual<br>structure of the database. Here are the major characteristics of the banking enterprise.<br>• The bank is organized into branches. Each branch is located in a particular<br>city and is identiﬁed by a unique name. The bank monitors the assets of each<br>branch.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>I. Data Models<br>2. Entity−Relationship <br>Model<br>69<br>© The McGraw−Hill <br>Companies, 2001<br>60<br>Chapter 2<br>Entity-Relationship Model<br>• Bank customers are identiﬁed by their customer-id values. The bank stores each<br>customer’s name, and the street and city where the customer lives. Customers<br>may have accounts and can take out loans. A customer may be associated with<br>a particular banker, who may act as a loan ofﬁcer or personal banker for that<br>customer.<br>• Bank employees are identiﬁed by their employee-id values. The bank adminis-<br>tration stores the name and telephone number of each employee, the names<br>of the employee’s dependents, and the employee-id number of the employee’s<br>manager. The bank also keeps track of the employee’s start date and, thus,<br>length of employment.<br>• The bank offers two types of accounts—savings and checking accounts. Ac-<br>counts can be held by more than one customer, and a customer can have more<br>than one account. Each account is assigned a unique account number. The<br>bank maintains a record of each account’s balance, and the most recent date on<br>which the account was accessed by each customer holding the account. In ad-<br>dition, each savings account has an interest rate, and overdrafts are recorded<br>for each checking account.<br>• A loan originates at a particular branch and can be held by one or more cus-<br>tomers. A loan is identiﬁed by a unique loan number. For each loan, the bank<br>keeps track of the loan amount and the loan payments. Although a loan-<br>payment number does not uniquely identify a particular payment among<br>those for all the bank’s loans, a payment number does identify a particular<br>payment for a speciﬁc loan. The date and amount are recorded for each pay-<br>ment.<br>In a real banking enterprise, the bank would keep track of deposits and with-<br>drawals from savings and checking accounts, just as it keeps track of payments to<br>loan accounts. Since the modeling requirements for that tracking are similar, and we<br>would like to keep our example application small, we do not keep track of such de-<br>posits and withdrawals in our model.<br>2.8.2.2<br>Entity Sets Designation<br>Our speciﬁcation of data requirements serves as the starting point for constructing a<br>conceptual schema for the database. From the characteristics listed in Section 2.8.2.1,<br>we begin to identify entity sets and their attributes:<br>• The branch entity set, with attributes branch-name, branch-city, and assets.<br>• The customer entity set, with attributes customer-id, customer-name, customer-<br>street; and customer-city. A possible additional attribute is banker-name.<br>• The employee entity set, with attributes employee-id, employee-name, telephone-<br>number, salary, and manager. Additional descriptive f</span><br><br><span style="background-color: #FFD6A5;" title="Chunk 9 | Start: 180018 | End: 200018 | Tokens: 3154">eatures are the multival-<br>ued attribute dependent-name, the base attribute start-date, and the derived at-<br>tribute employment-length.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>I. Data Models<br>2. Entity−Relationship <br>Model<br>70<br>© The McGraw−Hill <br>Companies, 2001<br>2.8<br>Design of an E-R Database Schema<br>61<br>• Two account entity sets—savings-account and checking-account—with the com-<br>mon attributes of account-number and balance; in addition, savings-account has<br>the attribute interest-rate and checking-account has the attribute overdraft-amount.<br>• The loan entity set, with the attributes loan-number, amount, and originating-<br>branch.<br>• The weak entity set loan-payment, with attributes payment-number, payment-<br>date, and payment-amount.<br>2.8.2.3<br>Relationship Sets Designation<br>We now return to the rudimentary design scheme of Section 2.8.2.2 and specify the<br>following relationship sets and mapping cardinalities. In the process, we also reﬁne<br>some of the decisions we made earlier regarding attributes of entity sets.<br>• borrower, a many-to-many relationship set between customer and loan.<br>• loan-branch, a many-to-one relationship set that indicates in which branch a<br>loan originated. Note that this relationship set replaces the attribute originating-<br>branch of the entity set loan.<br>• loan-payment, a one-to-many relationship from loan to payment, which docu-<br>ments that a payment is made on a loan.<br>• depositor, with relationship attribute access-date, a many-to-many relationship<br>set between customer and account, indicating that a customer owns an account.<br>• cust-banker, with relationship attribute type, a many-to-one relationship set ex-<br>pressing that a customer can be advised by a bank employee, and that a bank<br>employee can advise one or more customers. Note that this relationship set<br>has replaced the attribute banker-name of the entity set customer.<br>• works-for, a relationship set between employee entities with role indicators man-<br>ager and worker; the mapping cardinalities express that an employee works<br>for only one manager and that a manager supervises one or more employees.<br>Note that this relationship set has replaced the manager attribute of employee.<br>2.8.2.4<br>E-R Diagram<br>Drawing on the discussions in Section 2.8.2.3, we now present the completed E-R di-<br>agram for our example banking enterprise. Figure 2.22 depicts the full representation<br>of a conceptual model of a bank, expressed in terms of E-R concepts. The diagram in-<br>cludes the entity sets, attributes, relationship sets, and mapping cardinalities arrived<br>at through the design processes of Sections 2.8.2.1 and 2.8.2.2, and reﬁned in Section<br>2.8.2.3.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>I. Data Models<br>2. Entity−Relationship <br>Model<br>71<br>© The McGraw−Hill <br>Companies, 2001<br>62<br>Chapter 2<br>Entity-Relationship Model<br>interest-rate<br>overdraft-amount<br>account-number<br>balance<br>ISA<br>customer-name<br>customer-street<br>customer-id<br>customer-city<br>customer<br>branch-city<br>branch-name<br>assets<br>loan-number<br>amount<br>payment-number<br>loan-<br>payment<br>payment-date<br>type<br>dependent-name<br>employee-id<br>employment-<br>length<br> access-date<br>borrower<br>loan-branch<br>cust-banker<br>depositor<br>works-for<br>manager<br>worker<br>employee-name<br>telephone-number<br>start-date<br>branch<br>loan<br>payment<br>payment-amount<br>account<br>checking-account<br>savings-account<br>employee<br>Figure 2.22<br>E-R diagram for a banking enterprise.<br>2.9<br>Reduction of an E-R Schema to Tables<br>We can represent a database that conforms to an E-R database schema by a collection<br>of tables. For each entity set and for each relationship set in the database, there is a<br>unique table to which we assign the name of the corresponding entity set or relation-<br>ship set. Each table has multiple columns, each of which has a unique name.<br>Both the E-R model and the relational-database model are abstract, logical rep-<br>resentations of real-world enterprises. Because the two models employ similar de-<br>sign principles, we can convert an E-R design into a relational design. Converting a<br>database representation from an E-R diagram to a table format is the way we arrive<br>at a relational-database design from an E-R diagram. Although important differences<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>I. Data Models<br>2. Entity−Relationship <br>Model<br>72<br>© The McGraw−Hill <br>Companies, 2001<br>2.9<br>Reduction of an E-R Schema to Tables<br>63<br>exist between a relation and a table, informally, a relation can be considered to be a<br>table of values.<br>In this section, we describe how an E-R schema can be represented by tables; and<br>in Chapter 3, we show how to generate a relational-database schema from an E-R<br>schema.<br>The constraints speciﬁed in an E-R diagram, such as primary keys and cardinality<br>constraints, are mapped to constraints on the tables generated from the E-R diagram.<br>We provide more details about this mapping in Chapter 6 after describing how to<br>specify constraints on tables.<br>2.9.1<br>Tabular Representation of Strong Entity Sets<br>Let E be a strong entity set with descriptive attributes a1, a2, . . . , an. We represent<br>this entity by a table called E with n distinct columns, each of which corresponds to<br>one of the attributes of E. Each row in this table corresponds to one entity of the entity<br>set E.<br>As an illustration, consider the entity set loan of the E-R diagram in Figure 2.8. This<br>entity set has two attributes: loan-number and amount. We represent this entity set by<br>a table called loan, with two columns, as in Figure 2.23. The row<br>(L-17, 1000)<br>in the loan table means that loan number L-17 has a loan amount of $1000. We can<br>add a new entity to the database by inserting a row into a table. We can also delete or<br>modify rows.<br>Let D1 denote the set of all loan numbers, and let D2 denote the set of all balances.<br>Any row of the loan table must consist of a 2-tuple (v1, v2), where v1 is a loan (that<br>is, v1 is in set D1) and v2 is an amount (that is, v2 is in set D2). In general, the loan<br>table will contain only a subset of the set of all possible rows. We refer to the set of all<br>possible rows of loan as the Cartesian product of D1 and D2, denoted by<br>D1 × D2<br>In general, if we have a table of n columns, we denote the Cartesian product of<br>D1, D2, · · · , Dn by<br>D1 × D2 × · · · × Dn−1 × Dn<br>loan-number<br>amount<br>L-11<br>900<br>L-14<br>1500<br>L-15<br>1500<br>L-16<br>1300<br>L-17<br>1000<br>L-23<br>2000<br>L-93<br>500<br>Figure 2.23<br>The loan table.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>I. Data Models<br>2. Entity−Relationship <br>Model<br>73<br>© The McGraw−Hill <br>Companies, 2001<br>64<br>Chapter 2<br>Entity-Relationship Model<br>customer-id<br>customer-name<br>customer-street<br>customer-city<br>019-28-3746<br>182-73-6091<br>192-83-7465<br>244-66-8800<br>321-12-3123<br>335-57-7991<br>336-66-9999<br>677-89-9011<br>963-96-3963<br>Smith<br>Turner<br>Johnson<br>Curry<br>Jones<br>Adams<br>Lindsay<br>Hayes<br>Williams<br>North<br>Putnam<br>Alma<br>North<br>Main<br>Spring<br>Park<br>Main<br>Nassau<br>Rye<br>Stamford<br>Palo Alto<br>Rye<br>Harrison<br>Pittsfield<br>Pittsfield<br>Harrison<br>Princeton<br>Figure 2.24<br>The customer table.<br>As another example, consider the entity set customer of the E-R diagram in Fig-<br>ure 2.8. This entity set has the attributes customer-id, customer-name, customer-street,<br>and customer-city. The table corresponding to customer has four columns, as in Fig-<br>ure 2.24.<br>2.9.2<br>Tabular Representation of Weak Entity Sets<br>Let A be a weak entity set with attributes a1, a2, . . . , am. Let B be the strong entity set<br>on which A depends. Let the primary key of B consist of attributes b1, b2, . . . , bn. We<br>represent the entity set A by a table called A with one column for each attribute of<br>the set:<br>{a1, a2, . . . , am} ∪{b1, b2, . . . , bn}<br>As an illustration, consider the entity set payment in the E-R diagram of Figure 2.16.<br>This entity set has three attributes: payment-number, payment-date, and payment-amount.<br>The primary key of the loan entity set, on which payment depends, is loan-number.<br>Thus, we represent payment by a table with four columns labeled loan-number, payment-<br>number, payment-date, and payment-amount, as in Figure 2.25.<br>2.9.3<br>Tabular Representation of Relationship Sets<br>Let R be a relationship set, let a1, a2, . . . , am be the set of attributes formed by the<br>union of the primary keys of each of the entity sets participating in R, and let the<br>descriptive attributes (if any) of R be b1, b2, . . . , bn. We represent this relationship set<br>by a table called R with one column for each attribute of the set:<br>{a1, a2, . . . , am} ∪{b1, b2, . . . , bn}<br>As an illustration, consider the relationship set borrower in the E-R diagram of Fig-<br>ure 2.8. This relationship set involves the following two entity sets:<br>• customer, with the primary key customer-id<br>• loan, with the primary key loan-number<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>I. Data Models<br>2. Entity−Relationship <br>Model<br>74<br>© The McGraw−Hill <br>Companies, 2001<br>2.9<br>Reduction of an E-R Schema to Tables<br>65<br>loan-number<br>payment-number<br>payment-date<br>payment-amount<br>L-11<br>53<br>7 June 2001<br>125<br>L-14<br>69<br>28 May 2001<br>500<br>L-15<br>22<br>23 May 2001<br>300<br>L-16<br>58<br>18 June 2001<br>135<br>L-17<br>5<br>10 May 2001<br>50<br>L-17<br>6<br>7 June 2001<br>50<br>L-17<br>7<br>17 June 2001<br>100<br>L-23<br>11<br>17 May 2001<br>75<br>L-93<br>103<br>3 June 2001<br>900<br>L-93<br>104<br>13 June 2001<br>200<br>Figure 2.25<br>The payment table.<br>Since the relationship set has no attributes, the borrower table has two columns, la-<br>beled customer-id and loan-number, as shown in Figure 2.26.<br>2.9.3.1<br>Redundancy of Tables<br>A relationship set linking a weak entity set to the corresponding strong entity set is<br>treated specially. As we noted in Section 2.6, these relationships are many-to-one and<br>have no descriptive attributes. Furthermore, the primary key of a weak entity set in-<br>cludes the primary key of the strong entity set. In the E-R diagram of Figure 2.16, the<br>weak entity set payment is dependent on the strong entity set loan via the relation-<br>ship set loan-payment. The primary key of payment is {loan-number, payment-number},<br>and the primary key of loan is {loan-number}. Since loan-payment has no descriptive<br>attributes, the loan-payment table would have two columns, loan-number and payment-<br>number. The table for the entity set payment has four columns, loan-number, payment-<br>number, payment-date, and payment-amount. Every (loan-number, payment-number) com-<br>bination in loan-payment would also be present in the payment table, and vice versa.<br>Thus, the loan-payment table is redundant. In general, the table for the relationship set<br>customer-id<br>loan-number<br>019-28-3746<br>L-11<br>019-28-3746<br>L-23<br>244-66-8800<br>L-93<br>321-12-3123<br>L-17<br>335-57-7991<br>L-16<br>555-55-5555<br>L-14<br>677-89-9011<br>L-15<br>963-96-3963<br>L-17<br>Figure 2.26<br>The borrower table.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>I. Data Models<br>2. Entity−Relationship <br>Model<br>75<br>© The McGraw−Hill <br>Companies, 2001<br>66<br>Chapter 2<br>Entity-Relationship Model<br>linking a weak entity set to its corresponding strong entity set is redundant and does<br>not need to be present in a tabular representation of an E-R diagram.<br>2.9.3.2<br>Combination of Tables<br>Consider a many-to-one relationship set AB from entity set A to entity set B. Using<br>our table-construction scheme outlined previously, we get three tables: A, B, and AB.<br>Suppose further that the participation of A in the relationship is total; that is, every<br>entity a in the entity set A must participate in the relationship AB. Then we can<br>combine the tables A and AB to form a single table consisting of the union of columns<br>of both tables.<br>As an illustration, consider the E-R diagram of Figure 2.27. The double line in the<br>E-R diagram indicates that the participation of account in the account-branch is total.<br>Hence, an account cannot exist without being associated with a particular branch.<br>Further, the relationship set account-branch is many to one from account to branch.<br>Therefore, we can combine the table for account-branch with the table for account and<br>require only the following two tables:<br>• account, with attributes account-number, balance, and branch-name<br>• branch, with attributes branch-name, branch-city, and assets<br>2.9.4<br>Composite Attributes<br>We handle composite attributes by creating a separate attribute for each of the com-<br>ponent attributes; we do not create a separate column for the composite attribute<br>itself. Suppose address is a composite attribute of entity set customer, and the com-<br>ponents of address are street and city. The table generated from customer would then<br>contain columns address-street and address-city; there is no separate column for address.<br>2.9.5<br>Multivalued Attributes<br>We have seen that attributes in an E-R diagram generally map directly into columns<br>for the appropriate tables. Multivalued attributes, however, are an exception; new<br>tables are created for these attributes.<br>account-number<br>balance<br>account<br>branch-name<br>branch-city<br>branch<br>account-<br>branch<br>assets<br>Figure 2.27<br>E-R diagram.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>I. Data Models<br>2. Entity−Relationship <br>Model<br>76<br>© The McGraw−Hill <br>Companies, 2001<br>2.9<br>Reduction of an E-R Schema to Tables<br>67<br>For a multivalued attribute M, we create a table T with a column C that corre-<br>sponds to M and columns corresponding to the primary key of the entity set or rela-<br>tionship set of which M is an attribute. As an illustration, consider the E-R diagram<br>in Figure 2.22. The diagram includes the multivalued attribute dependent-name. For<br>this multivalued attribute, we create a table dependent-name, with columns dname, re-<br>ferring to the dependent-name attribute of employee, and employee-id, representing the<br>primary key of the entity set employee. Each dependent of an employee is represented<br>as a unique row in the table.<br>2.9.6<br>Tabular Representation of Generalization<br>There are two different methods for transforming to a tabular form an E-R diagram<br>that includes generalization. Although we refer to the generalization in Figure 2.17<br>in this discussion, we simplify it by including only the ﬁrst tier of lower-level entity<br>sets—that is, savings-account and checking-account.<br>1. Create a table for the higher-level entity set. For each lower-level entity set,<br>create a table that includes a column for each of the attributes of that entity set<br>plus a column for each attribute of the primary key of the higher-level entity<br>set. Thus, for the E-R diagram of Figure 2.17, we have three tables:<br>• account, with attributes account-number and balance<br>• savings-account, with attributes account-number and interest-rate<br>• checking-account, with attributes account-number and overdraft-amount<br>2. An alternative representation is possible, if the generalization is disjoint and<br>complete—that is, if no entity is a member of two lower-level entity sets di-<br>rectly below a higher-level entity set, and if every entity in the higher level<br>entity set is also a member of one of the lower-level entity sets. Here, do not<br>create a table for the higher-level entity set. Instead, for each lower-level en-<br>tity set, create a table that includes a column for each of the attributes of that<br>entity set plus a column for each attribute of the higher-level entity set. Then,<br>for the E-R diagram of Figure 2.17, we have two tables.<br>• savings-account, with attributes account-number, balance, and interest-rate<br>• checking-account, with attributes account-number, balance, and overdraft-<br>amount<br>The savings-account and checking-account relations corresponding to these<br>tables both have account-number as the primary key.<br>If the second method were used for an overlapping generalization, some values<br>such as balance would be stored twice unnecessarily. Similarly, if the generalization<br>were not complete—that is, if some accounts were neither savings nor checking<br>accounts—then such accounts could not be represented with the second method.<br>2.9.7<br>Tabular Representation of Aggregation<br>Transforming an E-R diagram containing aggregation to a tabular form is straight-<br>forward. Consider the diagram of Figure 2.19. The table for the relationship set<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>I. Data Models<br>2. Entity−Relationship <br>Model<br>77<br>© The McGraw−Hill <br>Companies, 2001<br>68<br>Chapter 2<br>Entity-Relationship Model<br>manages between the aggregation of works-on and the entity set manager includes a<br>column for each attribute in the primary keys of the entity set manager and the rela-<br>tionship set works-on. It would also include a column for any descriptive attributes,<br>if they exist, of the relationship set manages. We then transform the relationship sets<br>and entity sets within the aggregated entity.<br>2.10<br>The Uniﬁed Modeling Language UML∗∗<br>Entity-relationship diagrams help model the data representation component of a soft-<br>ware system. Data representation, however, forms only one part of an overall system<br>design. Other components include models of user interactions with the system, spec-<br>iﬁcation of functional modules of the system and their interaction, etc. The Uniﬁed<br>Modeling Language (UML), is a proposed standard for creating speciﬁcations of var-<br>ious components of a software system. Some of the parts of UML are:<br>• Class diagram. A class diagram is similar to an E-R diagram. Later in this<br>section we illustrate a few features of class diagrams and how they relate to<br>E-R diagrams.<br>• Use case diagram. Use case diagrams show the interaction between users and<br>the system, in particular the steps of tasks that users perform (such as with-<br>drawing money or registering for a course).<br>• Activity diagram. Activity diagrams depict the ﬂow of tasks between various<br>components of a system.<br>• Implementation diagram. Implementation diagrams show the system com-<br>ponents and their interconnections, both at the software component level and<br>the hardware component level.<br>We do not attempt to provide detailed coverage of the different parts of UML here.<br>See the bibliographic notes for references on UML. Instead we illustrate some features<br>of UML through examples.<br>Figure 2.28 shows several E-R diagram constructs and their equivalent UML class<br>diagram constructs. We describe these constructs below. UML shows entity sets as<br>boxes and, unlike E-R, shows attributes within the box rather than as separate el-<br>lipses. UML actually models objects, whereas E-R models entities. Objects are like<br>entities, and have attributes, but additionally provide a set of functions (called meth-<br>ods) that can be invoked to compute values on the basis of attributes of the objects,<br>or to update the object itself. Class diagrams can depict methods in addition to at-<br>tributes. We cover objects in Chapter 8.<br>We represent binary relationship sets in UML by just drawing a line connecting<br>the entity sets. We write the relationship set name adjacent to the line. We may also<br>specify the role played by an entity set in a relationship set by writing the role name<br>on the line, adjacent to the entity set. Alternatively, we may write the relationship set<br>name in a box, along with attributes of the relationship set, and connect the box by a<br>dotted line to the line depicting the relationship set. This box can then be treated as<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>I. Data Models<br>2. Entity−Relationship <br>Model<br>78<br>© The McGraw−Hill <br>Companies, 2001<br>2.10<br>The Uniﬁed Modeling Language UML∗∗<br>69<br>disjoint<br>E1<br>E2<br>R<br>role1<br>role2<br>E2<br>E2<br>person<br>customer<br>employee<br>person<br>customer<br>employee<br>person<br>ISA<br>customer<br>employee<br>person<br>ISA<br>customer<br>employee<br>1. entity sets<br>    and attributes<br>customer-id<br>customer-name<br>customer-street<br>customer-city<br>customer<br>customer-city<br>customer-street<br>customer<br>2. relationships<br>a1<br>role1<br>role2<br>a1<br>a2<br>R<br>R<br>3. cardinality<br>    constraints <br>R<br>0..1<br>R<br>E1<br>E2<br>E1<br>E1<br>E2<br>E2<br>role1<br>role2<br>role1<br>role2<br>4. generalization and<br>    specialization<br>class diagram in UML<br>E-R diagram<br>R<br>0..*<br>0..*<br>0..1<br>customer-name<br>customer-id<br>(overlapping <br>generalization)<br>(disjoint <br>generalization)<br>a2<br>E1<br>E1<br>Figure 2.28<br>Symbols used in the UML class diagram notation.<br>an entity set, in the same way as an aggregation in E-R diagrams and can participate<br>in relationships with other entity sets.<br>Nonbinar</span><br><br><span style="background-color: #FDFFB6;" title="Chunk 10 | Start: 200020 | End: 220020 | Tokens: 3182">y relationships cannot be directly represented in UML—they have to<br>be converted to binary relationships by the technique we have seen earlier in Sec-<br>tion 2.4.3.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>I. Data Models<br>2. Entity−Relationship <br>Model<br>79<br>© The McGraw−Hill <br>Companies, 2001<br>70<br>Chapter 2<br>Entity-Relationship Model<br>Cardinality constraints are speciﬁed in UML in the same way as in E-R diagrams, in<br>the form l..h, where l denotes the minimum and h the maximum number of relation-<br>ships an entity can participate in. However, you should be aware that the positioning<br>of the constraints is exactly the reverse of the positioning of constraints in E-R dia-<br>grams, as shown in Figure 2.28. The constraint 0..∗on the E2 side and 0..1 on the E1<br>side means that each E2 entity can participate in at most one relationship, whereas<br>each E1 entity can participate in many relationships; in other words, the relationship<br>is many to one from E2 to E1.<br>Single values such as 1 or ∗may be written on edges; the single value 1 on an edge<br>is treated as equivalent to 1..1, while ∗is equivalent to 0..∗.<br>We represent generalization and specialization in UML by connecting entity sets<br>by a line with a triangle at the end corresponding to the more general entity set.<br>For instance, the entity set person is a generalization of customer and employee. UML<br>diagrams can also represent explicitly the constraints of disjoint/overlapping on gen-<br>eralizations. Figure 2.28 shows disjoint and overlapping generalizations of customer<br>and employee to person. Recall that if the customer/employee to person generalization is<br>disjoint, it means that no one can be both a customer and an employee. An overlapping<br>generalization allows a person to be both a customer and an employee.<br>2.11<br>Summary<br>• The entity-relationship (E-R) data model is based on a perception of a real<br>world that consists of a set of basic objects called entities, and of relationships<br>among these objects.<br>• The model is intended primarily for the database-design process. It was de-<br>veloped to facilitate database design by allowing the speciﬁcation of an en-<br>terprise schema. Such a schema represents the overall logical structure of the<br>database. This overall structure can be expressed graphically by an E-R dia-<br>gram.<br>• An entity is an object that exists in the real world and is distinguishable from<br>other objects. We express the distinction by associating with each entity a set<br>of attributes that describes the object.<br>• A relationship is an association among several entities. The collection of all<br>entities of the same type is an entity set, and the collection of all relationships<br>of the same type is a relationship set.<br>• Mapping cardinalities express the number of entities to which another entity<br>can be associated via a relationship set.<br>• A superkey of an entity set is a set of one or more attributes that, taken collec-<br>tively, allows us to identify uniquely an entity in the entity set. We choose a<br>minimal superkey for each entity set from among its superkeys; the minimal<br>superkey is termed the entity set’s primary key. Similarly, a relationship set<br>is a set of one or more attributes that, taken collectively, allows us to identify<br>uniquely a relationship in the relationship set. Likewise, we choose a mini-<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>I. Data Models<br>2. Entity−Relationship <br>Model<br>80<br>© The McGraw−Hill <br>Companies, 2001<br>2.11<br>Summary<br>71<br>mal superkey for each relationship set from among its superkeys; this is the<br>relationship set’s primary key.<br>• An entity set that does not have sufﬁcient attributes to form a primary key<br>is termed a weak entity set. An entity set that has a primary key is termed a<br>strong entity set.<br>• Specialization and generalization deﬁne a containment relationship between<br>a higher-level entity set and one or more lower-level entity sets. Specialization<br>is the result of taking a subset of a higher-level entity set to form a lower-<br>level entity set. Generalization is the result of taking the union of two or more<br>disjoint (lower-level) entity sets to produce a higher-level entity set. The at-<br>tributes of higher-level entity sets are inherited by lower-level entity sets.<br>• Aggregation is an abstraction in which relationship sets (along with their as-<br>sociated entity sets) are treated as higher-level entity sets, and can participate<br>in relationships.<br>• The various features of the E-R model offer the database designer numerous<br>choices in how to best represent the enterprise being modeled. Concepts and<br>objects may, in certain cases, be represented by entities, relationships, or at-<br>tributes. Aspects of the overall structure of the enterprise may be best de-<br>scribed by using weak entity sets, generalization, specialization, or aggrega-<br>tion. Often, the designer must weigh the merits of a simple, compact model<br>versus those of a more precise, but more complex, one.<br>• A database that conforms to an E-R diagram can be represented by a collection<br>of tables. For each entity set and for each relationship set in the database, there<br>is a unique table that is assigned the name of the corresponding entity set or<br>relationship set. Each table has a number of columns, each of which has a<br>unique name. Converting database representation from an E-R diagram to a<br>table format is the basis for deriving a relational-database design from an E-R<br>diagram.<br>• The uniﬁed modeling language (UML) provides a graphical means of model-<br>ing various components of a software system. The class diagram component<br>of UML is based on E-R diagrams. However, there are some differences be-<br>tween the two that one must beware of.<br>Review Terms<br>• Entity-relationship data model<br>• Entity<br>• Entity set<br>• Attributes<br>• Domain<br>• Simple and composite attributes<br>• Single-valued and multivalued at-<br>tributes<br>• Null value<br>• Derived attribute<br>• Relationship, and relationship set<br>• Role<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>I. Data Models<br>2. Entity−Relationship <br>Model<br>81<br>© The McGraw−Hill <br>Companies, 2001<br>72<br>Chapter 2<br>Entity-Relationship Model<br>• Recursive relationship set<br>• Descriptive attributes<br>• Binary relationship set<br>• Degree of relationship set<br>• Mapping cardinality:<br>  One-to-one relationship<br>  One-to-many relationship<br>  Many-to-one relationship<br>  Many-to-many relationship<br>• Participation<br>  Total participation<br>  Partial participation<br>• Superkey, candidate key, and pri-<br>mary key<br>• Weak entity sets and strong entity<br>sets<br>  Discriminator attributes<br>  Identifying relationship<br>• Specialization and generalization<br>  Superclass and subclass<br>  Attribute inheritance<br>  Single and multiple inheri-<br>tance<br>  Condition-deﬁned and user-<br>deﬁned membership<br>  Disjoint and overlapping gen-<br>eralization<br>• Completeness constraint<br>  Total and partial generaliza-<br>tion<br>• Aggregation<br>• E-R diagram<br>• Uniﬁed Modeling Language (UML)<br>Exercises<br>2.1 Explain the distinctions among the terms primary key, candidate key, and su-<br>perkey.<br>2.2 Construct an E-R diagram for a car-insurance company whose customers own<br>one or more cars each. Each car has associated with it zero to any number of<br>recorded accidents.<br>2.3 Construct an E-R diagram for a hospital with a set of patients and a set of medi-<br>cal doctors. Associate with each patient a log of the various tests and examina-<br>tions conducted.<br>2.4 A university registrar’s ofﬁce maintains data about the following entities: (a)<br>courses, including number, title, credits, syllabus, and prerequisites; (b) course<br>offerings, including course number, year, semester, section number, instructor(s),<br>timings, and classroom; (c) students, including student-id, name, and program;<br>and (d) instructors, including identiﬁcation number, name, department, and ti-<br>tle. Further, the enrollment of students in courses and grades awarded to stu-<br>dents in each course they are enrolled for must be appropriately modeled.<br>Construct an E-R diagram for the registrar’s ofﬁce. Document all assumptions<br>that you make about the mapping constraints.<br>2.5 Consider a database used to record the marks that students get in different ex-<br>ams of different course offerings.<br>a. Construct an E-R diagram that models exams as entities, and uses a ternary<br>relationship, for the above database.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>I. Data Models<br>2. Entity−Relationship <br>Model<br>82<br>© The McGraw−Hill <br>Companies, 2001<br>Exercises<br>73<br>b. Construct an alternative E-R diagram that uses only a binary relationship<br>between students and course-offerings. Make sure that only one relationship<br>exists between a particular student and course-offering pair, yet you can<br>represent the marks that a student gets in different exams of a course offer-<br>ing.<br>2.6 Construct appropriate tables for each of the E-R diagrams in Exercises 2.2 to 2.4.<br>2.7 Design an E-R diagram for keeping track of the exploits of your favourite sports<br>team. You should store the matches played, the scores in each match, the players<br>in each match and individual player statistics for each match. Summary statis-<br>tics should be modeled as derived attributes<br>2.8 Extend the E-R diagram of the previous question to track the same information<br>for all teams in a league.<br>2.9 Explain the difference between a weak and a strong entity set.<br>2.10 We can convert any weak entity set to a strong entity set by simply adding ap-<br>propriate attributes. Why, then, do we have weak entity sets?<br>2.11 Deﬁne the concept of aggregation. Give two examples of where this concept is<br>useful.<br>2.12 Consider the E-R diagram in Figure 2.29, which models an online bookstore.<br>a. List the entity sets and their primary keys.<br>b. Suppose the bookstore adds music cassettes and compact disks to its col-<br>lection. The same music item may be present in cassette or compact disk<br>format, with differing prices. Extend the E-R diagram to model this addi-<br>tion, ignoring the effect on shopping baskets.<br>c. Now extend the E-R diagram, using generalization, to model the case where<br>a shopping basket may contain any combination of books, music cassettes,<br>or compact disks.<br>2.13 Consider an E-R diagram in which the same entity set appears several times.<br>Why is allowing this redundancy a bad practice that one should avoid whenever<br>possible?<br>2.14 Consider a university database for the scheduling of classrooms for ﬁnal exams.<br>This database could be modeled as the single entity set exam, with attributes<br>course-name, section-number, room-number, and time. Alternatively, one or more<br>additional entity sets could be deﬁned, along with relationship sets to replace<br>some of the attributes of the exam entity set, as<br>• course with attributes name, department, and c-number<br>• section with attributes s-number and enrollment, and dependent as a weak<br>entity set on course<br>• room with attributes r-number, capacity, and building<br>a. Show an E-R diagram illustrating the use of all three additional entity sets<br>listed.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>I. Data Models<br>2. Entity−Relationship <br>Model<br>83<br>© The McGraw−Hill <br>Companies, 2001<br>74<br>Chapter 2<br>Entity-Relationship Model<br>basketID<br>email<br>basket-of<br>ISBN<br>code<br>name<br>URL<br>address<br>name<br>address<br>phone<br>URL<br>publisher<br>published-by<br>written-by<br>title<br>price<br>number<br>book<br>contains<br>phone<br>customer<br>address<br>name<br>phone<br>stocks<br>warehouse<br>address<br>number<br>author<br>year<br>shopping-basket<br>Figure 2.29<br>E-R diagram for Exercise 2.12.<br>b. Explain what application characteristics would inﬂuence a decision to in-<br>clude or not to include each of the additional entity sets.<br>2.15 When designing an E-R diagram for a particular enterprise, you have several<br>alternatives from which to choose.<br>a. What criteria should you consider in making the appropriate choice?<br>b. Design three alternative E-R diagrams to represent the university registrar’s<br>ofﬁce of Exercise 2.4. List the merits of each. Argue in favor of one of the<br>alternatives.<br>2.16 An E-R diagram can be viewed as a graph. What do the following mean in terms<br>of the structure of an enterprise schema?<br>a. The graph is disconnected.<br>b. The graph is acyclic.<br>2.17 In Section 2.4.3, we represented a ternary relationship (Figure 2.30a) using bi-<br>nary relationships, as shown in Figure 2.30b. Consider the alternative shown in<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>I. Data Models<br>2. Entity−Relationship <br>Model<br>84<br>© The McGraw−Hill <br>Companies, 2001<br>Exercises<br>75<br>R<br>RB<br>RA<br>RC<br>B<br>A<br>C<br>R1<br>R<br>R2<br>3<br>A<br>B<br>C<br>B<br>C<br>A<br>E<br>(c)<br>(b)<br>(a)<br>Figure 2.30<br>E-R diagram for Exercise 2.17 (attributes not shown).<br>Figure 2.30c. Discuss the relative merits of these two alternative representations<br>of a ternary relationship by binary relationships.<br>2.18 Consider the representation of a ternary relationship using binary relationships<br>as described in Section 2.4.3 (shown in Figure 2.30b.)<br>a. Show a simple instance of E, A, B, C, RA, RB, and RC that cannot corre-<br>spond to any instance of A, B, C, and R.<br>b. Modify the E-R diagram of Figure 2.30b to introduce constraints that will<br>guarantee that any instance of E, A, B, C, RA, RB, and RC that satisﬁes the<br>constraints will correspond to an instance of A, B, C, and R.<br>c. Modify the translation above to handle total participation constraints on the<br>ternary relationship.<br>d. The above representation requires that we create a primary key attribute for<br>E. Show how to treat E as a weak entity set so that a primary key attribute<br>is not required.<br>2.19 A weak entity set can always be made into a strong entity set by adding to its<br>attributes the primary key attributes of its identifying entity set. Outline what<br>sort of redundancy will result if we do so.<br>2.20 Design a generalization–specialization hierarchy for a motor-vehicle sales com-<br>pany. The company sells motorcycles, passenger cars, vans, and buses. Justify<br>your placement of attributes at each level of the hierarchy. Explain why they<br>should not be placed at a higher or lower level.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>I. Data Models<br>2. Entity−Relationship <br>Model<br>85<br>© The McGraw−Hill <br>Companies, 2001<br>76<br>Chapter 2<br>Entity-Relationship Model<br>2.21 Explain the distinction between condition-deﬁned and user-deﬁned constraints.<br>Which of these constraints can the system check automatically? Explain your<br>answer.<br>2.22 Explain the distinction between disjoint and overlapping constraints.<br>2.23 Explain the distinction between total and partial constraints.<br>2.24 Figure 2.31 shows a lattice structure of generalization and specialization. For<br>entity sets A, B, and C, explain how attributes are inherited from the higher-<br>level entity sets X and Y . Discuss how to handle a case where an attribute of X<br>has the same name as some attribute of Y .<br>2.25 Draw the UML equivalents of the E-R diagrams of Figures 2.9c, 2.10, 2.12, 2.13<br>and 2.17.<br>2.26 Consider two separate banks that decide to merge. Assume that both banks<br>use exactly the same E-R database schema—the one in Figure 2.22. (This as-<br>sumption is, of course, highly unrealistic; we consider the more realistic case in<br>Section 19.8.) If the merged bank is to have a single database, there are several<br>potential problems:<br>• The possibility that the two original banks have branches with the same<br>name<br>• The possibility that some customers are customers of both original banks<br>• The possibility that some loan or account numbers were used at both origi-<br>nal banks (for different loans or accounts, of course)<br>For each of these potential problems, describe why there is indeed a potential<br>for difﬁculties. Propose a solution to the problem. For your solution, explain any<br>changes that would have to be made and describe what their effect would be on<br>the schema and the data.<br>2.27 Reconsider the situation described for Exercise 2.26 under the assumption that<br>one bank is in the United States and the other is in Canada. As before, the<br>banks use the schema of Figure 2.22, except that the Canadian bank uses the<br>social-insurance number assigned by the Canadian government, whereas the U.S.<br>bank uses the social-security number to identify customers. What problems (be-<br>X<br>B<br>Y<br>A<br>C<br>ISA<br>ISA<br>Figure 2.31<br>E-R diagram for Exercise 2.24 (attributes not shown).<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>I. Data Models<br>2. Entity−Relationship <br>Model<br>86<br>© The McGraw−Hill <br>Companies, 2001<br>Bibliographical Notes<br>77<br>yond those identiﬁed in Exercise 2.24) might occur in this multinational case?<br>How would you resolve them? Be sure to consider both the scheme and the<br>actual data values in constructing your answer.<br>Bibliographical Notes<br>The E-R data model was introduced by Chen [1976]. A logical design methodology for<br>relational databases using the extended E-R model is presented by Teorey et al. [1986].<br>Mapping from extended E-R models to the relational model is discussed by Lyngbaek<br>and Vianu [1987] and Markowitz and Shoshani [1992]. Various data-manipulation<br>languages for the E-R model have been proposed: GERM (Benneworth et al. [1981]),<br>GORDAS (Elmasri and Wiederhold [1981]), and ERROL (Markowitz and Raz [1983]). A<br>graphical query language for the E-R database was proposed by Zhang and Mendel-<br>zon [1983] and Elmasri and Larson [1985].<br>Smith and Smith [1977] introduced the concepts of generalization, specialization,<br>and aggregation and Hammer and McLeod [1980] expanded them. Lenzerini and<br>Santucci [1983] used the concepts in deﬁning cardinality constraints in the E-R model.<br>Thalheim [2000] provides a detailed textbook coverage of research in E-R mod-<br>eling. Basic textbook discussions are offered by Batini et al. [1992] and Elmasri and<br>Navathe [2000]. Davis et al. [1983] provide a collection of papers on the E-R model.<br>Tools<br>Many database systems provide tools for database design that support E-R diagrams.<br>These tools help a designer create E-R diagrams, and they can automatically cre-<br>ate corresponding tables in a database. See bibliographic notes of Chapter 1 for<br>references to database system vendor’s Web sites. There are also some database-<br>independent data modeling tools that support E-R diagrams and UML class diagrams.<br>These include Rational Rose (www.rational.com/products/rose), Visio Enterprise (see<br>www.visio.com), and ERwin (search for ERwin at the site www.cai.com/products).<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>I. Data Models<br>3. Relational Model<br>87<br>© The McGraw−Hill <br>Companies, 2001<br>C<br>H<br>A<br>P<br>T<br>E<br>R<br>3<br>Relational Model<br>The relational model is today the primary data model for commercial data-processing<br>applications. It has attained its primary position because of its simplicity, which eases<br>the job of the programmer, as compared to earlier data models such as the network<br>model or the hierarchical model.<br>In this chapter, we ﬁrst study the fundamentals of the relational model, which pro-<br>vides a very simple yet powerful way of representing data. We then describe three<br>formal query languages; query languages are used to specify requests for informa-<br>tion. The three we cover in this chapter are not user-friendly, but instead serve as<br>the formal basis for user-friendly query languages that we study later. We cover the<br>ﬁrst query language, relational algebra, in great detail. The relational algebra forms<br>the basis of the widely used SQL query language. We then provide overviews of the<br>other two formal languages, the tuple relational calculus and the domain relational<br>calculus, which are declarative query languages based on mathematical logic. The<br>domain relational calculus is the basis of the QBE query language.<br>A substantial theory exists for relational databases. We study the part of this theory<br>dealing with queries in this chapter. In Chapter 7 we shall examine aspects of rela-<br>tional database theory that help in the design of relational database schemas, while in<br>Chapters 13 and 14 we discuss aspects of the theory dealing with efﬁcient processing<br>of queries.<br>3.1<br>Structure of Relational Databases<br>A relational database consists of a co</span><br><br><span style="background-color: #CAFFBF;" title="Chunk 11 | Start: 220022 | End: 240022 | Tokens: 3261">llection of tables, each of which is assigned a<br>unique name. Each table has a structure similar to that presented in Chapter 2, where<br>we represented E-R databases by tables. A row in a table represents a relationship<br>among a set of values. Since a table is a collection of such relationships, there is a<br>close correspondence between the concept of table and the mathematical concept of<br>79<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>I. Data Models<br>3. Relational Model<br>88<br>© The McGraw−Hill <br>Companies, 2001<br>80<br>Chapter 3<br>Relational Model<br>relation, from which the relational data model takes its name. In what follows, we<br>introduce the concept of relation.<br>In this chapter, we shall be using a number of different relations to illustrate the<br>various concepts underlying the relational data model. These relations represent part<br>of a banking enterprise. They differ slightly from the tables that were used in Chap-<br>ter 2, so that we can simplify our presentation. We shall discuss criteria for the ap-<br>propriateness of relational structures in great detail in Chapter 7.<br>3.1.1<br>Basic Structure<br>Consider the account table of Figure 3.1. It has three column headers: account-number,<br>branch-name, and balance. Following the terminology of the relational model, we refer<br>to these headers as attributes (as we did for the E-R model in Chapter 2). For each<br>attribute, there is a set of permitted values, called the domain of that attribute. For<br>the attribute branch-name, for example, the domain is the set of all branch names. Let<br>D1 denote the set of all account numbers, D2 the set of all branch names, and D3<br>the set of all balances. As we saw in Chapter 2, any row of account must consist of<br>a 3-tuple (v1, v2, v3), where v1 is an account number (that is, v1 is in domain D1),<br>v2 is a branch name (that is, v2 is in domain D2), and v3 is a balance (that is, v3 is in<br>domain D3). In general, account will contain only a subset of the set of all possible<br>rows. Therefore, account is a subset of<br>D1 × D2 × D3<br>In general, a table of n attributes must be a subset of<br>D1 × D2 × · · · × Dn−1 × Dn<br>Mathematicians deﬁne a relation to be a subset of a Cartesian product of a list of<br>domains. This deﬁnition corresponds almost exactly with our deﬁnition of table. The<br>only difference is that we have assigned names to attributes, whereas mathematicians<br>rely on numeric “names,” using the integer 1 to denote the attribute whose domain<br>appears ﬁrst in the list of domains, 2 for the attribute whose domain appears second,<br>and so on. Because tables are essentially relations, we shall use the mathematical<br>account-number<br>branch-name<br>balance<br>A-101<br>Downtown<br>500<br>A-102<br>Perryridge<br>400<br>A-201<br>Brighton<br>900<br>A-215<br>Mianus<br>700<br>A-217<br>Brighton<br>750<br>A-222<br>Redwood<br>700<br>A-305<br>Round Hill<br>350<br>Figure 3.1<br>The account relation.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>I. Data Models<br>3. Relational Model<br>89<br>© The McGraw−Hill <br>Companies, 2001<br>3.1<br>Structure of Relational Databases<br>81<br>account-number<br>branch-name<br>balance<br>A-101<br>Downtown<br>500<br>A-215<br>Mianus<br>700<br>A-102<br>Perryridge<br>400<br>A-305<br>Round Hill<br>350<br>A-201<br>Brighton<br>900<br>A-222<br>Redwood<br>700<br>A-217<br>Brighton<br>750<br>Figure 3.2<br>The account relation with unordered tuples.<br>terms relation and tuple in place of the terms table and row. A tuple variable is a<br>variable that stands for a tuple; in other words, a tuple variable is a variable whose<br>domain is the set of all tuples.<br>In the account relation of Figure 3.1, there are seven tuples. Let the tuple variable t<br>refer to the ﬁrst tuple of the relation. We use the notation t[account-number] to denote<br>the value of t on the account-number attribute. Thus, t[account-number] = “A-101,” and<br>t[branch-name] = “Downtown”. Alternatively, we may write t[1] to denote the value<br>of tuple t on the ﬁrst attribute (account-number), t[2] to denote branch-name, and so on.<br>Since a relation is a set of tuples, we use the mathematical notation of t ∈r to denote<br>that tuple t is in relation r.<br>The order in which tuples appear in a relation is irrelevant, since a relation is a<br>set of tuples. Thus, whether the tuples of a relation are listed in sorted order, as in<br>Figure 3.1, or are unsorted, as in Figure 3.2, does not matter; the relations in the two<br>ﬁgures above are the same, since both contain the same set of tuples.<br>We require that, for all relations r, the domains of all attributes of r be atomic. A<br>domain is atomic if elements of the domain are considered to be indivisible units.<br>For example, the set of integers is an atomic domain, but the set of all sets of integers<br>is a nonatomic domain. The distinction is that we do not normally consider inte-<br>gers to have subparts, but we consider sets of integers to have subparts—namely,<br>the integers composing the set. The important issue is not what the domain itself is,<br>but rather how we use domain elements in our database. The domain of all integers<br>would be nonatomic if we considered each integer to be an ordered list of digits. In<br>all our examples, we shall assume atomic domains. In Chapter 9, we shall discuss<br>extensions to the relational data model to permit nonatomic domains.<br>It is possible for several attributes to have the same domain. For example, sup-<br>pose that we have a relation customer that has the three attributes customer-name,<br>customer-street, and customer-city, and a relation employee that includes the attribute<br>employee-name. It is possible that the attributes customer-name and employee-name will<br>have the same domain: the set of all person names, which at the physical level is<br>the set of all character strings. The domains of balance and branch-name, on the other<br>hand, certainly ought to be distinct. It is perhaps less clear whether customer-name<br>and branch-name should have the same domain. At the physical level, both customer<br>names and branch names are character strings. However, at the logical level, we may<br>want customer-name and branch-name to have distinct domains.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>I. Data Models<br>3. Relational Model<br>90<br>© The McGraw−Hill <br>Companies, 2001<br>82<br>Chapter 3<br>Relational Model<br>One domain value that is a member of any possible domain is the null value,<br>which signiﬁes that the value is unknown or does not exist. For example, suppose<br>that we include the attribute telephone-number in the customer relation. It may be that<br>a customer does not have a telephone number, or that the telephone number is un-<br>listed. We would then have to resort to null values to signify that the value is un-<br>known or does not exist. We shall see later that null values cause a number of difﬁ-<br>culties when we access or update the database, and thus should be eliminated if at<br>all possible. We shall assume null values are absent initially, and in Section 3.3.4, we<br>describe the effect of nulls on different operations.<br>3.1.2<br>Database Schema<br>When we talk about a database, we must differentiate between the database schema,<br>which is the logical design of the database, and a database instance, which is a snap-<br>shot of the data in the database at a given instant in time.<br>The concept of a relation corresponds to the programming-language notion of a<br>variable. The concept of a relation schema corresponds to the programming-language<br>notion of type deﬁnition.<br>It is convenient to give a name to a relation schema, just as we give names to type<br>deﬁnitions in programming languages. We adopt the convention of using lower-<br>case names for relations, and names beginning with an uppercase letter for rela-<br>tion schemas. Following this notation, we use Account-schema to denote the relation<br>schema for relation account. Thus,<br>Account-schema = (account-number, branch-name, balance)<br>We denote the fact that account is a relation on Account-schema by<br>account(Account-schema)<br>In general, a relation schema consists of a list of attributes and their corresponding<br>domains. We shall not be concerned about the precise deﬁnition of the domain of<br>each attribute until we discuss the SQL language in Chapter 4.<br>The concept of a relation instance corresponds to the programming language no-<br>tion of a value of a variable. The value of a given variable may change with time;<br>similarly the contents of a relation instance may change with time as the relation is<br>updated. However, we often simply say “relation” when we actually mean “relation<br>instance.”<br>As an example of a relation instance, consider the branch relation of Figure 3.3. The<br>schema for that relation is<br>Branch-schema = (branch-name, branch-city, assets)<br>Note that the attribute branch-name appears in both Branch-schema and Account-<br>schema. This duplication is not a coincidence. Rather, using common attributes in<br>relation schemas is one way of relating tuples of distinct relations. For example, sup-<br>pose we wish to ﬁnd the information about all of the accounts maintained in branches<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>I. Data Models<br>3. Relational Model<br>91<br>© The McGraw−Hill <br>Companies, 2001<br>3.1<br>Structure of Relational Databases<br>83<br>branch-name<br>branch-city<br>assets<br>Brighton<br>Brooklyn<br>7100000<br>Downtown<br>Brooklyn<br>9000000<br>Mianus<br>Horseneck<br>400000<br>North Town<br>Rye<br>3700000<br>Perryridge<br>Horseneck<br>1700000<br>Pownal<br>Bennington<br>300000<br>Redwood<br>Palo Alto<br>2100000<br>Round Hill<br>Horseneck<br>8000000<br>Figure 3.3<br>The branch relation.<br>located in Brooklyn. We look ﬁrst at the branch relation to ﬁnd the names of all the<br>branches located in Brooklyn. Then, for each such branch, we would look in the ac-<br>count relation to ﬁnd the information about the accounts maintained at that branch.<br>This is not surprising—recall that the primary key attributes of a strong entity set<br>appear in the table created to represent the entity set, as well as in the tables created<br>to represent relationships that the entity set participates in.<br>Let us continue our banking example. We need a relation to describe information<br>about customers. The relation schema is<br>Customer-schema = (customer-name, customer-street, customer-city)<br>Figure 3.4 shows a sample relation customer (Customer-schema). Note that we have<br>omitted the customer-id attribute, which we used Chapter 2, because now we want to<br>have smaller relation schemas in our running example of a bank database. We assume<br>that the customer name uniquely identiﬁes a customer—obviously this may not be<br>true in the real world, but the assumption makes our examples much easier to read.<br>customer-name<br>customer-street<br>customer-city<br>Adams<br>Spring<br>Pittsfield<br>Brooks<br>Senator<br>Brooklyn<br>Curry<br>North<br>Rye<br>Glenn<br>Sand Hill<br>Woodside<br>Green<br>Walnut<br>Stamford<br>Hayes<br>Main<br>Harrison<br>Johnson<br>Alma<br>Palo Alto<br>Jones<br>Main<br>Harrison<br>Lindsay<br>Park<br>Pittsfield<br>Smith<br>North<br>Rye<br>Turner<br>Putnam<br>Stamford<br>Williams<br>Nassau<br>Princeton<br>Figure 3.4<br>The customer relation.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>I. Data Models<br>3. Relational Model<br>92<br>© The McGraw−Hill <br>Companies, 2001<br>84<br>Chapter 3<br>Relational Model<br>In a real-world database, the customer-id (which could be a social-security number, or<br>an identiﬁer generated by the bank) would serve to uniquely identify customers.<br>We also need a relation to describe the association between customers and ac-<br>counts. The relation schema to describe this association is<br>Depositor-schema = (customer-name, account-number)<br>Figure 3.5 shows a sample relation depositor (Depositor-schema).<br>It would appear that, for our banking example, we could have just one relation<br>schema, rather than several. That is, it may be easier for a user to think in terms of<br>one relation schema, rather than in terms of several. Suppose that we used only one<br>relation for our example, with schema<br>(branch-name, branch-city, assets, customer-name, customer-street<br>customer-city, account-number, balance)<br>Observe that, if a customer has several accounts, we must list her address once for<br>each account. That is, we must repeat certain information several times. This repeti-<br>tion is wasteful and is avoided by the use of several relations, as in our example.<br>In addition, if a branch has no accounts (a newly created branch, say, that has no<br>customers yet), we cannot construct a complete tuple on the preceding single rela-<br>tion, because no data concerning customer and account are available yet. To represent<br>incomplete tuples, we must use null values that signify that the value is unknown or<br>does not exist. Thus, in our example, the values for customer-name, customer-street, and<br>so on must be null. By using several relations, we can represent the branch informa-<br>tion for a bank with no customers without using null values. We simply use a tuple<br>on Branch-schema to represent the information about the branch, and create tuples on<br>the other schemas only when the appropriate information becomes available.<br>In Chapter 7, we shall study criteria to help us decide when one set of relation<br>schemas is more appropriate than another, in terms of information repetition and<br>the existence of null values. For now, we shall assume that the relation schemas are<br>given.<br>We include two additional relations to describe data about loans maintained in the<br>various branches in the bank:<br>customer-name<br>account-number<br>Hayes<br>A-102<br>Johnson<br>A-101<br>Johnson<br>A-201<br>Jones<br>A-217<br>Lindsay<br>A-222<br>Smith<br>A-215<br>Turner<br>A-305<br>Figure 3.5<br>The depositor relation.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>I. Data Models<br>3. Relational Model<br>93<br>© The McGraw−Hill <br>Companies, 2001<br>3.1<br>Structure of Relational Databases<br>85<br>loan-number<br>branch-name<br>amount<br>L-11<br>Round Hill<br>900<br>L-14<br>Downtown<br>1500<br>L-15<br>Perryridge<br>1500<br>L-16<br>Perryridge<br>1300<br>L-17<br>Downtown<br>1000<br>L-23<br>Redwood<br>2000<br>L-93<br>Mianus<br>500<br>Figure 3.6<br>The loan relation.<br>Loan-schema = (loan-number, branch-name, amount)<br>Borrower-schema = (customer-name, loan-number)<br>Figures 3.6 and 3.7, respectively, show the sample relations loan (Loan-schema) and<br>borrower (Borrower-schema).<br>The E-R diagram in Figure 3.8 depicts the banking enterprise that we have just<br>described. The relation schemas correspond to the set of tables that we might gener-<br>ate by the method outlined in Section 2.9. Note that the tables for account-branch and<br>loan-branch have been combined into the tables for account and loan respectively. Such<br>combining is possible since the relationships are many to one from account and loan,<br>respectively, to branch, and, further, the participation of account and loan in the corre-<br>sponding relationships is total, as the double lines in the ﬁgure indicate. Finally, we<br>note that the customer relation may contain information about customers who have<br>neither an account nor a loan at the bank.<br>The banking enterprise described here will serve as our primary example in this<br>chapter and in subsequent ones. On occasion, we shall need to introduce additional<br>relation schemas to illustrate particular points.<br>3.1.3<br>Keys<br>The notions of superkey, candidate key, and primary key, as discussed in Chapter 2,<br>are also applicable to the relational model. For example, in Branch-schema, {branch-<br>customer-name<br>loan-number<br>Adams<br>L-16<br>Curry<br>L-93<br>Hayes<br>L-15<br>Jackson<br>L-14<br>Jones<br>L-17<br>Smith<br>L-11<br>Smith<br>L-23<br>Williams<br>L-17<br>Figure 3.7<br>The borrower relation.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>I. Data Models<br>3. Relational Model<br>94<br>© The McGraw−Hill <br>Companies, 2001<br>86<br>Chapter 3<br>Relational Model<br>account-number<br>balance<br>account<br>branch-name<br>assets<br>branch<br>account-branch<br>customer-name<br>customer-street<br>customer-city<br>customer<br>loan-number<br>amount<br>depositor<br>branch-city<br>loan-branch<br>loan<br>borrower<br>Figure 3.8<br>E-R diagram for the banking enterprise.<br>name} and {branch-name, branch-city} are both superkeys. {branch-name, branch-city}<br>is not a candidate key, because {branch-name} is a subset of {branch-name, branch-<br>city} and {branch-name} itself is a superkey. However, {branch-name} is a candidate<br>key, and for our purpose also will serve as a primary key. The attribute branch-city is<br>not a superkey, since two branches in the same city may have different names (and<br>different asset ﬁgures).<br>Let R be a relation schema. If we say that a subset K of R is a superkey for R, we<br>are restricting consideration to relations r(R) in which no two distinct tuples have<br>the same values on all attributes in K. That is, if t1 and t2 are in r and t1 ̸= t2, then<br>t1[K] ̸= t2[K].<br>If a relational database schema is based on tables derived from an E-R schema, it<br>is possible to determine the primary key for a relation schema from the primary keys<br>of the entity or relationship sets from which the schema is derived:<br>• Strong entity set. The primary key of the entity set becomes the primary key<br>of the relation.<br>• Weak entity set. The table, and thus the relation, corresponding to a weak<br>entity set includes<br>  The attributes of the weak entity set<br>  The primary key of the strong entity set on which the weak entity set<br>depends<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>I. Data Models<br>3. Relational Model<br>95<br>© The McGraw−Hill <br>Companies, 2001<br>3.1<br>Structure of Relational Databases<br>87<br>The primary key of the relation consists of the union of the primary key of the<br>strong entity set and the discriminator of the weak entity set.<br>• Relationship set. The union of the primary keys of the related entity sets be-<br>comes a superkey of the relation. If the relationship is many-to-many, this su-<br>perkey is also the primary key. Section 2.4.2 describes how to determine the<br>primary keys in other cases. Recall from Section 2.9.3 that no table is gener-<br>ated for relationship sets linking a weak entity set to the corresponding strong<br>entity set.<br>• Combined tables. Recall from Section 2.9.3 that a binary many-to-one rela-<br>tionship set from A to B can be represented by a table consisting of the at-<br>tributes of A and attributes (if any exist) of the relationship set. The primary<br>key of the “many” entity set becomes the primary key of the relation (that is,<br>if the relationship set is many to one from A to B, the primary key of A is<br>the primary key of the relation). For one-to-one relationship sets, the relation<br>is constructed like that for a many-to-one relationship set. However, we can<br>choose either entity set’s primary key as the primary key of the relation, since<br>both are candidate keys.<br>• Multivalued attributes. Recall from Section 2.9.5 that a multivalued attribute<br>M is represented by a table consisting of the primary key of the entity set or<br>relationship set of which M is an attribute plus a column C holding an indi-<br>vidual value of M. The primary key of the entity or relationship set, together<br>with the attribute C, becomes the primary key for the relation.<br>From the preceding list, we see that a relation schema, say r1, derived from an E-R<br>schema may include among its attributes the primary key of another relation schema,<br>say r2. This attribute is called a foreign key from r1, referencing r2. The relation r1<br>is also called the referencing relation of the foreign key dependency, and r2 is called<br>the referenced relation of the foreign key. For example, the attribute branch-name in<br>Account-schema is a foreign key from Account-schema referencing Branch-schema, since<br>branch-name is the primary key of Branch-schema. In any database instance, given any<br>tuple, say ta, from the account relation, there must be some tuple, say tb, in the branch<br>relation such that the value of the branch-name attribute of ta is the same as the value<br>of the primary key, branch-name, of tb.<br>It is customary to list the primary key attributes of a relation schema before the<br>other attributes; for example, the branch-name attribute of Branch-schema is listed ﬁrst,<br>since it is the primary key.<br>3.1.4<br>Schema Diagram<br>A database schema, along with primary key and foreign key dependencies, can be<br>depicted pictorially by schema diagrams. Figure 3.9 shows the schema diagram for<br>our banking enterprise. Each relation appears as a box, with the attributes listed in-<br>side it and the relation name above it. If there are primary key attributes, a horizontal<br>line crosses </span><br><br><span style="background-color: #9BF6FF;" title="Chunk 12 | Start: 240024 | End: 260024 | Tokens: 3221">the box, with the primary key attributes listed above the line. Foreign<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>I. Data Models<br>3. Relational Model<br>96<br>© The McGraw−Hill <br>Companies, 2001<br>88<br>Chapter 3<br>Relational Model<br>customer–name<br>account–number<br>depositor<br>loan–number<br>branch–name<br>amount<br>loan<br>customer–name<br>customer–street<br>customer–city<br>customer<br>branch–name<br>branch–city<br>assets<br>branch<br>account–number<br>branch–name<br>balance<br>account<br>borrower<br>loan–number<br>customer–name<br>Figure 3.9<br>Schema diagram for the banking enterprise.<br>key dependencies appear as arrows from the foreign key attributes of the referencing<br>relation to the primary key of the referenced relation.<br>Do not confuse a schema diagram with an E-R diagram. In particular, E-R diagrams<br>do not show foreign key attributes explicitly, whereas schema diagrams show them<br>explicity.<br>Many database systems provide design tools with a graphical user interface for<br>creating schema diagrams.<br>3.1.5<br>Query Languages<br>A query language is a language in which a user requests information from the data-<br>base. These languages are usually on a level higher than that of a standard program-<br>ming language. Query languages can be categorized as either procedural or non-<br>procedural. In a procedural language, the user instructs the system to perform a<br>sequence of operations on the database to compute the desired result. In a nonproce-<br>dural language, the user describes the desired information without giving a speciﬁc<br>procedure for obtaining that information.<br>Most commercial relational-database systems offer a query language that includes<br>elements of both the procedural and the nonprocedural approaches. We shall study<br>the very widely used query language SQL in Chapter 4. Chapter 5 covers the query<br>languages QBE and Datalog, the latter a query language that resembles the Prolog<br>programming language.<br>In this chapter, we examine “pure” languages: The relational algebra is procedu-<br>ral, whereas the tuple relational calculus and domain relational calculus are nonpro-<br>cedural. These query languages are terse and formal, lacking the “syntactic sugar” of<br>commercial languages, but they illustrate the fundamental techniques for extracting<br>data from the database.<br>Although we shall be concerned with only queries initially, a complete data-<br>manipulation language includes not only a query language, but also a language for<br>database modiﬁcation. Such languages include commands to insert and delete tuples,<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>I. Data Models<br>3. Relational Model<br>97<br>© The McGraw−Hill <br>Companies, 2001<br>3.2<br>The Relational Algebra<br>89<br>as well as commands to modify parts of existing tuples. We shall examine database<br>modiﬁcation after we complete our discussion of queries.<br>3.2<br>The Relational Algebra<br>The relational algebra is a procedural query language. It consists of a set of operations<br>that take one or two relations as input and produce a new relation as their result. The<br>fundamental operations in the relational algebra are select, project, union, set difference,<br>Cartesian product, and rename. In addition to the fundamental operations, there are<br>several other operations—namely, set intersection, natural join, division, and assign-<br>ment. We will deﬁne these operations in terms of the fundamental operations.<br>3.2.1<br>Fundamental Operations<br>The select, project, and rename operations are called unary operations, because they<br>operate on one relation. The other three operations operate on pairs of relations and<br>are, therefore, called binary operations.<br>3.2.1.1<br>The Select Operation<br>The select operation selects tuples that satisfy a given predicate. We use the lowercase<br>Greek letter sigma (σ) to denote selection. The predicate appears as a subscript to σ.<br>The argument relation is in parentheses after the σ. Thus, to select those tuples of the<br>loan relation where the branch is “Perryridge,” we write<br>σbranch-name = “Perryridge” (loan)<br>If the loan relation is as shown in Figure 3.6, then the relation that results from the<br>preceding query is as shown in Figure 3.10.<br>We can ﬁnd all tuples in which the amount lent is more than $1200 by writing<br>σamount&gt;1200 (loan)<br>In general, we allow comparisons using =, ̸=, &lt;, ≤, &gt;, ≥in the selection predicate.<br>Furthermore, we can combine several predicates into a larger predicate by using the<br>connectives and (∧), or (∨), and not (¬). Thus, to ﬁnd those tuples pertaining to loans<br>of more than $1200 made by the Perryridge branch, we write<br>σbranch-name = “Perryridge” ∧amount&gt;1200 (loan)<br>loan-number<br>branch-name<br>amount<br>L-15<br>Perryridge<br>1500<br>L-16<br>Perryridge<br>1300<br>Figure 3.10<br>Result of σbranch-name = “Perryridge” (loan).<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>I. Data Models<br>3. Relational Model<br>98<br>© The McGraw−Hill <br>Companies, 2001<br>90<br>Chapter 3<br>Relational Model<br>The selection predicate may include comparisons between two attributes. To illus-<br>trate, consider the relation loan-ofﬁcer that consists of three attributes: customer-name,<br>banker-name, and loan-number, which speciﬁes that a particular banker is the loan of-<br>ﬁcer for a loan that belongs to some customer. To ﬁnd all customers who have the<br>same name as their loan ofﬁcer, we can write<br>σcustomer-name = banker-name(loan-oﬃcer)<br>3.2.1.2<br>The Project Operation<br>Suppose we want to list all loan numbers and the amount of the loans, but do not<br>care about the branch name. The project operation allows us to produce this relation.<br>The project operation is a unary operation that returns its argument relation, with<br>certain attributes left out. Since a relation is a set, any duplicate rows are eliminated.<br>Projection is denoted by the uppercase Greek letter pi (Π). We list those attributes that<br>we wish to appear in the result as a subscript to Π. The argument relation follows in<br>parentheses. Thus, we write the query to list all loan numbers and the amount of the<br>loan as<br>Πloan-number, amount(loan)<br>Figure 3.11 shows the relation that results from this query.<br>3.2.1.3<br>Composition of Relational Operations<br>The fact that the result of a relational operation is itself a relation is important. Con-<br>sider the more complicated query “Find those customers who live in Harrison.” We<br>write:<br>Πcustomer-name (σcustomer-city = “Harrison” (customer))<br>Notice that, instead of giving the name of a relation as the argument of the projection<br>operation, we give an expression that evaluates to a relation.<br>In general, since the result of a relational-algebra operation is of the same type<br>(relation) as its inputs, relational-algebra operations can be composed together into<br>loan-number<br>amount<br>L-11<br>900<br>L-14<br>1500<br>L-15<br>1500<br>L-16<br>1300<br>L-17<br>1000<br>L-23<br>2000<br>L-93<br>500<br>Figure 3.11<br>Loan number and the amount of the loan.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>I. Data Models<br>3. Relational Model<br>99<br>© The McGraw−Hill <br>Companies, 2001<br>3.2<br>The Relational Algebra<br>91<br>a relational-algebra expression. Composing relational-algebra operations into rela-<br>tional-algebra expressions is just like composing arithmetic operations (such as +, −,<br>∗, and ÷) into arithmetic expressions. We study the formal deﬁnition of relational-<br>algebra expressions in Section 3.2.2.<br>3.2.1.4<br>The Union Operation<br>Consider a query to ﬁnd the names of all bank customers who have either an account<br>or a loan or both. Note that the customer relation does not contain the information,<br>since a customer does not need to have either an account or a loan at the bank. To<br>answer this query, we need the information in the depositor relation (Figure 3.5) and<br>in the borrower relation (Figure 3.7). We know how to ﬁnd the names of all customers<br>with a loan in the bank:<br>Πcustomer-name (borrower)<br>We also know how to ﬁnd the names of all customers with an account in the bank:<br>Πcustomer-name (depositor)<br>To answer the query, we need the union of these two sets; that is, we need all cus-<br>tomer names that appear in either or both of the two relations. We ﬁnd these data by<br>the binary operation union, denoted, as in set theory, by ∪. So the expression needed<br>is<br>Πcustomer-name (borrower) ∪Πcustomer-name (depositor)<br>The result relation for this query appears in Figure 3.12. Notice that there are 10 tuples<br>in the result, even though there are seven distinct borrowers and six depositors. This<br>apparent discrepancy occurs because Smith, Jones, and Hayes are borrowers as well<br>as depositors. Since relations are sets, duplicate values are eliminated.<br>customer-name<br>Adams<br>Curry<br>Hayes<br>Jackson<br>Jones<br>Smith<br>Williams<br>Lindsay<br>Johnson<br>Turner<br>Figure 3.12<br>Names of all customers who have either a loan or an account.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>I. Data Models<br>3. Relational Model<br>100<br>© The McGraw−Hill <br>Companies, 2001<br>92<br>Chapter 3<br>Relational Model<br>Observe that, in our example, we took the union of two sets, both of which con-<br>sisted of customer-name values. In general, we must ensure that unions are taken be-<br>tween compatible relations. For example, it would not make sense to take the union of<br>the loan relation and the borrower relation. The former is a relation of three attributes;<br>the latter is a relation of two. Furthermore, consider a union of a set of customer<br>names and a set of cities. Such a union would not make sense in most situations.<br>Therefore, for a union operation r ∪s to be valid, we require that two conditions<br>hold:<br>1. The relations r and s must be of the same arity. That is, they must have the<br>same number of attributes.<br>2. The domains of the ith attribute of r and the ith attribute of s must be the same,<br>for all i.<br>Note that r and s can be, in general, temporary relations that are the result of relational-<br>algebra expressions.<br>3.2.1.5<br>The Set Difference Operation<br>The set-difference operation, denoted by −, allows us to ﬁnd tuples that are in one<br>relation but are not in another. The expression r −s produces a relation containing<br>those tuples in r but not in s.<br>We can ﬁnd all customers of the bank who have an account but not a loan by<br>writing<br>Πcustomer-name (depositor) −Πcustomer-name (borrower)<br>The result relation for this query appears in Figure 3.13.<br>As with the union operation, we must ensure that set differences are taken be-<br>tween compatible relations. Therefore, for a set difference operation r −s to be valid,<br>we require that the relations r and s be of the same arity, and that the domains of the<br>ith attribute of r and the ith attribute of s be the same.<br>3.2.1.6<br>The Cartesian-Product Operation<br>The Cartesian-product operation, denoted by a cross (×), allows us to combine in-<br>formation from any two relations. We write the Cartesian product of relations r1 and<br>r2 as r1 × r2.<br>customer-name<br>Johnson<br>Lindsay<br>Turner<br>Figure 3.13<br>Customers with an account but no loan.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>I. Data Models<br>3. Relational Model<br>101<br>© The McGraw−Hill <br>Companies, 2001<br>3.2<br>The Relational Algebra<br>93<br>Recall that a relation is by deﬁnition a subset of a Cartesian product of a set of<br>domains. From that deﬁnition, we should already have an intuition about the deﬁ-<br>nition of the Cartesian-product operation. However, since the same attribute name<br>may appear in both r1 and r2, we need to devise a naming schema to distinguish<br>between these attributes. We do so here by attaching to an attribute the name of the<br>relation from which the attribute originally came. For example, the relation schema<br>for r = borrower × loan is<br>(borrower.customer-name, borrower.loan-number, loan.loan-number,<br>loan.branch-name, loan.amount)<br>With this schema, we can distinguish borrower.loan-number from loan.loan-number. For<br>those attributes that appear in only one of the two schemas, we shall usually drop<br>the relation-name preﬁx. This simpliﬁcation does not lead to any ambiguity. We can<br>then write the relation schema for r as<br>(customer-name, borrower.loan-number, loan.loan-number,<br>branch-name, amount)<br>This naming convention requires that the relations that are the arguments of the<br>Cartesian-product operation have distinct names. This requirement causes problems<br>in some cases, such as when the Cartesian product of a relation with itself is desired.<br>A similar problem arises if we use the result of a relational-algebra expression in a<br>Cartesian product, since we shall need a name for the relation so that we can refer<br>to the relation’s attributes. In Section 3.2.1.7, we see how to avoid these problems by<br>using a rename operation.<br>Now that we know the relation schema for r = borrower × loan, what tuples ap-<br>pear in r? As you may suspect, we construct a tuple of r out of each possible pair of<br>tuples: one from the borrower relation and one from the loan relation. Thus, r is a large<br>relation, as you can see from Figure 3.14, which includes only a portion of the tuples<br>that make up r.<br>Assume that we have n1 tuples in borrower and n2 tuples in loan. Then, there are<br>n1 ∗n2 ways of choosing a pair of tuples—one tuple from each relation; so there<br>are n1 ∗n2 tuples in r. In particular, note that for some tuples t in r, it may be that<br>t[borrower.loan-number] ̸= t[loan.loan-number].<br>In general, if we have relations r1(R1) and r2(R2), then r1 × r2 is a relation whose<br>schema is the concatenation of R1 and R2. Relation R contains all tuples t for which<br>there is a tuple t1 in r1 and a tuple t2 in r2 for which t[R1] = t1[R1] and t[R2] =<br>t2[R2].<br>Suppose that we want to ﬁnd the names of all customers who have a loan at the<br>Perryridge branch. We need the information in both the loan relation and the borrower<br>relation to do so. If we write<br>σbranch-name = “Perryridge”(borrower × loan)<br>then the result is the relation in Figure 3.15. We have a relation that pertains to only<br>the Perryridge branch. However, the customer-name column may contain customers<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>I. Data Models<br>3. Relational Model<br>102<br>© The McGraw−Hill <br>Companies, 2001<br>94<br>Chapter 3<br>Relational Model<br>customer-name<br>borrower.<br>loan.<br>.<br>branch-name<br>amount<br>loan-number<br>loan-number<br>Adams<br>L-16<br>L-11<br>Round Hill<br>Round Hill<br>900<br>Adams<br>L-16<br>L-14<br>Downtown<br>Downtown<br>Downtown<br>1500<br>Adams<br>L-16<br>L-15<br>Perryridge<br>Perryridge<br>1500<br>Adams<br>L-16<br>L-16<br>1300<br>Adams<br>L-16<br>L-17<br>1000<br>Adams<br>L-16<br>L-23<br>Redwood<br>2000<br>Adams<br>L-16<br>L-93<br>Mianus<br>Round Hill<br>Downtown<br>Downtown<br>Perryridge<br>Perryridge<br>Redwood<br>Mianus<br>Round Hill<br>Downtown<br>Downtown<br>Perryridge<br>Perryridge<br>Redwood<br>Mianus<br>Downtown<br>Downtown<br>Perryridge<br>Perryridge<br>Redwood<br>Mianus<br>500<br>Curry<br>L-93<br>L-11<br>900<br>Curry<br>L-93<br>L-14<br>1500<br>Curry<br>L-93<br>L-15<br>1500<br>Curry<br>L-93<br>L-16<br>1300<br>Curry<br>L-93<br>L-17<br>1000<br>Curry<br>L-93<br>L-23<br>2000<br>Curry<br>L-93<br>L-93<br>500<br>Hayes<br>L-15<br>L-11<br>900<br>Hayes<br>L-15<br>L-14<br>1500<br>Hayes<br>L-15<br>L-15<br>1500<br>Hayes<br>L-15<br>L-16<br>1300<br>Hayes<br>L-15<br>L-17<br>1000<br>Hayes<br>L-15<br>L-23<br>2000<br>Hayes<br>L-15<br>L-93<br>500<br>. . .<br>. . .<br>. . .<br>. . .<br>. . .<br>. . .<br>. . .<br>. . .<br>. . .<br>. . .<br>. . .<br>. . .<br>. . .<br>. . .<br>. . .<br>Smith<br>L-23<br>L-11<br>900<br>Smith<br>L-23<br>L-14<br>1500<br>Smith<br>L-23<br>L-15<br>1500<br>Smith<br>L-23<br>L-16<br>1300<br>Smith<br>L-23<br>L-17<br>1000<br>Smith<br>L-23<br>L-23<br>2000<br>Smith<br>L-23<br>L-93<br>500<br>Williams<br>L-17<br>L-11<br>900<br>Williams<br>L-17<br>L-14<br>1500<br>Williams<br>L-17<br>L-15<br>1500<br>Williams<br>L-17<br>L-16<br>1300<br>Williams<br>L-17<br>L-17<br>1000<br>Williams<br>L-17<br>L-23<br>2000<br>Williams<br>L-17<br>L-93<br>500<br>Figure 3.14<br>Result of borrower × loan.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>I. Data Models<br>3. Relational Model<br>103<br>© The McGraw−Hill <br>Companies, 2001<br>3.2<br>The Relational Algebra<br>95<br>customer-name<br>borrower.<br>loan.<br>branch-name<br>amount<br>loan-number<br>loan-number<br>Adams<br>L-16<br>L-15<br>Perryridge<br>1500<br>Adams<br>L-16<br>L-16<br>Perryridge<br>1300<br>Curry<br>L-93<br>L-15<br>Perryridge<br>1500<br>Curry<br>L-93<br>L-16<br>Perryridge<br>1300<br>Hayes<br>L-15<br>L-15<br>Perryridge<br>1500<br>Hayes<br>L-15<br>L-16<br>Perryridge<br>1300<br>Jackson<br>L-14<br>L-15<br>Perryridge<br>1500<br>Jackson<br>L-14<br>L-16<br>Perryridge<br>1300<br>Jones<br>L-17<br>L-15<br>Perryridge<br>1500<br>Jones<br>L-17<br>L-16<br>Perryridge<br>1300<br>Smith<br>L-11<br>L-15<br>Perryridge<br>1500<br>Smith<br>L-11<br>L-16<br>Perryridge<br>1300<br>Smith<br>L-23<br>L-15<br>Perryridge<br>1500<br>Smith<br>L-23<br>L-16<br>Perryridge<br>1300<br>Williams<br>L-17<br>L-15<br>Perryridge<br>1500<br>Williams<br>L-17<br>L-16<br>Perryridge<br>1300<br>Figure 3.15<br>Result of σbranch-name = “Perryridge” (borrower × loan).<br>who do not have a loan at the Perryridge branch. (If you do not see why that is true,<br>recall that the Cartesian product takes all possible pairings of one tuple from borrower<br>with one tuple of loan.)<br>Since the Cartesian-product operation associates every tuple of loan with every tu-<br>ple of borrower, we know that, if a customer has a loan in the Perryridge branch, then<br>there is some tuple in borrower × loan that contains his name, and borrower.loan-number<br>= loan.loan-number. So, if we write<br>σborrower.loan-number = loan.loan-number<br>(σbranch-name = “Perryridge”(borrower × loan))<br>we get only those tuples of borrower × loan that pertain to customers who have a<br>loan at the Perryridge branch.<br>Finally, since we want only customer-name, we do a projection:<br>Πcustomer-name (σborrower.loan-number = loan.loan-number<br>(σbranch-name = “Perryridge” (borrower × loan)))<br>The result of this expression, shown in Figure 3.16, is the correct answer to our query.<br>3.2.1.7<br>The Rename Operation<br>Unlike relations in the database, the results of relational-algebra expressions do not<br>have a name that we can use to refer to them. It is useful to be able to give them<br>names; the rename operator, denoted by the lowercase Greek letter rho (ρ), lets us do<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>I. Data Models<br>3. Relational Model<br>104<br>© The McGraw−Hill <br>Companies, 2001<br>96<br>Chapter 3<br>Relational Model<br>customer-name<br>Adams<br>Hayes<br>Figure 3.16<br>Result of Πcustomer-name<br>(σborrower.loan-number = loan.loan-number<br>(σbranch-name = “Perryridge” (borrower × loan))).<br>this. Given a relational-algebra expression E, the expression<br>ρx (E)<br>returns the result of expression E under the name x.<br>A relation r by itself is considered a (trivial) relational-algebra expression. Thus,<br>we can also apply the rename operation to a relation r to get the same relation under<br>a new name.<br>A second form of the rename operation is as follows. Assume that a relational-<br>algebra expression E has arity n. Then, the expression<br>ρx(A1,A2,...,An) (E)<br>returns the result of expression E under the name x, and with the attributes renamed<br>to A1, A2, . . . , An.<br>To illustrate renaming a relation, we consider the query “Find the largest account<br>balance in the bank.” Our strategy is to (1) compute ﬁrst a temporary relation consist-<br>ing of those balances that are not the largest and (2) take the set difference between<br>the relation Πbalance (account) and the temporary relation just computed, to obtain<br>the result.<br>Step 1: To compute the temporary relation, we need to compare the values of<br>all account balances. We do this comparison by computing the Cartesian product<br>account × account and forming a selection to compare the value of any two balances<br>appearing in one tuple. First, we need to devise a mechanism to distinguish between<br>the two balance attributes. We shall use the rename operation to rename one reference<br>to the account relation; thus we can reference the relation twice without ambiguity.<br>balance<br>500<br>400<br>700<br>750<br>350<br>Figure 3.17<br>Result of the subexpression<br>Πaccount.balance (σaccount.balance &lt; d.balance (account × ρd (account))).<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>I. Data Models<br>3. Relational Model<br>105<br>© The McGraw−Hill <br>Companies, 2001<br>3.2<br>The Relational Algebra<br>97<br>balance<br>900<br>Figure 3.18<br>Largest account balance in the bank.<br>We can now write the temporary relation that consists of the balances that are not<br>the largest:<br>Πaccount.balance (σaccount.balance &lt; d.balance (account × ρd (account)))<br>This expression gives those balances in the account relation for which a larger balance<br>appears somewhere in the account relation (renamed as d). The result contains all<br>balances except the largest one. Figure 3.17 shows this relation.<br>Step 2: The query to ﬁnd the largest account balance in the bank can be written as:<br>Πbalance (account) −<br>Πaccount.balance (σaccount.balance &lt; d.balance (account × ρd (account)))<br>Figur</span><br><br><span style="background-color: #A0C4FF;" title="Chunk 13 | Start: 260026 | End: 280026 | Tokens: 3228">e 3.18 shows the result of this query.<br>As one more example of the rename operation, consider the query “Find the names<br>of all customers who live on the same street and in the same city as Smith.” We can<br>obtain Smith’s street and city by writing<br>Πcustomer-street, customer-city (σcustomer-name = “Smith” (customer))<br>However, in order to ﬁnd other customers with this street and city, we must refer-<br>ence the customer relation a second time. In the following query, we use the rename<br>operation on the preceding expression to give its result the name smith-addr, and to<br>rename its attributes to street and city, instead of customer-street and customer-city:<br>Πcustomer.customer-name<br>(σcustomer.customer-street=smith-addr.street ∧customer.customer-city=smith-addr.city<br>(customer × ρsmith-addr(street,city)<br>(Πcustomer-street, customer-city (σcustomer-name = “Smith”(customer)))))<br>The result of this query, when we apply it to the customer relation of Figure 3.4, ap-<br>pears in Figure 3.19.<br>The rename operation is not strictly required, since it is possible to use a positional<br>notation for attributes. We can name attributes of a relation implicitly by using a po-<br>sitional notation, where $1, $2, . . . refer to the ﬁrst attribute, the second attribute, and<br>so on. The positional notation also applies to results of relational-algebra operations.<br>customer-name<br>Curry<br>Smith<br>Figure 3.19<br>Customers who live on the same street and in the same city as Smith.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>I. Data Models<br>3. Relational Model<br>106<br>© The McGraw−Hill <br>Companies, 2001<br>98<br>Chapter 3<br>Relational Model<br>The following relational-algebra expression illustrates the use of positional notation<br>with the unary operator σ:<br>σ$2=$3(R × R)<br>If a binary operation needs to distinguish between its two operand relations, a similar<br>positional notation can be used for relation names as well. For example, $R1 could<br>refer to the ﬁrst operand, and $R2 could refer to the second operand. However, the<br>positional notation is inconvenient for humans, since the position of the attribute is a<br>number, rather than an easy-to-remember attribute name. Hence, we do not use the<br>positional notation in this textbook.<br>3.2.2<br>Formal Deﬁnition of the Relational Algebra<br>The operations in Section 3.2.1 allow us to give a complete deﬁnition of an expression<br>in the relational algebra. A basic expression in the relational algebra consists of either<br>one of the following:<br>• A relation in the database<br>• A constant relation<br>A constant relation is written by listing its tuples within { }, for example { (A-101,<br>Downtown, 500) (A-215, Mianus, 700) }.<br>A general expression in relational algebra is constructed out of smaller subexpres-<br>sions. Let E1 and E2 be relational-algebra expressions. Then, these are all relational-<br>algebra expressions:<br>• E1 ∪E2<br>• E1 −E2<br>• E1 × E2<br>• σP (E1), where P is a predicate on attributes in E1<br>• ΠS(E1), where S is a list consisting of some of the attributes in E1<br>• ρx (E1), where x is the new name for the result of E1<br>3.2.3<br>Additional Operations<br>The fundamental operations of the relational algebra are sufﬁcient to express any<br>relational-algebra query.1 However, if we restrict ourselves to just the fundamental<br>operations, certain common queries are lengthy to express. Therefore, we deﬁne ad-<br>ditional operations that do not add any power to the algebra, but simplify common<br>queries. For each new operation, we give an equivalent expression that uses only the<br>fundamental operations.<br>1.<br>In Section 3.3, we introduce operations that extend the power of the relational algebra, to handle null<br>and aggregate values.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>I. Data Models<br>3. Relational Model<br>107<br>© The McGraw−Hill <br>Companies, 2001<br>3.2<br>The Relational Algebra<br>99<br>3.2.3.1<br>The Set-Intersection Operation<br>The ﬁrst additional-relational algebra operation that we shall deﬁne is set intersec-<br>tion (∩). Suppose that we wish to ﬁnd all customers who have both a loan and an<br>account. Using set intersection, we can write<br>Πcustomer-name (borrower) ∩Πcustomer-name (depositor)<br>The result relation for this query appears in Figure 3.20.<br>Note that we can rewrite any relational algebra expression that uses set intersec-<br>tion by replacing the intersection operation with a pair of set-difference operations<br>as:<br>r ∩s = r −(r −s)<br>Thus, set intersection is not a fundamental operation and does not add any power<br>to the relational algebra. It is simply more convenient to write r ∩s than to write<br>r −(r −s).<br>3.2.3.2<br>The Natural-Join Operation<br>It is often desirable to simplify certain queries that require a Cartesian product. Usu-<br>ally, a query that involves a Cartesian product includes a selection operation on the<br>result of the Cartesian product. Consider the query “Find the names of all customers<br>who have a loan at the bank, along with the loan number and the loan amount.” We<br>ﬁrst form the Cartesian product of the borrower and loan relations. Then, we select<br>those tuples that pertain to only the same loan-number, followed by the projection of<br>the resulting customer-name, loan-number, and amount:<br>Πcustomer-name, loan.loan-number, amount<br>(σborrower.loan-number = loan.loan-number (borrower × loan))<br>The natural join is a binary operation that allows us to combine certain selections and<br>a Cartesian product into one operation. It is denoted by the “join” symbol<br>. The<br>natural-join operation forms a Cartesian product of its two arguments, performs a<br>selection forcing equality on those attributes that appear in both relation schemas,<br>and ﬁnally removes duplicate attributes.<br>Although the deﬁnition of natural join is complicated, the operation is easy to<br>apply. As an illustration, consider again the example “Find the names of all customers<br>who have a loan at the bank, and ﬁnd the amount of the loan.” We express this query<br>customer-name<br>Hayes<br>Jones<br>Smith<br>Figure 3.20<br>Customers with both an account and a loan at the bank.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>I. Data Models<br>3. Relational Model<br>108<br>© The McGraw−Hill <br>Companies, 2001<br>100<br>Chapter 3<br>Relational Model<br>customer-name<br>loan-number<br>amount<br>Adams<br>L-16<br>1300<br>Curry<br>L-93<br>500<br>Hayes<br>L-15<br>1500<br>Jackson<br>L-14<br>1500<br>Jones<br>L-17<br>1000<br>Smith<br>L-23<br>2000<br>Smith<br>L-11<br>900<br>Williams<br>L-17<br>1000<br>Figure 3.21<br>Result of Πcustomer-name, loan-number, amount (borrower<br> loan).<br>by using the natural join as follows:<br>Πcustomer-name, loan-number, amount (borrower<br> loan)<br>Since the schemas for borrower and loan (that is, Borrower-schema and Loan-schema)<br>have the attribute loan-number in common, the natural-join operation considers only<br>pairs of tuples that have the same value on loan-number. It combines each such pair<br>of tuples into a single tuple on the union of the two schemas (that is, customer-name,<br>branch-name, loan-number, amount). After performing the projection, we obtain the re-<br>lation in Figure 3.21.<br>Consider two relation schemas R and S—which are, of course, lists of attribute<br>names. If we consider the schemas to be sets, rather than lists, we can denote those<br>attribute names that appear in both R and S by R ∩S, and denote those attribute<br>names that appear in R, in S, or in both by R ∪S. Similarly, those attribute names that<br>appear in R but not S are denoted by R −S, whereas S −R denotes those attribute<br>names that appear in S but not in R. Note that the union, intersection, and difference<br>operations here are on sets of attributes, rather than on relations.<br>We are now ready for a formal deﬁnition of the natural join. Consider two relations<br>r(R) and s(S). The natural join of r and s, denoted by r<br> s, is a relation on schema<br>R ∪S formally deﬁned as follows:<br>r<br> s = ΠR ∪S (σr.A1 = s.A1 ∧r.A2 = s.A2 ∧... ∧r.An = s.An r × s)<br>where R ∩S = {A1, A2, . . . , An}.<br>Because the natural join is central to much of relational-database theory and prac-<br>tice, we give several examples of its use.<br>branch-name<br>Brighton<br>Perryridge<br>Figure 3.22<br>Result of<br>Πbranch-name(σcustomer-city = “Harrison” (customer<br> account<br> depositor)).<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>I. Data Models<br>3. Relational Model<br>109<br>© The McGraw−Hill <br>Companies, 2001<br>3.2<br>The Relational Algebra<br>101<br>• Find the names of all branches with customers who have an account in the<br>bank and who live in Harrison.<br>Πbranch-name<br>(σcustomer-city = “Harrison” (customer<br> account<br> depositor))<br>The result relation for this query appears in Figure 3.22.<br>Notice that we wrote customer<br> account<br> depositor without inserting<br>parentheses to specify the order in which the natural-join operations on the<br>three relations should be executed. In the preceding case, there are two possi-<br>bilities:<br>  (customer<br> account)<br> depositor<br>  customer<br> (account<br> depositor)<br>We did not specify which expression we intended, because the two are equiv-<br>alent. That is, the natural join is associative.<br>• Find all customers who have both a loan and an account at the bank.<br>Πcustomer-name (borrower<br> depositor)<br>Note that in Section 3.2.3.1 we wrote an expression for this query by using set<br>intersection. We repeat this expression here.<br>Πcustomer-name (borrower) ∩Πcustomer-name (depositor)<br>The result relation for this query appeared earlier in Figure 3.20. This example<br>illustrates a general fact about the relational algebra: It is possible to write<br>several equivalent relational-algebra expressions that are quite different from<br>one another.<br>• Let r(R) and s(S) be relations without any attributes in common; that is,<br>R ∩S = ∅. (∅denotes the empty set.) Then, r<br> s = r × s.<br>The theta join operation is an extension to the natural-join operation that allows<br>us to combine a selection and a Cartesian product into a single operation. Consider<br>relations r(R) and s(S), and let θ be a predicate on attributes in the schema R ∪S.<br>The theta join operation r<br>θ s is deﬁned as follows:<br>r<br>θ s = σθ(r × s)<br>3.2.3.3<br>The Division Operation<br>The division operation, denoted by ÷, is suited to queries that include the phrase<br>“for all.” Suppose that we wish to ﬁnd all customers who have an account at all the<br>branches located in Brooklyn. We can obtain all branches in Brooklyn by the expres-<br>sion<br>r1 = Πbranch-name (σbranch-city = “Brooklyn” (branch))<br>The result relation for this expression appears in Figure 3.23.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>I. Data Models<br>3. Relational Model<br>110<br>© The McGraw−Hill <br>Companies, 2001<br>102<br>Chapter 3<br>Relational Model<br>branch-name<br>Brighton<br>Downtown<br>Figure 3.23<br>Result of Πbranch-name(σbranch-city = “Brooklyn” (branch).<br>We can ﬁnd all (customer-name, branch-name) pairs for which the customer has an<br>account at a branch by writing<br>r2 = Πcustomer-name, branch-name (depositor<br> account)<br>Figure 3.24 shows the result relation for this expression.<br>Now, we need to ﬁnd customers who appear in r2 with every branch name in<br>r1. The operation that provides exactly those customers is the divide operation. We<br>formulate the query by writing<br>Πcustomer-name, branch-name (depositor<br> account)<br>÷ Πbranch-name (σbranch-city = “Brooklyn” (branch))<br>The result of this expression is a relation that has the schema (customer-name) and that<br>contains the tuple (Johnson).<br>Formally, let r(R) and s(S) be relations, and let S ⊆R; that is, every attribute of<br>schema S is also in schema R. The relation r ÷ s is a relation on schema R −S (that<br>is, on the schema containing all attributes of schema R that are not in schema S). A<br>tuple t is in r ÷ s if and only if both of two conditions hold:<br>1. t is in ΠR−S(r)<br>2. For every tuple ts in s, there is a tuple tr in r satisfying both of the following:<br>a. tr[S] = ts[S]<br>b. tr[R −S] = t<br>It may surprise you to discover that, given a division operation and the schemas of<br>the relations, we can, in fact, deﬁne the division operation in terms of the fundamen-<br>tal operations. Let r(R) and s(S) be given, with S ⊆R:<br>r ÷ s = ΠR−S (r) −ΠR−S ((ΠR−S (r) × s) −ΠR−S,S(r))<br>customer-name<br>branch-name<br>Hayes<br>Perryridge<br>Johnson<br>Downtown<br>Johnson<br>Brighton<br>Jones<br>Brighton<br>Lindsay<br>Redwood<br>Smith<br>Mianus<br>Turner<br>Round Hill<br>Figure 3.24<br>Result of Πcustomer-name, branch-name (depositor<br> account).<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>I. Data Models<br>3. Relational Model<br>111<br>© The McGraw−Hill <br>Companies, 2001<br>3.3<br>Extended Relational-Algebra Operations<br>103<br>To see that this expression is true, we observe that ΠR−S (r) gives us all tuples t that<br>satisfy the ﬁrst condition of the deﬁnition of division. The expression on the right<br>side of the set difference operator<br>ΠR−S ((ΠR−S (r) × s) −ΠR−S,S(r))<br>serves to eliminate those tuples that fail to satisfy the second condition of the deﬁni-<br>tion of division. Let us see how it does so. Consider ΠR−S (r) × s. This relation is<br>on schema R, and pairs every tuple in ΠR−S (r) with every tuple in s. The expression<br>ΠR−S,S(r) merely reorders the attributes of r.<br>Thus, (ΠR−S (r) × s) −ΠR−S,S(r) gives us those pairs of tuples from ΠR−S (r)<br>and s that do not appear in r. If a tuple tj is in<br>ΠR−S ((ΠR−S (r) × s) −ΠR−S,S(r))<br>then there is some tuple ts in s that does not combine with tuple tj to form a tuple in<br>r. Thus, tj holds a value for attributes R −S that does not appear in r ÷ s. It is these<br>values that we eliminate from ΠR−S (r).<br>3.2.3.4<br>The Assignment Operation<br>It is convenient at times to write a relational-algebra expression by assigning parts of<br>it to temporary relation variables. The assignment operation, denoted by ←, works<br>like assignment in a programming language. To illustrate this operation, consider the<br>deﬁnition of division in Section 3.2.3.3. We could write r ÷ s as<br>temp1 ←ΠR−S (r)<br>temp2 ←ΠR−S ((temp1 × s) −ΠR−S,S(r))<br>result = temp1 −temp2<br>The evaluation of an assignment does not result in any relation being displayed to<br>the user. Rather, the result of the expression to the right of the ←is assigned to the<br>relation variable on the left of the ←. This relation variable may be used in subsequent<br>expressions.<br>With the assignment operation, a query can be written as a sequential program<br>consisting of a series of assignments followed by an expression whose value is dis-<br>played as the result of the query. For relational-algebra queries, assignment must<br>always be made to a temporary relation variable. Assignments to permanent rela-<br>tions constitute a database modiﬁcation. We discuss this issue in Section 3.4. Note<br>that the assignment operation does not provide any additional power to the algebra.<br>It is, however, a convenient way to express complex queries.<br>3.3<br>Extended Relational-Algebra Operations<br>The basic relational-algebra operations have been extended in several ways. A simple<br>extension is to allow arithmetic operations as part of projection. An important exten-<br>sion is to allow aggregate operations such as computing the sum of the elements of a<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>I. Data Models<br>3. Relational Model<br>112<br>© The McGraw−Hill <br>Companies, 2001<br>104<br>Chapter 3<br>Relational Model<br>customer-name<br>limit<br>credit-balance<br>Curry<br>2000<br>1750<br>Hayes<br>1500<br>1500<br>Jones<br>6000<br>700<br>Smith<br>2000<br>400<br>Figure 3.25<br>The credit-info relation.<br>set, or their average. Another important extension is the outer-join operation, which<br>allows relational-algebra expressions to deal with null values, which model missing<br>information.<br>3.3.1<br>Generalized Projection<br>The generalized-projection operation extends the projection operation by allowing<br>arithmetic functions to be used in the projection list. The generalized projection op-<br>eration has the form<br>ΠF1,F2,...,Fn(E)<br>where E is any relational-algebra expression, and each of F1, F2, . . . , Fn is an arith-<br>metic expression involving constants and attributes in the schema of E. As a special<br>case, the arithmetic expression may be simply an attribute or a constant.<br>For example, suppose we have a relation credit-info, as in Figure 3.25, which lists<br>the credit limit and expenses so far (the credit-balance on the account). If we want to<br>ﬁnd how much more each person can spend, we can write the following expression:<br>Πcustomer-name, limit −credit-balance (credit-info)<br>The attribute resulting from the expression limit −credit-balance does not have a<br>name. We can apply the rename operation to the result of generalized projection in<br>order to give it a name. As a notational convenience, renaming of attributes can be<br>combined with generalized projection as illustrated below:<br>Πcustomer-name, (limit −credit-balance) as credit-available (credit-info)<br>The second attribute of this generalized projection has been given the name credit-<br>available. Figure 3.26 shows the result of applying this expression to the relation in<br>Figure 3.25.<br>3.3.2<br>Aggregate Functions<br>Aggregate functions take a collection of values and return a single value as a result.<br>For example, the aggregate function sum takes a collection of values and returns the<br>sum of the values. Thus, the function sum applied on the collection<br>{1, 1, 3, 4, 4, 11}<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>I. Data Models<br>3. Relational Model<br>113<br>© The McGraw−Hill <br>Companies, 2001<br>3.3<br>Extended Relational-Algebra Operations<br>105<br>customer-name<br>credit-available<br>Curry<br>250<br>Jones<br>5300<br>Smith<br>1600<br>Hayes<br>0<br>Figure 3.26<br>The result of Πcustomer-name, (limit −credit-balance) as credit-available<br>(credit-info).<br>returns the value 24. The aggregate function avg returns the average of the values.<br>When applied to the preceding collection, it returns the value 4. The aggregate func-<br>tion count returns the number of the elements in the collection, and returns 6 on<br>the preceding collection. Other common aggregate functions include min and max,<br>which return the minimum and maximum values in a collection; they return 1 and<br>11, respectively, on the preceding collection.<br>The collections on which aggregate functions operate can have multiple occur-<br>rences of a value; the order in which the values appear is not relevant. Such collec-<br>tions are called multisets. Sets are a special case of multisets where there is only one<br>copy of each element.<br>To illustrate the concept of aggregation, we shall use the pt-works relation in Fig-<br>ure 3.27, for part-time employees. Suppose that we want to ﬁnd out the total sum of<br>salaries of all the part-time employees in the bank. The relational-algebra expression<br>for this query is:<br>Gsum(salary)(pt-works)<br>The symbol G is the letter G in calligraphic font; read it as “calligraphic G.” The<br>relational-algebra operation G signiﬁes that aggregation is to be applied, and its sub-<br>script speciﬁes the aggregate operation to be applied. The result of the expression<br>above is a relation with a single attribute, containing a single row with a numerical<br>value corresponding to the sum of all the salaries of all employees working part-time<br>in the bank.<br>employee-name<br>branch-name<br>salary<br>Adams<br>Perryridge<br>1500<br>Brown<br>Perryridge<br>1300<br>Gopal<br>Perryridge<br>5300<br>Johnson<br>Downtown<br>1500<br>Loreena<br>Downtown<br>1300<br>Peterson<br>Downtown<br>2500<br>Rao<br>Austin<br>1500<br>Sato<br>Austin<br>1600<br>Figure 3.27<br>The pt-works relation.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>I. Data Models<br>3. Relational Model<br>114<br>© The McGraw−Hill <br>Companies, 2001<br>106<br>Chapter 3<br>Relational Model<br>There are cases where we must eliminate multiple occurrences of a value before<br>computing an aggregate function. If we do want to eliminate duplicates, we use the<br>same function names as before, with the addition of the hyphenated string “distinct”<br>appended to the end of the function name (for example, count-distinct). An example<br>arises in the query “Find the number of branches appearing in the pt-works relation.”<br>In this case, a branch name counts only once, regardless of the number of employees<br>working that branch. We write this query as follows:<br>Gcount-distinct(branch-name)(pt-works)<br>For the relation in Figure 3.27, the result of this query is a single row containing the<br>value 3.<br>Suppose we want to ﬁnd the total salary sum of all part-time emp</span><br><br><span style="background-color: #BDB2FF;" title="Chunk 14 | Start: 280028 | End: 300028 | Tokens: 3277">loyees at each<br>branch of the bank separately, rather than the sum for the entire bank. To do so, we<br>need to partition the relation pt-works into groups based on the branch, and to apply<br>the aggregate function on each group.<br>The following expression using the aggregation operator G achieves the desired<br>result:<br>branch-nameGsum(salary)(pt-works)<br>In the expression, the attribute branch-name in the left-hand subscript of G indicates<br>that the input relation pt-works must be divided into groups based on the value of<br>branch-name. Figure 3.28 shows the resulting groups. The expression sum(salary) in<br>the right-hand subscript of G indicates that for each group of tuples (that is, each<br>branch), the aggregation function sum must be applied on the collection of values of<br>the salary attribute. The output relation consists of tuples with the branch name, and<br>the sum of the salaries for the branch, as shown in Figure 3.29.<br>The general form of the aggregation operation G is as follows:<br>G1,G2,...,GnGF1(A1), F2(A2),..., Fm(Am)(E)<br>where E is any relational-algebra expression; G1, G2, . . . , Gn constitute a list of at-<br>tributes on which to group; each Fi is an aggregate function; and each Ai is an at-<br>employee-name<br>branch-name<br>salary<br>Rao<br>Austin<br>1500<br>Sato<br>Austin<br>1600<br>Johnson<br>Downtown<br>1500<br>Loreena<br>Downtown<br>1300<br>Peterson<br>Downtown<br>2500<br>Adams<br>Perryridge<br>1500<br>Brown<br>Perryridge<br>1300<br>Gopal<br>Perryridge<br>5300<br>Figure 3.28<br>The pt-works relation after grouping.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>I. Data Models<br>3. Relational Model<br>115<br>© The McGraw−Hill <br>Companies, 2001<br>3.3<br>Extended Relational-Algebra Operations<br>107<br>branch-name<br>sum of salary<br>Austin<br>3100<br>Downtown<br>5300<br>Perryridge<br>8100<br>Figure 3.29<br>Result of branch-nameGsum(salary)(pt-works).<br>tribute name. The meaning of the operation is as follows. The tuples in the result of<br>expression E are partitioned into groups in such a way that<br>1. All tuples in a group have the same values for G1, G2, . . . , Gn.<br>2. Tuples in different groups have different values for G1, G2, . . . , Gn.<br>Thus, the groups can be identiﬁed by the values of attributes G1, G2, . . . , Gn. For each<br>group (g1, g2, . . . , gn), the result has a tuple (g1, g2, . . . , gn, a1, a2, . . . , am) where, for<br>each i, ai is the result of applying the aggregate function Fi on the multiset of values<br>for attribute Ai in the group.<br>As a special case of the aggregate operation, the list of attributes G1, G2, . . . , Gn can<br>be empty, in which case there is a single group containing all tuples in the relation.<br>This corresponds to aggregation without grouping.<br>Going back to our earlier example, if we want to ﬁnd the maximum salary for<br>part-time employees at each branch, in addition to the sum of the salaries, we write<br>the expression<br>branch-nameGsum(salary),max(salary)(pt-works)<br>As in generalized projection, the result of an aggregation operation does not have a<br>name. We can apply a rename operation to the result in order to give it a name. As<br>a notational convenience, attributes of an aggregation operation can be renamed as<br>illustrated below:<br>branch-nameGsum(salary) as sum-salary,max(salary) as max-salary(pt-works)<br>Figure 3.30 shows the result of the expression.<br>branch-name<br>sum-salary<br>max-salary<br>Austin<br>3100<br>1600<br>Downtown<br>5300<br>2500<br>Perryridge<br>8100<br>5300<br>Figure 3.30<br>Result of<br>branch-nameGsum(salary) as sum-salary,max(salary) as max-salary(pt-works).<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>I. Data Models<br>3. Relational Model<br>116<br>© The McGraw−Hill <br>Companies, 2001<br>108<br>Chapter 3<br>Relational Model<br>employee-name<br>street<br>city<br>Coyote<br>Toon<br>Hollywood<br>Rabbit<br>Tunnel<br>Carrotville<br>Smith<br>Revolver<br>Death Valley<br>Williams<br>Seaview<br>Seattle<br>employee-name<br>branch-name<br>salary<br>Coyote<br>Mesa<br>1500<br>Rabbit<br>Mesa<br>1300<br>Gates<br>Redmond<br>5300<br>Williams<br>Redmond<br>1500<br>Figure 3.31<br>The employee and ft-works relations.<br>3.3.3<br>Outer Join<br>The outer-join operation is an extension of the join operation to deal with missing<br>information. Suppose that we have the relations with the following schemas, which<br>contain data on full-time employees:<br>employee (employee-name, street, city)<br>ft-works (employee-name, branch-name, salary)<br>Consider the employee and ft-works relations in Figure 3.31. Suppose that we want<br>to generate a single relation with all the information (street, city, branch name, and<br>salary) about full-time employees. A possible approach would be to use the natural-<br>join operation as follows:<br>employee<br> ft-works<br>The result of this expression appears in Figure 3.32. Notice that we have lost the street<br>and city information about Smith, since the tuple describing Smith is absent from<br>the ft-works relation; similarly, we have lost the branch name and salary information<br>about Gates, since the tuple describing Gates is absent from the employee relation.<br>We can use the outer-join operation to avoid this loss of information. There are<br>actually three forms of the operation: left outer join, denoted<br>; right outer join, de-<br>noted<br> ; and full outer join, denoted<br> . All three forms of outer join compute the<br>join, and add extra tuples to the result of the join. The results of the expressions<br>employee-name<br>street<br>city<br>branch-name<br>salary<br>Coyote<br>Toon<br>Hollywood<br>Mesa<br>1500<br>Rabbit<br>Tunnel<br>Carrotville<br>Mesa<br>1300<br>Williams<br>Seaview<br>Seattle<br>Redmond<br>1500<br>Figure 3.32<br>The result of employee<br> ft-works.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>I. Data Models<br>3. Relational Model<br>117<br>© The McGraw−Hill <br>Companies, 2001<br>3.3<br>Extended Relational-Algebra Operations<br>109<br>employee-name<br>street<br>city<br>branch-name<br>salary<br>Coyote<br>Toon<br>Hollywood<br>Mesa<br>1500<br>Rabbit<br>Tunnel<br>Carrotville<br>Mesa<br>1300<br>Williams<br>Seaview<br>Seattle<br>Redmond<br>1500<br>Smith<br>Revolver<br>Death Valley<br>null<br>null<br>Figure 3.33<br>Result of employee<br> ft-works.<br>employee<br> ft-works,, employee<br> ft-works, and employee<br> ft-works appear in<br>Figures 3.33, 3.34, and 3.35, respectively.<br>The left outer join (<br>) takes all tuples in the left relation that did not match with<br>any tuple in the right relation, pads the tuples with null values for all other attributes<br>from the right relation, and adds them to the result of the natural join. In Figure 3.33,<br>tuple (Smith, Revolver, Death Valley, null, null) is such a tuple. All information from<br>the left relation is present in the result of the left outer join.<br>The right outer join ( ) is symmetric with the left outer join: It pads tuples from<br>the right relation that did not match any from the left relation with nulls and adds<br>them to the result of the natural join. In Figure 3.34, tuple (Gates, null, null, Redmond,<br>5300) is such a tuple. Thus, all information from the right relation is present in the<br>result of the right outer join.<br>The full outer join(<br> ) does both of those operations, padding tuples from the<br>left relation that did not match any from the right relation, as well as tuples from the<br>right relation that did not match any from the left relation, and adding them to the<br>result of the join. Figure 3.35 shows the result of a full outer join.<br>Since outer join operations may generate results containing null values, we need<br>to specify how the different relational-algebra operations deal with null values. Sec-<br>tion 3.3.4 deals with this issue.<br>It is interesting to note that the outer join operations can be expressed by the basic<br>relational-algebra operations. For instance, the left outer join operation, r<br> s, can<br>be written as<br>(r<br> s) ∪(r −ΠR(r<br> s)) × {(null, . . . , null)}<br>where the constant relation {(null, . . . , null)} is on the schema S −R.<br>employee-name<br>street<br>city<br>branch-name<br>salary<br>Coyote<br>Toon<br>Hollywood<br>Mesa<br>1500<br>Rabbit<br>Tunnel<br>Carrotville<br>Mesa<br>1300<br>Williams<br>Seaview<br>Seattle<br>Redmond<br>1500<br>Gates<br>null<br>null<br>Redmond<br>5300<br>Figure 3.34<br>Result of employee<br> ft-works.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>I. Data Models<br>3. Relational Model<br>118<br>© The McGraw−Hill <br>Companies, 2001<br>110<br>Chapter 3<br>Relational Model<br>employee-name<br>street<br>city<br>branch-name<br>salary<br>Coyote<br>Toon<br>Hollywood<br>Mesa<br>1500<br>Rabbit<br>Tunnel<br>Carrotville<br>Mesa<br>1300<br>Williams<br>Seaview<br>Seattle<br>Redmond<br>1500<br>Smith<br>Revolver<br>Death Valley<br>null<br>null<br>Gates<br>null<br>null<br>Redmond<br>5300<br>Figure 3.35<br>Result of employee<br> ft-works.<br>3.3.4<br>Null Values∗∗<br>In this section, we deﬁne how the various relational algebra operations deal with null<br>values and complications that arise when a null value participates in an arithmetic<br>operation or in a comparison. As we shall see, there is often more than one possible<br>way of dealing with null values, and as a result our deﬁnitions can sometimes be<br>arbitrary. Operations and comparisons on null values should therefore be avoided,<br>where possible.<br>Since the special value null indicates “value unknown or nonexistent,” any arith-<br>metic operations (such as +, −, ∗, /) involving null values must return a null result.<br>Similarly, any comparisons (such as &lt;, &lt;=, &gt;, &gt;=, ̸=) involving a null value eval-<br>uate to special value unknown; we cannot say for sure whether the result of the<br>comparison is true or false, so we say that the result is the new truth value unknown.<br>Comparisons involving nulls may occur inside Boolean expressions involving the<br>and, or, and not operations. We must therefore deﬁne how the three Boolean opera-<br>tions deal with the truth value unknown.<br>• and: (true and unknown) = unknown; (false and unknown) = false; (unknown and<br>unknown) = unknown.<br>• or: (true or unknown) = true; (false or unknown) = unknown; (unknown or un-<br>known) = unknown.<br>• not: (not unknown) = unknown.<br>We are now in a position to outline how the different relational operations deal<br>with null values. Our deﬁnitions follow those used in the SQL language.<br>• select: The selection operation evaluates predicate P in σP (E) on each tuple t<br>in E. If the predicate returns the value true, t is added to the result. Otherwise,<br>if the predicate returns unknown or false, t is not added to the result.<br>• join: Joins can be expressed as a cross product followed by a selection. Thus,<br>the deﬁnition of how selection handles nulls also deﬁnes how join operations<br>handle nulls.<br>In a natural join, say r<br> s, we can see from the above deﬁnition that if two<br>tuples, tr ∈r and ts ∈s, both have a null value in a common attribute, then<br>the tuples do not match.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>I. Data Models<br>3. Relational Model<br>119<br>© The McGraw−Hill <br>Companies, 2001<br>3.4<br>Modiﬁcation of the Database<br>111<br>• projection: The projection operation treats nulls just like any other value when<br>eliminating duplicates. Thus, if two tuples in the projection result are exactly<br>the same, and both have nulls in the same ﬁelds, they are treated as duplicates.<br>The decision is a little arbitrary since, without knowing the actual value,<br>we do not know if the two instances of null are duplicates or not.<br>• union, intersection, difference: These operations treat nulls just as the projec-<br>tion operation does; they treat tuples that have the same values on all ﬁelds as<br>duplicates even if some of the ﬁelds have null values in both tuples.<br>The behavior is rather arbitrary, especially in the case of intersection and<br>difference, since we do not know if the actual values (if any) represented by<br>the nulls are the same.<br>• generalized projection: We outlined how nulls are handled in expressions<br>at the beginning of Section 3.3.4. Duplicate tuples containing null values are<br>handled as in the projection operation.<br>• aggregate: When nulls occur in grouping attributes, the aggregate operation<br>treats them just as in projection: If two tuples are the same on all grouping<br>attributes, the operation places them in the same group, even if some of their<br>attribute values are null.<br>When nulls occur in aggregated attributes, the operation deletes null values<br>at the outset, before applying aggregation. If the resultant multiset is empty,<br>the aggregate result is null.<br>Note that the treatment of nulls here is different from that in ordinary arith-<br>metic expressions; we could have deﬁned the result of an aggregate operation<br>as null if even one of the aggregated values is null. However, this would mean<br>a single unknown value in a large group could make the aggregate result on<br>the group to be null, and we would lose a lot of useful information.<br>• outer join: Outer join operations behave just like join operations, except on<br>tuples that do not occur in the join result. Such tuples may be added to the<br>result (depending on whether the operation is<br>,<br> , or<br> ), padded with<br>nulls.<br>3.4<br>Modiﬁcation of the Database<br>We have limited our attention until now to the extraction of information from the<br>database. In this section, we address how to add, remove, or change information in<br>the database.<br>We express database modiﬁcations by using the assignment operation. We make<br>assignments to actual database relations by using the same notation as that described<br>in Section 3.2.3 for assignment.<br>3.4.1<br>Deletion<br>We express a delete request in much the same way as a query. However, instead of<br>displaying tuples to the user, we remove the selected tuples from the database. We<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>I. Data Models<br>3. Relational Model<br>120<br>© The McGraw−Hill <br>Companies, 2001<br>112<br>Chapter 3<br>Relational Model<br>can delete only whole tuples; we cannot delete values on only particular attributes.<br>In relational algebra a deletion is expressed by<br>r ←r −E<br>where r is a relation and E is a relational-algebra query.<br>Here are several examples of relational-algebra delete requests:<br>• Delete all of Smith’s account records.<br>depositor ←depositor −σcustomer-name = “Smith” (depositor)<br>• Delete all loans with amount in the range 0 to 50.<br>loan ←loan −σamount≥0 and amount≤50 (loan)<br>• Delete all accounts at branches located in Needham.<br>r1 ←σbranch-city = “Needham” (account<br> branch)<br>r2 ←Πbranch-name, account-number, balance (r1)<br>account ←account −r2<br>Note that, in the ﬁnal example, we simpliﬁed our expression by using assign-<br>ment to temporary relations (r1 and r2).<br>3.4.2<br>Insertion<br>To insert data into a relation, we either specify a tuple to be inserted or write a query<br>whose result is a set of tuples to be inserted. Obviously, the attribute values for in-<br>serted tuples must be members of the attribute’s domain. Similarly, tuples inserted<br>must be of the correct arity. The relational algebra expresses an insertion by<br>r ←r ∪E<br>where r is a relation and E is a relational-algebra expression. We express the insertion<br>of a single tuple by letting E be a constant relation containing one tuple.<br>Suppose that we wish to insert the fact that Smith has $1200 in account A-973 at<br>the Perryridge branch. We write<br>account ←account ∪{(A-973, “Perryridge”, 1200)}<br>depositor ←depositor ∪{(“Smith”, A-973)}<br>More generally, we might want to insert tuples on the basis of the result of a query.<br>Suppose that we want to provide as a gift for all loan customers of the Perryridge<br>branch a new $200 savings account. Let the loan number serve as the account number<br>for this savings account. We write<br>r1 ←(σbranch-name = “Perryridge” (borrower<br> loan))<br>r2 ←Πloan-number, branch-name (r1)<br>account ←account ∪(r2 × {(200)})<br>depositor ←depositor ∪Πcustomer-name, loan-number (r1)<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>I. Data Models<br>3. Relational Model<br>121<br>© The McGraw−Hill <br>Companies, 2001<br>3.5<br>Views<br>113<br>Instead of specifying a tuple as we did earlier, we specify a set of tuples that is in-<br>serted into both the account and depositor relation. Each tuple in the account relation<br>has an account-number (which is the same as the loan number), a branch-name (Per-<br>ryridge), and the initial balance of the new account ($200). Each tuple in the depositor<br>relation has as customer-name the name of the loan customer who is being given the<br>new account and the same account number as the corresponding account tuple.<br>3.4.3<br>Updating<br>In certain situations, we may wish to change a value in a tuple without changing all<br>values in the tuple. We can use the generalized-projection operator to do this task:<br>r ←ΠF1,F2,...,Fn(r)<br>where each Fi is either the ith attribute of r, if the ith attribute is not updated, or, if<br>the attribute is to be updated, Fi is an expression, involving only constants and the<br>attributes of r, that gives the new value for the attribute.<br>If we want to select some tuples from r and to update only them, we can use<br>the following expression; here, P denotes the selection condition that chooses which<br>tuples to update:<br>r ←ΠF1,F2,...,Fn(σP (r)) ∪(r −σP (r))<br>To illustrate the use of the update operation, suppose that interest payments are<br>being made, and that all balances are to be increased by 5 percent. We write<br>account ←Πaccount-number, branch-name, balance ∗1.05 (account)<br>Now suppose that accounts with balances over $10,000 receive 6 percent interest,<br>whereas all others receive 5 percent. We write<br>account ←ΠAN,BN, balance ∗1.06 (σbalance&gt;10000 (account))<br>∪ΠAN, BN balance ∗1.05 (σbalance≤10000 (account))<br>where the abbreviations AN and BN stand for account-number and branch-name, re-<br>spectively.<br>3.5<br>Views<br>In our examples up to this point, we have operated at the logical-model level. That<br>is, we have assumed that the relations in thecollection we are given are the actual<br>relations stored in the database.<br>It is not desirable for all users to see the entire logical model. Security consider-<br>ations may require that certain data be hidden from users. Consider a person who<br>needs to know a customer’s loan number and branch name, but has no need to see<br>the loan amount. This person should see a relation described, in the relational alge-<br>bra, by<br>Πcustomer-name, loan-number, branch-name (borrower<br> loan)<br>Aside from security concerns, we may wish to create a personalized collection of<br>relations that is better matched to a certain user’s intuition than is the logical model.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>I. Data Models<br>3. Relational Model<br>122<br>© The McGraw−Hill <br>Companies, 2001<br>114<br>Chapter 3<br>Relational Model<br>An employee in the advertising department, for example, might like to see a relation<br>consisting of the customers who have either an account or a loan at the bank, and<br>the branches with which they do business. The relation that we would create for that<br>employee is<br>Πbranch-name, customer-name (depositor<br> account)<br>∪Πbranch-name, customer-name (borrower<br> loan)<br>Any relation that is not part of the logical model, but is made visible to a user as a<br>virtual relation, is called a view. It is possible to support a large number of views on<br>top of any given set of actual relations.<br>3.5.1<br>View Deﬁnition<br>We deﬁne a view by using the create view statement. To deﬁne a view, we must give<br>the view a name, and must state the query that computes the view. The form of the<br>create view statement is<br>create view v as &lt;query expression&gt;<br>where &lt;query expression&gt; is any legal relational-algebra query expression. The view<br>name is represented by v.<br>As an example, consider the view consisting of branches and their customers. We<br>wish this view to be called all-customer. We deﬁne this view as follows:<br>create view all-customer as<br>Πbranch-name, customer-name (depositor<br> account)<br>∪Πbranch-name, customer-name (borrower<br> loan)<br>Once we have deﬁned a view, we can use the view name to refer to the virtual re-<br>lation that the view generates. Using the view all-customer, we can ﬁnd all customers<br>of the Perryridge branch by writing<br>Πcustomer-name (σbranch-name = “Perryridge” (all-customer))<br>Recall that we wrote the same query in Section 3.2.1 without using views.<br>View names may appear in any place where a relation name may appear, so long<br>as no update operations are executed on the views. We study the issue of update<br>operations on views in Section 3.5.2.<br>View deﬁnition differs from the relational-algebra assignment operation. Suppose<br>that we deﬁne relation r1 as follows:<br>r1 ←Πbranch-name, customer-name (depositor<br> account)<br>∪Πbranch-name, customer-name(borrower<br> loan</span><br><br><span style="background-color: #FFC6FF;" title="Chunk 15 | Start: 300030 | End: 320030 | Tokens: 3345">)<br>We evaluate the assignment operation once, and r1 does not change when we up-<br>date the relations depositor, account, loan, or borrower. In contrast, any modiﬁcation<br>we make to these relations changes the set of tuples in the view all-customer as well.<br>Intuitively, at any given time, the set of tuples in the view relation is the result of<br>evaluation of the query expression that deﬁnes the view at that time.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>I. Data Models<br>3. Relational Model<br>123<br>© The McGraw−Hill <br>Companies, 2001<br>3.5<br>Views<br>115<br>Thus, if a view relation is computed and stored, it may become out of date if the<br>relations used to deﬁne it are modiﬁed. To avoid this, views are usually implemented<br>as follows. When we deﬁne a view, the database system stores the deﬁnition of the<br>view itself, rather than the result of evaluation of the relational-algebra expression<br>that deﬁnes the view. Wherever a view relation appears in a query, it is replaced by<br>the stored query expression. Thus, whenever we evaluate the query, the view relation<br>gets recomputed.<br>Certain database systems allow view relations to be stored, but they make sure<br>that, if the actual relations used in the view deﬁnition change, the view is kept up<br>to date. Such views are called materialized views. The process of keeping the view<br>up to date is called view maintenance, covered in Section 14.5. Applications that use<br>a view frequently beneﬁt from the use of materialized views, as do applications that<br>demand fast response to certain view-based queries. Of course, the beneﬁts to queries<br>from the materialization of a view must be weighed against the storage costs and the<br>added overhead for updates.<br>3.5.2<br>Updates through Views and Null Values<br>Although views are a useful tool for queries, they present serious problems if we ex-<br>press updates, insertions, or deletions with them. The difﬁculty is that a modiﬁcation<br>to the database expressed in terms of a view must be translated to a modiﬁcation to<br>the actual relations in the logical model of the database.<br>To illustrate the problem, consider a clerk who needs to see all loan data in the loan<br>relation, except loan-amount. Let loan-branch be the view given to the clerk. We deﬁne<br>this view as<br>create view loan-branch as<br>Πloan-number, branch-name (loan)<br>Since we allow a view name to appear wherever a relation name is allowed, the clerk<br>can write:<br>loan-branch ←loan-branch ∪{(L-37, “Perryridge”)}<br>This insertion must be represented by an insertion into the relation loan, since loan is<br>the actual relation from which the database system constructs the view loan-branch.<br>However, to insert a tuple into loan, we must have some value for amount. There are<br>two reasonable approaches to dealing with this insertion:<br>• Reject the insertion, and return an error message to the user.<br>• Insert a tuple (L-37, “Perryridge”, null) into the loan relation.<br>Another problem with modiﬁcation of the database through views occurs with a<br>view such as<br>create view loan-info as<br>Πcustomer-name, amount(borrower<br> loan)<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>I. Data Models<br>3. Relational Model<br>124<br>© The McGraw−Hill <br>Companies, 2001<br>116<br>Chapter 3<br>Relational Model<br>loan-number<br>branch-name<br>amount<br>L-11<br>RoundHill<br>900<br>L-14<br>Downtown<br>1500<br>L-15<br>Perryridge<br>Perryridge<br>1500<br>L-16<br>1300<br>L-17<br>Downtown<br>1000<br>L-23<br>Redwood<br>2000<br>L-93<br>Mianus<br>500<br>null<br>null<br>1900<br>customer-name<br>loan-number<br>Adams<br>L-16<br>Curry<br>L-93<br>Hayes<br>L-15<br>Jackson<br>L-14<br>Jones<br>L-17<br>Smith<br>L-11<br>Smith<br>L-23<br>Williams<br>L-17<br>Johnson<br>null<br>Figure 3.36<br>Tuples inserted into loan and borrower.<br>This view lists the loan amount for each loan that any customer of the bank has.<br>Consider the following insertion through this view:<br>loan-info ←loan-info ∪{(“Johnson”, 1900)}<br>The only possible method of inserting tuples into the borrower and loan relations is to<br>insert (“Johnson”, null) into borrower and (null, null, 1900) into loan. Then, we obtain<br>the relations shown in Figure 3.36. However, this update does not have the desired<br>effect, since the view relation loan-info still does not include the tuple (“Johnson”,<br>1900). Thus, there is no way to update the relations borrower and loan by using nulls<br>to get the desired update on loan-info.<br>Because of problems such as these, modiﬁcations are generally not permitted on<br>view relations, except in limited cases. Different database systems specify different<br>conditions under which they permit updates on view relations; see the database<br>system manuals for details. The general problem of database modiﬁcation through<br>views has been the subject of substantial research, and the bibliographic notes pro-<br>vide pointers to some of this research.<br>3.5.3<br>Views Deﬁned by Using Other Views<br>In Section 3.5.1 we mentioned that view relations may appear in any place that a<br>relation name may appear, except for restrictions on the use of views in update ex-<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>I. Data Models<br>3. Relational Model<br>125<br>© The McGraw−Hill <br>Companies, 2001<br>3.5<br>Views<br>117<br>pressions. Thus, one view may be used in the expression deﬁning another view. For<br>example, we can deﬁne the view perryridge-customer as follows:<br>create view perryridge-customer as<br>Πcustomer-name (σbranch-name = “Perryridge” (all-customer))<br>where all-customer is itself a view relation.<br>View expansion is one way to deﬁne the meaning of views deﬁned in terms of<br>other views. The procedure assumes that view deﬁnitions are not recursive; that is,<br>no view is used in its own deﬁnition, whether directly, or indirectly through other<br>view deﬁnitions. For example, if v1 is used in the deﬁnition of v2, v2 is used in the<br>deﬁnition of v3, and v3 is used in the deﬁnition of v1, then each of v1, v2, and v3<br>is recursive. Recursive view deﬁnitions are useful in some situations, and we revisit<br>them in the context of the Datalog language, in Section 5.2.<br>Let view v1 be deﬁned by an expression e1 that may itself contain uses of view<br>relations. A view relation stands for the expression deﬁning the view, and therefore<br>a view relation can be replaced by the expression that deﬁnes it. If we modify an ex-<br>pression by replacing a view relation by the latter’s deﬁnition, the resultant expres-<br>sion may still contain other view relations. Hence, view expansion of an expression<br>repeats the replacement step as follows:<br>repeat<br>Find any view relation vi in e1<br>Replace the view relation vi by the expression deﬁning vi<br>until no more view relations are present in e1<br>As long as the view deﬁnitions are not recursive, this loop will terminate. Thus, an<br>expression e containing view relations can be understood as the expression resulting<br>from view expansion of e, which does not contain any view relations.<br>As an illustration of view expansion, consider the following expression:<br>σcustomer-name=“John”( perryridge-customer)<br>The view-expansion procedure initially generates<br>σcustomer-name=“John”(Πcustomer-name (σbranch-name = “Perryridge”<br>(all-customer)))<br>It then generates<br>σcustomer-name=“John” (Πcustomer-name (σbranch-name = “Perryridge”<br>(Πbranch-name, customer-name (depositor<br> account)<br>∪Πbranch-name, customer-name (borrower<br> loan))))<br>There are no more uses of view relations, and view expansion terminates.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>I. Data Models<br>3. Relational Model<br>126<br>© The McGraw−Hill <br>Companies, 2001<br>118<br>Chapter 3<br>Relational Model<br>3.6<br>The Tuple Relational Calculus<br>When we write a relational-algebra expression, we provide a sequence of procedures<br>that generates the answer to our query. The tuple relational calculus, by contrast, is a<br>nonprocedural query language. It describes the desired information without giving<br>a speciﬁc procedure for obtaining that information.<br>A query in the tuple relational calculus is expressed as<br>{t | P(t)}<br>that is, it is the set of all tuples t such that predicate P is true for t. Following our<br>earlier notation, we use t[A] to denote the value of tuple t on attribute A, and we use<br>t ∈r to denote that tuple t is in relation r.<br>Before we give a formal deﬁnition of the tuple relational calculus, we return to<br>some of the queries for which we wrote relational-algebra expressions in Section 3.2.<br>3.6.1<br>Example Queries<br>Say that we want to ﬁnd the branch-name, loan-number, and amount for loans of over<br>$1200:<br>{t | t ∈loan ∧t[amount] &gt; 1200}<br>Suppose that we want only the loan-number attribute, rather than all attributes of the<br>loan relation. To write this query in the tuple relational calculus, we need to write<br>an expression for a relation on the schema (loan-number). We need those tuples on<br>(loan-number) such that there is a tuple in loan with the amount attribute &gt; 1200. To<br>express this request, we need the construct “there exists” from mathematical logic.<br>The notation<br>∃t ∈r (Q(t))<br>means “there exists a tuple t in relation r such that predicate Q(t) is true.”<br>Using this notation, we can write the query “Find the loan number for each loan<br>of an amount greater than $1200” as<br>{t | ∃s ∈loan (t[loan-number] = s[loan-number]<br>∧s[amount] &gt; 1200)}<br>In English, we read the preceding expression as “The set of all tuples t such that there<br>exists a tuple s in relation loan for which the values of t and s for the loan-number<br>attribute are equal, and the value of s for the amount attribute is greater than $1200.”<br>Tuple variable t is deﬁned on only the loan-number attribute, since that is the only<br>attribute having a condition speciﬁed for t. Thus, the result is a relation on (loan-<br>number).<br>Consider the query “Find the names of all customers who have a loan from the<br>Perryridge branch.” This query is slightly more complex than the previous queries,<br>since it involves two relations: borrower and loan. As we shall see, however, all it<br>requires is that we have two “there exists” clauses in our tuple-relational-calculus<br>expression, connected by and (∧). We write the query as follows:<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>I. Data Models<br>3. Relational Model<br>127<br>© The McGraw−Hill <br>Companies, 2001<br>3.6<br>The Tuple Relational Calculus<br>119<br>{t | ∃s ∈borrower (t[customer-name] = s[customer-name]<br>∧∃u ∈loan (u[loan-number] = s[loan-number]<br>∧u[branch-name] = “Perryridge”))}<br>In English, this expression is “The set of all (customer-name) tuples for which the cus-<br>tomer has a loan that is at the Perryridge branch.” Tuple variable u ensures that the<br>customer is a borrower at the Perryridge branch. Tuple variable s is restricted to per-<br>tain to the same loan number as s. Figure 3.37 shows the result of this query.<br>To ﬁnd all customers who have a loan, an account, or both at the bank, we used<br>the union operation in the relational algebra. In the tuple relational calculus, we shall<br>need two “there exists” clauses, connected by or (∨):<br>{t | ∃s ∈borrower (t[customer-name] = s[customer-name])<br>∨∃u ∈depositor (t[customer-name] = u[customer-name])}<br>This expression gives us the set of all customer-name tuples for which at least one of<br>the following holds:<br>• The customer-name appears in some tuple of the borrower relation as a borrower<br>from the bank.<br>• The customer-name appears in some tuple of the depositor relation as a deposi-<br>tor of the bank.<br>If some customer has both a loan and an account at the bank, that customer appears<br>only once in the result, because the mathematical deﬁnition of a set does not allow<br>duplicate members. The result of this query appeared earlier in Figure 3.12.<br>If we now want only those customers who have both an account and a loan at the<br>bank, all we need to do is to change the or (∨) to and (∧) in the preceding expression.<br>{t | ∃s ∈borrower (t[customer-name] = s[customer-name])<br>∧∃u ∈depositor (t[customer-name] = u[customer-name])}<br>The result of this query appeared in Figure 3.20.<br>Now consider the query “Find all customers who have an account at the bank but<br>do not have a loan from the bank.” The tuple-relational-calculus expression for this<br>query is similar to the expressions that we have just seen, except for the use of the not<br>(¬) symbol:<br>{t | ∃u ∈depositor (t[customer-name] = u[customer-name])<br>∧¬ ∃s ∈borrower (t[customer-name] = s[customer-name])}<br>customer-name<br>Adams<br>Hayes<br>Figure 3.37<br>Names of all customers who have a loan at the Perryridge branch.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>I. Data Models<br>3. Relational Model<br>128<br>© The McGraw−Hill <br>Companies, 2001<br>120<br>Chapter 3<br>Relational Model<br>This tuple-relational-calculus expression uses the ∃u<br>∈<br>depositor (. . .) clause to<br>require that the customer have an account at the bank, and it uses the ¬ ∃s<br>∈<br>borrower (. . .) clause to eliminate those customers who appear in some tuple of the<br>borrower relation as having a loan from the bank. The result of this query appeared in<br>Figure 3.13.<br>The query that we shall consider next uses implication, denoted by ⇒. The formula<br>P<br>⇒Q means “P implies Q”; that is, “if P is true, then Q must be true.” Note that<br>P ⇒Q is logically equivalent to ¬P ∨Q. The use of implication rather than not and<br>or often suggests a more intuitive interpretation of a query in English.<br>Consider the query that we used in Section 3.2.3 to illustrate the division opera-<br>tion: “Find all customers who have an account at all branches located in Brooklyn.” To<br>write this query in the tuple relational calculus, we introduce the “for all” construct,<br>denoted by ∀. The notation<br>∀t ∈r (Q(t))<br>means “Q is true for all tuples t in relation r.”<br>We write the expression for our query as follows:<br>{t | ∃r ∈customer (r[customer-name] = t[customer-name]) ∧<br>( ∀u ∈branch (u[branch-city] = “ Brooklyn” ⇒<br>∃s ∈depositor (t[customer-name] = s[customer-name]<br>∧∃w ∈account (w[account-number] = s[account-number]<br>∧w[branch-name] = u[branch-name]))))}<br>In English, we interpret this expression as “The set of all customers (that is, (customer-<br>name) tuples t) such that, for all tuples u in the branch relation, if the value of u on at-<br>tribute branch-city is Brooklyn, then the customer has an account at the branch whose<br>name appears in the branch-name attribute of u.”<br>Note that there is a subtlety in the above query: If there is no branch in Brooklyn,<br>all customer names satisfy the condition. The ﬁrst line of the query expression is crit-<br>ical in this case—without the condition<br>∃r ∈customer (r[customer-name] = t[customer-name])<br>if there is no branch in Brooklyn, any value of t (including values that are not cus-<br>tomer names in the depositor relation) would qualify.<br>3.6.2<br>Formal Deﬁnition<br>We are now ready for a formal deﬁnition. A tuple-relational-calculus expression is of<br>the form<br>{t | P(t)}<br>where P is a formula. Several tuple variables may appear in a formula. A tuple vari-<br>able is said to be a free variable unless it is quantiﬁed by a ∃or ∀. Thus, in<br>t ∈loan ∧∃s ∈customer(t[branch-name] = s[branch-name])<br>t is a free variable. Tuple variable s is said to be a bound variable.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>I. Data Models<br>3. Relational Model<br>129<br>© The McGraw−Hill <br>Companies, 2001<br>3.6<br>The Tuple Relational Calculus<br>121<br>A tuple-relational-calculus formula is built up out of atoms. An atom has one of<br>the following forms:<br>• s ∈r, where s is a tuple variable and r is a relation (we do not allow use of the<br>/∈operator)<br>• s[x] Θ u[y], where s and u are tuple variables, x is an attribute on which s is<br>deﬁned, y is an attribute on which u is deﬁned, and Θ is a comparison operator<br>(&lt;, ≤, =, ̸=, &gt;, ≥); we require that attributes x and y have domains whose<br>members can be compared by Θ<br>• s[x] Θ c, where s is a tuple variable, x is an attribute on which s is deﬁned, Θ is<br>a comparison operator, and c is a constant in the domain of attribute x<br>We build up formulae from atoms by using the following rules:<br>• An atom is a formula.<br>• If P1 is a formula, then so are ¬P1 and (P1).<br>• If P1 and P2 are formulae, then so are P1 ∨P2, P1 ∧P2, and P1 ⇒P2.<br>• If P1(s) is a formula containing a free tuple variable s, and r is a relation, then<br>∃s ∈r (P1(s)) and ∀s ∈r (P1(s))<br>are also formulae.<br>As we could for the relational algebra, we can write equivalent expressions that<br>are not identical in appearance. In the tuple relational calculus, these equivalences<br>include the following three rules:<br>1. P1 ∧P2 is equivalent to ¬ (¬(P1) ∨¬(P2)).<br>2. ∀t ∈r (P1(t)) is equivalent to ¬ ∃t ∈r (¬P1(t)).<br>3. P1 ⇒P2 is equivalent to ¬(P1) ∨P2.<br>3.6.3<br>Safety of Expressions<br>There is one ﬁnal issue to be addressed. A tuple-relational-calculus expression may<br>generate an inﬁnite relation. Suppose that we write the expression<br>{t |¬ (t ∈loan)}<br>There are inﬁnitely many tuples that are not in loan. Most of these tuples contain<br>values that do not even appear in the database! Clearly, we do not wish to allow such<br>expressions.<br>To help us deﬁne a restriction of the tuple relational calculus, we introduce the<br>concept of the domain of a tuple relational formula, P. Intuitively, the domain of<br>P, denoted dom(P), is the set of all values referenced by P. They include values<br>mentioned in P itself, as well as values that appear in a tuple of a relation men-<br>tioned in P. Thus, the domain of P is the set of all values that appear explicitly in<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>I. Data Models<br>3. Relational Model<br>130<br>© The McGraw−Hill <br>Companies, 2001<br>122<br>Chapter 3<br>Relational Model<br>P or that appear in one or more relations whose names appear in P. For example,<br>dom(t ∈loan ∧t[amount] &gt; 1200) is the set containing 1200 as well as the set of all<br>values appearing in loan. Also, dom(¬ (t ∈loan)) is the set of all values appearing<br>in loan, since the relation loan is mentioned in the expression.<br>We say that an expression {t | P(t)} is safe if all values that appear in the result<br>are values from dom(P). The expression {t |¬ (t<br>∈<br>loan)} is not safe. Note that<br>dom(¬ (t ∈loan)) is the set of all values appearing in loan. However, it is possible<br>to have a tuple t not in loan that contains values that do not appear in loan. The other<br>examples of tuple-relational-calculus expressions that we have written in this section<br>are safe.<br>3.6.4<br>Expressive Power of Languages<br>The tuple relational calculus restricted to safe expressions is equivalent in expressive<br>power to the basic relational algebra (with the operators ∪, −, ×, σ, and ρ, but without<br>the extended relational operators such as generalized projection G and the outer-join<br>operations) Thus, for every relational-algebra expression using only the basic opera-<br>tions, there is an equivalent expression in the tuple relational calculus, and for every<br>tuple-relational-calculus expression, there is an equivalent relational-algebra expres-<br>sion. We will not prove this assertion here; the bibliographic notes contain references<br>to the proof. Some parts of the proof are included in the exercises. We note that the<br>tuple relational calculus does not have any equivalent of the aggregate operation, but<br>it can be extended to support aggregation. Extending the tuple relational calculus to<br>handle arithmetic expressions is straightforward.<br>3.7<br>The Domain Relational Calculus∗∗<br>A second form of relational calculus, called domain relational calculus, uses domain<br>variables that take on values from an attributes domain, rather than values for an<br>entire tuple. The domain relational calculus, however, is closely related to the tuple<br>relational calculus.<br>Domain relational calculus serves as the theoretical basis of the widely used QBE<br>language, just as relational algebra serves as the basis for the SQL language.<br>3.7.1<br>Formal Deﬁnition<br>An expression in the domain relational calculus is of the form<br>{&lt; x1, x2, . . . , xn &gt; | P(x1, x2, . . . , xn)}<br>where x1, x2, . . . , xn represent domain variables. P represents a formula composed<br>of atoms, as was the case in the tuple relational calculus. An atom in the domain<br>relational calculus has one of the following forms:<br>• &lt; x1, x2, . . . , xn &gt; ∈r, where r is a relation on n attributes and x1, x2, . . . , xn<br></span><br><br><span style="background-color: #FFADAD;" title="Chunk 16 | Start: 320032 | End: 340032 | Tokens: 3444">are domain variables or domain constants.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>I. Data Models<br>3. Relational Model<br>131<br>© The McGraw−Hill <br>Companies, 2001<br>3.7<br>The Domain Relational Calculus∗∗<br>123<br>• x Θ y, where x and y are domain variables and Θ is a comparison operator<br>(&lt;, ≤, =, ̸=, &gt;, ≥). We require that attributes x and y have domains that can<br>be compared by Θ.<br>• x Θ c, where x is a domain variable, Θ is a comparison operator, and c is a<br>constant in the domain of the attribute for which x is a domain variable.<br>We build up formulae from atoms by using the following rules:<br>• An atom is a formula.<br>• If P1 is a formula, then so are ¬P1 and (P1).<br>• If P1 and P2 are formulae, then so are P1 ∨P2, P1 ∧P2, and P1 ⇒P2.<br>• If P1(x) is a formula in x, where x is a domain variable, then<br>∃x (P1(x)) and ∀x (P1(x))<br>are also formulae.<br>As a notational shorthand, we write<br>∃a, b, c (P(a, b, c))<br>for<br>∃a (∃b (∃c (P(a, b, c))))<br>3.7.2<br>Example Queries<br>We now give domain-relational-calculus queries for the examples that we consid-<br>ered earlier. Note the similarity of these expressions and the corresponding tuple-<br>relational-calculus expressions.<br>• Find the loan number, branch name, and amount for loans of over $1200:<br>{&lt; l, b, a &gt; | &lt; l, b, a &gt; ∈loan ∧a &gt; 1200}<br>• Find all loan numbers for loans with an amount greater than $1200:<br>{&lt; l &gt; | ∃b, a (&lt; l, b, a &gt; ∈loan ∧a &gt; 1200)}<br>Although the second query appears similar to the one that we wrote for the tuple<br>relational calculus, there is an important difference. In the tuple calculus, when we<br>write ∃s for some tuple variable s, we bind it immediately to a relation by writing<br>∃s ∈r. However, when we write ∃b in the domain calculus, b refers not to a tuple,<br>but rather to a domain value. Thus, the domain of variable b is unconstrained until<br>the subformula &lt; l, b, a &gt; ∈loan constrains b to branch names that appear in the<br>loan relation. For example,<br>• Find the names of all customers who have a loan from the Perryridge branch<br>and ﬁnd the loan amount:<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>I. Data Models<br>3. Relational Model<br>132<br>© The McGraw−Hill <br>Companies, 2001<br>124<br>Chapter 3<br>Relational Model<br>{&lt; c, a &gt; | ∃l (&lt; c, l &gt; ∈borrower<br>∧∃b (&lt; l, b, a &gt; ∈loan ∧b = “Perryridge”))}<br>• Find the names of all customers who have a loan, an account, or both at the<br>Perryridge branch:<br>{&lt; c &gt; | ∃l (&lt; c, l &gt; ∈borrower<br>∧∃b, a (&lt; l, b, a &gt; ∈loan ∧b = “Perryridge”))<br>∨∃a (&lt; c, a &gt; ∈depositor<br>∧∃b, n (&lt; a, b, n &gt; ∈account ∧b = “Perryridge”))}<br>• Find the names of all customers who have an account at all the branches lo-<br>cated in Brooklyn:<br>{&lt; c &gt; | ∃n (&lt; c, n &gt; ∈customer) ∧<br>∀x, y, z (&lt; x, y, z &gt; ∈branch ∧y = “Brooklyn” ⇒<br>∃a, b (&lt; a, x, b &gt; ∈account ∧&lt; c, a &gt; ∈depositor))}<br>In English, we interpret this expression as “The set of all (customer-name) tu-<br>ples c such that, for all (branch-name, branch-city, assets) tuples, x, y, z, if the<br>branch city is Brooklyn, then the following is true”:<br>  There exists a tuple in the relation account with account number a and<br>branch name x.<br>  There exists a tuple in the relation depositor with customer c and account<br>number a.”<br>3.7.3<br>Safety of Expressions<br>We noted that, in the tuple relational calculus (Section 3.6), it is possible to write ex-<br>pressions that may generate an inﬁnite relation. That led us to deﬁne safety for tuple-<br>relational-calculus expressions. A similar situation arises for the domain relational<br>calculus. An expression such as<br>{&lt; l, b, a &gt; | ¬(&lt; l, b, a &gt; ∈loan)}<br>is unsafe, because it allows values in the result that are not in the domain of the<br>expression.<br>For the domain relational calculus, we must be concerned also about the form of<br>formulae within “there exists” and “for all” clauses. Consider the expression<br>{&lt; x &gt; | ∃y (&lt; x, y &gt;∈r) ∧∃z (¬(&lt; x, z &gt;∈r) ∧P(x, z))}<br>where P is some formula involving x and z. We can test the ﬁrst part of the formula,<br>∃y (&lt; x, y &gt; ∈r), by considering only the values in r. However, to test the second<br>part of the formula, ∃z (¬ (&lt; x, z &gt; ∈r) ∧P(x, z)), we must consider values for<br>z that do not appear in r. Since all relations are ﬁnite, an inﬁnite number of values<br>do not appear in r. Thus, it is not possible, in general, to test the second part of the<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>I. Data Models<br>3. Relational Model<br>133<br>© The McGraw−Hill <br>Companies, 2001<br>3.7<br>The Domain Relational Calculus∗∗<br>125<br>formula, without considering an inﬁnite number of potential values for z. Instead,<br>we add restrictions to prohibit expressions such as the preceding one.<br>In the tuple relational calculus, we restricted any existentially quantiﬁed variable<br>to range over a speciﬁc relation. Since we did not do so in the domain calculus, we<br>add rules to the deﬁnition of safety to deal with cases like our example. We say that<br>an expression<br>{&lt; x1, x2, . . . , xn &gt; | P (x1, x2, . . . , xn)}<br>is safe if all of the following hold:<br>1. All values that appear in tuples of the expression are values from dom(P).<br>2. For every “there exists” subformula of the form ∃x (P1(x)), the subformula is<br>true if and only if there is a value x in dom(P1) such that P1(x) is true.<br>3. For every “for all” subformula of the form ∀x (P1(x)), the subformula is true<br>if and only if P1(x) is true for all values x from dom(P1).<br>The purpose of the additional rules is to ensure that we can test “for all” and “there<br>exists” subformulae without having to test inﬁnitely many possibilities. Consider the<br>second rule in the deﬁnition of safety. For ∃x (P1(x)) to be true, we need to ﬁnd only<br>one x for which P1(x) is true. In general, there would be inﬁnitely many values to<br>test. However, if the expression is safe, we know that we can restrict our attention to<br>values from dom(P1). This restriction reduces to a ﬁnite number the tuples we must<br>consider.<br>The situation for subformulae of the form ∀x (P1(x)) is similar. To assert that<br>∀x (P1(x)) is true, we must, in general, test all possible values, so we must exam-<br>ine inﬁnitely many values. As before, if we know that the expression is safe, it is<br>sufﬁcient for us to test P1(x) for those values taken from dom(P1).<br>All the domain-relational-calculus expressions that we have written in the exam-<br>ple queries of this section are safe.<br>3.7.4<br>Expressive Power of Languages<br>When the domain relational calculus is restricted to safe expressions, it is equivalent<br>in expressive power to the tuple relational calculus restricted to safe expressions.<br>Since we noted earlier that the restricted tuple relational calculus is equivalent to the<br>relational algebra, all three of the following are equivalent:<br>• The basic relational algebra (without the extended relational algebra opera-<br>tions)<br>• The tuple relational calculus restricted to safe expressions<br>• The domain relational calculus restricted to safe expressions<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>I. Data Models<br>3. Relational Model<br>134<br>© The McGraw−Hill <br>Companies, 2001<br>126<br>Chapter 3<br>Relational Model<br>We note that the domain relational calculus also does not have any equivalent of the<br>aggregate operation, but it can be extended to support aggregation, and extending it<br>to handle arithmatic expressions is straightforward.<br>3.8<br>Summary<br>• The relational data model is based on a collection of tables. The user of the<br>database system may query these tables, insert new tuples, delete tuples, and<br>update (modify) tuples. There are several languages for expressing these op-<br>erations.<br>• The relational algebra deﬁnes a set of algebraic operations that operate on<br>tables, and output tables as their results. These operations can be combined<br>to get expressions that express desired queries. The algebra deﬁnes the basic<br>operations used within relational query languages.<br>• The operations in relational algebra can be divided into<br>  Basic operations<br>  Additional operations that can be expressed in terms of the basic opera-<br>tions<br>  Extended operations, some of which add further expressive power to re-<br>lational algebra<br>• Databases can be modiﬁed by insertion, deletion, or update of tuples. We<br>used the relational algebra with the assignment operator to express these<br>modiﬁcations.<br>• Different users of a shared database may beneﬁt from individualized views of<br>the database. Views are “virtual relations” deﬁned by a query expression. We<br>evaluate queries involving views by replacing the view with the expression<br>that deﬁnes the view.<br>• Views are useful mechanisms for simplifying database queries, but modiﬁca-<br>tion of the database through views may cause problems. Therefore, database<br>systems severely restrict updates through views.<br>• For reasons of query-processing efﬁciency, a view may be materialized—that<br>is, the query is evaluated and the result stored physically. When database re-<br>lations are updated, the materialized view must be correspondingly updated.<br>• The tuple relational calculus and the domain relational calculus are non-<br>procedural languages that represent the basic power required in a relational<br>query language. The basic relational algebra is a procedural language that is<br>equivalent in power to both forms of the relational calculus when they are<br>restricted to safe expressions.<br>• The relational algebra and the relational calculi are terse, formal languages<br>that are inappropriate for casual users of a database system. Commercial data-<br>base systems, therefore, use languages with more “syntactic sugar.” In Chap-<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>I. Data Models<br>3. Relational Model<br>135<br>© The McGraw−Hill <br>Companies, 2001<br>Exercises<br>127<br>ters 4 and 5, we shall consider the three most inﬂuential languages: SQL,<br>which is based on relational algebra, and QBE and Datalog, which are based<br>on domain relational calculus.<br>Review Terms<br>• Table<br>• Relation<br>• Tuple variable<br>• Atomic domain<br>• Null value<br>• Database schema<br>• Database instance<br>• Relation schema<br>• Relation instance<br>• Keys<br>• Foreign key<br>  Referencing relation<br>  Referenced relation<br>• Schema diagram<br>• Query language<br>• Procedural language<br>• Nonprocedural language<br>• Relational algebra<br>• Relational algebra operations<br>  Select σ<br>  Project Π<br>  Union ∪<br>  Set difference −<br>  Cartesian product ×<br>  Rename ρ<br>• Additional operations<br>  Set-intersection ∩<br>  Natural-join<br><br>  Division /<br>• Assignment operation<br>• Extended relational-algebra<br>operations<br>  Generalized projection Π<br>  Outer join<br>–– Left outer join<br><br>–– Right outer join<br><br>–– Full outer join<br><br>  Aggregation G<br>• Multisets<br>• Grouping<br>• Null values<br>• Modiﬁcation of the database<br>  Deletion<br>  Insertion<br>  Updating<br>• Views<br>• View deﬁnition<br>• Materialized views<br>• View update<br>• View expansion<br>• Recursive views<br>• Tuple relational calculus<br>• Domain relational calculus<br>• Safety of expressions<br>• Expressive power of languages<br>Exercises<br>3.1 Design a relational database for a university registrar’s ofﬁce. The ofﬁce main-<br>tains data about each class, including the instructor, the number of students<br>enrolled, and the time and place of the class meetings. For each student–class<br>pair, a grade is recorded.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>I. Data Models<br>3. Relational Model<br>136<br>© The McGraw−Hill <br>Companies, 2001<br>128<br>Chapter 3<br>Relational Model<br>person<br>owns<br>car<br>participated<br>accident<br>address<br>damage-amount<br>model<br>year<br>license<br>name<br>report-number<br>date<br>location<br>driver-id<br>driver<br>Figure 3.38<br>E-R diagram.<br>3.2 Describe the differences in meaning between the terms relation and relation schema.<br>Illustrate your answer by referring to your solution to Exercise 3.1.<br>3.3 Design a relational database corresponding to the E-R diagram of Figure 3.38.<br>3.4 In Chapter 2, we saw how to represent many-to-many, many-to-one, one-to-<br>many, and one-to-one relationship sets. Explain how primary keys help us to<br>represent such relationship sets in the relational model.<br>3.5 Consider the relational database of Figure 3.39, where the primary keys are un-<br>derlined. Give an expression in the relational algebra to express each of the fol-<br>lowing queries:<br>a. Find the names of all employees who work for First Bank Corporation.<br>b. Find the names and cities of residence of all employees who work for First<br>Bank Corporation.<br>c. Find the names, street address, and cities of residence of all employees who<br>work for First Bank Corporation and earn more than $10,000 per annum.<br>d. Find the names of all employees in this database who live in the same city<br>as the company for which they work.<br>e. Find the names of all employees who live in the same city and on the same<br>street as do their managers.<br>f. Find the names of all employees in this database who do not work for First<br>Bank Corporation.<br>g. Find the names of all employees who earn more than every employee of<br>Small Bank Corporation.<br>h. Assume the companies may be located in several cities. Find all companies<br>located in every city in which Small Bank Corporation is located.<br>3.6 Consider the relation of Figure 3.21, which shows the result of the query “Find<br>the names of all customers who have a loan at the bank.” Rewrite the query<br>to include not only the name, but also the city of residence for each customer.<br>Observe that now customer Jackson no longer appears in the result, even though<br>Jackson does in fact have a loan from the bank.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>I. Data Models<br>3. Relational Model<br>137<br>© The McGraw−Hill <br>Companies, 2001<br>Exercises<br>129<br>employee (person-name, street, city)<br>works (person-name, company-name, salary)<br>company (company-name, city)<br>manages (person-name, manager-name)<br>Figure 3.39<br>Relational database for Exercises 3.5, 3.8 and 3.10.<br>a. Explain why Jackson does not appear in the result.<br>b. Suppose that you want Jackson to appear in the result. How would you<br>modify the database to achieve this effect?<br>c. Again, suppose that you want Jackson to appear in the result. Write a query<br>using an outer join that accomplishes this desire without your having to<br>modify the database.<br>3.7 The outer-join operations extend the natural-join operation so that tuples from<br>the participating relations are not lost in the result of the join. Describe how the<br>theta join operation can be extended so that tuples from the left, right, or both<br>relations are not lost from the result of a theta join.<br>3.8 Consider the relational database of Figure 3.39. Give an expression in the rela-<br>tional algebra for each request:<br>a. Modify the database so that Jones now lives in Newtown.<br>b. Give all employees of First Bank Corporation a 10 percent salary raise.<br>c. Give all managers in this database a 10 percent salary raise.<br>d. Give all managers in this database a 10 percent salary raise, unless the salary<br>would be greater than $100,000. In such cases, give only a 3 percent raise.<br>e. Delete all tuples in the works relation for employees of Small Bank Corpora-<br>tion.<br>3.9 Using the bank example, write relational-algebra queries to ﬁnd the accounts<br>held by more than two customers in the following ways:<br>a. Using an aggregate function.<br>b. Without using any aggregate functions.<br>3.10 Consider the relational database of Figure 3.39. Give a relational-algebra expres-<br>sion for each of the following queries:<br>a. Find the company with the most employees.<br>b. Find the company with the smallest payroll.<br>c. Find those companies whose employees earn a higher salary, on average,<br>than the average salary at First Bank Corporation.<br>3.11 List two reasons why we may choose to deﬁne a view.<br>3.12 List two major problems with processing update operations expressed in terms<br>of views.<br>3.13 Let the following relation schemas be given:<br>R<br>= (A, B, C)<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>I. Data Models<br>3. Relational Model<br>138<br>© The McGraw−Hill <br>Companies, 2001<br>130<br>Chapter 3<br>Relational Model<br>S<br>= (D, E, F)<br>Let relations r(R) and s(S) be given. Give an expression in the tuple relational<br>calculus that is equivalent to each of the following:<br>a. ΠA(r)<br>b. σB = 17 (r)<br>c. r × s<br>d. ΠA,F (σC = D(r × s))<br>3.14 Let R<br>=<br>(A, B, C), and let r1 and r2 both be relations on schema R. Give<br>an expression in the domain relational calculus that is equivalent to each of the<br>following:<br>a. ΠA(r1)<br>b. σB = 17 (r1)<br>c. r1 ∪r2<br>d. r1 ∩r2<br>e. r1 −r2<br>f. ΠA,B(r1)<br> ΠB,C(r2)<br>3.15 Repeat Exercise 3.5 using the tuple relational calculus and the domain relational<br>calculus.<br>3.16 Let R = (A, B) and S = (A, C), and let r(R) and s(S) be relations. Write<br>relational-algebra expressions equivalent to the following domain-relational-<br>calculus expressions:<br>a. {&lt; a &gt; | ∃b (&lt; a, b &gt; ∈r ∧b = 17)}<br>b. {&lt; a, b, c &gt; | &lt; a, b &gt; ∈r ∧&lt; a, c &gt; ∈s}<br>c. {&lt; a &gt; | ∃b (&lt; a, b &gt; ∈r) ∨∀c (∃d (&lt; d, c &gt; ∈s) ⇒&lt; a, c &gt; ∈s)}<br>d. {&lt; a &gt;<br>| ∃c (&lt; a, c &gt; ∈<br>s ∧∃b1, b2 (&lt; a, b1 &gt; ∈<br>r ∧<br>&lt; c, b2 &gt;<br>∈r ∧b1 &gt; b2))}<br>3.17 Let R = (A, B) and S = (A, C), and let r(R) and s(S) be relations. Using<br>the special constant null, write tuple-relational-calculus expressions equivalent<br>to each of the following:<br>a. r<br> s<br>b. r<br> s<br>c. r<br> s<br>3.18 List two reasons why null values might be introduced into the database.<br>3.19 Certain systems allow marked nulls. A marked null ⊥i is equal to itself, but if<br>i ̸= j, then ⊥i ̸= ⊥j. One application of marked nulls is to allow certain updates<br>through views. Consider the view loan-info (Section 3.5). Show how you can use<br>marked nulls to allow the insertion of the tuple (“Johnson”, 1900) through loan-<br>info.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>I. Data Models<br>3. Relational Model<br>139<br>© The McGraw−Hill <br>Companies, 2001<br>Bibliographical Notes<br>131<br>Bibliographical Notes<br>E. F. Codd of the IBM San Jose Research Laboratory proposed the relational model in<br>the late 1960s; Codd [1970]. This work led to the prestigious ACM Turing Award to<br>Codd in 1981; Codd [1982].<br>After Codd published his original paper, several research projects were formed<br>with the goal of constructing practical relational database systems, including System<br>R at the IBM San Jose Research Laboratory, Ingres at the University of California at<br>Berkeley, Query-by-Example at the IBM T. J. Watson Research Center, and the Pe-<br>terlee Relational Test Vehicle (PRTV) at the IBM Scientiﬁc Center in Peterlee, United<br>Kingdom. System R is discussed in Astrahan et al. [1976], Astrahan et al. [1979],<br>and Chamberlin et al. [1981]. Ingres is discussed in Stonebraker [1980], Stonebraker<br>[1986b], and Stonebraker et al. [1976]. Query-by-example is described in Zloof [1977].<br>PRTV is described in Todd [1976].<br>Many relational-database products are now commercially available. These include<br>IBM’s DB2, Ingres, Oracle, Sybase, Informix, and Microsoft SQL Server. Database<br>products for personal computers include Microsoft Access, dBase, and FoxPro. In-<br>formation about the products can be found in their respective manuals.<br>General discussion of the relational data model appears in most database texts.<br>Atzeni and Antonellis [1993] and Maier [1983] are texts devoted exclusively to the<br>relational data model. The original deﬁnition of relational algebra is in Codd [1970];<br>that of tuple relational calculus is in Codd [1972]. A formal proof of the equivalence<br>of tuple relational calculus and relational algebra is in Codd [1972].<br>Several extensions to the relational calculus have been proposed. Klug [1982] and<br>Escobar-Molano et al. [1993] describe extensions to scalar aggregate functions. Ex-<br>tensions to the relational model and discussions of incorporation of null values in<br>the relational algebra (the RM/T model), as well as outer joins, are in Codd [1979].<br>Codd [1990] is a compendium of E. F. Codd’s papers on the relational model. Outer<br>joins are also discussed in Date [1993b]. The problem of updating relational databases<br>through views is addressed by Bancilhon and Spyratos [1981], Cosmadakis and Pa-<br>padimitriou [1984], Dayal and Bernstein [1978], and Langerak [1990]. Section 14.5<br>covers materialized view maintenance, and references to literature on view mainte-<br>nance can be </span><br><br><span style="background-color: #FFD6A5;" title="Chunk 17 | Start: 340034 | End: 360034 | Tokens: 3195">found at the end of that chapter.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>II. Relational Databases<br>Introduction<br>140<br>© The McGraw−Hill <br>Companies, 2001<br>P<br>A<br>R<br>T<br>2<br>Relational Databases<br>A relational database is a shared repository of data. To make data from a relational<br>database available to users, we have to address several issues. One is how users spec-<br>ify requests for data: Which of the various query languages do they use? Chapter 4<br>covers the SQL language, which is the most widely used query language today. Chap-<br>ter 5 covers two other query languages, QBE and Datalog, which offer alternative<br>approaches to querying relational data.<br>Another issue is data integrity and security; databases need to protect data from<br>damage by user actions, whether unintentional or intentional. The integrity main-<br>tenance component of a database ensures that updates do not violate integrity con-<br>straints that have been speciﬁed on the data. The security component of a database<br>includes authentication of users, and access control, to restrict the permissible actions<br>for each user. Chapter 6 covers integrity and security issues. Security and integrity<br>issues are present regardless of the data model, but for concreteness we study them<br>in the context of the relational model. Integrity constraints form the basis of relational<br>database design, which we study in Chapter 7.<br>Relational database design—the design of the relational schema—is the ﬁrst step<br>in building a database application. Schema design was covered informally in ear-<br>lier chapters. There are, however, principles that can be used to distinguish good<br>database designs from bad ones. These are formalized by means of several “normal<br>forms,” which offer different tradeoffs between the possibility of inconsistencies and<br>the efﬁciency of certain queries. Chapter 7 describes the formal design of relational<br>schemas.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>II. Relational Databases<br>4. SQL<br>141<br>© The McGraw−Hill <br>Companies, 2001<br>C<br>H<br>A<br>P<br>T<br>E<br>R<br>4<br>SQL<br>The formal languages described in Chapter 3 provide a concise notation for repre-<br>senting queries. However, commercial database systems require a query language<br>that is more user friendly. In this chapter, we study SQL, the most inﬂuential commer-<br>cially marketed query language, SQL. SQL uses a combination of relational-algebra<br>and relational-calculus constructs.<br>Although we refer to the SQL language as a “query language,” it can do much<br>more than just query a database. It can deﬁne the structure of the data, modify data<br>in the database, and specify security constraints.<br>It is not our intention to provide a complete users’ guide for SQL. Rather, we<br>present SQL’s fundamental constructs and concepts. Individual implementations of<br>SQL may differ in details, or may support only a subset of the full language.<br>4.1<br>Background<br>IBM developed the original version of SQL at its San Jose Research Laboratory (now<br>the Almaden Research Center). IBM implemented the language, originally called Se-<br>quel, as part of the System R project in the early 1970s. The Sequel language has<br>evolved since then, and its name has changed to SQL (Structured Query Language).<br>Many products now support the SQL language. SQL has clearly established itself as<br>the standard relational-database language.<br>In 1986, the American National Standards Institute (ANSI) and the International<br>Organization for Standardization (ISO) published an SQL standard, called SQL-86.<br>IBM published its own corporate SQL standard, the Systems Application Architec-<br>ture Database Interface (SAA-SQL) in 1987. ANSI published an extended standard for<br>SQL, SQL-89, in 1989. The next version of the standard was SQL-92 standard, and the<br>most recent version is SQL:1999. The bibliographic notes provide references to these<br>standards.<br>135<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>II. Relational Databases<br>4. SQL<br>142<br>© The McGraw−Hill <br>Companies, 2001<br>136<br>Chapter 4<br>SQL<br>In this chapter, we present a survey of SQL, based mainly on the widely imple-<br>mented SQL-92 standard. The SQL:1999 standard is a superset of the SQL-92 standard;<br>we cover some features of SQL:1999 in this chapter, and provide more detailed cov-<br>erage in Chapter 9. Many database systems support some of the new constructs in<br>SQL:1999, although currently no database system supports all the new constructs. You<br>should also be aware that some database systems do not even support all the fea-<br>tures of SQL-92, and that many databases provide nonstandard features that we do<br>not cover here.<br>The SQL language has several parts:<br>• Data-deﬁnition language (DDL). The SQL DDL provides commands for deﬁn-<br>ing relation schemas, deleting relations, and modifying relation schemas.<br>• Interactive data-manipulation language (DML). The SQL DML includes a<br>query language based on both the relational algebra and the tuple relational<br>calculus. It includes also commands to insert tuples into, delete tuples from,<br>and modify tuples in the database.<br>• View deﬁnition. The SQL DDL includes commands for deﬁning views.<br>• Transaction control. SQL includes commands for specifying the beginning<br>and ending of transactions.<br>• Embedded SQL and dynamic SQL. Embedded and dynamic SQL deﬁne how<br>SQL statements can be embedded within general-purpose programming lan-<br>guages, such as C, C++, Java, PL/I, Cobol, Pascal, and Fortran.<br>• Integrity. The SQL DDL includes commands for specifying integrity constraints<br>that the data stored in the database must satisfy. Updates that violate integrity<br>constraints are disallowed.<br>• Authorization. The SQL DDL includes commands for specifying access rights<br>to relations and views.<br>In this chapter, we cover the DML and the basic DDL features of SQL. We also<br>brieﬂy outline embedded and dynamic SQL, including the ODBC and JDBC standards<br>for interacting with a database from programs written in the C and Java languages.<br>SQL features supporting integrity and authorization are described in Chapter 6, while<br>Chapter 9 outlines object-oriented extensions to SQL.<br>The enterprise that we use in the examples in this chapter, and later chapters, is a<br>banking enterprise with the following relation schemas:<br>Branch-schema = (branch-name, branch-city, assets)<br>Customer-schema = (customer-name, customer-street, customer-city)<br>Loan-schema = (loan-number, branch-name, amount)<br>Borrower-schema = (customer-name, loan-number)<br>Account-schema = (account-number, branch-name, balance)<br>Depositor-schema = (customer-name, account-number)<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>II. Relational Databases<br>4. SQL<br>143<br>© The McGraw−Hill <br>Companies, 2001<br>4.2<br>Basic Structure<br>137<br>Note that in this chapter, as elsewhere in the text, we use hyphenated names for<br>schema, relations, and attributes for ease of reading. In actual SQL systems, however,<br>hyphens are not valid parts of a name (they are treated as the minus operator). A<br>simple way of translating the names we use to valid SQL names is to replace all hy-<br>phens by the underscore symbol (“ ”). For example, we use branch name in place of<br>branch-name.<br>4.2<br>Basic Structure<br>A relational database consists of a collection of relations, each of which is assigned<br>a unique name. Each relation has a structure similar to that presented in Chapter 3.<br>SQL allows the use of null values to indicate that the value either is unknown or does<br>not exist. It allows a user to specify which attributes cannot be assigned null values,<br>as we shall discuss in Section 4.11.<br>The basic structure of an SQL expression consists of three clauses: select, from, and<br>where.<br>• The select clause corresponds to the projection operation of the relational al-<br>gebra. It is used to list the attributes desired in the result of a query.<br>• The from clause corresponds to the Cartesian-product operation of the rela-<br>tional algebra. It lists the relations to be scanned in the evaluation of the ex-<br>pression.<br>• The where clause corresponds to the selection predicate of the relational alge-<br>bra. It consists of a predicate involving attributes of the relations that appear<br>in the from clause.<br>That the term select has different meaning in SQL than in the relational algebra is an<br>unfortunate historical fact. We emphasize the different interpretations here to mini-<br>mize potential confusion.<br>A typical SQL query has the form<br>select A1, A2, . . . , An<br>from r1, r2, . . . , rm<br>where P<br>Each Ai represents an attribute, and each ri a relation. P is a predicate. The query is<br>equivalent to the relational-algebra expression<br>ΠA1, A2,...,An(σP (r1 × r2 × · · · × rm))<br>If the where clause is omitted, the predicate P is true. However, unlike the result of a<br>relational-algebra expression, the result of the SQL query may contain multiple copies<br>of some tuples; we shall return to this issue in Section 4.2.8.<br>SQL forms the Cartesian product of the relations named in the from clause,<br>performs a relational-algebra selection using the where clause predicate, and then<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>II. Relational Databases<br>4. SQL<br>144<br>© The McGraw−Hill <br>Companies, 2001<br>138<br>Chapter 4<br>SQL<br>projects the result onto the attributes of the select clause. In practice, SQL may con-<br>vert the expression into an equivalent form that can be processed more efﬁciently.<br>However, we shall defer concerns about efﬁciency to Chapters 13 and 14.<br>4.2.1<br>The select Clause<br>The result of an SQL query is, of course, a relation. Let us consider a simple query<br>using our banking example, “Find the names of all branches in the loan relation”:<br>select branch-name<br>from loan<br>The result is a relation consisting of a single attribute with the heading branch-name.<br>Formal query languages are based on the mathematical notion of a relation being<br>a set. Thus, duplicate tuples never appear in relations. In practice, duplicate elimina-<br>tion is time-consuming. Therefore, SQL (like most other commercial query languages)<br>allows duplicates in relations as well as in the results of SQL expressions. Thus, the<br>preceding query will list each branch-name once for every tuple in which it appears in<br>the loan relation.<br>In those cases where we want to force the elimination of duplicates, we insert the<br>keyword distinct after select. We can rewrite the preceding query as<br>select distinct branch-name<br>from loan<br>if we want duplicates removed.<br>SQL allows us to use the keyword all to specify explicitly that duplicates are not<br>removed:<br>select all branch-name<br>from loan<br>Since duplicate retention is the default, we will not use all in our examples. To ensure<br>the elimination of duplicates in the results of our example queries, we will use dis-<br>tinct whenever it is necessary. In most queries where distinct is not used, the exact<br>number of duplicate copies of each tuple present in the query result is not important.<br>However, the number is important in certain applications; we return to this issue in<br>Section 4.2.8.<br>The asterisk symbol “ * ” can be used to denote “all attributes.” Thus, the use of<br>loan.* in the preceding select clause would indicate that all attributes of loan are to be<br>selected. A select clause of the form select * indicates that all attributes of all relations<br>appearing in the from clause are selected.<br>The select clause may also contain arithmetic expressions involving the operators<br>+, −, ∗, and / operating on constants or attributes of tuples. For example, the query<br>select loan-number, branch-name, amount * 100<br>from loan<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>II. Relational Databases<br>4. SQL<br>145<br>© The McGraw−Hill <br>Companies, 2001<br>4.2<br>Basic Structure<br>139<br>will return a relation that is the same as the loan relation, except that the attribute<br>amount is multiplied by 100.<br>SQL also provides special data types, such as various forms of the date type, and<br>allows several arithmetic functions to operate on these types.<br>4.2.2<br>The where Clause<br>Let us illustrate the use of the where clause in SQL. Consider the query “Find all loan<br>numbers for loans made at the Perryridge branch with loan amounts greater that<br>$1200.” This query can be written in SQL as:<br>select loan-number<br>from loan<br>where branch-name = ’Perryridge’ and amount &gt; 1200<br>SQL uses the logical connectives and, or, and not—rather than the mathematical<br>symbols ∧, ∨, and ¬ —in the where clause. The operands of the logical connectives<br>can be expressions involving the comparison operators &lt;, &lt;=, &gt;, &gt;=, =, and &lt;&gt;.<br>SQL allows us to use the comparison operators to compare strings and arithmetic<br>expressions, as well as special types, such as date types.<br>SQL includes a between comparison operator to simplify where clauses that spec-<br>ify that a value be less than or equal to some value and greater than or equal to some<br>other value. If we wish to ﬁnd the loan number of those loans with loan amounts<br>between $90,000 and $100,000, we can use the between comparison to write<br>select loan-number<br>from loan<br>where amount between 90000 and 100000<br>instead of<br>select loan-number<br>from loan<br>where amount &lt;= 100000 and amount &gt;= 90000<br>Similarly, we can use the not between comparison operator.<br>4.2.3<br>The from Clause<br>Finally, let us discuss the use of the from clause. The from clause by itself deﬁnes a<br>Cartesian product of the relations in the clause. Since the natural join is deﬁned in<br>terms of a Cartesian product, a selection, and a projection, it is a relatively simple<br>matter to write an SQL expression for the natural join.<br>We write the relational-algebra expression<br>Πcustomer-name, loan-number, amount (borrower<br> loan)<br>for the query “For all customers who have a loan from the bank, ﬁnd their names,<br>loan numbers and loan amount.” In SQL, this query can be written as<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>II. Relational Databases<br>4. SQL<br>146<br>© The McGraw−Hill <br>Companies, 2001<br>140<br>Chapter 4<br>SQL<br>select customer-name, borrower.loan-number, amount<br>from borrower, loan<br>where borrower.loan-number = loan.loan-number<br>Notice that SQL uses the notation relation-name.attribute-name, as does the relational<br>algebra, to avoid ambiguity in cases where an attribute appears in the schema of more<br>than one relation. We could have written borrower.customer-name instead of customer-<br>name in the select clause. However, since the attribute customer-name appears in only<br>one of the relations named in the from clause, there is no ambiguity when we write<br>customer-name.<br>We can extend the preceding query and consider a more complicated case in which<br>we require also that the loan be from the Perryridge branch: “Find the customer<br>names, loan numbers, and loan amounts for all loans at the Perryridge branch.” To<br>write this query, we need to state two constraints in the where clause, connected by<br>the logical connective and:<br>select customer-name, borrower.loan-number, amount<br>from borrower, loan<br>where borrower.loan-number = loan.loan-number and<br>branch-name = ’Perryridge’<br>SQL includes extensions to perform natural joins and outer joins in the from clause.<br>We discuss these extensions in Section 4.10.<br>4.2.4<br>The Rename Operation<br>SQL provides a mechanism for renaming both relations and attributes. It uses the as<br>clause, taking the form:<br>old-name as new-name<br>The as clause can appear in both the select and from clauses.<br>Consider again the query that we used earlier:<br>select customer-name, borrower.loan-number, amount<br>from borrower, loan<br>where borrower.loan-number = loan.loan-number<br>The result of this query is a relation with the following attributes:<br>customer-name, loan-number, amount.<br>The names of the attributes in the result are derived from the names of the attributes<br>in the relations in the from clause.<br>We cannot, however, always derive names in this way, for several reasons: First,<br>two relations in the from clause may have attributes with the same name, in which<br>case an attribute name is duplicated in the result. Second, if we used an arithmetic<br>expression in the select clause, the resultant attribute does not have a name. Third,<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>II. Relational Databases<br>4. SQL<br>147<br>© The McGraw−Hill <br>Companies, 2001<br>4.2<br>Basic Structure<br>141<br>even if an attribute name can be derived from the base relations as in the preced-<br>ing example, we may want to change the attribute name in the result. Hence, SQL<br>provides a way of renaming the attributes of a result relation.<br>For example, if we want the attribute name loan-number to be replaced with the<br>name loan-id, we can rewrite the preceding query as<br>select customer-name, borrower.loan-number as loan-id, amount<br>from borrower, loan<br>where borrower.loan-number = loan.loan-number<br>4.2.5<br>Tuple Variables<br>The as clause is particularly useful in deﬁning the notion of tuple variables, as is<br>done in the tuple relational calculus. A tuple variable in SQL must be associated with<br>a particular relation. Tuple variables are deﬁned in the from clause by way of the as<br>clause. To illustrate, we rewrite the query “For all customers who have a loan from<br>the bank, ﬁnd their names, loan numbers, and loan amount” as<br>select customer-name, T.loan-number, S.amount<br>from borrower as T, loan as S<br>where T.loan-number = S.loan-number<br>Note that we deﬁne a tuple variable in the from clause by placing it after the name of<br>the relation with which it is associated, with the keyword as in between (the keyword<br>as is optional). When we write expressions of the form relation-name.attribute-name,<br>the relation name is, in effect, an implicitly deﬁned tuple variable.<br>Tuple variables are most useful for comparing two tuples in the same relation.<br>Recall that, in such cases, we could use the rename operation in the relational algebra.<br>Suppose that we want the query “Find the names of all branches that have assets<br>greater than at least one branch located in Brooklyn.” We can write the SQL expression<br>select distinct T.branch-name<br>from branch as T, branch as S<br>where T.assets &gt; S.assets and S.branch-city = ’Brooklyn’<br>Observe that we could not use the notation branch.asset, since it would not be clear<br>which reference to branch is intended.<br>SQL permits us to use the notation (v1, v2, . . . , vn) to denote a tuple of arity n con-<br>taining values v1, v2, . . . , vn. The comparison operators can be used on tuples, and<br>the ordering is deﬁned lexicographically. For example, (a1, a2) &lt;= (b1, b2) is true if<br>a1 &lt; b1, or (a1 = b1) ∧(a2 &lt;= b2); similarly, the two tuples are equal if all their<br>attributes are equal.<br>4.2.6<br>String Operations<br>SQL speciﬁes strings by enclosing them in single quotes, for example, ’Perryridge’,<br>as we saw earlier. A single quote character that is part of a string can be speciﬁed by<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>II. Relational Databases<br>4. SQL<br>148<br>© The McGraw−Hill <br>Companies, 2001<br>142<br>Chapter 4<br>SQL<br>using two single quote characters; for example the string “It’s right” can be speciﬁed<br>by ’It”s right’.<br>The most commonly used operation on strings is pattern matching using the op-<br>erator like. We describe patterns by using two special characters:<br>• Percent (%): The % character matches any substring.<br>• Underscore ( ): The<br>character matches any character.<br>Patterns are case sensitive; that is, uppercase characters do not match lowercase char-<br>acters, or vice versa. To illustrate pattern matching, we consider the following exam-<br>ples:<br>• ’Perry%’ matches any string beginning with “Perry”.<br>• ’%idge%’ matches any string containing “idge” as a substring, for example,<br>’Perryridge’, ’Rock Ridge’, ’Mianus Bridge’, and ’Ridgeway’.<br>• ’<br>’ matches any string of exactly three characters.<br>• ’<br>%’ matches any string of at least three characters.<br>SQL expresses patterns by using the like comparison operator. Consider the query<br>“Find the names of all customers whose street address includes the substring ‘Main’.”<br>This query can be written as<br>select customer-name<br>from customer<br>where customer-street like ’%Main%’<br>For pattern</span><br><br><span style="background-color: #FDFFB6;" title="Chunk 18 | Start: 360036 | End: 380036 | Tokens: 3370">s to include the special pattern characters (that is, % and<br>), SQL allows<br>the speciﬁcation of an escape character. The escape character is used immediately<br>before a special pattern character to indicate that the special pattern character is to be<br>treated like a normal character. We deﬁne the escape character for a like comparison<br>using the escape keyword. To illustrate, consider the following patterns, which use a<br>backslash (\) as the escape character:<br>• like ’ab\%cd%’ escape ’\’ matches all strings beginning with “ab%cd”.<br>• like ’ab\\cd%’ escape ’\’ matches all strings beginning with “ab\cd”.<br>SQL allows us to search for mismatches instead of matches by using the not like<br>comparison operator.<br>SQL also permits a variety of functions on character strings, such as concatenat-<br>ing (using “∥”), extracting substrings, ﬁnding the length of strings, converting be-<br>tween uppercase and lowercase, and so on. SQL:1999 also offers a similar to opera-<br>tion, which provides more powerful pattern matching than the like operation; the<br>syntax for specifying patterns is similar to that used in Unix regular expressions.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>II. Relational Databases<br>4. SQL<br>149<br>© The McGraw−Hill <br>Companies, 2001<br>4.2<br>Basic Structure<br>143<br>4.2.7<br>Ordering the Display of Tuples<br>SQL offers the user some control over the order in which tuples in a relation are dis-<br>played. The order by clause causes the tuples in the result of a query to appear in<br>sorted order. To list in alphabetic order all customers who have a loan at the Per-<br>ryridge branch, we write<br>select distinct customer-name<br>from borrower, loan<br>where borrower.loan-number = loan.loan-number and<br>branch-name = ’Perryridge’<br>order by customer-name<br>By default, the order by clause lists items in ascending order. To specify the sort order,<br>we may specify desc for descending order or asc for ascending order. Furthermore,<br>ordering can be performed on multiple attributes. Suppose that we wish to list the<br>entire loan relation in descending order of amount. If several loans have the same<br>amount, we order them in ascending order by loan number. We express this query in<br>SQL as follows:<br>select *<br>from loan<br>order by amount desc, loan-number asc<br>To fulﬁll an order by request, SQL must perform a sort. Since sorting a large num-<br>ber of tuples may be costly, it should be done only when necessary.<br>4.2.8<br>Duplicates<br>Using relations with duplicates offers advantages in several situations. Accordingly,<br>SQL formally deﬁnes not only what tuples are in the result of a query, but also how<br>many copies of each of those tuples appear in the result. We can deﬁne the duplicate<br>semantics of an SQL query using multiset versions of the relational operators. Here,<br>we deﬁne the multiset versions of several of the relational-algebra operators. Given<br>multiset relations r1 and r2,<br>1. If there are c1 copies of tuple t1 in r1, and t1 satisﬁes selection σθ, then there<br>are c1 copies of t1 in σθ(r1).<br>2. For each copy of tuple t1 in r1, there is a copy of tuple ΠA(t1) in ΠA(r1), where<br>ΠA(t1) denotes the projection of the single tuple t1.<br>3. If there are c1 copies of tuple t1 in r1 and c2 copies of tuple t2 in r2, there are<br>c1 ∗c2 copies of the tuple t1.t2 in r1 × r2.<br>For example, suppose that relations r1 with schema (A, B) and r2 with schema (C)<br>are the following multisets:<br>r1 = {(1, a), (2, a)}<br>r2 = {(2), (3), (3)}<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>II. Relational Databases<br>4. SQL<br>150<br>© The McGraw−Hill <br>Companies, 2001<br>144<br>Chapter 4<br>SQL<br>Then ΠB(r1) would be {(a), (a)}, whereas ΠB(r1) × r2 would be<br>{(a, 2), (a, 2), (a, 3), (a, 3), (a, 3), (a, 3)}<br>We can now deﬁne how many copies of each tuple occur in the result of an SQL<br>query. An SQL query of the form<br>select A1, A2, . . . , An<br>from r1, r2, . . . , rm<br>where P<br>is equivalent to the relational-algebra expression<br>ΠA1, A2,...,An(σP (r1 × r2 × · · · × rm))<br>using the multiset versions of the relational operators σ, Π, and ×.<br>4.3<br>Set Operations<br>The SQL operations union, intersect, and except operate on relations and correspond<br>to the relational-algebra operations ∪, ∩, and −. Like union, intersection, and set<br>difference in relational algebra, the relations participating in the operations must be<br>compatible; that is, they must have the same set of attributes.<br>Let us demonstrate how several of the example queries that we considered in<br>Chapter 3 can be written in SQL. We shall now construct queries involving the union,<br>intersect, and except operations of two sets: the set of all customers who have an<br>account at the bank, which can be derived by<br>select customer-name<br>from depositor<br>and the set of customers who have a loan at the bank, which can be derived by<br>select customer-name<br>from borrower<br>We shall refer to the relations obtained as the result of the preceding queries as<br>d and b, respectively.<br>4.3.1<br>The Union Operation<br>To ﬁnd all customers having a loan, an account, or both at the bank, we write<br>(select customer-name<br>from depositor)<br>union<br>(select customer-name<br>from borrower)<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>II. Relational Databases<br>4. SQL<br>151<br>© The McGraw−Hill <br>Companies, 2001<br>4.3<br>Set Operations<br>145<br>The union operation automatically eliminates duplicates, unlike the select clause.<br>Thus, in the preceding query, if a customer—say, Jones—has several accounts or<br>loans (or both) at the bank, then Jones will appear only once in the result.<br>If we want to retain all duplicates, we must write union all in place of union:<br>(select customer-name<br>from depositor)<br>union all<br>(select customer-name<br>from borrower)<br>The number of duplicate tuples in the result is equal to the total number of duplicates<br>that appear in both d and b. Thus, if Jones has three accounts and two loans at the<br>bank, then there will be ﬁve tuples with the name Jones in the result.<br>4.3.2<br>The Intersect Operation<br>To ﬁnd all customers who have both a loan and an account at the bank, we write<br>(select distinct customer-name<br>from depositor)<br>intersect<br>(select distinct customer-name<br>from borrower)<br>The intersect operation automatically eliminates duplicates. Thus, in the preceding<br>query, if a customer—say, Jones—has several accounts and loans at the bank, then<br>Jones will appear only once in the result.<br>If we want to retain all duplicates, we must write intersect all in place of intersect:<br>(select customer-name<br>from depositor)<br>intersect all<br>(select customer-name<br>from borrower)<br>The number of duplicate tuples that appear in the result is equal to the minimum<br>number of duplicates in both d and b. Thus, if Jones has three accounts and two loans<br>at the bank, then there will be two tuples with the name Jones in the result.<br>4.3.3<br>The Except Operation<br>To ﬁnd all customers who have an account but no loan at the bank, we write<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>II. Relational Databases<br>4. SQL<br>152<br>© The McGraw−Hill <br>Companies, 2001<br>146<br>Chapter 4<br>SQL<br>(select distinct customer-name<br>from depositor)<br>except<br>(select customer-name<br>from borrower)<br>The except operation automatically eliminates duplicates. Thus, in the preceding<br>query, a tuple with customer name Jones will appear (exactly once) in the result only<br>if Jones has an account at the bank, but has no loan at the bank.<br>If we want to retain all duplicates, we must write except all in place of except:<br>(select customer-name<br>from depositor)<br>except all<br>(select customer-name<br>from borrower)<br>The number of duplicate copies of a tuple in the result is equal to the number of<br>duplicate copies of the tuple in d minus the number of duplicate copies of the tuple<br>in b, provided that the difference is positive. Thus, if Jones has three accounts and<br>one loan at the bank, then there will be two tuples with the name Jones in the result.<br>If, instead, this customer has two accounts and three loans at the bank, there will be<br>no tuple with the name Jones in the result.<br>4.4<br>Aggregate Functions<br>Aggregate functions are functions that take a collection (a set or multiset) of values as<br>input and return a single value. SQL offers ﬁve built-in aggregate functions:<br>• Average: avg<br>• Minimum: min<br>• Maximum: max<br>• Total: sum<br>• Count: count<br>The input to sum and avg must be a collection of numbers, but the other operators<br>can operate on collections of nonnumeric data types, such as strings, as well.<br>As an illustration, consider the query “Find the average account balance at the<br>Perryridge branch.” We write this query as follows:<br>select avg (balance)<br>from account<br>where branch-name = ’Perryridge’<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>II. Relational Databases<br>4. SQL<br>153<br>© The McGraw−Hill <br>Companies, 2001<br>4.4<br>Aggregate Functions<br>147<br>The result of this query is a relation with a single attribute, containing a single tu-<br>ple with a numerical value corresponding to the average balance at the Perryridge<br>branch. Optionally, we can give a name to the attribute of the result relation by using<br>the as clause.<br>There are circumstances where we would like to apply the aggregate function not<br>only to a single set of tuples, but also to a group of sets of tuples; we specify this wish<br>in SQL using the group by clause. The attribute or attributes given in the group by<br>clause are used to form groups. Tuples with the same value on all attributes in the<br>group by clause are placed in one group.<br>As an illustration, consider the query “Find the average account balance at each<br>branch.” We write this query as follows:<br>select branch-name, avg (balance)<br>from account<br>group by branch-name<br>Retaining duplicates is important in computing an average. Suppose that the ac-<br>count balances at the (small) Brighton branch are $1000, $3000, $2000, and $1000. The<br>average balance is $7000/4 = $1750.00. If duplicates were eliminated, we would ob-<br>tain the wrong answer ($6000/3 = $2000).<br>There are cases where we must eliminate duplicates before computing an aggre-<br>gate function. If we do want to eliminate duplicates, we use the keyword distinct in<br>the aggregate expression. An example arises in the query “Find the number of de-<br>positors for each branch.” In this case, a depositor counts only once, regardless of the<br>number of accounts that depositor may have. We write this query as follows:<br>select branch-name, count (distinct customer-name)<br>from depositor, account<br>where depositor.account-number = account.account-number<br>group by branch-name<br>At times, it is useful to state a condition that applies to groups rather than to tu-<br>ples. For example, we might be interested in only those branches where the average<br>account balance is more than $1200. This condition does not apply to a single tuple;<br>rather, it applies to each group constructed by the group by clause. To express such a<br>query, we use the having clause of SQL. SQL applies predicates in the having clause<br>after groups have been formed, so aggregate functions may be used. We express this<br>query in SQL as follows:<br>select branch-name, avg (balance)<br>from account<br>group by branch-name<br>having avg (balance) &gt; 1200<br>At times, we wish to treat the entire relation as a single group. In such cases, we<br>do not use a group by clause. Consider the query “Find the average balance for all<br>accounts.” We write this query as follows:<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>II. Relational Databases<br>4. SQL<br>154<br>© The McGraw−Hill <br>Companies, 2001<br>148<br>Chapter 4<br>SQL<br>select avg (balance)<br>from account<br>We use the aggregate function count frequently to count the number of tuples in<br>a relation. The notation for this function in SQL is count (*). Thus, to ﬁnd the number<br>of tuples in the customer relation, we write<br>select count (*)<br>from customer<br>SQL does not allow the use of distinct with count(*). It is legal to use distinct with<br>max and min, even though the result does not change. We can use the keyword all<br>in place of distinct to specify duplicate retention, but, since all is the default, there is<br>no need to do so.<br>If a where clause and a having clause appear in the same query, SQL applies the<br>predicate in the where clause ﬁrst. Tuples satisfying the where predicate are then<br>placed into groups by the group by clause. SQL then applies the having clause, if it<br>is present, to each group; it removes the groups that do not satisfy the having clause<br>predicate. The select clause uses the remaining groups to generate tuples of the result<br>of the query.<br>To illustrate the use of both a having clause and a where clause in the same query,<br>we consider the query “Find the average balance for each customer who lives in<br>Harrison and has at least three accounts.”<br>select depositor.customer-name, avg (balance)<br>from depositor, account, customer<br>where depositor.account-number = account.account-number and<br>depositor.customer-name = customer.customer-name and<br>customer-city = ’Harrison’<br>group by depositor.customer-name<br>having count (distinct depositor.account-number) &gt;= 3<br>4.5<br>Null Values<br>SQL allows the use of null values to indicate absence of information about the value<br>of an attribute.<br>We can use the special keyword null in a predicate to test for a null value. Thus,<br>to ﬁnd all loan numbers that appear in the loan relation with null values for amount,<br>we write<br>select loan-number<br>from loan<br>where amount is null<br>The predicate is not null tests for the absence of a null value.<br>The use of a null value in arithmetic and comparison operations causes several<br>complications. In Section 3.3.4 we saw how null values are handled in the relational<br>algebra. We now outline how SQL handles null values.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>II. Relational Databases<br>4. SQL<br>155<br>© The McGraw−Hill <br>Companies, 2001<br>4.6<br>Nested Subqueries<br>149<br>The result of an arithmetic expression (involving, for example +, −, ∗or /) is null<br>if any of the input values is null. SQL treats as unknown the result of any comparison<br>involving a null value (other than is null and is not null).<br>Since the predicate in a where clause can involve Boolean operations such as and,<br>or, and not on the results of comparisons, the deﬁnitions of the Boolean operations<br>are extended to deal with the value unknown, as outlined in Section 3.3.4.<br>• and: The result of true and unknown is unknown, false and unknown is false,<br>while unknown and unknown is unknown.<br>• or: The result of true or unknown is true, false or unknown is unknown, while<br>unknown or unknown is unknown.<br>• not: The result of not unknown is unknown.<br>SQL deﬁnes the result of an SQL statement of the form<br>select . . . from R1, · · · , Rn where P<br>to contain (projections of) tuples in R1 × · · · × Rn for which predicate P evaluates to<br>true. If the predicate evaluates to either false or unknown for a tuple in R1 ×· · ·×Rn<br>(the projection of) the tuple is not added to the result.<br>SQL also allows us to test whether the result of a comparison is unknown, rather<br>than true or false, by using the clauses is unknown and is not unknown.<br>Null values, when they exist, also complicate the processing of aggregate opera-<br>tors. For example, assume that some tuples in the loan relation have a null value for<br>amount. Consider the following query to total all loan amounts:<br>select sum (amount)<br>from loan<br>The values to be summed in the preceding query include null values, since some<br>tuples have a null value for amount. Rather than say that the overall sum is itself null,<br>the SQL standard says that the sum operator should ignore null values in its input.<br>In general, aggregate functions treat nulls according to the following rule: All ag-<br>gregate functions except count(*) ignore null values in their input collection. As a<br>result of null values being ignored, the collection of values may be empty. The count<br>of an empty collection is deﬁned to be 0, and all other aggregate operations return a<br>value of null when applied on an empty collection. The effect of null values on some<br>of the more complicated SQL constructs can be subtle.<br>A boolean type data, which can take values true, false, and unknown, was in-<br>troduced in SQL:1999. The aggregate functions some and every, which mean exactly<br>what you would intuitively expect, can be applied on a collection of Boolean values.<br>4.6<br>Nested Subqueries<br>SQL provides a mechanism for nesting subqueries. A subquery is a select-from-<br>where expression that is nested within another query. A common use of subqueries<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>II. Relational Databases<br>4. SQL<br>156<br>© The McGraw−Hill <br>Companies, 2001<br>150<br>Chapter 4<br>SQL<br>is to perform tests for set membership, make set comparisons, and determine set car-<br>dinality. We shall study these uses in subsequent sections.<br>4.6.1<br>Set Membership<br>SQL draws on the relational calculus for operations that allow testing tuples for mem-<br>bership in a relation. The in connective tests for set membership, where the set is a<br>collection of values produced by a select clause. The not in connective tests for the<br>absence of set membership. As an illustration, reconsider the query “Find all cus-<br>tomers who have both a loan and an account at the bank.” Earlier, we wrote such a<br>query by intersecting two sets: the set of depositors at the bank, and the set of bor-<br>rowers from the bank. We can take the alternative approach of ﬁnding all account<br>holders at the bank who are members of the set of borrowers from the bank. Clearly,<br>this formulation generates the same results as the previous one did, but it leads us<br>to write our query using the in connective of SQL. We begin by ﬁnding all account<br>holders, and we write the subquery<br>(select customer-name<br>from depositor)<br>We then need to ﬁnd those customers who are borrowers from the bank and who<br>appear in the list of account holders obtained in the subquery. We do so by nesting<br>the subquery in an outer select. The resulting query is<br>select distinct customer-name<br>from borrower<br>where customer-name in (select customer-name<br>from depositor)<br>This example shows that it is possible to write the same query several ways in<br>SQL. This ﬂexibility is beneﬁcial, since it allows a user to think about the query in<br>the way that seems most natural. We shall see that there is a substantial amount of<br>redundancy in SQL.<br>In the preceding example, we tested membership in a one-attribute relation. It is<br>also possible to test for membership in an arbitrary relation in SQL. We can thus write<br>the query “Find all customers who have both an account and a loan at the Perryridge<br>branch” in yet another way:<br>select distinct customer-name<br>from borrower, loan<br>where borrower.loan-number = loan.loan-number and<br>branch-name = ’Perryridge’ and<br>(branch-name, customer-name) in<br>(select branch-name, customer-name<br>from depositor, account<br>where depositor.account-number = account.account-number)<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>II. Relational Databases<br>4. SQL<br>157<br>© The McGraw−Hill <br>Companies, 2001<br>4.6<br>Nested Subqueries<br>151<br>We use the not in construct in a similar way. For example, to ﬁnd all customers<br>who do have a loan at the bank, but do not have an account at the bank, we can write<br>select distinct customer-name<br>from borrower<br>where customer-name not in (select customer-name<br>from depositor)<br>The in and not in operators can also be used on enumerated sets. The following<br>query selects the names of customers who have a loan at the bank, and whose names<br>are neither Smith nor Jones.<br>select distinct customer-name<br>from borrower<br>where customer-name not in (’Smith’, ’Jones’)<br>4.6.2<br>Set Comparison<br>As an example of the ability of a nested subquery to compare sets, consider the query<br>“Find the names of all branches that have assets greater than those of at least one<br>branch located in Brooklyn.” In Section 4.2.5, we wrote this query as follows:<br>select distinct T.branch-name<br>from branch as T, branch as S<br>where T.assets &gt; S.assets and S.branch-city = ’Brooklyn’<br>SQL does, however, offer an alternative style for writing the preceding query. The<br>phras</span><br><br><span style="background-color: #CAFFBF;" title="Chunk 19 | Start: 380038 | End: 400038 | Tokens: 3247">e “greater than at least one” is represented in SQL by &gt; some. This construct<br>allows us to rewrite the query in a form that resembles closely our formulation of the<br>query in English.<br>select branch-name<br>from branch<br>where assets &gt; some (select assets<br>from branch<br>where branch-city = ’Brooklyn’)<br>The subquery<br>(select assets<br>from branch<br>where branch-city = ’Brooklyn’)<br>generates the set of all asset values for all branches in Brooklyn. The &gt; some<br>comparison in the where clause of the outer select is true if the assets value of the<br>tuple is greater than at least one member of the set of all asset values for branches in<br>Brooklyn.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>II. Relational Databases<br>4. SQL<br>158<br>© The McGraw−Hill <br>Companies, 2001<br>152<br>Chapter 4<br>SQL<br>SQL also allows &lt; some, &lt;= some, &gt;= some, = some, and &lt;&gt; some comparisons.<br>As an exercise, verify that = some is identical to in, whereas &lt;&gt; some is not the same<br>as not in. The keyword any is synonymous to some in SQL. Early versions of SQL<br>allowed only any. Later versions added the alternative some to avoid the linguistic<br>ambiguity of the word any in English.<br>Now we modify our query slightly. Let us ﬁnd the names of all branches that<br>have an asset value greater than that of each branch in Brooklyn. The construct &gt; all<br>corresponds to the phrase “greater than all.” Using this construct, we write the query<br>as follows:<br>select branch-name<br>from branch<br>where assets &gt; all (select assets<br>from branch<br>where branch-city = ’Brooklyn’)<br>As it does for some, SQL also allows &lt; all, &lt;= all, &gt;= all, = all, and &lt;&gt; all compar-<br>isons. As an exercise, verify that &lt;&gt; all is identical to not in.<br>As another example of set comparisons, consider the query “Find the branch that<br>has the highest average balance.” Aggregate functions cannot be composed in SQL.<br>Thus, we cannot use max (avg (. . .)). Instead, we can follow this strategy: We begin<br>by writing a query to ﬁnd all average balances, and then nest it as a subquery of a<br>larger query that ﬁnds those branches for which the average balance is greater than<br>or equal to all average balances:<br>select branch-name<br>from account<br>group by branch-name<br>having avg (balance) &gt;= all (select avg (balance)<br>from account<br>group by branch-name)<br>4.6.3<br>Test for Empty Relations<br>SQL includes a feature for testing whether a subquery has any tuples in its result. The<br>exists construct returns the value true if the argument subquery is nonempty. Using<br>the exists construct, we can write the query “Find all customers who have both an<br>account and a loan at the bank” in still another way:<br>select customer-name<br>from borrower<br>where exists (select *<br>from depositor<br>where depositor.customer-name = borrower.customer-name)<br>We can test for the nonexistence of tuples in a subquery by using the not ex-<br>ists construct. We can use the not exists construct to simulate the set containment<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>II. Relational Databases<br>4. SQL<br>159<br>© The McGraw−Hill <br>Companies, 2001<br>4.6<br>Nested Subqueries<br>153<br>(that is, superset) operation: We can write “relation A contains relation B” as “not<br>exists (B except A).” (Although it is not part of the SQL-92 and SQL:1999 standards,<br>the contains operator was present in some early relational systems.) To illustrate the<br>not exists operator, consider again the query “Find all customers who have an ac-<br>count at all the branches located in Brooklyn.” For each customer, we need to see<br>whether the set of all branches at which that customer has an account contains the<br>set of all branches in Brooklyn. Using the except construct, we can write the query as<br>follows:<br>select distinct S.customer-name<br>from depositor as S<br>where not exists ((select branch-name<br>from branch<br>where branch-city = ’Brooklyn’)<br>except<br>(select R.branch-name<br>from depositor as T, account as R<br>where T.account-number = R.account-number and<br>S.customer-name = T.customer-name))<br>Here, the subquery<br>(select branch-name<br>from branch<br>where branch-city = ’Brooklyn’)<br>ﬁnds all the branches in Brooklyn. The subquery<br>(select R.branch-name<br>from depositor as T, account as R<br>where T.account-number = R.account-number and<br>S.customer-name = T.customer-name)<br>ﬁnds all the branches at which customer S.customer-name has an account. Thus, the<br>outer select takes each customer and tests whether the set of all branches at which<br>that customer has an account contains the set of all branches located in Brooklyn.<br>In queries that contain subqueries, a scoping rule applies for tuple variables. In<br>a subquery, according to the rule, it is legal to use only tuple variables deﬁned in<br>the subquery itself or in any query that contains the subquery. If a tuple variable<br>is deﬁned both locally in a subquery and globally in a containing query, the local<br>deﬁnition applies. This rule is analogous to the usual scoping rules used for variables<br>in programming languages.<br>4.6.4<br>Test for the Absence of Duplicate Tuples<br>SQL includes a feature for testing whether a subquery has any duplicate tuples in its<br>result. The unique construct returns the value true if the argument subquery contains<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>II. Relational Databases<br>4. SQL<br>160<br>© The McGraw−Hill <br>Companies, 2001<br>154<br>Chapter 4<br>SQL<br>no duplicate tuples. Using the unique construct, we can write the query “Find all<br>customers who have at most one account at the Perryridge branch” as follows:<br>select T.customer-name<br>from depositor as T<br>where unique (select R.customer-name<br>from account, depositor as R<br>where T.customer-name = R.customer-name and<br>R.account-number = account.account-number and<br>account.branch-name = ’Perryridge’)<br>We can test for the existence of duplicate tuples in a subquery by using the not<br>unique construct. To illustrate this construct, consider the query “Find all customers<br>who have at least two accounts at the Perryridge branch,” which we write as<br>select distinct T.customer-name<br>from depositor T<br>where not unique (select R.customer-name<br>from account, depositor as R<br>where T.customer-name = R.customer-name and<br>R.account-number = account.account-number and<br>account.branch-name = ’Perryridge’)<br>Formally, the unique test on a relation is deﬁned to fail if and only if the relation<br>contains two tuples t1 and t2 such that t1 = t2. Since the test t1 = t2 fails if any of the<br>ﬁelds of t1 or t2 are null, it is possible for unique to be true even if there are multiple<br>copies of a tuple, as long as at least one of the attributes of the tuple is null.<br>4.7<br>Views<br>We deﬁne a view in SQL by using the create view command. To deﬁne a view, we<br>must give the view a name and must state the query that computes the view. The<br>form of the create view command is<br>create view v as &lt;query expression&gt;<br>where &lt;query expression&gt; is any legal query expression. The view name is repre-<br>sented by v. Observe that the notation that we used for view deﬁnition in the rela-<br>tional algebra (see Chapter 3) is based on that of SQL.<br>As an example, consider the view consisting of branch names and the names of<br>customers who have either an account or a loan at that branch. Assume that we want<br>this view to be called all-customer. We deﬁne this view as follows:<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>II. Relational Databases<br>4. SQL<br>161<br>© The McGraw−Hill <br>Companies, 2001<br>4.8<br>Complex Queries<br>155<br>create view all-customer as<br>(select branch-name, customer-name<br>from depositor, account<br>where depositor.account-number = account.account-number)<br>union<br>(select branch-name, customer-name<br>from borrower, loan<br>where borrower.loan-number = loan.loan-number)<br>The attribute names of a view can be speciﬁed explicitly as follows:<br>create view branch-total-loan(branch-name, total-loan) as<br>select branch-name, sum(amount)<br>from loan<br>groupby branch-name<br>The preceding view gives for each branch the sum of the amounts of all the loans<br>at the branch. Since the expression sum(amount) does not have a name, the attribute<br>name is speciﬁed explicitly in the view deﬁnition.<br>View names may appear in any place that a relation name may appear. Using the<br>view all-customer, we can ﬁnd all customers of the Perryridge branch by writing<br>select customer-name<br>from all-customer<br>where branch-name = ’Perryridge’<br>4.8<br>Complex Queries<br>Complex queries are often hard or impossible to write as a single SQL block or a<br>union/intersection/difference of SQL blocks. (An SQL block consists of a single select<br>from where statement, possibly with groupby and having clauses.) We study here<br>two ways of composing multiple SQL blocks to express a complex query: derived<br>relations and the with clause.<br>4.8.1<br>Derived Relations<br>SQL allows a subquery expression to be used in the from clause. If we use such an<br>expression, then we must give the result relation a name, and we can rename the<br>attributes. We do this renaming by using the as clause. For example, consider the<br>subquery<br>(select branch-name, avg (balance)<br>from account<br>group by branch-name)<br>as result (branch-name, avg-balance)<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>II. Relational Databases<br>4. SQL<br>162<br>© The McGraw−Hill <br>Companies, 2001<br>156<br>Chapter 4<br>SQL<br>This subquery generates a relation consisting of the names of all branches and their<br>corresponding average account balances. The subquery result is named result, with<br>the attributes branch-name and avg-balance.<br>To illustrate the use of a subquery expression in the from clause, consider the<br>query “Find the average account balance of those branches where the average ac-<br>count balance is greater than $1200.” We wrote this query in Section 4.4 by using the<br>having clause. We can now rewrite this query, without using the having clause, as<br>follows:<br>select branch-name, avg-balance<br>from (select branch-name, avg (balance)<br>from account<br>group by branch-name)<br>as branch-avg (branch-name, avg-balance)<br>where avg-balance &gt; 1200<br>Note that we do not need to use the having clause, since the subquery in the from<br>clause computes the average balance, and its result is named as branch-avg; we can<br>use the attributes of branch-avg directly in the where clause.<br>As another example, suppose we wish to ﬁnd the maximum across all branches of<br>the total balance at each branch. The having clause does not help us in this task, but<br>we can write this query easily by using a subquery in the from clause, as follows:<br>select max(tot-balance)<br>from (select branch-name, sum(balance)<br>from account<br>group by branch-name) as branch-total (branch-name, tot-balance)<br>4.8.2<br>The with Clause<br>Complex queries are much easier to write and to understand if we structure them<br>by breaking them into smaller views that we then combine, just as we structure pro-<br>grams by breaking their task into procedures. However, unlike a procedure deﬁni-<br>tion, a create view clause creates a view deﬁnition in the database, and the view<br>deﬁnition stays in the database until a command drop view view-name is executed.<br>The with clause provides a way of deﬁning a temporary view whose deﬁnition is<br>available only to the query in which the with clause occurs. Consider the following<br>query, which selects accounts with the maximum balance; if there are many accounts<br>with the same maximum balance, all of them are selected.<br>with max-balance (value) as<br>select max(balance)<br>from account<br>select account-number<br>from account, max-balance<br>where account.balance = max-balance.value<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>II. Relational Databases<br>4. SQL<br>163<br>© The McGraw−Hill <br>Companies, 2001<br>4.9<br>Modiﬁcation of the Database<br>157<br>The with clause introduced in SQL:1999, is currently supported only by some data-<br>bases.<br>We could have written the above query by using a nested subquery in either the<br>from clause or the where clause. However, using nested subqueries would have<br>made the query harder to read and understand. The with clause makes the query<br>logic clearer; it also permits a view deﬁnition to be used in multiple places within a<br>query.<br>For example, suppose we want to ﬁnd all branches where the total account deposit<br>is less than the average of the total account deposits at all branches. We can write the<br>query using the with clause as follows.<br>with branch-total (branch-name, value) as<br>select branch-name, sum(balance)<br>from account<br>group by branch-name<br>with branch-total-avg(value) as<br>select avg(value)<br>from branch-total<br>select branch-name<br>from branch-total, branch-total-avg<br>where branch-total.value &gt;= branch-total-avg.value<br>We can, of course, create an equivalent query without the with clause, but it would<br>be more complicated and harder to understand. You can write the equivalent query<br>as an exercise.<br>4.9<br>Modiﬁcation of the Database<br>We have restricted our attention until now to the extraction of information from the<br>database. Now, we show how to add, remove, or change information with SQL.<br>4.9.1<br>Deletion<br>A delete request is expressed in much the same way as a query. We can delete only<br>whole tuples; we cannot delete values on only particular attributes. SQL expresses a<br>deletion by<br>delete from r<br>where P<br>where P represents a predicate and r represents a relation. The delete statement ﬁrst<br>ﬁnds all tuples t in r for which P(t) is true, and then deletes them from r. The where<br>clause can be omitted, in which case all tuples in r are deleted.<br>Note that a delete command operates on only one relation. If we want to delete<br>tuples from several relations, we must use one delete command for each relation.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>II. Relational Databases<br>4. SQL<br>164<br>© The McGraw−Hill <br>Companies, 2001<br>158<br>Chapter 4<br>SQL<br>The predicate in the where clause may be as complex as a select command’s where<br>clause. At the other extreme, the where clause may be empty. The request<br>delete from loan<br>deletes all tuples from the loan relation. (Well-designed systems will seek conﬁrma-<br>tion from the user before executing such a devastating request.)<br>Here are examples of SQL delete requests:<br>• Delete all account tuples in the Perryridge branch.<br>delete from account<br>where branch-name = ’Perryridge’<br>• Delete all loans with loan amounts between $1300 and $1500.<br>delete from loan<br>where amount between 1300 and 1500<br>• Delete all account tuples at every branch located in Needham.<br>delete from account<br>where branch-name in (select branch-name<br>from branch<br>where branch-city = ’Needham’)<br>This delete request ﬁrst ﬁnds all branches in Needham, and then deletes all<br>account tuples pertaining to those branches.<br>Note that, although we may delete tuples from only one relation at a time, we may<br>reference any number of relations in a select-from-where nested in the where clause<br>of a delete. The delete request can contain a nested select that references the relation<br>from which tuples are to be deleted. For example, suppose that we want to delete the<br>records of all accounts with balances below the average at the bank. We could write<br>delete from account<br>where balance &lt; (select avg (balance)<br>from account)<br>The delete statement ﬁrst tests each tuple in the relation account to check whether the<br>account has a balance less than the average at the bank. Then, all tuples that fail the<br>test—that is, represent an account with a lower-than-average balance—are deleted.<br>Performing all the tests before performing any deletion is important—if some tuples<br>are deleted before other tuples have been tested, the average balance may change,<br>and the ﬁnal result of the delete would depend on the order in which the tuples were<br>processed!<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>II. Relational Databases<br>4. SQL<br>165<br>© The McGraw−Hill <br>Companies, 2001<br>4.9<br>Modiﬁcation of the Database<br>159<br>4.9.2<br>Insertion<br>To insert data into a relation, we either specify a tuple to be inserted or write a query<br>whose result is a set of tuples to be inserted. Obviously, the attribute values for in-<br>serted tuples must be members of the attribute’s domain. Similarly, tuples inserted<br>must be of the correct arity.<br>The simplest insert statement is a request to insert one tuple. Suppose that we<br>wish to insert the fact that there is an account A-9732 at the Perryridge branch and<br>that is has a balance of $1200. We write<br>insert into account<br>values (’A-9732’, ’Perryridge’, 1200)<br>In this example, the values are speciﬁed in the order in which the corresponding<br>attributes are listed in the relation schema. For the beneﬁt of users who may not<br>remember the order of the attributes, SQL allows the attributes to be speciﬁed as part<br>of the insert statement. For example, the following SQL insert statements are identical<br>in function to the preceding one:<br>insert into account (account-number, branch-name, balance)<br>values (’A-9732’, ’Perryridge’, 1200)<br>insert into account (branch-name, account-number, balance)<br>values (’Perryridge’, ’A-9732’, 1200)<br>More generally, we might want to insert tuples on the basis of the result of a query.<br>Suppose that we want to present a new $200 savings acocunt as a gift to all loan<br>customers of the Perryridge branch, for each loan they have. Let the loan number<br>serve as the account number for the savings account. We write<br>insert into account<br>select loan-number, branch-name, 200<br>from loan<br>where branch-name = ’Perryridge’<br>Instead of specifying a tuple as we did earlier in this section, we use a select to specify<br>a set of tuples. SQL evaluates the select statement ﬁrst, giving a set of tuples that is<br>then inserted into the account relation. Each tuple has a loan-number (which serves as<br>the account number for the new account), a branch-name (Perryridge), and an initial<br>balance of the new account ($200).<br>We also need to add tuples to the depositor relation; we do so by writing<br>insert into depositor<br>select customer-name, loan-number<br>from borrower, loan<br>where borrower.loan-number = loan.loan-number and<br>branch-name = ’Perryridge’<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>II. Relational Databases<br>4. SQL<br>166<br>© The McGraw−Hill <br>Companies, 2001<br>160<br>Chapter 4<br>SQL<br>This query inserts a tuple (customer-name, loan-number) into the depositor relation for<br>each customer-name who has a loan in the Perryridge branch with loan number loan-<br>number.<br>It is important that we evaluate the select statement fully before we carry out<br>any insertions. If we carry out some insertions even as the select statement is being<br>evaluated, a request such as<br>insert into account<br>select *<br>from account<br>might insert an inﬁnite number of tuples! The request would insert the ﬁrst tuple in<br>account again, creating a second copy of the tuple. Since this second copy is part of<br>account now, the select statement may ﬁnd it, and a third copy would be inserted into<br>account. The select statement may then ﬁnd this third copy and insert a fourth copy,<br>and so on, forever. Evaluating the select statement completely before performing<br>insertions avoids such problems.<br>Our discussion of the insert statement considered only examples in which a value<br>is given for every attribute in inserted tuples. It is possible, as we saw in Chapter 3,<br>for inserted tuples to be given values on only some attributes of the schema. The<br>remaining attributes are assigned a null value denoted by null. Consider the request<br>insert into account<br>values (’A-401’, null, 1200)<br>We know that account A-401 has $1200, but the branch name is not known. Consider<br>the query<br>select account-number<br>from account<br>where branch-name = ’Perryridge’<br>Since the branch at which account A-401 is maintained is not known, we cannot de-<br>termine whether it is equal to “Perryridge”.<br>We can prohibit the insertion of null values on speciﬁed attributes by using the<br>SQL DDL, which we discuss in Section 4.11.<br>4.9.3<br>Updates<br>In certain situations, we may wish to change a value in a tuple without changing all<br>values in the tuple. For this purpose, the update statement can be used. As we could<br>for insert and delete, we can choose the tuples to be updated by using a query.<br>Suppose that annual interest </span><br><br><span style="background-color: #9BF6FF;" title="Chunk 20 | Start: 400040 | End: 420040 | Tokens: 3280">payments are being made, and all balances are to be<br>increased by 5 percent. We write<br>update account<br>set balance = balance * 1.05<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>II. Relational Databases<br>4. SQL<br>167<br>© The McGraw−Hill <br>Companies, 2001<br>4.9<br>Modiﬁcation of the Database<br>161<br>The preceding update statement is applied once to each of the tuples in account rela-<br>tion.<br>If interest is to be paid only to accounts with a balance of $1000 or more, we can<br>write<br>update account<br>set balance = balance * 1.05<br>where balance &gt;= 1000<br>In general, the where clause of the update statement may contain any construct<br>legal in the where clause of the select statement (including nested selects). As with<br>insert and delete, a nested select within an update statement may reference the re-<br>lation that is being updated. As before, SQL ﬁrst tests all tuples in the relation to see<br>whether they should be updated, and carries out the updates afterward. For exam-<br>ple, we can write the request “Pay 5 percent interest on accounts whose balance is<br>greater than average” as follows:<br>update account<br>set balance = balance * 1.05<br>where balance &gt; select avg (balance)<br>from account<br>Let us now suppose that all accounts with balances over $10,000 receive 6 percent<br>interest, whereas all others receive 5 percent. We could write two update statements:<br>update account<br>set balance = balance * 1.06<br>where balance &gt; 10000<br>update account<br>set balance = balance * 1.05<br>where balance &lt;= 10000<br>Note that, as we saw in Chapter 3, the order of the two update statements is impor-<br>tant. If we changed the order of the two statements, an account with a balance just<br>under $10,000 would receive 11.3 percent interest.<br>SQL provides a case construct, which we can use to perform both the updates with<br>a single update statement, avoiding the problem with order of updates.<br>update account<br>set balance = case<br>when balance &lt;= 10000 then balance * 1.05<br>else balance * 1.06<br>end<br>The general form of the case statement is as follows.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>II. Relational Databases<br>4. SQL<br>168<br>© The McGraw−Hill <br>Companies, 2001<br>162<br>Chapter 4<br>SQL<br>case<br>when pred1 then result1<br>when pred2 then result2<br>. . .<br>when predn then resultn<br>else result0<br>end<br>The operation returns resulti, where i is the ﬁrst of pred1, pred2, . . . , predn that is sat-<br>isﬁed; if none of the predicates is satisﬁed, the operation returns result0. Case state-<br>ments can be used in any place where a value is expected.<br>4.9.4<br>Update of a View<br>The view-update anomaly that we discussed in Chapter 3 exists also in SQL. As an<br>illustration, consider the following view deﬁnition:<br>create view loan-branch as<br>select branch-name, loan-number<br>from loan<br>Since SQL allows a view name to appear wherever a relation name is allowed, we can<br>write<br>insert into loan-branch<br>values (’Perryridge’, ’L-307’)<br>SQL represents this insertion by an insertion into the relation loan, since loan is the<br>actual relation from which the view loan-branch is constructed. We must, therefore,<br>have some value for amount. This value is a null value. Thus, the preceding insert<br>results in the insertion of the tuple<br>(’L-307’, ’Perryridge’, null)<br>into the loan relation.<br>As we saw in Chapter 3, the view-update anomaly becomes more difﬁcult to han-<br>dle when a view is deﬁned in terms of several relations. As a result, many SQL-based<br>database systems impose the following constraint on modiﬁcations allowed through<br>views:<br>• A modiﬁcation is permitted through a view only if the view in question is<br>deﬁned in terms of one relation of the actual relational database—that is, of<br>the logical-level database.<br>Under this constraint, the update, insert, and delete operations would be forbidden<br>on the example view all-customer that we deﬁned previously.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>II. Relational Databases<br>4. SQL<br>169<br>© The McGraw−Hill <br>Companies, 2001<br>4.10<br>Joined Relations∗∗<br>163<br>4.9.5<br>Transactions<br>A transaction consists of a sequence of query and/or update statements. The SQL<br>standard speciﬁes that a transaction begins implicitly when an SQL statement is exe-<br>cuted. One of the following SQL statements must end the transaction:<br>• Commit work commits the current transaction; that is, it makes the updates<br>performed by the transaction become permanent in the database. After the<br>transaction is committed, a new transaction is automatically started.<br>• Rollback work causes the current transaction to be rolled back; that is, it un-<br>does all the updates performed by the SQL statements in the transaction. Thus,<br>the database state is restored to what it was before the ﬁrst statement of the<br>transaction was executed.<br>The keyword work is optional in both the statements.<br>Transaction rollback is useful if some error condition is detected during execution<br>of a transaction. Commit is similar, in a sense, to saving changes to a document that<br>is being edited, while rollback is similar to quitting the edit session without saving<br>changes. Once a transaction has executed commit work, its effects can no longer be<br>undone by rollback work. The database system guarantees that in the event of some<br>failure, such as an error in one of the SQL statements, a power outage, or a system<br>crash, a transaction’s effects will be rolled back if it has not yet executed commit<br>work. In the case of power outage or other system crash, the rollback occurs when<br>the system restarts.<br>For instance, to transfer money from one account to another we need to update<br>two account balances. The two update statements would form a transaction. An error<br>while a transaction executes one of its statements would result in undoing of the<br>effects of the earlier statements of the transaction, so that the database is not left in a<br>partially updated state. We study further properties of transactions in Chapter 15.<br>If a program terminates without executing either of these commands, the updates<br>are either committed or rolled back. The standard does not specify which of the two<br>happens, and the choice is implementation dependent. In many SQL implementa-<br>tions, by default each SQL statement is taken to be a transaction on its own, and gets<br>committed as soon as it is executed. Automatic commit of individual SQL statements<br>must be turned off if a transaction consisting of multiple SQL statements needs to be<br>executed. How to turn off automatic commit depends on the speciﬁc SQL implemen-<br>tation.<br>A better alternative, which is part of the SQL:1999 standard (but supported by only<br>some SQL implementations currently), is to allow multiple SQL statements to be en-<br>closed between the keywords begin atomic . . . end. All the statements between the<br>keywords then form a single transaction.<br>4.10<br>Joined Relations∗∗<br>SQL provides not only the basic Cartesian-product mechanism for joining tuples of<br>relations found in its earlier versions, but, SQL also provides various other mecha-<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>II. Relational Databases<br>4. SQL<br>170<br>© The McGraw−Hill <br>Companies, 2001<br>164<br>Chapter 4<br>SQL<br>loan-number<br>branch-name<br>amount<br>L-170<br>Downtown<br>3000<br>L-230<br>Redwood<br>4000<br>L-260<br>Perryridge<br>1700<br>loan<br>customer-name<br>loan-number<br>Jones<br>L-170<br>Smith<br>L-230<br>Hayes<br>L-155<br>borrower<br>Figure 4.1<br>The loan and borrower relations.<br>nisms for joining relations, including condition joins and natural joins, as well as var-<br>ious forms of outer joins. These additional operations are typically used as subquery<br>expressions in the from clause.<br>4.10.1<br>Examples<br>We illustrate the various join operations by using the relations loan and borrower in<br>Figure 4.1. We start with a simple example of inner joins. Figure 4.2 shows the result<br>of the expression<br>loan inner join borrower on loan.loan-number = borrower.loan-number<br>The expression computes the theta join of the loan and the borrower relations, with the<br>join condition being loan.loan-number = borrower.loan-number. The attributes of the<br>result consist of the attributes of the left-hand-side relation followed by the attributes<br>of the right-hand-side relation.<br>Note that the attribute loan-number appears twice in the ﬁgure—the ﬁrst occur-<br>rence is from loan, and the second is from borrower. The SQL standard does not require<br>attribute names in such results to be unique. An as clause should be used to assign<br>unique names to attributes in query and subquery results.<br>We rename the result relation of a join and the attributes of the result relation by<br>using an as clause, as illustrated here:<br>loan inner join borrower on loan.loan-number = borrower.loan-number<br>as lb(loan-number, branch, amount, cust, cust-loan-num)<br>We rename the second occurrence of loan-number to cust-loan-num. The ordering of<br>the attributes in the result of the join is important for the renaming.<br>Next, we consider an example of the left outer join operation:<br>loan left outer join borrower on loan.loan-number = borrower.loan-number<br>loan-number<br>branch-name<br>amount<br>customer-name<br>loan-number<br>L-170<br>Downtown<br>3000<br>Jones<br>L-170<br>L-230<br>Redwood<br>4000<br>Smith<br>L-230<br>Figure 4.2<br>The result of loan inner join borrower on<br>loan.loan-number = borrower.loan-number.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>II. Relational Databases<br>4. SQL<br>171<br>© The McGraw−Hill <br>Companies, 2001<br>4.10<br>Joined Relations∗∗<br>165<br>loan-number<br>branch-name<br>amount<br>customer-name<br>loan-number<br>L-170<br>Downtown<br>3000<br>Jones<br>L-170<br>L-230<br>Redwood<br>4000<br>Smith<br>L-230<br>L-260<br>Perryridge<br>1700<br>null<br>null<br>Figure 4.3<br>The result of loan left outer join borrower on<br>loan.loan-number = borrower.loan-number.<br>We can compute the left outer join operation logically as follows. First, compute the<br>result of the inner join as before. Then, for every tuple t in the left-hand-side relation<br>loan that does not match any tuple in the right-hand-side relation borrower in the inner<br>join, add a tuple r to the result of the join: The attributes of tuple r that are derived<br>from the left-hand-side relation are ﬁlled in with the values from tuple t, and the<br>remaining attributes of r are ﬁlled with null values. Figure 4.3 shows the resultant<br>relation. The tuples (L-170, Downtown, 3000) and (L-230, Redwood, 4000) join with<br>tuples from borrower and appear in the result of the inner join, and hence in the result<br>of the left outer join. On the other hand, the tuple (L-260, Perryridge, 1700) did not<br>match any tuple from borrower in the inner join, and hence a tuple (L-260, Perryridge,<br>1700, null, null) is present in the result of the left outer join.<br>Finally, we consider an example of the natural join operation:<br>loan natural inner join borrower<br>This expression computes the natural join of the two relations. The only attribute<br>name common to loan and borrower is loan-number. Figure 4.4 shows the result of the<br>expression. The result is similar to the result of the inner join with the on condition in<br>Figure 4.2, since they have, in effect, the same join condition. However, the attribute<br>loan-number appears only once in the result of the natural join, whereas it appears<br>twice in the result of the join with the on condition.<br>4.10.2<br>Join Types and Conditions<br>In Section 4.10.1, we saw examples of the join operations permitted in SQL. Join op-<br>erations take two relations and return another relation as the result. Although outer-<br>join expressions are typically used in the from clause, they can be used anywhere<br>that a relation can be used.<br>Each of the variants of the join operations in SQL consists of a join type and a join<br>condition. The join condition deﬁnes which tuples in the two relations match and what<br>attributes are present in the result of the join. The join type deﬁnes how tuples in each<br>loan-number<br>branch-name<br>amount<br>customer-name<br>L-170<br>Downtown<br>3000<br>Jones<br>L-230<br>Redwood<br>4000<br>Smith<br>Figure 4.4<br>The result of loan natural inner join borrower.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>II. Relational Databases<br>4. SQL<br>172<br>© The McGraw−Hill <br>Companies, 2001<br>166<br>Chapter 4<br>SQL<br>Join types<br>inner join<br>left outer join<br>right outer join<br>full outer join<br>Join Conditions<br>natural<br>on &lt; predicate&gt;<br>using (A1, A1, . . ., An)<br>Figure 4.5<br>Join types and join conditions.<br>relation that do not match any tuple in the other relation (based on the join condition)<br>are treated. Figure 4.5 shows some of the allowed join types and join conditions. The<br>ﬁrst join type is the inner join, and the other three are the outer joins. Of the three join<br>conditions, we have seen the natural join and the on condition before, and we shall<br>discuss the using condition, later in this section.<br>The use of a join condition is mandatory for outer joins, but is optional for inner<br>joins (if it is omitted, a Cartesian product results). Syntactically, the keyword natural<br>appears before the join type, as illustrated earlier, whereas the on and using con-<br>ditions appear at the end of the join expression. The keywords inner and outer are<br>optional, since the rest of the join type enables us to deduce whether the join is an<br>inner join or an outer join.<br>The meaning of the join condition natural, in terms of which tuples from the two<br>relations match, is straightforward. The ordering of the attributes in the result of a<br>natural join is as follows. The join attributes (that is, the attributes common to both<br>relations) appear ﬁrst, in the order in which they appear in the left-hand-side relation.<br>Next come all nonjoin attributes of the left-hand-side relation, and ﬁnally all nonjoin<br>attributes of the right-hand-side relation.<br>The right outer join is symmetric to the left outer join. Tuples from the right-hand-<br>side relation that do not match any tuple in the left-hand-side relation are padded<br>with nulls and are added to the result of the right outer join.<br>Here is an example of combining the natural join condition with the right outer<br>join type:<br>loan natural right outer join borrower<br>Figure 4.6 shows the result of this expression. The attributes of the result are deﬁned<br>by the join type, which is a natural join; hence, loan-number appears only once. The<br>ﬁrst two tuples in the result are from the inner natural join of loan and borrower. The<br>tuple (Hayes, L-155) from the right-hand-side relation does not match any tuple from<br>the left-hand-side relation loan in the natural inner join. Hence, the tuple (L-155, null,<br>null, Hayes) appears in the join result.<br>The join condition using(A1, A2, . . . , An) is similar to the natural join condition, ex-<br>cept that the join attributes are the attributes A1, A2, . . . , An, rather than all attributes<br>that are common to both relations. The attributes A1, A2, . . . , An must consist of only<br>attributes that are common to both relations, and they appear only once in the result<br>of the join.<br>The full outer join is a combination of the left and right outer-join types. After<br>the operation computes the result of the inner join, it extends with nulls tuples from<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>II. Relational Databases<br>4. SQL<br>173<br>© The McGraw−Hill <br>Companies, 2001<br>4.10<br>Joined Relations∗∗<br>167<br>loan-number<br>branch-name<br>amount<br>customer-name<br>L-170<br>Downtown<br>3000<br>Jones<br>L-230<br>Redwood<br>4000<br>Smith<br>L-155<br>null<br>null<br>Hayes<br>Figure 4.6<br>The result of loan natural right outer join borrower.<br>the left-hand-side relation that did not match with any from the right-hand-side, and<br>adds them to the result. Similarly, it extends with nulls tuples from the right-hand-<br>side relation that did not match with any tuples from the left-hand-side relation and<br>adds them to the result.<br>For example, Figure 4.7 shows the result of the expression<br>loan full outer join borrower using (loan-number)<br>As another example of the use of the outer-join operation, we can write the query<br>“Find all customers who have an account but no loan at the bank” as<br>select d-CN<br>from (depositor left outer join borrower<br>on depositor.customer-name = borrower.customer-name)<br>as db1 (d-CN, account-number, b-CN, loan-number)<br>where b-CN is null<br>Similarly, we can write the query “Find all customers who have either an account<br>or a loan (but not both) at the bank,” with natural full outer joins as:<br>select customer-name<br>from (depositor natural full outer join borrower)<br>where account-number is null or loan-number is null<br>SQL-92 also provides two other join types, called cross join and union join. The<br>ﬁrst is equivalent to an inner join without a join condition; the second is equivalent<br>to a full outer join on the “false” condition—that is, where the inner join is empty.<br>loan-number<br>branch-name<br>amount<br>customer-name<br>L-170<br>Downtown<br>3000<br>Jones<br>L-230<br>Redwood<br>4000<br>Smith<br>L-260<br>Perryridge<br>1700<br>null<br>L-155<br>null<br>null<br>Hayes<br>Figure 4.7<br>The result of loan full outer join borrower using(loan-number).<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>II. Relational Databases<br>4. SQL<br>174<br>© The McGraw−Hill <br>Companies, 2001<br>168<br>Chapter 4<br>SQL<br>4.11<br>Data-Deﬁnition Language<br>In most of our discussions of SQL and relational databases, we have accepted a set of<br>relations as given. Of course, the set of relations in a database must be speciﬁed to<br>the system by means of a data deﬁnition language (DDL).<br>The SQL DDL allows speciﬁcation of not only a set of relations, but also information<br>about each relation, including<br>• The schema for each relation<br>• The domain of values associated with each attribute<br>• The integrity constraints<br>• The set of indices to be maintained for each relation<br>• The security and authorization information for each relation<br>• The physical storage structure of each relation on disk<br>We discuss here schema deﬁnition and domain values; we defer discussion of the<br>other SQL DDL features to Chapter 6.<br>4.11.1<br>Domain Types in SQL<br>The SQL standard supports a variety of built-in domain types, including:<br>• char(n): A ﬁxed-length character string with user-speciﬁed length n. The full<br>form, character, can be used instead.<br>• varchar(n): A variable-length character string with user-speciﬁed maximum<br>length n. The full form, character varying, is equivalent.<br>• int: An integer (a ﬁnite subset of the integers that is machine dependent). The<br>full form, integer, is equivalent.<br>• smallint: A small integer (a machine-dependent subset of the integer domain<br>type).<br>• numeric(p, d): A ﬁxed-point number with user-speciﬁed precision. The num-<br>ber consists of p digits (plus a sign), and d of the p digits are to the right of the<br>decimal point. Thus, numeric(3,1) allows 44.5 to be stored exactly, but neither<br>444.5 or 0.32 can be stored exactly in a ﬁeld of this type.<br>• real, double precision: Floating-point and double-precision ﬂoating-point<br>numbers with machine-dependent precision.<br>• ﬂoat(n): A ﬂoating-point number, with precision of at least n digits.<br>• date: A calendar date containing a (four-digit) year, month, and day of the<br>month.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>II. Relational Databases<br>4. SQL<br>175<br>© The McGraw−Hill <br>Companies, 2001<br>4.11<br>Data-Deﬁnition Language<br>169<br>• time: The time of day, in hours, minutes, and seconds. A variant, time(p), can<br>be used to specify the number of fractional digits for seconds (the default be-<br>ing 0). It is also possible to store time zone information along with the time.<br>• timestamp: A combination of date and time. A variant, timestamp(p), can be<br>used to specify the number of fractional digits for seconds (the default here<br>being 6).<br>Date and time values can be speciﬁed like this:<br>date ’2001-04-25’<br>time ’09:30:00’<br>timestamp ’2001-04-25 10:29:01.45’<br>Dates must be speciﬁed in the format year followed by month followed by day, as<br>shown. The seconds ﬁeld of time or timestamp can have a fractional part, as in the<br>timestamp above. We can use an expression of the form cast e as t to convert a char-<br>acter string (or string valued expression) e to the type t, where t is one of date, time,<br>or timestamp. The string must be in the appropriate format as illustrated at the be-<br>ginning of this paragraph.<br>To extract individual ﬁelds of a date or tim</span><br><br><span style="background-color: #A0C4FF;" title="Chunk 21 | Start: 420042 | End: 440042 | Tokens: 3325">e value d, we can use extract (ﬁeld from<br>d), where ﬁeld can be one of year, month, day, hour, minute, or second.<br>SQL allows comparison operations on all the domains listed here, and it allows<br>both arithmetic and comparison operations on the various numeric domains. SQL<br>also provides a data type called interval, and it allows computations based on dates<br>and times and on intervals. For example, if x and y are of type date, then x −y is an<br>interval whose value is the number of days from date x to date y. Similarly, adding<br>or subtracting an interval to a date or time gives back a date or time, respectively.<br>It is often useful to compare values from compatible domains. For example, since<br>every small integer is an integer, a comparison x &lt; y, where x is a small integer and<br>y is an integer (or vice versa), makes sense. We make such a comparison by casting<br>small integer x as an integer. A transformation of this sort is called a type coercion.<br>Type coercion is used routinely in common programming languages, as well as in<br>database systems.<br>As an illustration, suppose that the domain of customer-name is a character string<br>of length 20, and the domain of branch-name is a character string of length 15. Al-<br>though the string lengths might differ, standard SQL will consider the two domains<br>compatible.<br>As we discussed in Chapter 3, the null value is a member of all domains. For cer-<br>tain attributes, however, null values may be inappropriate. Consider a tuple in the<br>customer relation where customer-name is null. Such a tuple gives a street and city for<br>an anonymous customer; thus, it does not contain useful information. In cases such<br>as this, we wish to forbid null values, and we do so by restricting the domain of<br>customer-name to exclude null values.<br>SQL allows the domain declaration of an attribute to include the speciﬁcation not<br>null and thus prohibits the insertion of a null value for this attribute. Any database<br>modiﬁcation that would cause a null to be inserted in a not null domain generates<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>II. Relational Databases<br>4. SQL<br>176<br>© The McGraw−Hill <br>Companies, 2001<br>170<br>Chapter 4<br>SQL<br>an error diagnostic. There are many situations where we want to avoid null values.<br>In particular, it is essential to prohibit null values in the primary key of a relation<br>schema. Thus, in our bank example, in the customer relation, we must prohibit a null<br>value for the attribute customer-name, which is the primary key for customer.<br>4.11.2<br>Schema Deﬁnition in SQL<br>We deﬁne an SQL relation by using the create table command:<br>create table r(A1D1, A2D2, . . . , AnDn,<br>⟨integrity-constraint1⟩,<br>. . . ,<br>⟨integrity-constraintk⟩)<br>where r is the name of the relation, each Ai is the name of an attribute in the schema<br>of relation r, and Di is the domain type of values in the domain of attribute Ai. The<br>allowed integrity constraints include<br>• primary key (Aj1, Aj2, . . . , Ajm): The primary key speciﬁcation says that at-<br>tributes Aj1, Aj2, . . . , Ajm form the primary key for the relation. The primary<br>key attributes are required to be non-null and unique; that is, no tuple can have<br>a null value for a primary key attribute, and no two tuples in the relation can<br>be equal on all the primary-key attributes.1 Although the primary key speciﬁ-<br>cation is optional, it is generally a good idea to specify a primary key for each<br>relation.<br>• check(P): The check clause speciﬁes a predicate P that must be satisﬁed by<br>every tuple in the relation.<br>The create table command also includes other integrity constraints, which we shall<br>discuss in Chapter 6.<br>Figure 4.8 presents a partial SQL DDL deﬁnition of our bank database. Note that,<br>as in earlier chapters, we do not attempt to model precisely the real world in the<br>bank-database example. In the real world, multiple people may have the same name,<br>so customer-name would not be a primary key customer; a customer-id would more<br>likely be used as a primary key. We use customer-name as a primary key to keep our<br>database schema simple and short.<br>If a newly inserted or modiﬁed tuple in a relation has null values for any primary-<br>key attribute, or if the tuple has the same value on the primary-key attributes as does<br>another tuple in the relation, SQL ﬂags an error and prevents the update. Similarly, it<br>ﬂags an error and prevents the update if the check condition on the tuple fails.<br>By default null is a legal value for every attribute in SQL, unless the attribute is<br>speciﬁcally stated to be not null. An attribute can be declared to be not null in the<br>following way:<br>account-number char(10) not null<br>1.<br>In SQL-89, primary-key attributes were not implicitly declared to be not null; an explicit not null<br>declaration was required.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>II. Relational Databases<br>4. SQL<br>177<br>© The McGraw−Hill <br>Companies, 2001<br>4.11<br>Data-Deﬁnition Language<br>171<br>create table customer<br>(customer-name<br>char(20),<br>customer-street<br>char(30),<br>customer-city<br>char(30),<br>primary key (customer-name))<br>create table branch<br>(branch-name<br>char(15),<br>branch-city<br>char(30),<br>assets<br>integer,<br>primary key (branch-name),<br>check (assets &gt;= 0))<br>create table account<br>(account-number char(10),<br>branch-name<br>char(15),<br>balance<br>integer,<br>primary key (account-number),<br>check (balance &gt;= 0))<br>create table depositor<br>(customer-name<br>char(20),<br>account-number<br>char(10),<br>primary key (customer-name, account-number))<br>Figure 4.8<br>SQL data deﬁnition for part of the bank database.<br>SQL also supports an integrity constraint<br>unique (Aj1, Aj2, . . . , Ajm)<br>The unique speciﬁcation says that attributes Aj1, Aj2, . . . , Ajm form a candidate key;<br>that is, no two tuples in the relation can be equal on all the primary-key attributes.<br>However, candidate key attributes are permitted to be null unless they have explicitly<br>been declared to be not null. Recall that a null value does not equal any other value.<br>The treatment of nulls here is the same as that of the unique construct deﬁned in<br>Section 4.6.4.<br>A common use of the check clause is to ensure that attribute values satisfy spec-<br>iﬁed conditions, in effect creating a powerful type system. For instance, the check<br>clause in the create table command for relation branch checks that the value of assets<br>is nonnegative. As another example, consider the following:<br>create table student<br>(name<br>char(15) not null,<br>student-id<br>char(10),<br>degree-level<br>char(15),<br>primary key (student-id),<br>check (degree-level in (’Bachelors’, ’Masters’, ’Doctorate’)))<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>II. Relational Databases<br>4. SQL<br>178<br>© The McGraw−Hill <br>Companies, 2001<br>172<br>Chapter 4<br>SQL<br>Here, we use the check clause to simulate an enumerated type, by specifying that<br>degree-level must be one of ’Bachelors’, ’Masters’, or ’Doctorate’. We consider more<br>general forms of check conditions, as well as a class of constraints called referential<br>integrity constraints, in Chapter 6.<br>A newly created relation is empty initially. We can use the insert command to load<br>data into the relation. Many relational-database products have special bulk loader<br>utilities to load an initial set of tuples into a relation.<br>To remove a relation from an SQL database, we use the drop table command. The<br>drop table command deletes all information about the dropped relation from the<br>database. The command<br>drop table r<br>is a more drastic action than<br>delete from r<br>The latter retains relation r, but deletes all tuples in r. The former deletes not only all<br>tuples of r, but also the schema for r. After r is dropped, no tuples can be inserted<br>into r unless it is re-created with the create table command.<br>We use the alter table command to add attributes to an existing relation. All tuples<br>in the relation are assigned null as the value for the new attribute. The form of the<br>alter table command is<br>alter table r add A D<br>where r is the name of an existing relation, A is the name of the attribute to be added,<br>and D is the domain of the added attribute. We can drop attributes from a relation by<br>the command<br>alter table r drop A<br>where r is the name of an existing relation, and A is the name of an attribute of the<br>relation. Many database systems do not support dropping of attributes, although<br>they will allow an entire table to be dropped.<br>4.12<br>Embedded SQL<br>SQL provides a powerful declarative query language. Writing queries in SQL is usu-<br>ally much easier than coding the same queries in a general-purpose programming<br>language. However, a programmer must have access to a database from a general-<br>purpose programming language for at least two reasons:<br>1. Not all queries can be expressed in SQL, since SQL does not provide the full<br>expressive power of a general-purpose language. That is, there exist queries<br>that can be expressed in a language such as C, Java, or Cobol that cannot be<br>expressed in SQL. To write such queries, we can embed SQL within a more<br>powerful language.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>II. Relational Databases<br>4. SQL<br>179<br>© The McGraw−Hill <br>Companies, 2001<br>4.12<br>Embedded SQL<br>173<br>SQL is designed so that queries written in it can be optimized automatically<br>and executed efﬁciently—and providing the full power of a programming<br>language makes automatic optimization exceedingly difﬁcult.<br>2. Nondeclarative actions—such as printing a report, interacting with a user, or<br>sending the results of a query to a graphical user interface—cannot be done<br>from within SQL. Applications usually have several components, and query-<br>ing or updating data is only one component; other components are written in<br>general-purpose programming languages. For an integrated application, the<br>programs written in the programming language must be able to access the<br>database.<br>The SQL standard deﬁnes embeddings of SQL in a variety of programming lan-<br>guages, such as C, Cobol, Pascal, Java, PL/I, and Fortran. A language in which SQL<br>queries are embedded is referred to as a host language, and the SQL structures per-<br>mitted in the host language constitute embedded SQL.<br>Programs written in the host language can use the embedded SQL syntax to ac-<br>cess and update data stored in a database. This embedded form of SQL extends the<br>programmer’s ability to manipulate the database even further. In embedded SQL, all<br>query processing is performed by the database system, which then makes the result<br>of the query available to the program one tuple (record) at a time.<br>An embedded SQL program must be processed by a special preprocessor prior to<br>compilation. The preprocessor replaces embedded SQL requests with host-language<br>declarations and procedure calls that allow run-time execution of the database ac-<br>cesses. Then, the resulting program is compiled by the host-language compiler. To<br>identify embedded SQL requests to the preprocessor, we use the EXEC SQL statement;<br>it has the form<br>EXEC SQL &lt;embedded SQL statement &gt; END-EXEC<br>The exact syntax for embedded SQL requests depends on the language in which<br>SQL is embedded. For instance, a semicolon is used instead of END-EXEC when SQL<br>is embedded in C. The Java embedding of SQL (called SQLJ) uses the syntax<br># SQL { &lt;embedded SQL statement &gt; };<br>We place the statement SQL INCLUDE in the program to identify the place where<br>the preprocessor should insert the special variables used for communication between<br>the program and the database system. Variables of the host language can be used<br>within embedded SQL statements, but they must be preceded by a colon (:) to distin-<br>guish them from SQL variables.<br>Embedded SQL statements are similar in form to the SQL statements that we de-<br>scribed in this chapter. There are, however, several important differences, as we note<br>here.<br>To write a relational query, we use the declare cursor statement. The result of the<br>query is not yet computed. Rather, the program must use the open and fetch com-<br>mands (discussed later in this section) to obtain the result tuples.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>II. Relational Databases<br>4. SQL<br>180<br>© The McGraw−Hill <br>Companies, 2001<br>174<br>Chapter 4<br>SQL<br>Consider the banking schema that we have used in this chapter. Assume that we<br>have a host-language variable amount, and that we wish to ﬁnd the names and cities<br>of residence of customers who have more than amount dollars in any account. We can<br>write this query as follows:<br>EXEC SQL<br>declare c cursor for<br>select customer-name, customer-city<br>from depositor, customer, account<br>where depositor.customer-name = customer.customer-name and<br>account.account-number = depositor.account-number and<br>account.balance &gt; :amount<br>END-EXEC<br>The variable c in the preceding expression is called a cursor for the query. We use<br>this variable to identify the query in the open statement, which causes the query to<br>be evaluated, and in the fetch statement, which causes the values of one tuple to be<br>placed in host-language variables.<br>The open statement for our sample query is as follows:<br>EXEC SQL open c END-EXEC<br>This statement causes the database system to execute the query and to save the results<br>within a temporary relation. The query has a host-language variable (:amount); the<br>query uses the value of the variable at the time the open statement was executed.<br>If the SQL query results in an error, the database system stores an error diagnostic<br>in the SQL communication-area (SQLCA) variables, whose declarations are inserted<br>by the SQL INCLUDE statement.<br>An embedded SQL program executes a series of fetch statements to retrieve tuples<br>of the result. The fetch statement requires one host-language variable for each at-<br>tribute of the result relation. For our example query, we need one variable to hold the<br>customer-name value and another to hold the customer-city value. Suppose that those<br>variables are cn and cc, respectively. Then the statement:<br>EXEC SQL fetch c into :cn, :cc END-EXEC<br>produces a tuple of the result relation. The program can then manipulate the vari-<br>ables cn and cc by using the features of the host programming language.<br>A single fetch request returns only one tuple. To obtain all tuples of the result,<br>the program must contain a loop to iterate over all tuples. Embedded SQL assists the<br>programmer in managing this iteration. Although a relation is conceptually a set, the<br>tuples of the result of a query are in some ﬁxed physical order. When the program<br>executes an open statement on a cursor, the cursor is set to point to the ﬁrst tuple<br>of the result. Each time it executes a fetch statement, the cursor is updated to point<br>to the next tuple of the result. When no further tuples remain to be processed, the<br>variable SQLSTATE in the SQLCA is set to ’02000’ (meaning “no data”). Thus, we can<br>use a while loop (or equivalent loop) to process each tuple of the result.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>II. Relational Databases<br>4. SQL<br>181<br>© The McGraw−Hill <br>Companies, 2001<br>4.13<br>Dynamic SQL<br>175<br>We must use the close statement to tell the database system to delete the tempo-<br>rary relation that held the result of the query. For our example, this statement takes<br>the form<br>EXEC SQL close c END-EXEC<br>SQLJ, the Java embedding of SQL, provides a variation of the above scheme, where<br>Java iterators are used in place of cursors. SQLJ associates the results of a query with<br>an iterator, and the next() method of the Java iterator interface can be used to step<br>through the result tuples, just as the preceding examples use fetch on the cursor.<br>Embedded SQL expressions for database modiﬁcation (update, insert, and delete)<br>do not return a result. Thus, they are somewhat simpler to express. A database-<br>modiﬁcation request takes the form<br>EXEC SQL &lt; any valid update, insert, or delete&gt; END-EXEC<br>Host-language variables, preceded by a colon, may appear in the SQL database-<br>modiﬁcation expression. If an error condition arises in the execution of the statement,<br>a diagnostic is set in the SQLCA.<br>Database relations can also be updated through cursors. For example, if we want<br>to add 100 to the balance attribute of every account where the branch name is “Per-<br>ryridge”, we could declare a cursor as follows.<br>declare c cursor for<br>select *<br>from account<br>where branch-name = ‘Perryridge‘<br>for update<br>We then iterate through the tuples by performing fetch operations on the cursor (as<br>illustrated earlier), and after fetching each tuple we execute the following code<br>update account<br>set balance = balance + 100<br>where current of c<br>Embedded SQL allows a host-language program to access the database, but it pro-<br>vides no assistance in presenting results to the user or in generating reports. Most<br>commercial database products include tools to assist application programmers in<br>creating user interfaces and formatted reports. We discuss such tools in Chapter 5<br>(Section 5.3).<br>4.13<br>Dynamic SQL<br>The dynamic SQL component of SQL allows programs to construct and submit SQL<br>queries at run time. In contrast, embedded SQL statements must be completely present<br>at compile time; they are compiled by the embedded SQL preprocessor. Using dy-<br>namic SQL, programs can create SQL queries as strings at run time (perhaps based on<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>II. Relational Databases<br>4. SQL<br>182<br>© The McGraw−Hill <br>Companies, 2001<br>176<br>Chapter 4<br>SQL<br>input from the user) and can either have them executed immediately or have them<br>prepared for subsequent use. Preparing a dynamic SQL statement compiles it, and<br>subsequent uses of the prepared statement use the compiled version.<br>SQL deﬁnes standards for embedding dynamic SQL calls in a host language, such<br>as C, as in the following example.<br>char * sqlprog = ”update account set balance = balance ∗1.05<br>where account-number = ?”<br>EXEC SQL prepare dynprog from :sqlprog;<br>char account[10] = ”A-101”;<br>EXEC SQL execute dynprog using :account;<br>The dynamic SQL program contains a ?, which is a place holder for a value that is<br>provided when the SQL program is executed.<br>However, the syntax above requires extensions to the language or a preprocessor<br>for the extended language. An alternative that is very widely used is to use an appli-<br>cation program interface to send SQL queries or updates to a database system, and<br>not make any changes in the programming language itself.<br>In the rest of this section, we look at two standards for connecting to an SQL<br>database and performing queries and updates. One, ODBC, is an application pro-<br>gram interface for the C language, while the other, JDBC, is an application program<br>interface for the Java language.<br>To understand these standards, we need to understand the concept of SQL ses-<br>sions. The user or application connects to an SQL server, establishing a session; exe-<br>cutes a series of statements; and ﬁnally disconnects the session. Thus, all activities of<br>the user or application are in the context of an SQL session. In addition to the normal<br>SQL commands, a session can also contain commands to commit the work carried out<br>in the session, or to rollback the work carried out in the session.<br>4.13.1<br>ODBC∗∗<br>The Open DataBase Connectivity (ODBC) standard deﬁnes a way for an application<br>program to communicate with a database server. ODBC deﬁnes an application pro-<br>gram interface (API) that applications can use to open a connection with a database,<br>send queries and updates, and get back results. Applications such as graphical user<br>interfaces, statistics packages, and spreadsheets can make use of the same ODBC API<br>to connect to any database server that supports ODBC.<br>Each database system supporting ODBC provides a library that must be linked<br>with the client program. When the client program makes an ODBC API call, the code<br>in the library communicates with the server to carry out the requested action, and<br>fetch results.<br>Figure 4.9 shows an example of C code using the ODBC API. The ﬁrst step in using<br>ODBC to communicate with a server is to set up a connection with the server. To do<br>so, the program ﬁrst allocates an SQ</span><br><br><span style="background-color: #BDB2FF;" title="Chunk 22 | Start: 440044 | End: 460044 | Tokens: 3175">L environment, then a database connection han-<br>dle. ODBC deﬁnes the types HENV, HDBC, and RETCODE. The program then opens<br>the database connection by using SQLConnect. This call takes several parameters, in-<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>II. Relational Databases<br>4. SQL<br>183<br>© The McGraw−Hill <br>Companies, 2001<br>4.13<br>Dynamic SQL<br>177<br>int ODBCexample()<br>{<br>RETCODE error;<br>HENV env; /* environment */<br>HDBC conn; /* database connection */<br>SQLAllocEnv(&amp;env);<br>SQLAllocConnect(env, &amp;conn);<br>SQLConnect(conn, ”aura.bell-labs.com”, SQL NTS, ”avi”, SQL NTS,<br>”avipasswd”, SQL NTS);<br>{<br>char branchname[80];<br>ﬂoat balance;<br>int lenOut1, lenOut2;<br>HSTMT stmt;<br>SQLAllocStmt(conn, &amp;stmt);<br>char * sqlquery = ”select branch name, sum (balance)<br>from account<br>group by branch name”;<br>error = SQLExecDirect(stmt, sqlquery, SQL NTS);<br>if (error == SQL SUCCESS) {<br>SQLBindCol(stmt, 1, SQL C CHAR, branchname , 80, &amp;lenOut1);<br>SQLBindCol(stmt, 2, SQL C FLOAT, &amp;balance, 0 , &amp;lenOut2);<br>while (SQLFetch(stmt) &gt;= SQL SUCCESS) {<br>printf (” %s %g\n”, branchname, balance);<br>}<br>}<br>}<br>SQLFreeStmt(stmt, SQL DROP);<br>SQLDisconnect(conn);<br>SQLFreeConnect(conn);<br>SQLFreeEnv(env);<br>}<br>Figure 4.9<br>ODBC code example.<br>cluding the connection handle, the server to which to connect, the user identiﬁer,<br>and the password for the database. The constant SQL NTS denotes that the previous<br>argument is a null-terminated string.<br>Once the connection is set up, the program can send SQL commands to the database<br>by using SQLExecDirect C language variables can be bound to attributes of the query<br>result, so that when a result tuple is fetched using SQLFetch, its attribute values are<br>stored in corresponding C variables. The SQLBindCol function does this task; the sec-<br>ond argument identiﬁes the position of the attribute in the query result, and the third<br>argument indicates the type conversion required from SQL to C. The next argument<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>II. Relational Databases<br>4. SQL<br>184<br>© The McGraw−Hill <br>Companies, 2001<br>178<br>Chapter 4<br>SQL<br>gives the address of the variable. For variable-length types like character arrays, the<br>last two arguments give the maximum length of the variable and a location where<br>the actual length is to be stored when a tuple is fetched. A negative value returned<br>for the length ﬁeld indicates that the value is null.<br>The SQLFetch statement is in a while loop that gets executed until SQLFetch re-<br>turns a value other than SQL SUCCESS. On each fetch, the program stores the values<br>in C variables as speciﬁed by the calls on SQLBindCol and prints out these values.<br>At the end of the session, the program frees the statement handle, disconnects<br>from the database, and frees up the connection and SQL environment handles. Good<br>programming style requires that the result of every function call must be checked to<br>make sure there are no errors; we have omitted most of these checks for brevity.<br>It is possible to create an SQL statement with parameters; for example, consider<br>the statement insert into account values(?,?,?). The question marks are placeholders<br>for values which will be supplied later. The above statement can be “prepared,” that<br>is, compiled at the database, and repeatedly executed by providing actual values for<br>the placeholders—in this case, by providing an account number, branch name, and<br>balance for the relation account.<br>ODBC deﬁnes functions for a variety of tasks, such as ﬁnding all the relations in the<br>database and ﬁnding the names and types of columns of a query result or a relation<br>in the database.<br>By default, each SQL statement is treated as a separate transaction that is commit-<br>ted automatically. The call SQLSetConnectOption(conn, SQL AUTOCOMMIT, 0) turns<br>off automatic commit on connection conn, and transactions must then be committed<br>explicitly by SQLTransact(conn, SQL COMMIT) or rolled back by SQLTransact(conn,<br>SQL ROLLBACK).<br>The more recent versions of the ODBC standard add new functionality. Each ver-<br>sion deﬁnes conformance levels, which specify subsets of the functionality deﬁned by<br>the standard. An ODBC implementation may provide only core level features, or it<br>may provide more advanced (level 1 or level 2) features. Level 1 requires support<br>for fetching information about the catalog, such as information about what relations<br>are present and the types of their attributes. Level 2 requires further features, such as<br>ability to send and retrieve arrays of parameter values and to retrieve more detailed<br>catalog information.<br>The more recent SQL standards (SQL-92 and SQL:1999) deﬁne a call level interface<br>(CLI) that is similar to the ODBC interface, but with some minor differences.<br>4.13.2<br>JDBC∗∗<br>The JDBC standard deﬁnes an API that Java programs can use to connect to database<br>servers. (The word JDBC was originally an abbreviation for “Java Database Connec-<br>tivity”, but the full form is no longer used.) Figure 4.10 shows an example Java pro-<br>gram that uses the JDBC interface. The program must ﬁrst open a connection to a<br>database, and can then execute SQL statements, but before opening a connection,<br>it loads the appropriate drivers for the database by using Class.forName. The ﬁrst<br>parameter to the getConnection call speciﬁes the machine name where the server<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>II. Relational Databases<br>4. SQL<br>185<br>© The McGraw−Hill <br>Companies, 2001<br>4.13<br>Dynamic SQL<br>179<br>public static void JDBCexample(String dbid, String userid, String passwd)<br>{<br>try<br>{<br>Class.forName (”oracle.jdbc.driver.OracleDriver”);<br>Connection conn = DriverManager.getConnection(<br>”jdbc:oracle:thin:@aura.bell-labs.com:2000:bankdb”,<br>userid, passwd);<br>Statement stmt = conn.createStatement();<br>try {<br>stmt.executeUpdate(<br>”insert into account values(’A-9732’, ’Perryridge’, 1200)”);<br>} catch (SQLException sqle)<br>{<br>System.out.println(”Could not insert tuple. ” + sqle);<br>}<br>ResultSet rset = stmt.executeQuery(<br>”select branch name, avg (balance)<br>from account<br>group by branch name”);<br>while (rset.next()) {<br>System.out.println(rset.getString(”branch name”) + ” ” +<br>rset.getFloat(2));<br>}<br>stmt.close();<br>conn.close();<br>}<br>catch (SQLException sqle)<br>{<br>System.out.println(”SQLException : ” + sqle);<br>}<br>}<br>Figure 4.10<br>An example of JDBC code.<br>runs (in our example, aura.bell-labs.com), the port number it uses for communica-<br>tion (in our example, 2000). The parameter also speciﬁes which schema on the server<br>is to be used (in our example, bankdb), since a database server may support multiple<br>schemas. The ﬁrst parameter also speciﬁes the protocol to be used to communicate<br>with the database (in our example, jdbc:oracle:thin:). Note that JDBC speciﬁes only<br>the API, not the communication protocol. A JDBC driver may support multiple pro-<br>tocols, and we must specify one supported by both the database and the driver. The<br>other two arguments to getConnection are a user identiﬁer and a password.<br>The program then creates a statement handle on the connection and uses it to<br>execute an SQL statement and get back results. In our example, stmt.executeUpdate<br>executes an update statement. The try { . . . } catch { . . . } construct permits us to<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>II. Relational Databases<br>4. SQL<br>186<br>© The McGraw−Hill <br>Companies, 2001<br>180<br>Chapter 4<br>SQL<br>PreparedStatement pStmt = conn.prepareStatement(<br>”insert into account values(?,?,?)”);<br>pStmt.setString(1, ”A-9732”);<br>pStmt.setString(2, ”Perryridge”);<br>pStmt.setInt(3, 1200);<br>pStmt.executeUpdate();<br>pStmt.setString(1, ”A-9733”);<br>pStmt.executeUpdate();<br>Figure 4.11<br>Prepared statements in JDBC code.<br>catch any exceptions (error conditions) that arise when JDBC calls are made, and print<br>an appropriate message to the user.<br>The program can execute a query by using stmt.executeQuery. It can retrieve the<br>set of rows in the result into a ResultSet and fetch them one tuple at a time using the<br>next() function on the result set. Figure 4.10 shows two ways of retrieving the values<br>of attributes in a tuple: using the name of the attribute (branch-name) and using the<br>position of the attribute (2, to denote the second attribute).<br>We can also create a prepared statement in which some values are replaced by “?”,<br>thereby specifying that actual values will be provided later. We can then provide the<br>values by using setString(). The database can compile the query when it is prepared,<br>and each time it is executed (with new values), the database can reuse the previously<br>compiled form of the query. The code fragment in Figure 4.11 shows how prepared<br>statements can be used.<br>JDBC provides a number of other features, such as updatable result sets. It can<br>create an updatable result set from a query that performs a selection and/or a pro-<br>jection on a database relation. An update to a tuple in the result set then results in<br>an update to the corresponding tuple of the database relation. JDBC also provides an<br>API to examine database schemas and to ﬁnd the types of attributes of a result set.<br>For more information about JDBC, refer to the bibliographic information at the end<br>of the chapter.<br>4.14<br>Other SQL Features ∗∗<br>The SQL language has grown over the past two decades from a simple language with<br>a few features to a rather complex language with features to satisfy many different<br>types of users. We covered the basics of SQL earlier in this chapter. In this section we<br>introduce the reader to some of the more complex features of SQL.<br>4.14.1<br>Schemas, Catalogs, and Environments<br>To understand the motivation for schemas and catalogs, consider how ﬁles are named<br>in a ﬁle system. Early ﬁle systems were ﬂat; that is, all ﬁles were stored in a single<br>directory. Current generation ﬁle systems of course have a directory structure, with<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>II. Relational Databases<br>4. SQL<br>187<br>© The McGraw−Hill <br>Companies, 2001<br>4.14<br>Other SQL Features ∗∗<br>181<br>ﬁles stored within subdirectories. To name a ﬁle uniquely, we must specify the full<br>path name of the ﬁle, for example, /users/avi/db-book/chapter4.tex.<br>Like early ﬁle systems, early database systems also had a single name space for all<br>relations. Users had to coordinate to make sure they did not try to use the same name<br>for different relations. Contemporary database systems provide a three-level hierar-<br>chy for naming relations. The top level of the hierarchy consists of catalogs, each of<br>which can contain schemas. SQL objects such as relations and views are contained<br>within a schema.<br>In order to perform any actions on a database, a user (or a program) must ﬁrst<br>connect to the database. The user must provide the user name and usually, a secret<br>password for verifying the identity of the user, as we saw in the ODBC and JDBC<br>examples in Sections 4.13.1 and 4.13.2. Each user has a default catalog and schema,<br>and the combination is unique to the user. When a user connects to a database system,<br>the default catalog and schema are set up for for the connection; this corresponds to<br>the current directory being set to the user’s home directory when the user logs into<br>an operating system.<br>To identify a relation uniquely, a three-part name must be used, for example,<br>catalog5.bank-schema.account<br>We may omit the catalog component, in which case the catalog part of the name is<br>considered to be the default catalog for the connection. Thus if catalog5 is the default<br>catalog, we can use bank-schema.account to identify the same relation uniquely. Fur-<br>ther, we may also omit the schema name, and the schema part of the name is again<br>considered to be the default schema for the connection. Thus we can use just account<br>if the default catalog is catalog5 and the default schema is bank-schema.<br>With multiple catalogs and schemas available, different applications and differ-<br>ent users can work independently without worrying about name clashes. Moreover,<br>multiple versions of an application—one a production version, other test versions—<br>can run on the same database system.<br>The default catalog and schema are part of an SQL environment that is set up<br>for each connection. The environment additionally contains the user identiﬁer (also<br>referred to as the authorization identiﬁer). All the usual SQL statements, including the<br>DDL and DML statements, operate in the context of a schema. We can create and<br>drop schemas by means of create schema and drop schema statements. Creation and<br>dropping of catalogs is implementation dependent and not part of the SQL standard.<br>4.14.2<br>Procedural Extensions and Stored Procedures<br>SQL provides a module language, which allows procedures to be deﬁned in SQL.<br>A module typically contains multiple SQL procedures. Each procedure has a name,<br>optional arguments, and an SQL statement. An extension of the SQL-92 standard lan-<br>guage also permits procedural constructs, such as for, while, and if-then-else, and<br>compound SQL statements (multiple SQL statements between a begin and an end).<br>We can store procedures in the database and then execute them by using the call<br>statement. Such procedures are also called stored procedures. Stored procedures<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>II. Relational Databases<br>4. SQL<br>188<br>© The McGraw−Hill <br>Companies, 2001<br>182<br>Chapter 4<br>SQL<br>are particularly useful because they permit operations on the database to be made<br>available to external applications, without exposing any of the internal details of the<br>database.<br>Chapter 9 covers procedural extensions of SQL as well as many other new features<br>of SQL:1999.<br>4.15<br>Summary<br>• Commercial database systems do not use the terse, formal query languages<br>covered in Chapter 3. The widely used SQL language, which we studied in<br>this chapter, is based on the formal relational algebra, but includes much “syn-<br>tactic sugar.”<br>• SQL includes a variety of language constructs for queries on the database. All<br>the relational-algebra operations, including the extended relational-algebra<br>operations, can be expressed by SQL. SQL also allows ordering of query re-<br>sults by sorting on speciﬁed attributes.<br>• View relations can be deﬁned as relations containing the result of queries.<br>Views are useful for hiding unneeded information, and for collecting together<br>information from more than one relation into a single view.<br>• Temporary views deﬁned by using the with clause are also useful for breaking<br>up complex queries into smaller and easier-to-understand parts.<br>• SQL provides constructs for updating, inserting, and deleting information. A<br>transaction consists of a sequence of operations, which must appear to be<br>atomic. That is, all the operations are carried out successfully, or none is car-<br>ried out. In practice, if a transaction cannot complete successfully, any partial<br>actions it carried out are undone.<br>• Modiﬁcations to the database may lead to the generation of null values in<br>tuples. We discussed how nulls can be introduced, and how the SQL query<br>language handles queries on relations containing null values.<br>• The SQL data deﬁnition language is used to create relations with speciﬁed<br>schemas. The SQL DDL supports a number of types including date and time<br>types. Further details on the SQL DDL, in particular its support for integrity<br>constraints, appear in Chapter 6.<br>• SQL queries can be invoked from host languages, via embedded and dynamic<br>SQL. The ODBC and JDBC standards deﬁne application program interfaces to<br>access SQL databases from C and Java language programs. Increasingly, pro-<br>grammers use these APIs to access databases.<br>• We also saw a brief overview of some advanced features of SQL, such as pro-<br>cedural extensions, catalogs, schemas and stored procedures.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>II. Relational Databases<br>4. SQL<br>189<br>© The McGraw−Hill <br>Companies, 2001<br>Exercises<br>183<br>Review Terms<br>• DDL: data deﬁnition language<br>• DML: data manipulation<br>language<br>• select clause<br>• from clause<br>• where clause<br>• as clause<br>• Tuple variable<br>• order by clause<br>• Duplicates<br>• Set operations<br>  union, intersect, except<br>• Aggregate functions<br>  avg, min, max, sum, count<br>  group by<br>• Null values<br>  Truth value “unknown”<br>• Nested subqueries<br>• Set operations<br>  {&lt;, &lt;=, &gt;, &gt;=} { some, all }<br>  exists<br>  unique<br>• Views<br>• Derived relations (in from clause)<br>• with clause<br>• Database modiﬁcation<br>  delete, insert, update<br>  View update<br>• Join types<br>  Inner and outer join<br>  left, right and full outer join<br>  natural, using, and on<br>• Transaction<br>• Atomicity<br>• Index<br>• Schema<br>• Domains<br>• Embedded SQL<br>• Dynamic SQL<br>• ODBC<br>• JDBC<br>• Catalog<br>• Stored procedures<br>Exercises<br>4.1 Consider the insurance database of Figure 4.12, where the primary keys are un-<br>derlined. Construct the following SQL queries for this relational database.<br>a. Find the total number of people who owned cars that were involved in ac-<br>cidents in 1989.<br>b. Find the number of accidents in which the cars belonging to “John Smith”<br>were involved.<br>c. Add a new accident to the database; assume any values for required at-<br>tributes.<br>d. Delete the Mazda belonging to “John Smith”.<br>e. Update the damage amount for the car with license number “AABB2000” in<br>the accident with report number “AR2197” to $3000.<br>4.2 Consider the employee database of Figure 4.13, where the primary keys are un-<br>derlined. Give an expression in SQL for each of the following queries.<br>a. Find the names of all employees who work for First Bank Corporation.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>II. Relational Databases<br>4. SQL<br>190<br>© The McGraw−Hill <br>Companies, 2001<br>184<br>Chapter 4<br>SQL<br>person (driver-id#, name, address)<br>car (license, model, year)<br>accident (report-number, date, location)<br>owns (driver-id#, license)<br>participated (driver-id, car, report-number, damage-amount)<br>Figure 4.12<br>Insurance database.<br>employee (employee-name, street, city)<br>works (employee-name, company-name, salary)<br>company (company-name, city)<br>manages (employee-name, manager-name)<br>Figure 4.13<br>Employee database.<br>b. Find the names and cities of residence of all employees who work for First<br>Bank Corporation.<br>c. Find the names, street addresses, and cities of residence of all employees<br>who work for First Bank Corporation and earn more than $10,000.<br>d. Find all employees in the database who live in the same cities as the com-<br>panies for which they work.<br>e. Find all employees in the database who live in the same cities and on the<br>same streets as do their managers.<br>f. Find all employees in the database who do not work for First Bank Corpo-<br>ration.<br>g. Find all employees in the database who earn more than each employee of<br>Small Bank Corporation.<br>h. Assume that the companies may be located in several cities. Find all com-<br>panies located in every city in which Small Bank Corporation is located.<br>i. Find all employees who earn more than the average salary of all employees<br>of their company.<br>j. Find the company that has the most employees.<br>k. Find the company that has the smallest payroll.<br>l. Find those companies whose employees earn a higher salary, on average,<br>than the average salary at First Bank Corporation.<br>4.3 Consider the relational database of Figure 4.13. Give an expression in SQL for<br>each of the following queries.<br>a. Modify the database so that Jones now lives in Newtown.<br>b. Give all employees of First Bank Corporation a 10 percent raise.<br>c. Give all managers of First Bank Corporation a 10 percent raise.<br>d. Give all managers of First Bank Corporation a 10 percent raise unless the<br>salary becomes greater than $100,000; in such cases, give only a 3 percent<br>raise.<br>e. Delete all tuples in the works relation for employees of Small Bank Corpora-<br>tion.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>II. Relational Databases<br>4. SQL<br>191<br>© The McGraw−Hill <br>Companies, 2001<br>Exercises<br>185<br>4.4 Let the following relation schemas be given:<br>R = (A, B, C)<br>S = (D, E, F)<br>Let relations r(R) and s(S) be given. Give an expression in SQL that is equivalent<br>to each of the following queries.<br>a. ΠA(r)<br>b</span><br><br><span style="background-color: #FFC6FF;" title="Chunk 23 | Start: 460046 | End: 480046 | Tokens: 3307">. σB = 17 (r)<br>c. r × s<br>d. ΠA,F (σC = D(r × s))<br>4.5 Let R = (A, B, C), and let r1 and r2 both be relations on schema R. Give an<br>expression in SQL that is equivalent to each of the following queries.<br>a. r1 ∪r2<br>b. r1 ∩r2<br>c. r1 −r2<br>d. ΠAB(r1)<br> ΠBC(r2)<br>4.6 Let R = (A, B) and S = (A, C), and let r(R) and s(S) be relations. Write an<br>expression in SQL for each of the queries below:<br>a. {&lt; a &gt; | ∃b (&lt; a, b &gt; ∈r ∧b = 17)}<br>b. {&lt; a, b, c &gt; | &lt; a, b &gt; ∈r ∧&lt; a, c &gt; ∈s}<br>c. {&lt; a &gt; | ∃c (&lt; a, c &gt; ∈s ∧∃b1, b2 (&lt; a, b1 &gt; ∈r ∧&lt; c, b2 &gt; ∈r ∧b1 &gt;<br>b2))}<br>4.7 Show that, in SQL, &lt;&gt; all is identical to not in.<br>4.8 Consider the relational database of Figure 4.13. Using SQL, deﬁne a view con-<br>sisting of manager-name and the average salary of all employees who work for<br>that manager. Explain why the database system should not allow updates to be<br>expressed in terms of this view.<br>4.9 Consider the SQL query<br>select p.a1<br>from p, r1, r2<br>where p.a1 = r1.a1 or p.a1 = r2.a1<br>Under what conditions does the preceding query select values of p.a1 that are<br>either in r1 or in r2? Examine carefully the cases where one of r1 or r2 may be<br>empty.<br>4.10 Write an SQL query, without using a with clause, to ﬁnd all branches where<br>the total account deposit is less than the average total account deposit at all<br>branches,<br>a. Using a nested query in the from clauser.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>II. Relational Databases<br>4. SQL<br>192<br>© The McGraw−Hill <br>Companies, 2001<br>186<br>Chapter 4<br>SQL<br>b. Using a nested query in a having clause.<br>4.11 Suppose that we have a relation marks(student-id, score) and we wish to assign<br>grades to students based on the score as follows: grade F if score &lt; 40, grade C<br>if 40 ≤score &lt; 60, grade B if 60 ≤score &lt; 80, and grade A if 80 ≤score. Write<br>SQL queries to do the following:<br>a. Display the grade for each student, based on the marks relation.<br>b. Find the number of students with each grade.<br>4.12 SQL-92 provides an n-ary operation called coalesce, which is deﬁned as follows:<br>coalesce(A1, A2, . . . , An) returns the ﬁrst nonnull Ai in the list A1, A2, . . . , An,<br>and returns null if all of A1, A2, . . . , An are null. Show how to express the coa-<br>lesce operation using the case operation.<br>4.13 Let a and b be relations with the schemas A(name, address, title) and B(name, ad-<br>dress, salary), respectively. Show how to express a natural full outer join b using<br>the full outer join operation with an on condition and the coalesce operation.<br>Make sure that the result relation does not contain two copies of the attributes<br>name and address, and that the solution is correct even if some tuples in a and b<br>have null values for attributes name or address.<br>4.14 Give an SQL schema deﬁnition for the employee database of Figure 4.13. Choose<br>an appropriate domain for each attribute and an appropriate primary key for<br>each relation schema.<br>4.15 Write check conditions for the schema you deﬁned in Exercise 4.14 to ensure<br>that:<br>a. Every employee works for a company located in the same city as the city in<br>which the employee lives.<br>b. No employee earns a salary higher than that of his manager.<br>4.16 Describe the circumstances in which you would choose to use embedded SQL<br>rather than SQL alone or only a general-purpose programming language.<br>Bibliographical Notes<br>The original version of SQL, called Sequel 2, is described by Chamberlin et al. [1976].<br>Sequel 2 was derived from the languages Square Boyce et al. [1975] and Chamber-<br>lin and Boyce [1974]. The American National Standard SQL-86 is described in ANSI<br>[1986]. The IBM Systems Application Architecture deﬁnition of SQL is deﬁned by IBM<br>[1987]. The ofﬁcial standards for SQL-89 and SQL-92 are available as ANSI [1989] and<br>ANSI [1992], respectively.<br>Textbook descriptions of the SQL-92 language include Date and Darwen [1997],<br>Melton and Simon [1993], and Cannan and Otten [1993]. Melton and Eisenberg [2000]<br>provides a guide to SQLJ, JDBC, and related technologies. More information on SQLJ<br>and SQLJ software can be obtained from http://www.sqlj.org. Date and Darwen [1997]<br>and Date [1993a] include a critique of SQL-92.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>II. Relational Databases<br>4. SQL<br>193<br>© The McGraw−Hill <br>Companies, 2001<br>Bibliographical Notes<br>187<br>Eisenberg and Melton [1999] provide an overview of SQL:1999. The standard is<br>published as a sequence of ﬁve ISO/IEC standards documents, with several more<br>parts describing various extensions under development. Part 1 (SQL/Framework),<br>gives an overview of the other parts. Part 2 (SQL/Foundation) outlines the basics of<br>the language. Part 3 (SQL/CLI) describes the Call-Level Interface. Part 4 (SQL/PSM)<br>describes Persistent Stored Modules, and Part 5 (SQL/Bindings) describes host lan-<br>guage bindings. The standard is useful to database implementers but is very hard<br>to read. If you need them, you can purchase them electronically from the Web site<br>http://webstore.ansi.org.<br>Many database products support SQL features beyond those speciﬁed in the stan-<br>dards, and may not support some features of the standard. More information on<br>these features may be found in the SQL user manuals of the respective products.<br>http://java.sun.com/docs/books/tutorial is an excellent source for more (and up-to-<br>date) information on JDBC, and on Java in general. References to books on Java (in-<br>cluding JDBC) are also available at this URL. The ODBC API is described in Microsoft<br>[1997] and Sanders [1998].<br>The processing of SQL queries, including algorithms and performance issues, is<br>discussed in Chapters 13 and 14. Bibliographic references on these matters appear in<br>that chapter.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>II. Relational Databases<br>5. Other Relational <br>Languages<br>194<br>© The McGraw−Hill <br>Companies, 2001<br>C<br>H<br>A<br>P<br>T<br>E<br>R<br>5<br>Other Relational Languages<br>In Chapter 4, we described SQL—the most inﬂuential commercial relational-database<br>language. In this chapter, we study two more languages: QBE and Datalog. Unlike<br>SQL, QBE is a graphical language, where queries look like tables. QBE and its variants<br>are widely used in database systems on personal computers. Datalog has a syntax<br>modeled after the Prolog language. Although not used commercially at present, Dat-<br>alog has been used in several research database systems.<br>Here, we present fundamental constructs and concepts rather than a complete<br>users’ guide for these languages. Keep in mind that individual implementations of a<br>language may differ in details, or may support only a subset of the full language.<br>In this chapter, we also study forms interfaces and tools for generating reports and<br>analyzing data. While these are not strictly speaking languages, they form the main<br>interface to a database for many users. In fact, most users do not perform explicit<br>querying with a query language at all, and access data only via forms, reports, and<br>other data analysis tools.<br>5.1<br>Query-by-Example<br>Query-by-Example (QBE) is the name of both a data-manipulation language and an<br>early database system that included this language. The QBE database system was<br>developed at IBM’s T. J. Watson Research Center in the early 1970s. The QBE data-<br>manipulation language was later used in IBM’s Query Management Facility (QMF).<br>Today, many database systems for personal computers support variants of QBE lan-<br>guage. In this section, we consider only the data-manipulation language. It has two<br>distinctive features:<br>1. Unlike most query languages and programming languages, QBE has a two-<br>dimensional syntax: Queries look like tables. A query in a one-dimensional<br>189<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>II. Relational Databases<br>5. Other Relational <br>Languages<br>195<br>© The McGraw−Hill <br>Companies, 2001<br>190<br>Chapter 5<br>Other Relational Languages<br>language (for example, SQL) can be written in one (possibly long) line. A two-<br>dimensional language requires two dimensions for its expression. (There is a<br>one-dimensional version of QBE, but we shall not consider it in our discus-<br>sion).<br>2. QBE queries are expressed “by example.” Instead of giving a procedure for<br>obtaining the desired answer, the user gives an example of what is desired.<br>The system generalizes this example to compute the answer to the query.<br>Despite these unusual features, there is a close correspondence between QBE and the<br>domain relational calculus.<br>We express queries in QBE by skeleton tables. These tables show the relation<br>schema, as in Figure 5.1. Rather than clutter the display with all skeletons, the user se-<br>lects those skeletons needed for a given query and ﬁlls in the skeletons with example<br>rows. An example row consists of constants and example elements, which are domain<br>variables. To avoid confusion between the two, QBE uses an underscore character ( )<br>before domain variables, as in x, and lets constants appear without any qualiﬁcation.<br>branch<br>branch-name<br>branch-city<br>assets<br>customer<br>customer-name<br>customer-street<br>customer-city<br>loan<br>loan-number<br>branch-name<br>amount<br>borrower<br>customer-name<br>loan-number<br>account<br>account-number<br>branch-name<br>balance<br>depositor<br>customer-name<br>account-number<br>Figure 5.1<br>QBE skeleton tables for the bank example.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>II. Relational Databases<br>5. Other Relational <br>Languages<br>196<br>© The McGraw−Hill <br>Companies, 2001<br>5.1<br>Query-by-Example<br>191<br>This convention is in contrast to those in most other languages, in which constants<br>are quoted and variables appear without any qualiﬁcation.<br>5.1.1<br>Queries on One Relation<br>Returning to our ongoing bank example, to ﬁnd all loan numbers at the Perryridge<br>branch, we bring up the skeleton for the loan relation, and ﬁll it in as follows:<br>loan<br>loan-number<br>branch-name<br>amount<br>P. x<br>Perryridge<br>This query tells the system to look for tuples in loan that have “Perryridge” as the<br>value for the branch-name attribute. For each such tuple, the system assigns the value<br>of the loan-number attribute to the variable x. It “prints” (actually, displays) the value<br>of the variable x, because the command P. appears in the loan-number column next to<br>the variable x. Observe that this result is similar to what would be done to answer<br>the domain-relational-calculus query<br>{⟨x⟩| ∃b, a(⟨x, b, a⟩∈loan ∧b = “Perryridge”)}<br>QBE assumes that a blank position in a row contains a unique variable. As a result,<br>if a variable does not appear more than once in a query, it may be omitted. Our<br>previous query could thus be rewritten as<br>loan<br>loan-number<br>branch-name<br>amount<br>P.<br>Perryridge<br>QBE (unlike SQL) performs duplicate elimination automatically. To suppress du-<br>plicate elimination, we insert the command ALL. after the P. command:<br>loan<br>loan-number<br>branch-name<br>amount<br>P.ALL.<br>Perryridge<br>To display the entire loan relation, we can create a single row consisting of P. in<br>every ﬁeld. Alternatively, we can use a shorthand notation by placing a single P. in<br>the column headed by the relation name:<br>loan<br>loan-number<br>branch-name<br>amount<br>P.<br>QBE allows queries that involve arithmetic comparisons (for example, &gt;), rather<br>than equality comparisons, as in “Find the loan numbers of all loans with a loan<br>amount of more than $700”:<br>loan<br>loan-number<br>branch-name<br>amount<br>P.<br>&gt;700<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>II. Relational Databases<br>5. Other Relational <br>Languages<br>197<br>© The McGraw−Hill <br>Companies, 2001<br>192<br>Chapter 5<br>Other Relational Languages<br>Comparisons can involve only one arithmetic expression on the right-hand side of<br>the comparison operation (for example, &gt; ( x + y −20)). The expression can include<br>both variables and constants. The space on the left-hand side of the comparison op-<br>eration must be blank. The arithmetic operations that QBE supports are =, &lt;, ≤, &gt;,<br>≥, and ¬.<br>Note that requiring the left-hand side to be blank implies that we cannot compare<br>two distinct named variables. We shall deal with this difﬁculty shortly.<br>As yet another example, consider the query “Find the names of all branches that<br>are not located in Brooklyn.” This query can be written as follows:<br>branch<br>branch-name<br>branch-city<br>assets<br>P.<br>¬ Brooklyn<br>The primary purpose of variables in QBE is to force values of certain tuples to have<br>the same value on certain attributes. Consider the query “Find the loan numbers of<br>all loans made jointly to Smith and Jones”:<br>borrower<br>customer-name<br>loan-number<br>“Smith”<br>P. x<br>“Jones”<br>x<br>To execute this query, the system ﬁnds all pairs of tuples in borrower that agree on<br>the loan-number attribute, where the value for the customer-name attribute is “Smith”<br>for one tuple and “Jones” for the other. The system then displays the value of the<br>loan-number attribute.<br>In the domain relational calculus, the query would be written as<br>{⟨l⟩| ∃x (⟨x, l⟩∈borrower ∧x = “Smith”)<br>∧∃x (⟨x, l⟩∈borrower ∧x = “Jones”)}<br>As another example, consider the query “Find all customers who live in the same<br>city as Jones”:<br>customer<br>customer-name<br>customer-street<br>customer-city<br>P. x<br>y<br>Jones<br>y<br>5.1.2<br>Queries on Several Relations<br>QBE allows queries that span several different relations (analogous to Cartesian prod-<br>uct or natural join in the relational algebra). The connections among the various rela-<br>tions are achieved through variables that force certain tuples to have the same value<br>on certain attributes. As an illustration, suppose that we want to ﬁnd the names of all<br>customers who have a loan from the Perryridge branch. This query can be written as<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>II. Relational Databases<br>5. Other Relational <br>Languages<br>198<br>© The McGraw−Hill <br>Companies, 2001<br>5.1<br>Query-by-Example<br>193<br>loan<br>loan-number<br>branch-name<br>amount<br>x<br>Perryridge<br>borrower<br>customer-name<br>loan-number<br>P. y<br>x<br>To evaluate the preceding query, the system ﬁnds tuples in loan with “Perryridge”<br>as the value for the branch-name attribute. For each such tuple, the system ﬁnds tu-<br>ples in borrower with the same value for the loan-number attribute as the loan tuple. It<br>displays the values for the customer-name attribute.<br>We can use a technique similar to the preceding one to write the query “Find the<br>names of all customers who have both an account and a loan at the bank”:<br>depositor<br>customer-name<br>account-number<br>P. x<br>borrower<br>customer-name<br>loan-number<br>x<br>Now consider the query “Find the names of all customers who have an account<br>at the bank, but who do not have a loan from the bank.” We express queries that<br>involve negation in QBE by placing a not sign (¬) under the relation name and next<br>to an example row:<br>depositor<br>customer-name<br>account-number<br>P. x<br>borrower<br>customer-name<br>loan-number<br>x<br>¬<br>Compare the preceding query with our earlier query “Find the names of all cus-<br>tomers who have both an account and a loan at the bank.” The only difference is the ¬<br>appearing next to the example row in the borrower skeleton. This difference, however,<br>has a major effect on the processing of the query. QBE ﬁnds all x values for which<br>1. There is a tuple in the depositor relation whose customer-name is the domain<br>variable x.<br>2. There is no tuple in the borrower relation whose customer-name is the same as<br>in the domain variable x.<br>The ¬ can be read as “there does not exist.”<br>The fact that we placed the ¬ under the relation name, rather than under an at-<br>tribute name, is important. A ¬ under an attribute name is shorthand for ̸=. Thus, to<br>ﬁnd all customers who have at least two accounts, we write<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>II. Relational Databases<br>5. Other Relational <br>Languages<br>199<br>© The McGraw−Hill <br>Companies, 2001<br>194<br>Chapter 5<br>Other Relational Languages<br>depositor<br>customer-name<br>account-number<br>P. x<br>y<br>x<br>¬ y<br>In English, the preceding query reads “Display all customer-name values that ap-<br>pear in at least two tuples, with the second tuple having an account-number different<br>from the ﬁrst.”<br>5.1.3<br>The Condition Box<br>At times, it is either inconvenient or impossible to express all the constraints on the<br>domain variables within the skeleton tables. To overcome this difﬁculty, QBE includes<br>a condition box feature that allows the expression of general constraints over any of<br>the domain variables. QBE allows logical expressions to appear in a condition box.<br>The logical operators are the words and and or, or the symbols “&amp;” and “|”.<br>For example, the query “Find the loan numbers of all loans made to Smith, to Jones<br>(or to both jointly)” can be written as<br>borrower<br>customer-name<br>loan-number<br>n<br>P. x<br>conditions<br>n = Smith or n = Jones<br>It is possible to express the above query without using a condition box, by using<br>P. in multiple rows. However, queries with P. in multiple rows are sometimes hard to<br>understand, and are best avoided.<br>As yet another example, suppose that we modify the ﬁnal query in Section 5.1.2<br>to be “Find all customers who are not named ‘Jones’ and who have at least two ac-<br>counts.” We want to include an “x ̸= Jones” constraint in this query. We do that by<br>bringing up the condition box and entering the constraint “x ¬ = Jones”:<br>conditions<br>x ¬ = Jones<br>Turning to another example, to ﬁnd all account numbers with a balance between<br>$1300 and $1500, we write<br>account<br>account-number<br>branch-name<br>balance<br>P.<br>x<br>conditions<br>x ≥1300<br>x ≤1500<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>II. Relational Databases<br>5. Other Relational <br>Languages<br>200<br>© The McGraw−Hill <br>Companies, 2001<br>5.1<br>Query-by-Example<br>195<br>As another example, consider the query “Find all branches that have assets greater<br>than those of at least one branch located in Brooklyn.” This query can be written as<br>branch<br>branch-name<br>branch-city<br>assets<br>P. x<br>y<br>Brooklyn<br>z<br>conditions<br>y &gt;<br>z<br>QBE allows complex arithmetic expressions to appear in a condition box. We can<br>write the query “Find all branches that have assets that are at least twice as large as<br>the assets of one of the branches located in Brooklyn” much as we did in the preced-<br>ing query, by modifying the condition box to<br>conditions<br>y ≥2 * z<br>To ﬁnd all account numbers of account with a balance between $1300 and $2000,<br>but not exactly $1500, we write<br>account<br>account-number<br>branch-name<br>balance<br>P.<br>x<br>x<br>conditions<br>( ≥1300<br>≤2000<br>¬ 1500)<br>and<br>and<br>=<br>QBE uses the or construct in an unconventional way to allow comparison with a set<br>of constant values. To ﬁnd all branches that are located in either Brooklyn or Queens,<br>we write<br>branch<br>branch-name<br>branch-city<br>assets<br>P.<br>x<br>x = (Brooklyn or Queens)<br>conditions<br>5.1.4<br>The Result Relation<br>The queries that we have written thus far have one characteristic in common: The<br>results to be displayed appear in a single relation schema. If the result of a query<br>includes attributes from several relation schemas, we need a mechanism to display<br>the desired result in a single table. For this purpose, we can declare a temporary result<br>relation that includes all the attributes of the result of the query. We print the desired<br>result by including the command P. in only the result skeleton table.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>II. Relational Databases<br>5. Other Relational <br>Languages<br>201<br>© The McGraw−Hill <br>Companies, 2001<br>196<br>Chapter 5<br>Other Relational Languages<br>As an illustration, consider the query “Find the customer-name, account-number, and<br>balance for all accounts at the Perryridge branch.” In relational algebra, we would<br>construct this query as follows:<br>1. Join depositor and account.<br>2. Project customer-name, account-number, and balance.<br>To construct the same query in QBE, we proceed as follows:<br>1. Create a skeleton table, called result, with attributes customer-name, account-<br>number, and balance. The name of the newly created skeleton table (that is,<br>result) must be different from any of the previously existing database relation<br>names.<br>2. Write the query.<br>The resulting query is<br>account<br>account-number<br>branch-name<br>balance<br>y<br>Perryridge<br>z<br>depositor<br>customer-name<br>account-number<br>x<br>y<br>result<br>customer-name<br>account-number<br>balance<br>P.<br>x<br>y<br>z<br>5.1.5<br>Ordering of the Di</span><br><br><span style="background-color: #FFADAD;" title="Chunk 24 | Start: 480048 | End: 500048 | Tokens: 3320">splay of Tuples<br>QBE offers the user control over the order in which tuples in a relation are displayed.<br>We gain this control by inserting either the command AO. (ascending order) or the<br>command DO. (descending order) in the appropriate column. Thus, to list in ascend-<br>ing alphabetic order all customers who have an account at the bank, we write<br>depositor<br>customer-name<br>account-number<br>P.AO.<br>QBE provides a mechanism for sorting and displaying data in multiple columns.<br>We specify the order in which the sorting should be carried out by including, with<br>each sort operator (AO or DO), an integer surrounded by parentheses. Thus, to list all<br>account numbers at the Perryridge branch in ascending alphabetic order with their<br>respective account balances in descending order, we write<br>account<br>account-number<br>branch-name<br>balance<br>P.AO(1).<br>Perryridge<br>P.DO(2).<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>II. Relational Databases<br>5. Other Relational <br>Languages<br>202<br>© The McGraw−Hill <br>Companies, 2001<br>5.1<br>Query-by-Example<br>197<br>The command P.AO(1). speciﬁes that the account number should be sorted ﬁrst;<br>the command P.DO(2). speciﬁes that the balances for each account should then be<br>sorted.<br>5.1.6<br>Aggregate Operations<br>QBE includes the aggregate operators AVG, MAX, MIN, SUM, and CNT. We must post-<br>ﬁx these operators with ALL. to create a multiset on which the aggregate operation is<br>evaluated. The ALL. operator ensures that duplicates are not eliminated. Thus, to ﬁnd<br>the total balance of all the accounts maintained at the Perryridge branch, we write<br>account<br>account-number<br>branch-name<br>balance<br>Perryridge<br>P.SUM.ALL.<br>We use the operator UNQ to specify that we want duplicates eliminated. Thus, to<br>ﬁnd the total number of customers who have an account at the bank, we write<br>depositor<br>customer-name<br>account-number<br>P.CNT.UNQ.<br>QBE also offers the ability to compute functions on groups of tuples using the G.<br>operator, which is analogous to SQL’s group by construct. Thus, to ﬁnd the average<br>balance at each branch, we can write<br>account<br>account-number<br>branch-name<br>balance<br>P.G.<br>P.AVG.ALL. x<br>The average balance is computed on a branch-by-branch basis. The keyword ALL.<br>in the P.AVG.ALL. entry in the balance column ensures that all the balances are consid-<br>ered. If we wish to display the branch names in ascending order, we replace P.G. by<br>P.AO.G.<br>To ﬁnd the average account balance at only those branches where the average<br>account balance is more than $1200, we add the following condition box:<br>conditions<br>AVG.ALL. x &gt; 1200<br>As another example, consider the query “Find all customers who have accounts at<br>each of the branches located in Brooklyn”:<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>II. Relational Databases<br>5. Other Relational <br>Languages<br>203<br>© The McGraw−Hill <br>Companies, 2001<br>198<br>Chapter 5<br>Other Relational Languages<br>depositor<br>customer-name<br>account-number<br>P.G. x<br>y<br>account<br>account-number<br>branch-name<br>balance<br>y<br>z<br>branch<br>branch-name<br>branch-city<br>assets<br>z<br>Brooklyn<br>w<br>Brooklyn<br>conditions<br>CNT.UNQ. z =<br>CNT.UNQ. w<br>The domain variable w can hold the value of names of branches located in Brook-<br>lyn. Thus, CNT.UNQ. w is the number of distinct branches in Brooklyn. The domain<br>variable z can hold the value of branches in such a way that both of the following<br>hold:<br>• The branch is located in Brooklyn.<br>• The customer whose name is x has an account at the branch.<br>Thus, CNT.UNQ. z is the number of distinct branches in Brooklyn at which customer x<br>has an account. If CNT.UNQ. z = CNT.UNQ. w, then customer x must have an account<br>at all of the branches located in Brooklyn. In such a case, the displayed result includes<br>x (because of the P.).<br>5.1.7<br>Modiﬁcation of the Database<br>In this section, we show how to add, remove, or change information in QBE.<br>5.1.7.1<br>Deletion<br>Deletion of tuples from a relation is expressed in much the same way as a query. The<br>major difference is the use of D. in place of P. QBE (unlike SQL), lets us delete whole<br>tuples, as well as values in selected columns. When we delete information in only<br>some of the columns, null values, speciﬁed by −, are inserted.<br>We note that a D. command operates on only one relation. If we want to delete<br>tuples from several relations, we must use one D. operator for each relation.<br>Here are some examples of QBE delete requests:<br>• Delete customer Smith.<br>customer<br>customer-name<br>customer-street<br>customer-city<br>D.<br>Smith<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>II. Relational Databases<br>5. Other Relational <br>Languages<br>204<br>© The McGraw−Hill <br>Companies, 2001<br>5.1<br>Query-by-Example<br>199<br>• Delete the branch-city value of the branch whose name is “Perryridge.”<br>branch<br>branch-name<br>branch-city<br>assets<br>Perryridge<br>D.<br>Thus, if before the delete operation the branch relation contains the tuple<br>(Perryridge, Brooklyn, 50000), the delete results in the replacement of the pre-<br>ceding tuple with the tuple (Perryridge, −, 50000).<br>• Delete all loans with a loan amount between $1300 and $1500.<br>loan<br>loan-number<br>branch-name<br>amount<br>D.<br>y<br>x<br>borrower<br>customer-name<br>loan-number<br>D.<br>y<br>conditions<br>x = (≥ 1300<br> ≤ 1500)<br>and<br>Note that to delete loans we must delete tuples from both the loan and bor-<br>rower relations.<br>• Delete all accounts at all branches located in Brooklyn.<br>account<br>account-number<br>branch-name<br>balance<br>D.<br>y<br>x<br>depositor<br>customer-name<br>account-number<br>D.<br>y<br>branch<br>branch-name<br>branch-city<br>assets<br>x<br>Brooklyn<br>Note that, in expressing a deletion, we can reference relations other than those from<br>which we are deleting information.<br>5.1.7.2<br>Insertion<br>To insert data into a relation, we either specify a tuple to be inserted or write a query<br>whose result is a set of tuples to be inserted. We do the insertion by placing the I.<br>operator in the query expression. Obviously, the attribute values for inserted tuples<br>must be members of the attribute’s domain.<br>The simplest insert is a request to insert one tuple. Suppose that we wish to insert<br>the fact that account A-9732 at the Perryridge branch has a balance of $700. We write<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>II. Relational Databases<br>5. Other Relational <br>Languages<br>205<br>© The McGraw−Hill <br>Companies, 2001<br>200<br>Chapter 5<br>Other Relational Languages<br>account<br>account-number<br>branch-name<br>balance<br>I.<br>A-9732<br>Perryridge<br>700<br>We can also insert a tuple that contains only partial information. To insert infor-<br>mation into the branch relation about a new branch with name “Capital” and city<br>“Queens,” but with a null asset value, we write<br>branch<br>branch-name<br>branch-city<br>assets<br>I.<br>Capital<br>Queens<br>More generally, we might want to insert tuples on the basis of the result of a query.<br>Consider again the situation where we want to provide as a gift, for all loan cus-<br>tomers of the Perryridge branch, a new $200 savings account for every loan account<br>that they have, with the loan number serving as the account number for the savings<br>account. We write<br>account<br>account-number<br>branch-name<br>balance<br>I.<br>x<br>Perryridge<br>200<br>depositor<br>customer-name<br>account-number<br>I.<br>y<br>x<br>loan<br>loan-number<br>branch-name<br>amount<br>x<br>Perryridge<br>borrower<br>customer-name<br>loan-number<br>y<br>x<br>To execute the preceding insertion request, the system must get the appropriate<br>information from the borrower relation, then must use that information to insert the<br>appropriate new tuple in the depositor and account relations.<br>5.1.7.3<br>Updates<br>There are situations in which we wish to change one value in a tuple without chang-<br>ing all values in the tuple. For this purpose, we use the U. operator. As we could<br>for insert and delete, we can choose the tuples to be updated by using a query. QBE,<br>however, does not allow users to update the primary key ﬁelds.<br>Suppose that we want to update the asset value of the of the Perryridge branch to<br>$10,000,000. This update is expressed as<br>branch<br>branch-name<br>branch-city<br>assets<br>Perryridge<br>U.10000000<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>II. Relational Databases<br>5. Other Relational <br>Languages<br>206<br>© The McGraw−Hill <br>Companies, 2001<br>5.1<br>Query-by-Example<br>201<br>The blank ﬁeld of attribute branch-city implies that no updating of that value is<br>required.<br>The preceding query updates the assets of the Perryridge branch to $10,000,000,<br>regardless of the old value. There are circumstances, however, where we need to<br>update a value by using the previous value. Suppose that interest payments are being<br>made, and all balances are to be increased by 5 percent. We write<br>account<br>account-number<br>branch-name<br>balance<br>U. x * 1.05<br>This query speciﬁes that we retrieve one tuple at a time from the account relation,<br>determine the balance x, and update that balance to x * 1.05.<br>5.1.8<br>QBE in Microsoft Access<br>In this section, we survey the QBE version supported by Microsoft Access. While<br>the original QBE was designed for a text-based display environment, Access QBE is<br>designed for a graphical display environment, and accordingly is called graphical<br>query-by-example (GQBE).<br>Figure 5.2<br>An example query in Microsoft Access QBE.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>II. Relational Databases<br>5. Other Relational <br>Languages<br>207<br>© The McGraw−Hill <br>Companies, 2001<br>202<br>Chapter 5<br>Other Relational Languages<br>Figure 5.2 shows a sample GQBE query. The query can be described in English as<br>“Find the customer-name, account-number, and balance for all accounts at the Perryridge<br>branch.” Section 5.1.4 showed how it is expressed in QBE.<br>A minor difference in the GQBE version is that the attributes of a table are writ-<br>ten one below the other, instead of horizontally. A more signiﬁcant difference is that<br>the graphical version of QBE uses a line linking attributes of two tables, instead of a<br>shared variable, to specify a join condition.<br>An interesting feature of QBE in Access is that links between tables are created<br>automatically, on the basis of the attribute name. In the example in Figure 5.2, the two<br>tables account and depositor were added to the query. The attribute account-number is<br>shared between the two selected tables, and the system automatically inserts a link<br>between the two tables. In other words, a natural join condition is imposed by default<br>between the tables; the link can be deleted if it is not desired. The link can also be<br>speciﬁed to denote a natural outer-join, instead of a natural join.<br>Another minor difference in Access QBE is that it speciﬁes attributes to be printed<br>in a separate box, called the design grid, instead of using a P. in the table. It also<br>speciﬁes selections on attribute values in the design grid.<br>Queries involving group by and aggregation can be created in Access as shown in<br>Figure 5.3. The query in the ﬁgure ﬁnds the name, street, and city of all customers<br>who have more than one account at the bank; we saw the QBE version of the query<br>earlier in Section 5.1.6. The group by attributes as well as the aggregate functions<br>Figure 5.3<br>An aggregation query in Microsoft Access QBE.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>II. Relational Databases<br>5. Other Relational <br>Languages<br>208<br>© The McGraw−Hill <br>Companies, 2001<br>5.2<br>Datalog<br>203<br>are noted in the design grid. If an attribute is to be printed, it must appear in the<br>design grid, and must be speciﬁed in the “Total” row to be either a group by, or<br>have an aggregate function applied to it. SQL has a similar requirement. Attributes<br>that participate in selection conditions but are not to be printed can alternatively be<br>marked as “Where” in the row “Total”, indicating that the attribute is neither a group<br>by attribute, nor one to be aggregated on.<br>Queries are created through a graphical user interface, by ﬁrst selecting tables.<br>Attributes can then be added to the design grid by dragging and dropping them<br>from the tables. Selection conditions, grouping and aggregation can then be speciﬁed<br>on the attributes in the design grid. Access QBE supports a number of other features<br>too, including queries to modify the database through insertion, deletion, or update.<br>5.2<br>Datalog<br>Datalog is a nonprocedural query language based on the logic-programming lan-<br>guage Prolog. As in the relational calculus, a user describes the information desired<br>without giving a speciﬁc procedure for obtaining that information. The syntax of Dat-<br>alog resembles that of Prolog. However, the meaning of Datalog programs is deﬁned<br>in a purely declarative manner, unlike the more procedural semantics of Prolog, so<br>Datalog simpliﬁes writing simple queries and makes query optimization easier.<br>5.2.1<br>Basic Structure<br>A Datalog program consists of a set of rules. Before presenting a formal deﬁnition<br>of Datalog rules and their formal meaning, we consider examples. Consider a Dat-<br>alog rule to deﬁne a view relation v1 containing account numbers and balances for<br>accounts at the Perryridge branch with a balance of over $700:<br>v1(A, B) :– account(A, “Perryridge”, B), B &gt; 700<br>Datalog rules deﬁne views; the preceding rule uses the relation account, and de-<br>ﬁnes the view relation v1. The symbol :– is read as “if,” and the comma separating<br>the “account(A, “Perryridge”, B)” from “B &gt; 700” is read as “and.” Intuitively, the<br>rule is understood as follows:<br>for all A, B<br>if<br>(A, “Perryridge”, B) ∈account and B &gt; 700<br>then<br>(A, B) ∈v1<br>Suppose that the relation account is as shown in Figure 5.4. Then, the view relation<br>v1 contains the tuples in Figure 5.5.<br>To retrieve the balance of account number A-217 in the view relation v1, we can<br>write the following query:<br>? v1(“A-217”, B)<br>The answer to the query is<br>(A-217, 750)<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>II. Relational Databases<br>5. Other Relational <br>Languages<br>209<br>© The McGraw−Hill <br>Companies, 2001<br>204<br>Chapter 5<br>Other Relational Languages<br>account-number<br>branch-name<br>balance<br>A-101<br>Downtown<br>500<br>A-215<br>Mianus<br>700<br>A-102<br>Perryridge<br>Perryridge<br>Perryridge<br>400<br>A-305<br>Round Hill<br>350<br>A-201<br>900<br>A-222<br>Redwood<br>700<br>A-217<br>750<br>Figure 5.4<br>The account relation.<br>To get the account number and balance of all accounts in relation v1, where the bal-<br>ance is greater than 800, we can write<br>? v1(A, B), B &gt; 800<br>The answer to this query is<br>(A-201, 900)<br>In general, we need more than one rule to deﬁne a view relation. Each rule deﬁnes<br>a set of tuples that the view relation must contain. The set of tuples in the view re-<br>lation is then deﬁned as the union of all these sets of tuples. The following Datalog<br>program speciﬁes the interest rates for accounts:<br>interest-rate(A, 5) :– account(A, N, B), B &lt; 10000<br>interest-rate(A, 6) :– account(A, N, B), B &gt;= 10000<br>The program has two rules deﬁning a view relation interest-rate, whose attributes are<br>the account number and the interest rate. The rules say that, if the balance is less than<br>$10000, then the interest rate is 5 percent, and if the balance is greater than or equal<br>to $10000, the interest rate is 6 percent.<br>Datalog rules can also use negation. The following rules deﬁne a view relation c<br>that contains the names of all customers who have a deposit, but have no loan, at the<br>bank:<br>c(N) :– depositor(N,A), not is-borrower(N)<br>is-borrower(N) :– borrower(N, L),<br>Prolog and most Datalog implementations recognize attributes of a relation by po-<br>sition and omit attribute names. Thus, Datalog rules are compact, compared to SQL<br>account-number<br>balance<br>A-201<br>900<br>A-217<br>750<br>Figure 5.5<br>The v1 relation.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>II. Relational Databases<br>5. Other Relational <br>Languages<br>210<br>© The McGraw−Hill <br>Companies, 2001<br>5.2<br>Datalog<br>205<br>queries. However, when relations have a large number of attributes, or the order or<br>number of attributes of relations may change, the positional notation can be cum-<br>bersome and error prone. It is not hard to create a variant of Datalog syntax using<br>named attributes, rather than positional attributes. In such a system, the Datalog rule<br>deﬁning v1 can be written as<br>v1(account-number A, balance B) :–<br>account(account-number A, branch-name “Perryridge”, balance B),<br>B &gt; 700<br>Translation between the two forms can be done without signiﬁcant effort, given the<br>relation schema.<br>5.2.2<br>Syntax of Datalog Rules<br>Now that we have informally explained rules and queries, we can formally deﬁne<br>their syntax; we discuss their meaning in Section 5.2.3. We use the same conventions<br>as in the relational algebra for denoting relation names, attribute names, and con-<br>stants (such as numbers or quoted strings). We use uppercase (capital) letters and<br>words starting with uppercase letters to denote variable names, and lowercase let-<br>ters and words starting with lowercase letters to denote relation names and attribute<br>names. Examples of constants are 4, which is a number, and “John,” which is a string;<br>X and Name are variables. A positive literal has the form<br>p(t1, t2, . . . , tn)<br>where p is the name of a relation with n attributes, and t1, t2, . . . ,tn are either con-<br>stants or variables. A negative literal has the form<br>not p(t1, t2, . . . , tn)<br>where relation p has n attributes. Here is an example of a literal:<br>account(A, “Perryridge”, B)<br>Literals involving arithmetic operations are treated specially. For example, the lit-<br>eral B &gt; 700, although not in the syntax just described, can be conceptually un-<br>derstood to stand for &gt; (B, 700), which is in the required syntax, and where &gt; is a<br>relation.<br>But what does this notation mean for arithmetic operations such as “&gt;”? The re-<br>lation &gt; (conceptually) contains tuples of the form (x, y) for every possible pair of<br>values x, y such that x &gt; y. Thus, (2, 1) and (5, −33) are both tuples in &gt;. Clearly,<br>the (conceptual) relation &gt; is inﬁnite. Other arithmetic operations (such as &gt;, =, +<br>or −) are also treated conceptually as relations. For example, A = B + C stands con-<br>ceptually for +(B, C, A), where the relation + contains every tuple (x, y, z) such that<br>z = x + y.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>II. Relational Databases<br>5. Other Relational <br>Languages<br>211<br>© The McGraw−Hill <br>Companies, 2001<br>206<br>Chapter 5<br>Other Relational Languages<br>A fact is written in the form<br>p(v1, v2, . . . , vn)<br>and denotes that the tuple (v1, v2, . . . , vn) is in relation p. A set of facts for a relation<br>can also be written in the usual tabular notation. A set of facts for the relations in a<br>database schema is equivalent to an instance of the database schema. Rules are built<br>out of literals and have the form<br>p(t1, t2, . . . , tn) :– L1, L2, . . . , Ln<br>where each Li is a (positive or negative) literal. The literal p(t1, t2, . . . , tn) is referred<br>to as the head of the rule, and the rest of the literals in the rule constitute the body of<br>the rule.<br>A Datalog program consists of a set of rules; the order in which the rules are writ-<br>ten has no signiﬁcance. As mentioned earlier, there may be several rules deﬁning a<br>relation.<br>Figure 5.6 shows a Datalog program that deﬁnes the interest on each account in<br>the Perryridge branch. The ﬁrst rule of the program deﬁnes a view relation interest,<br>whose attributes are the account number and the interest earned on the account. It<br>uses the relation account and the view relation interest-rate. The last two rules of the<br>program are rules that we saw earlier.<br>A view relation v1 is said to depend directly on a view relation v2 if v2 is used<br>in the expression deﬁning v1. In the above program, view relation interest depends<br>directly on relations interest-rate and account. Relation interest-rate in turn depends<br>directly on account.<br>A view relation v1 is said to depend indirectly on view relation v2 if there is a<br>sequence of intermediate relations i1, i2, . . . , in, for some n, such that v1 depends di-<br>rectly on i1, i1 depends directly on i2, and so on till in−1 depends on in.<br>In the example in Figure 5.6, since we have a chain of dependencies from interest<br>to interest-rate to account, relation interest also depends indirectly on account.<br>Finally, a view relation v1 is said to depend on view relation v2 if v1 either depends<br>di</span><br><br><span style="background-color: #FFD6A5;" title="Chunk 25 | Start: 500050 | End: 520050 | Tokens: 3513">rectly or indirectly on v2.<br>A view relation v is said to be recursive if it depends on itself. A view relation that<br>is not recursive is said to be nonrecursive.<br>Consider the program in Figure 5.7. Here, the view relation empl depends on itself<br>(becasue of the second rule), and is therefore recursive. In contrast, the program in<br>Figure 5.6 is nonrecursive.<br>interest(A, I) :– account(A, “Perryridge”, B),<br>interest-rate(A, R), I = B ∗R/100.<br>interest-rate(A, 5) :– account(A, N, B), B &lt; 10000.<br>interest-rate(A, 6) :– account(A, N, B), B &gt;= 10000.<br>Figure 5.6<br>Datalog program that deﬁnes interest on Perryridge accounts.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>II. Relational Databases<br>5. Other Relational <br>Languages<br>212<br>© The McGraw−Hill <br>Companies, 2001<br>5.2<br>Datalog<br>207<br>empl(X, Y ) :– manager(X, Y ).<br>empl(X, Y ) :– manager(X, Z), empl(Z, Y ).<br>Figure 5.7<br>Recursive Datalog program.<br>5.2.3<br>Semantics of Nonrecursive Datalog<br>We consider the formal semantics of Datalog programs. For now, we consider only<br>programs that are nonrecursive. The semantics of recursive programs is somewhat<br>more complicated; it is discussed in Section 5.2.6. We deﬁne the semantics of a pro-<br>gram by starting with the semantics of a single rule.<br>5.2.3.1<br>Semantics of a Rule<br>A ground instantiation of a rule is the result of replacing each variable in the rule<br>by some constant. If a variable occurs multiple times in a rule, all occurrences of<br>the variable must be replaced by the same constant. Ground instantiations are often<br>simply called instantiations.<br>Our example rule deﬁning v1, and an instantiation of the rule, are:<br>v1(A, B) :– account(A, “Perryridge”, B), B &gt; 700<br>v1(“A-217”, 750) :– account(“A-217”, “Perryridge”, 750), 750 &gt; 700<br>Here, variable A was replaced by “A-217,” and variable B by 750.<br>A rule usually has many possible instantiations. These instantiations correspond<br>to the various ways of assigning values to each variable in the rule.<br>Suppose that we are given a rule R,<br>p(t1, t2, . . . , tn) :– L1, L2, . . . , Ln<br>and a set of facts I for the relations used in the rule (I can also be thought of as a<br>database instance). Consider any instantiation R′ of rule R:<br>p(v1, v2, . . . , vn) :– l1, l2, . . . , ln<br>where each literal li is either of the form qi(vi,1, v1,2, . . . , vi,ni) or of the form not qi(vi,1,<br>v1,2, . . . , vi,ni), and where each vi and each vi,j is a constant.<br>We say that the body of rule instantiation R′ is satisﬁed in I if<br>1. For each positive literal qi(vi,1, . . . , vi,ni) in the body of R′, the set of facts I<br>contains the fact q(vi,1, . . . , vi,ni).<br>2. For each negative literal not qj(vj,1, . . . , vj,nj) in the body of R′, the set of facts<br>I does not contain the fact qj(vj,1, . . . , vj,nj).<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>II. Relational Databases<br>5. Other Relational <br>Languages<br>213<br>© The McGraw−Hill <br>Companies, 2001<br>208<br>Chapter 5<br>Other Relational Languages<br>account-number<br>balance<br>A-201<br>900<br>A-217<br>750<br>Figure 5.8<br>Result of infer(R, I).<br>We deﬁne the set of facts that can be inferred from a given set of facts I using rule<br>R as<br>infer(R, I) = {p(t1, . . . , tni) | there is an instantiation R′ of R,<br>where p(t1, . . . , tni) is the head of R′, and<br>the body of R′ is satisﬁed in I}.<br>Given a set of rules R = {R1, R2, . . . , Rn}, we deﬁne<br>infer(R, I) = infer(R1, I) ∪infer(R2, I) ∪. . . ∪infer(Rn, I)<br>Suppose that we are given a set of facts I containing the tuples for relation account<br>in Figure 5.4. One possible instantiation of our running-example rule R is<br>v1(“A-217”, 750) :– account(“A-217”, “Perryridge”, 750), 750 &gt; 700.<br>The fact account(“A-217”, “Perryridge”, 750) is in the set of facts I. Further, 750 is<br>greater than 700, and hence conceptually (750, 700) is in the relation “&gt;”. Hence, the<br>body of the rule instantiation is satisﬁed in I. There are other possible instantiations<br>of R, and using them we ﬁnd that infer(R, I) has exactly the set of facts for v1 that<br>appears in Figure 5.8.<br>5.2.3.2<br>Semantics of a Program<br>When a view relation is deﬁned in terms of another view relation, the set of facts in<br>the ﬁrst view depends on the set of facts in the second one. We have assumed, in this<br>section, that the deﬁnition is nonrecursive; that is, no view relation depends (directly<br>or indirectly) on itself. Hence, we can layer the view relations in the following way,<br>and can use the layering to deﬁne the semantics of the program:<br>• A relation is in layer 1 if all relations used in the bodies of rules deﬁning it are<br>stored in the database.<br>• A relation is in layer 2 if all relations used in the bodies of rules deﬁning it<br>either are stored in the database or are in layer 1.<br>• In general, a relation p is in layer i + 1 if (1) it is not in layers 1, 2, . . . , i, and<br>(2) all relations used in the bodies of rules deﬁning p either are stored in the<br>database or are in layers 1, 2, . . . , i.<br>Consider the program in Figure 5.6. The layering of view relations in the program<br>appears in Figure 5.9. The relation account is in the database. Relation interest-rate is<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>II. Relational Databases<br>5. Other Relational <br>Languages<br>214<br>© The McGraw−Hill <br>Companies, 2001<br>5.2<br>Datalog<br>209<br>interest<br>account<br>interest-rate<br>perryridge-account<br>layer 2<br>layer 1<br>database<br>Figure 5.9<br>Layering of view relations.<br>in level 1, since all the relations used in the two rules deﬁning it are in the database.<br>Relation perryridge-account is similarly in layer 1. Finally, relation interest is in layer<br>2, since it is not in layer 1 and all the relations used in the rule deﬁning it are in the<br>database or in layers lower than 2.<br>We can now deﬁne the semantics of a Datalog program in terms of the layering of<br>view relations. Let the layers in a given program be 1, 2, . . . , n. Let Ri denote the set<br>of all rules deﬁning view relations in layer i.<br>• We deﬁne I0 to be the set of facts stored in the database, and deﬁne I1 as<br>I1 = I0 ∪infer(R1, I0)<br>• We proceed in a similar fashion, deﬁning I2 in terms of I1 and R2, and so on,<br>using the following deﬁnition:<br>Ii+1 = Ii ∪infer(Ri+1, Ii)<br>• Finally, the set of facts in the view relations deﬁned by the program (also called<br>the semantics of the program) is given by the set of facts In corresponding to<br>the highest layer n.<br>For the program in Figure 5.6, I0 is the set of facts in the database, and I1 is the set<br>of facts in the database along with all facts that we can infer from I0 using the rules for<br>relations interest-rate and perryridge-account. Finally, I2 contains the facts in I1 along<br>with the facts for relation interest that we can infer from the facts in I1 by the rule<br>deﬁning interest. The semantics of the program—that is, the set of those facts that are<br>in each of the view relations—is deﬁned as the set of facts I2.<br>Recall that, in Section 3.5.3, we saw how to deﬁne the meaning of nonrecursive<br>relational-algebra views by a technique known as view expansion. View expansion<br>can be used with nonrecursive Datalog views as well; conversely, the layering tech-<br>nique described here can also be used with relational-algebra views.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>II. Relational Databases<br>5. Other Relational <br>Languages<br>215<br>© The McGraw−Hill <br>Companies, 2001<br>210<br>Chapter 5<br>Other Relational Languages<br>5.2.4<br>Safety<br>It is possible to write rules that generate an inﬁnite number of answers. Consider the<br>rule<br>gt(X, Y ) :– X &gt; Y<br>Since the relation deﬁning &gt; is inﬁnite, this rule would generate an inﬁnite number<br>of facts for the relation gt, which calculation would, correspondingly, take an inﬁnite<br>amount of time and space.<br>The use of negation can also cause similar problems. Consider the rule:<br>not-in-loan(L, B, A) :– not loan(L, B, A)<br>The idea is that a tuple (loan-number, branch-name, amount) is in view relation not-in-<br>loan if the tuple is not present in the loan relation. However, if the set of possible ac-<br>count numbers, branch-names, and balances is inﬁnite, the relation not-in-loan would<br>be inﬁnite as well.<br>Finally, if we have a variable in the head that does not appear in the body, we may<br>get an inﬁnite number of facts where the variable is instantiated to different values.<br>So that these possibilities are avoided, Datalog rules are required to satisfy the<br>following safety conditions:<br>1. Every variable that appears in the head of the rule also appears in a nonarith-<br>metic positive literal in the body of the rule.<br>2. Every variable appearing in a negative literal in the body of the rule also ap-<br>pears in some positive literal in the body of the rule.<br>If all the rules in a nonrecursive Datalog program satisfy the preceding safety con-<br>ditions, then all the view relations deﬁned in the program can be shown to be ﬁnite,<br>as long as all the database relations are ﬁnite. The conditions can be weakened some-<br>what to allow variables in the head to appear only in an arithmetic literal in the body<br>in some cases. For example, in the rule<br>p(A) :– q(B), A = B + 1<br>we can see that if relation q is ﬁnite, then so is p, according to the properties of addi-<br>tion, even though variable A appears in only an arithmetic literal.<br>5.2.5<br>Relational Operations in Datalog<br>Nonrecursive Datalog expressions without arithmetic operations are equivalent in<br>expressive power to expressions using the basic operations in relational algebra (∪, −,<br>×, σ, Π and ρ). We shall not formally prove this assertion here. Rather, we shall show<br>through examples how the various relational-algebra operations can be expressed in<br>Datalog. In all cases, we deﬁne a view relation called query to illustrate the operations.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>II. Relational Databases<br>5. Other Relational <br>Languages<br>216<br>© The McGraw−Hill <br>Companies, 2001<br>5.2<br>Datalog<br>211<br>We have already seen how to do selection by using Datalog rules. We perform<br>projections simply by using only the required attributes in the head of the rule. To<br>project attribute account-name from account, we use<br>query(A) :– account(A, N, B)<br>We can obtain the Cartesian product of two relations r1 and r2 in Datalog as fol-<br>lows:<br>query(X1, X2, . . . , Xn, Y1, Y2, . . . , Ym) :– r1(X1, X2, . . . , Xn), r2(Y1, Y2, . . . , Ym)<br>where r1 is of arity n, and r2 is of arity m, and the X1, X2, . . . , Xn, Y1, Y2, . . . , Ym are<br>all distinct variable names.<br>We form the union of two relations r1 and r2 (both of arity n) in this way:<br>query(X1, X2, . . . , Xn) :– r1(X1, X2, . . . , Xn)<br>query(X1, X2, . . . , Xn) :– r2(X1, X2, . . . , Xn)<br>We form the set difference of two relations r1 and r2 in this way:<br>query(X1, X2, . . . , Xn) :– r1(X1, X2, . . . , Xn), not r2(X1, X2, . . . , Xn)<br>Finally, we note that with the positional notation used in Datalog, the renaming oper-<br>ator ρ is not needed. A relation can occur more than once in the rule body, but instead<br>of renaming to give distinct names to the relation occurrences, we can use different<br>variable names in the different occurrences.<br>It is possible to show that we can express any nonrecursive Datalog query without<br>arithmetic by using the relational-algebra operations. We leave this demonstration<br>as an exercise for you to carry out. You can thus establish the equivalence of the<br>basic operations of relational algebra and nonrecursive Datalog without arithmetic<br>operations.<br>Certain extensions to Datalog support the extended relational update operations<br>of insertion, deletion, and update. The syntax for such operations varies from imple-<br>mentation to implementation. Some systems allow the use of + or −in rule heads to<br>denote relational insertion and deletion. For example, we can move all accounts at<br>the Perryridge branch to the Johnstown branch by executing<br>+ account(A, “Johnstown”, B) :– account(A, “Perryridge”, B)<br>−account(A, “Perryridge”, B) :– account(A, “Perryridge”, B)<br>Some implementations of Datalog also support the aggregation operation of ex-<br>tended relational algebra. Again, there is no standard syntax for this operation.<br>5.2.6<br>Recursion in Datalog<br>Several database applications deal with structures that are similar to tree data struc-<br>tures. For example, consider employees in an organization. Some of the employees<br>are managers. Each manager manages a set of people who report to him or her. But<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>II. Relational Databases<br>5. Other Relational <br>Languages<br>217<br>© The McGraw−Hill <br>Companies, 2001<br>212<br>Chapter 5<br>Other Relational Languages<br>procedure Datalog-Fixpoint<br>I = set of facts in the database<br>repeat<br>Old I = I<br>I = I∪infer(R, I)<br>until I = Old I<br>Figure 5.10<br>Datalog-Fixpoint procedure.<br>each of these people may in turn be managers, and they in turn may have other peo-<br>ple who report to them. Thus employees may be organized in a structure similar to a<br>tree.<br>Suppose that we have a relation schema<br>Manager-schema = (employee-name, manager-name)<br>Let manager be a relation on the preceding schema.<br>Suppose now that we want to ﬁnd out which employees are supervised, directly<br>or indirectly by a given manager—say, Jones. Thus, if the manager of Alon is Barin-<br>sky, and the manager of Barinsky is Estovar, and the manager of Estovar is Jones,<br>then Alon, Barinsky, and Estovar are the employees controlled by Jones. People of-<br>ten write programs to manipulate tree data structures by recursion. Using the idea<br>of recursion, we can deﬁne the set of employees controlled by Jones as follows. The<br>people supervised by Jones are (1) people whose manager is Jones and (2) people<br>whose manager is supervised by Jones. Note that case (2) is recursive.<br>We can encode the preceding recursive deﬁnition as a recursive Datalog view,<br>called empl-jones:<br>empl-jones(X) :– manager(X, “Jones” )<br>empl-jones(X) :– manager(X, Y ), empl-jones(Y )<br>The ﬁrst rule corresponds to case (1); the second rule corresponds to case (2). The<br>view empl-jones depends on itself because of the second rule; hence, the preceding<br>Datalog program is recursive. We assume that recursive Datalog programs contain no<br>rules with negative literals. The reason will become clear later. The bibliographical<br>employee-name<br>manager-name<br>Alon<br>Barinsky<br>Barinsky<br>Estovar<br>Corbin<br>Duarte<br>Duarte<br>Jones<br>Estovar<br>Jones<br>Jones<br>Klinger<br>Rensal<br>Klinger<br>Figure 5.11<br>The manager relation.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>II. Relational Databases<br>5. Other Relational <br>Languages<br>218<br>© The McGraw−Hill <br>Companies, 2001<br>5.2<br>Datalog<br>213<br>Iteration number<br>Tuples in empl-jones<br>0<br>1<br>(Duarte), (Estovar)<br>2<br>(Duarte), (Estovar), (Barinsky), (Corbin)<br>3<br>(Duarte), (Estovar), (Barinsky), (Corbin), (Alon)<br>4<br>(Duarte), (Estovar), (Barinsky), (Corbin), (Alon)<br>Figure 5.12<br>Employees of Jones in iterations of procedure Datalog-Fixpoint.<br>notes refer to papers that describe where negation can be used in recursive Datalog<br>programs.<br>The view relations of a recursive program that contains a set of rules R are deﬁned<br>to contain exactly the set of facts I computed by the iterative procedure Datalog-<br>Fixpoint in Figure 5.10. The recursion in the Datalog program has been turned into<br>an iteration in the procedure. At the end of the procedure, infer(R, I) = I, and I is<br>called a ﬁxed point of the program.<br>Consider the program deﬁning empl-jones, with the relation manager, as in Fig-<br>ure 5.11. The set of facts computed for the view relation empl-jones in each iteration<br>appears in Figure 5.12. In each iteration, the program computes one more level of<br>employees under Jones and adds it to the set empl-jones. The procedure terminates<br>when there is no change to the set empl-jones, which the system detects by ﬁnding<br>I = Old I. Such a termination point must be reached, since the set of managers and<br>employees is ﬁnite. On the given manager relation, the procedure Datalog-Fixpoint<br>terminates after iteration 4, when it detects that no new facts have been inferred.<br>You should verify that, at the end of the iteration, the view relation empl-jones<br>contains exactly those employees who work under Jones. To print out the names of<br>the employees supervised by Jones deﬁned by the view, you can use the query<br>? empl-jones(N)<br>To understand procedure Datalog-Fixpoint, we recall that a rule infers new facts<br>from a given set of facts. Iteration starts with a set of facts I set to the facts in the<br>database. These facts are all known to be true, but there may be other facts that are<br>true as well.1 Next, the set of rules R in the given Datalog program is used to infer<br>what facts are true, given that facts in I are true. The inferred facts are added to I,<br>and the rules are used again to make further inferences. This process is repeated until<br>no new facts can be inferred.<br>For safe Datalog programs, we can show that there will be some point where no<br>more new facts can be derived; that is, for some k, Ik+1 = Ik. At this point, then, we<br>have the ﬁnal set of true facts. Further, given a Datalog program and a database, the<br>ﬁxed-point procedure infers all the facts that can be inferred to be true.<br>1.<br>The word “fact” is used in a technical sense to note membership of a tuple in a relation. Thus, in the<br>Datalog sense of “fact,” a fact may be true (the tuple is indeed in the relation) or false (the tuple is not in<br>the relation).<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>II. Relational Databases<br>5. Other Relational <br>Languages<br>219<br>© The McGraw−Hill <br>Companies, 2001<br>214<br>Chapter 5<br>Other Relational Languages<br>If a recursive program contains a rule with a negative literal, the following prob-<br>lem can arise. Recall that when we make an inference by using a ground instantiation<br>of a rule, for each negative literal notq in the rule body we check that q is not present<br>in the set of facts I. This test assumes that q cannot be inferred later. However, in<br>the ﬁxed-point iteration, the set of facts I grows in each iteration, and even if q is<br>not present in I at one iteration, it may appear in I later. Thus, we may have made<br>an inference in one iteration that can no longer be made at an earlier iteration, and<br>the inference was incorrect. We require that a recursive program should not contain<br>negative literals, in order to avoid such problems.<br>Instead of creating a view for the employees supervised by a speciﬁc manager<br>Jones, we can create a more general view relation empl that contains every tuple<br>(X, Y ) such that X is directly or indirectly managed by Y , using the following pro-<br>gram (also shown in Figure 5.7):<br>empl(X, Y ) :– manager(X, Y )<br>empl(X, Y ) :– manager(X, Z), empl(Z, Y )<br>To ﬁnd the direct and indirect subordinates of Jones, we simply use the query<br>? empl(X, “Jones”)<br>which gives the same set of values for X as the view empl-jones. Most Datalog imple-<br>mentations have sophisticated query optimizers and evaluation engines that can run<br>the preceding query at about the same speed they could evaluate the view empl-jones.<br>The view empl deﬁned previously is called the transitive closure of the relation<br>manager. If the relation manager were replaced by any other binary relation R, the<br>preceding program would deﬁne the transitive closure of R.<br>5.2.7<br>The Power of Recursion<br>Datalog with recursion has more expressive power than Datalog without recursion.<br>In other words, there are queries on the database that we can answer by using recur-<br>sion, but cannot answer without using it. For example, we cannot express transitive<br>closure in Datalog without using recursion (or for that matter, in SQL or QBE without<br>recursion). Consider the transitive closure of the relation manager. Intuitively, a ﬁxed<br>number of joins can ﬁnd only those employees that are some (other) ﬁxed number of<br>levels down from any manager (we will not attempt to prove this result here). Since<br>any given nonrecursive query has a ﬁxed number of joins, there is a limit on how<br>many levels of employees the query can ﬁnd. If the number of levels of employees<br>in the manager relation is more than the limit of the query, the </span><br><br><span style="background-color: #FDFFB6;" title="Chunk 26 | Start: 520052 | End: 540052 | Tokens: 3273">query will miss some<br>levels of employees. Thus, a nonrecursive Datalog program cannot express transitive<br>closure.<br>An alternative to recursion is to use an external mechanism, such as embedded<br>SQL, to iterate on a nonrecursive query. The iteration in effect implements the ﬁxed-<br>point loop of Figure 5.10. In fact, that is how such queries are implemented on data-<br>base systems that do not support recursion. However, writing such queries by iter-<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>II. Relational Databases<br>5. Other Relational <br>Languages<br>220<br>© The McGraw−Hill <br>Companies, 2001<br>5.2<br>Datalog<br>215<br>ation is more complicated than using recursion, and evaluation by recursion can be<br>optimized to run faster than evaluation by iteration.<br>The expressive power provided by recursion must be used with care. It is relatively<br>easy to write recursive programs that will generate an inﬁnite number of facts, as this<br>program illustrates:<br>number(0)<br>number(A) :– number(B), A = B + 1<br>The program generates number(n) for all positive integers n, which is clearly inﬁnite,<br>and will not terminate. The second rule of the program does not satisfy the safety<br>condition in Section 5.2.4. Programs that satisfy the safety condition will terminate,<br>even if they are recursive, provided that all database relations are ﬁnite. For such<br>programs, tuples in view relations can contain only constants from the database, and<br>hence the view relations must be ﬁnite. The converse is not true; that is, there are<br>programs that do not satisfy the safety conditions, but that do terminate.<br>5.2.8<br>Recursion in Other Languages<br>The SQL:1999 standard supports a limited form of recursion, using the with recursive<br>clause. Suppose the relation manager has attributes emp and mgr. We can ﬁnd every<br>pair (X, Y ) such that X is directly or indirectly managed by Y , using this SQL:1999<br>query:<br>with recursive empl(emp, mgr) as (<br>select emp, mgr<br>from manager<br>union<br>select emp, empl.mgr<br>from manager, empl<br>where manager.mgr = empl.emp<br>)<br>select ∗<br>from empl<br>Recall that the with clause is used to deﬁne a temporary view whose deﬁnition is<br>available only to the query where it is deﬁned. The additional keyword recursive<br>speciﬁes that the view is recursive. The SQL deﬁnition of the view empl above is<br>equivalent to the Datalog version we saw in Section 5.2.6.<br>The procedure Datalog-Fixpoint iteratively uses the function infer(R, I) to com-<br>pute what facts are true, given a recursive Datalog program. Although we consid-<br>ered only the case of Datalog programs without negative literals, the procedure can<br>also be used on views deﬁned in other languages, such as SQL or relational algebra,<br>provided that the views satisfy the conditions described next. Regardless of the lan-<br>guage used to deﬁne a view V, the view can be thought of as being deﬁned by an<br>expression EV that, given a set of facts I, returns a set of facts EV (I) for the view rela-<br>tion V. Given a set of view deﬁnitions R (in any language), we can deﬁne a function<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>II. Relational Databases<br>5. Other Relational <br>Languages<br>221<br>© The McGraw−Hill <br>Companies, 2001<br>216<br>Chapter 5<br>Other Relational Languages<br>infer(R, I) that returns I ∪<br>V ∈R EV (I). The preceding function has the same form<br>as the infer function for Datalog.<br>A view V is said to be monotonic if, given any two sets of facts I1 and I2 such<br>that I1 ⊆I2, then EV (I1) ⊆EV (I2), where EV is the expression used to deﬁne V .<br>Similarly, the function infer is said to be monotonic if<br>I1 ⊆I2 ⇒infer(R, I1) ⊆infer(R, I2)<br>Thus, if infer is monotonic, given a set of facts I0 that is a subset of the true facts, we<br>can be sure that all facts in infer(R, I0) are also true. Using the same reasoning as in<br>Section 5.2.6, we can then show that procedure Datalog-Fixpoint is sound (that is, it<br>computes only true facts), provided that the function infer is monotonic.<br>Relational-algebra expressions that use only the operators Π, σ, ×,<br>, ∪, ∩, or ρ are<br>monotonic. Recursive views can be deﬁned by using such expressions.<br>However, relational expressions that use the operator −are not monotonic. For ex-<br>ample, let manager 1 and manager 2 be relations with the same schema as the manager<br>relation. Let<br>I1 = { manager 1(“Alon”, “Barinsky”), manager 1(“Barinsky”, “Estovar”),<br>manager 2(“Alon”, “Barinsky”) }<br>and let<br>I2 = { manager 1(“Alon”, “Barinsky”), manager 1(“Barinsky”, “Estovar”),<br>manager 2(“Alon”, “Barinsky”), manager 2(“Barinsky”, “Estovar”)}<br>Consider the expression manager 1 −manager 2. Now the result of the preceding ex-<br>pression on I1 is (“Barinsky”, “Estovar”), whereas the result of the expression on I2 is<br>the empty relation. But I1 ⊆I2; hence, the expression is not monotonic. Expressions<br>using the grouping operation of extended relational algebra are also nonmonotonic.<br>The ﬁxed-point technique does not work on recursive views deﬁned with non-<br>monotonic expressions. However, there are instances where such views are useful,<br>particularly for deﬁning aggregates on “part–subpart” relationships. Such relation-<br>ships deﬁne what subparts make up each part. Subparts themselves may have further<br>subparts, and so on; hence, the relationships, like the manager relationship, have a<br>natural recursive structure. An example of an aggregate query on such a structure<br>would be to compute the total number of subparts of each part. Writing this query in<br>Datalog or in SQL (without procedural extensions) would require the use of a recur-<br>sive view on a nonmonotonic expression. The bibliographic notes provide references<br>to research on deﬁning such views.<br>It is possible to deﬁne some kinds of recursive queries without using views. For<br>example, extended relational operations have been proposed to deﬁne transitive clo-<br>sure, and extensions to the SQL syntax to specify (generalized) transitive closure have<br>been proposed. However, recursive view deﬁnitions provide more expressive power<br>than do the other forms of recursive queries.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>II. Relational Databases<br>5. Other Relational <br>Languages<br>222<br>© The McGraw−Hill <br>Companies, 2001<br>5.3<br>User Interfaces and Tools<br>217<br>5.3<br>User Interfaces and Tools<br>Although many people interact with databases, few people use a query language to<br>directly interact with a database system. Most people interact with a database system<br>through one of the following means:<br>1. Forms and graphical user interfaces allow users to enter values that com-<br>plete predeﬁned queries. The system executes the queries and appropriately<br>formats and displays the results to the user. Graphical user interfaces provide<br>an easy-to-use way to interact with the database system.<br>2. Report generators permit predeﬁned reports to be generated on the current<br>database contents. Analysts or managers view such reports in order to make<br>business decisions.<br>3. Data analysis tools permit users to interactively browse and analyze data.<br>It is worth noting that such interfaces use query languages to communicate with<br>database systems.<br>In this section, we provide an overview of forms, graphical user interfaces, and<br>report generators. Chapter 22 covers data analysis tools in more detail. Unfortunately,<br>there are no standards for user interfaces, and each database system usually provides<br>its own user interface. In this section, we describe the basic concepts, without going<br>into the details of any particular user interface product.<br>5.3.1<br>Forms and Graphical User Interfaces<br>Forms interfaces are widely used to enter data into databases, and extract informa-<br>tion from databases, via predeﬁned queries. For example, World Wide Web search<br>engines provide forms that are used to enter key words. Hitting a “submit” button<br>causes the search engine to execute a query using the entered key words and display<br>the result to the user.<br>As a more database-oriented example, you may connect to a university registra-<br>tion system, where you are asked to ﬁll in your roll number and password into a<br>form. The system uses this information to verify your identity, as well as to extract<br>information, such as your name and the courses you have registered for, from the<br>database and display it. There may be further links on the Web page that let you<br>search for courses and ﬁnd further information about courses such as the syllabus<br>and the instructor.<br>Web browsers supporting HTML constitute the most widely used forms and graph-<br>ical user interface today. Most database system vendors also provide proprietary<br>forms interfaces that offer facilities beyond those present in HTML forms.<br>Programmers can create forms and graphical user interfaces by using HTML or<br>programming languages such as C or Java. Most database system vendors also pro-<br>vide tools that simplify the creation of graphical user interfaces and forms. These<br>tools allow application developers to create forms in an easy declarative fashion, us-<br>ing form-editor programs. Users can deﬁne the type, size, and format of each ﬁeld in<br>a form by using the form editor. System actions can be associated with user actions,<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>II. Relational Databases<br>5. Other Relational <br>Languages<br>223<br>© The McGraw−Hill <br>Companies, 2001<br>218<br>Chapter 5<br>Other Relational Languages<br>such as ﬁlling in a ﬁeld, hitting a function key on the keyboard, or submitting a form.<br>For instance, the execution of a query to ﬁll in name and address ﬁelds may be asso-<br>ciated with ﬁlling in a roll number ﬁeld, and execution of an update statement may<br>be associated with submitting a form.<br>Simple error checks can be performed by deﬁning constraints on the ﬁelds in<br>the form.2 For example, a constraint on the course number ﬁeld may check that the<br>course number typed in by the user corresponds to an actual course. Although such<br>constraints can be checked when the transaction is executed, detecting errors early<br>helps the user to correct errors quickly. Menus that indicate the valid values that can<br>be entered in a ﬁeld can help eliminate the possibility of many types of errors. Sys-<br>tem developers ﬁnd that the ability to control such features declaratively with the<br>help of a user interface development tool, instead of creating a form directly by using<br>a scripting or programming language, makes their job much easier.<br>5.3.2<br>Report Generators<br>Report generators are tools to generate human-readable summary reports from a<br>database. They integrate querying the database with the creation of formatted text<br>and summary charts (such as bar or pie charts). For example, a report may show the<br>total sales in each of the past two months for each sales region.<br>The application developer can specify report formats by using the formatting fa-<br>cilities of the report generator. Variables can be used to store parameters such as the<br>month and the year and to deﬁne ﬁelds in the report. Tables, graphs, bar charts, or<br>other graphics can be deﬁned via queries on the database. The query deﬁnitions can<br>make use of the parameter values stored in the variables.<br>Once we have deﬁned a report structure on a report-generator facility, we can<br>store it, and can execute it at any time to generate a report. Report-generator systems<br>provide a variety of facilities for structuring tabular output, such as deﬁning table<br>and column headers, displaying subtotals for each group in a table, automatically<br>splitting long tables into multiple pages, and displaying subtotals at the end of each<br>page.<br>Figure 5.13 is an example of a formatted report. The data in the report are gener-<br>ated by aggregation on information about orders.<br>The Microsoft Ofﬁce suite provides a convenient way of embedding formatted<br>query results from a database, such as MS Access, into a document created with a<br>text editor, such as MS Word. The query results can be formatted in a tabular fashion<br>or graphically (as charts) by the report generator facility of MS Access. A feature<br>called OLE (Object Linking and Embedding) links the resulting structure into a text<br>document.<br>The collections of application-development tools provided by database systems,<br>such as forms packages and report generator, used to be referred to as fourth-generation<br>languages (4GLs). The name emphasizes that these tools offer a programming para-<br>digm that is different from the imperative programming paradigm offered by third-<br>2.<br>These are called “form triggers” in Oracle, but in this book we use the term “trigger” in a different<br>sense, which we cover in Chapter 6.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>II. Relational Databases<br>5. Other Relational <br>Languages<br>224<br>© The McGraw−Hill <br>Companies, 2001<br>5.4<br>Summary<br>219<br>Region<br>Category<br>Sales<br>North<br>Computer Hardware<br>1,000,000<br>Computer Software<br>500,000<br>All categories<br>1,500,000<br>South<br>Computer Hardware<br>200,000<br>Computer Software<br>400,000<br>All categories<br>600,000<br>2,100,000<br>Acme Supply Company Inc. <br>Quarterly Sales Report<br>Period:  Jan. 1 to March 31, 2001<br>Total Sales<br>Subtotal<br>Figure 5.13<br>A formatted report.<br>generation programming languages, such as Pascal and C. However, this term is less<br>relevant today, since forms and report generators are typically created with graphical<br>tools, rather than with programming languages.<br>5.4<br>Summary<br>• We have considered two query languages: QBE, and Datalog.<br>• QBE is based on a visual paradigm: The queries look much like tables.<br>• QBE and its variants have become popular with nonexpert database users be-<br>cause of the intuitive simplicity of the visual paradigm. The widely used Mi-<br>crosoft Access database system supports a graphical version of QBE, called<br>GQBE.<br>• Datalog is derived from Prolog, but unlike Prolog, it has a declarative seman-<br>tics, making simple queries easier to write and query evaluation easier to op-<br>timize.<br>• Deﬁning views is particularly easy in Datalog, and the recursive views that<br>Datalog supports makes it possible to write queries, such as transitive-closure<br>queries, that cannot be written without recursion or iteration. However, no<br>accepted standards exist for important features, such as grouping and aggre-<br>gation, in Datalog. Datalog remains mainly a research language.<br>• Most users interact with databases via forms and graphical user interfaces,<br>and there are numerous tools to simplify the construction of such interfaces.<br>Report generators are tools that help create human-readable reports from the<br>contents of the database.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>II. Relational Databases<br>5. Other Relational <br>Languages<br>225<br>© The McGraw−Hill <br>Companies, 2001<br>220<br>Chapter 5<br>Other Relational Languages<br>Review Terms<br>• Query-by-Example (QBE)<br>• Two-dimensional syntax<br>• Skeleton tables<br>• Example rows<br>• Condition box<br>• Result relation<br>• Microsoft Access<br>• Graphical Query-By-Example<br>(GQBE)<br>• Design grid<br>• Datalog<br>• Rules<br>• Uses<br>• Deﬁnes<br>• Positive literal<br>• Negative literal<br>• Fact<br>• Rule<br>  Head<br>  Body<br>• Datalog program<br>• Depend on<br>  Directly<br>  Indirectly<br>• Recursive view<br>• Nonrecursive view<br>• Instantiation<br>  Ground instantiation<br>  Satisﬁed<br>• Infer<br>• Semantics<br>  Of a rule<br>  Of a program<br>• Safety<br>• Fixed point<br>• Transitive closure<br>• Monotonic view deﬁnition<br>• Forms<br>• Graphical user interfaces<br>• Report generators<br>Exercises<br>5.1 Consider the insurance database of Figure 5.14, where the primary keys are un-<br>derlined. Construct the following QBE queries for this relational-database.<br>a. Find the total number of people who owned cars that were involved in ac-<br>cidents in 1989.<br>b. Find the number of accidents in which the cars belonging to “John Smith”<br>were involved.<br>c. Add a new accident to the database; assume any values for required at-<br>tributes.<br>d. Delete the Mazda belonging to “John Smith.”<br>e. Update the damage amount for the car with license number “AABB2000” in<br>the accident with report number “AR2197” to $3000.<br>5.2 Consider the employee database of Figure 5.15. Give expressions in QBE, and<br>Datalog for each of the following queries:<br>a. Find the names of all employees who work for First Bank Corporation.<br>b. Find the names and cities of residence of all employees who work for First<br>Bank Corporation.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>II. Relational Databases<br>5. Other Relational <br>Languages<br>226<br>© The McGraw−Hill <br>Companies, 2001<br>Exercises<br>221<br>person (driver-id#, name, address)<br>car (license, model, year)<br>accident (report-number, date, location)<br>owns (driver-id#, license)<br>participated (driver-id, car, report-number, damage-amount)<br>Figure 5.14<br>Insurance database.<br>c. Find the names, street addresses, and cities of residence of all employees<br>who work for First Bank Corporation and earn more than $10,000 per an-<br>num.<br>d. Find all employees who live in the same city as the company for which they<br>work is located.<br>e. Find all employees who live in the same city and on the same street as their<br>managers.<br>f. Find all employees in the database who do not work for First Bank Corpo-<br>ration.<br>g. Find all employees who earn more than every employee of Small Bank Cor-<br>poration.<br>h. Assume that the companies may be located in several cities. Find all com-<br>panies located in every city in which Small Bank Corporation is located.<br>5.3 Consider the relational database of Figure 5.15. where the primary keys are un-<br>derlined. Give expressions in QBE for each of the following queries:<br>a. Find all employees who earn more than the average salary of all employees<br>of their company.<br>b. Find the company that has the most employees.<br>c. Find the company that has the smallest payroll.<br>d. Find those companies whose employees earn a higher salary, on average,<br>than the average salary at First Bank Corporation.<br>5.4 Consider the relational database of Figure 5.15. Give expressions in QBE for each<br>of the following queries:<br>a. Modify the database so that Jones now lives in Newtown.<br>b. Give all employees of First Bank Corporation a 10 percent raise.<br>c. Give all managers in the database a 10 percent raise.<br>d. Give all managers in the database a 10 percent raise, unless the salary would<br>be greater than $100,000. In such cases, give only a 3 percent raise.<br>employee (person-name, street, city)<br>works (person-name, company-name, salary)<br>company (company-name, city)<br>manages (person-name, manager-name)<br>Figure 5.15<br>Employee database.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>II. Relational Databases<br>5. Other Relational <br>Languages<br>227<br>© The McGraw−Hill <br>Companies, 2001<br>222<br>Chapter 5<br>Other Relational Languages<br>e. Delete all tuples in the works relation for employees of Small Bank Corpora-<br>tion.<br>5.5 Let the following relation schemas be given:<br>R = (A, B, C)<br>S = (D, E, F)<br>Let relations r(R) and s(S) be given. Give expressions in QBE, and Datalog equiv-<br>alent to each of the following queries:<br>a. ΠA(r)<br>b. σB = 17 (r)<br>c. r × s<br>d. ΠA,F (σC = D(r × s))<br>5.6 Let R = (A, B, C), and let r1 and r2 both be relations on schema R. Give expres-<br>sions in QBE, and Datalog equivalent to each of the following queries:<br>a. r1 ∪r2<br>b. r1 ∩r2<br>c. r1 −r2<br>d. ΠAB(r1)<br> ΠBC(r2)<br>5.7 Let R = (A, B) and S = (A, C), and let r(R) and s(S) be relations. Write expres-<br>sions in QBE and Datalog for each of the following queries:<br>a. {&lt; a &gt; | ∃b (&lt; a, b &gt; ∈r ∧b = 17)}<br>b. {&lt; a, b, c &gt; | &lt; a, b &gt; ∈r ∧&lt; a, c &gt; ∈s}<br>c. {&lt; a &gt; | ∃c (&lt; a, c &gt; ∈s ∧∃b1, b2 (&lt; a, b1 &gt; ∈r ∧&lt; c, b2 &gt; ∈r ∧b1 &gt;<br>b2))}<br>5.8 Consider the relational database of Figure 5.15. Write a Datalog program for<br>each of the following queries:<br>a. Find all employees who work (directly or indirectly) under the manager<br>“Jones”.<br>b. Find all cities of residence of all employees who work (directly or indirectly)<br>under the manager “Jones”.<br>c. Find all pairs of employees who have a (direct or indirect) manager in com-<br>mon.<br>d. Find all pairs of employees who have a (direct or indirect) manager in com-<br>mon, and are at the same number of levels of supervision below the com-<br>mon manager.<br>5.9 Write an extended relational-algebra vi</span><br><br><span style="background-color: #CAFFBF;" title="Chunk 27 | Start: 540054 | End: 560054 | Tokens: 3226">ew equivalent to the Datalog rule<br>p(A, C, D) :– q1(A, B), q2(B, C), q3(4, B), D = B + 1 .<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>II. Relational Databases<br>5. Other Relational <br>Languages<br>228<br>© The McGraw−Hill <br>Companies, 2001<br>Bibliographical Notes<br>223<br>5.10 Describe how an arbitrary Datalog rule can be expressed as an extended relation-<br>al algebra view.<br>Bibliographical Notes<br>The experimental version of Query-by-Example is described in Zloof [1977]; the com-<br>mercial version is described in IBM [1978]. Numerous database systems—in partic-<br>ular, database systems that run on personal computers—implement QBE or variants.<br>Examples are Microsoft Access and Borland Paradox.<br>Implementations of Datalog include LDL system (described in Tsur and Zaniolo<br>[1986] and Naqvi and Tsur [1988]), Nail! (described in Derr et al. [1993]), and Coral<br>(described in Ramakrishnan et al. [1992b] and Ramakrishnan et al. [1993]). Early dis-<br>cussions concerning logic databases were presented in Gallaire and Minker [1978]<br>and Gallaire et al. [1984]. Ullman [1988] and Ullman [1989] provide extensive text-<br>book discussions of logic query languages and implementation techniques. Ramakr-<br>ishnan and Ullman [1995] provides a more recent survey on deductive databases.<br>Datalog programs that have both recursion and negation can be assigned a simple<br>semantics if the negation is “stratiﬁed”—that is, if there is no recursion through nega-<br>tion. Chandra and Harel [1982] and Apt and Pugin [1987] discuss stratiﬁed negation.<br>An important extension, called the modular-stratiﬁcation semantics, which handles a<br>class of recursive programs with negative literals, is discussed in Ross [1990]; an eval-<br>uation technique for such programs is described by Ramakrishnan et al. [1992a].<br>Tools<br>The Microsoft Access QBE is probably the most widely used implementation of QBE.<br>IBM DB2 QMF and Borland Paradox also support QBE.<br>The Coral system from the University of Wisconsin–Madison is a widely used<br>implementation of Datalog (see (http://www.cs.wisc.edu/coral). The XSB system from<br>the State University of New York (SUNY) Stony Brook (http://xsb.sourceforge.net) is<br>a widely used Prolog implementation that supports database querying; recall that<br>Datalog is a nonprocedural subset of Prolog.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>II. Relational Databases<br>6. Integrity and Security<br>229<br>© The McGraw−Hill <br>Companies, 2001<br>C<br>H<br>A<br>P<br>T<br>E<br>R<br>6<br>Integrity and Security<br>Integrity constraints ensure that changes made to the database by authorized users<br>do not result in a loss of data consistency. Thus, integrity constraints guard against<br>accidental damage to the database.<br>We have already seen two forms of integrity constraints for the E-R model in Chap-<br>ter 2:<br>• Key declarations—the stipulation that certain attributes form a candidate key<br>for a given entity set.<br>• Form of a relationship—many to many, one to many, one to one.<br>In general, an integrity constraint can be an arbitrary predicate pertaining to the<br>database. However, arbitrary predicates may be costly to test. Thus, we concentrate<br>on integrity constraints that can be tested with minimal overhead. We study some<br>such forms of integrity constraints in Sections 6.1 and 6.2, and cover a more complex<br>form in Section 6.3. In Chapter 7 we study another form of integrity constraint, called<br>“functional dependency,” which is primarily used in the process of schema design.<br>In Section 6.4 we study triggers, which are statements that are executed automati-<br>cally by the system as a side effect of a modiﬁcation to the database. Triggers are used<br>to ensure some types of integrity.<br>In addition to protecting against accidental introduction of inconsistency, the data<br>stored in the database need to be protected from unauthorized access and malicious<br>destruction or alteration. In Sections 6.5 through 6.7, we examine ways in which data<br>may be misused or intentionally made inconsistent, and present security mechanisms<br>to guard against such occurrences.<br>6.1<br>Domain Constraints<br>We have seen that a domain of possible values must be associated with every at-<br>tribute. In Chapter 4, we saw a number of standard domain types, such as integer<br>225<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>II. Relational Databases<br>6. Integrity and Security<br>230<br>© The McGraw−Hill <br>Companies, 2001<br>226<br>Chapter 6<br>Integrity and Security<br>types, character types, and date/time types deﬁned in SQL. Declaring an attribute to<br>be of a particular domain acts as a constraint on the values that it can take. Domain<br>constraints are the most elementary form of integrity constraint. They are tested eas-<br>ily by the system whenever a new data item is entered into the database.<br>It is possible for several attributes to have the same domain. For example, the at-<br>tributes customer-name and employee-name might have the same domain: the set of all<br>person names. However, the domains of balance and branch-name certainly ought to be<br>distinct. It is perhaps less clear whether customer-name and branch-name should have<br>the same domain. At the implementation level, both customer names and branch<br>names are character strings. However, we would normally not consider the query<br>“Find all customers who have the same name as a branch” to be a meaningful query.<br>Thus, if we view the database at the conceptual, rather than the physical, level,<br>customer-name and branch-name should have distinct domains.<br>From the above discussion, we can see that a proper deﬁnition of domain con-<br>straints not only allows us to test values inserted in the database, but also permits<br>us to test queries to ensure that the comparisons made make sense. The principle be-<br>hind attribute domains is similar to that behind typing of variables in programming<br>languages. Strongly typed programming languages allow the compiler to check the<br>program in greater detail.<br>The create domain clause can be used to deﬁne new domains. For example, the<br>statements:<br>create domain Dollars numeric(12,2)<br>create domain Pounds numeric(12,2)<br>deﬁne the domains Dollars and Pounds to be decimal numbers with a total of 12 digits,<br>two of which are placed after the decimal point. An attempt to assign a value of type<br>Dollars to a variable of type Pounds would result in a syntax error, although both are of<br>the same numeric type. Such an assignment is likely to be due to a programmer error,<br>where the programmer forgot about the differences in currency. Declaring different<br>domains for different currencies helps catch such errors.<br>Values of one domain can be cast (that is, converted) to another domain. If the<br>attribute A or relation r is of type Dollars, we can convert it to Pounds by writing<br>cast r.A as Pounds<br>In a real application we would of course multiply r.A by a currency conversion factor<br>before casting it to pounds. SQL also provides drop domain and alter domain clauses<br>to drop or modify domains that have been created earlier.<br>The check clause in SQL permits domains to be restricted in powerful ways that<br>most programming language type systems do not permit. Speciﬁcally, the check<br>clause permits the schema designer to specify a predicate that must be satisﬁed by<br>any value assigned to a variable whose type is the domain. For instance, a check<br>clause can ensure that an hourly wage domain allows only values greater than a<br>speciﬁed value (such as the minimum wage):<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>II. Relational Databases<br>6. Integrity and Security<br>231<br>© The McGraw−Hill <br>Companies, 2001<br>6.2<br>Referential Integrity<br>227<br>create domain HourlyWage numeric(5,2)<br>constraint wage-value-test check(value &gt;= 4.00)<br>The domain HourlyWage has a constraint that ensures that the hourly wage is greater<br>than 4.00. The clause constraint wage-value-test is optional, and is used to give the<br>name wage-value-test to the constraint. The name is used to indicate which constraint<br>an update violated.<br>The check clause can also be used to restrict a domain to not contain any null<br>values:<br>create domain AccountNumber char(10)<br>constraint account-number-null-test check(value not null )<br>As another example, the domain can be restricted to contain only a speciﬁed set of<br>values by using the in clause:<br>create domain AccountType char(10)<br>constraint account-type-test<br>check(value in (’Checking’, ’Saving’))<br>The preceding check conditions can be tested quite easily, when a tuple is inserted<br>or modiﬁed. However, in general, the check conditions can be more complex (and<br>harder to check), since subqueries that refer to other relations are permitted in the<br>check condition. For example, this constraint could be speciﬁed on the relation de-<br>posit:<br>check (branch-name in (select branch-name from branch))<br>The check condition veriﬁes that the branch-name in each tuple in the deposit relation<br>is actually the name of a branch in the branch relation. Thus, the condition has to be<br>checked not only when a tuple is inserted or modiﬁed in deposit, but also when the<br>relation branch changes (in this case, when a tuple is deleted or modiﬁed in relation<br>branch).<br>The preceding constraint is actually an example of a class of constraints called<br>referential-integrity constraints. We discuss such constraints, along with a simpler way<br>of specifying them in SQL, in Section 6.2.<br>Complex check conditions can be useful when we want to ensure integrity of data,<br>but we should use them with care, since they may be costly to test.<br>6.2<br>Referential Integrity<br>Often, we wish to ensure that a value that appears in one relation for a given set of<br>attributes also appears for a certain set of attributes in another relation. This condition<br>is called referential integrity.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>II. Relational Databases<br>6. Integrity and Security<br>232<br>© The McGraw−Hill <br>Companies, 2001<br>228<br>Chapter 6<br>Integrity and Security<br>6.2.1<br>Basic Concepts<br>Consider a pair of relations r(R) and s(S), and the natural join r<br> s. There may be a<br>tuple tr in r that does not join with any tuple in s. That is, there is no ts in s such that<br>tr[R ∩S] = ts[R ∩S]. Such tuples are called dangling tuples. Depending on the entity<br>set or relationship set being modeled, dangling tuples may or may not be acceptable.<br>In Section 3.3.3, we considered a modiﬁed form of join—the outer join—to operate<br>on relations containing dangling tuples. Here, our concern is not with queries, but<br>rather with when we should permit dangling tuples to exist in the database.<br>Suppose there is a tuple t1 in the account relation with t1[branch-name] = “Lu-<br>nartown,” but there is no tuple in the branch relation for the Lunartown branch. This<br>situation would be undesirable. We expect the branch relation to list all bank branches.<br>Therefore, tuple t1 would refer to an account at a branch that does not exist. Clearly,<br>we would like to have an integrity constraint that prohibits dangling tuples of this<br>sort.<br>Not all instances of dangling tuples are undesirable, however. Assume that there<br>is a tuple t2 in the branch relation with t2[branch-name] = “Mokan,” but there is no<br>tuple in the account relation for the Mokan branch. In this case, a branch exists that<br>has no accounts. Although this situation is not common, it may arise when a branch<br>is opened or is about to close. Thus, we do not want to prohibit this situation.<br>The distinction between these two examples arises from two facts:<br>• The attribute branch-name in Account-schema is a foreign key referencing the<br>primary key of Branch-schema.<br>• The attribute branch-name in Branch-schema is not a foreign key.<br>(Recall from Section 3.1.3 that a foreign key is a set of attributes in a relation schema<br>that forms a primary key for another schema.)<br>In the Lunartown example, tuple t1 in account has a value on the foreign key<br>branch-name that does not appear in branch. In the Mokan-branch example, tuple t2 in<br>branch has a value on branch-name that does not appear in account, but branch-name is<br>not a foreign key. Thus, the distinction between our two examples of dangling tuples<br>is the presence of a foreign key.<br>Let r1(R1) and r2(R2) be relations with primary keys K1 and K2, respectively. We<br>say that a subset α of R2 is a foreign key referencing K1 in relation r1 if it is required<br>that, for every t2 in r2, there must be a tuple t1 in r1 such that t1[K1] = t2[α]. Re-<br>quirements of this form are called referential integrity constraints, or subset depen-<br>dencies. The latter term arises because the preceding referential-integrity constraint<br>can be written as Πα (r2) ⊆ΠK1 (r1). Note that, for a referential-integrity constraint<br>to make sense, either α must be equal to K1, or α and K1 must be compatible sets of<br>attributes.<br>6.2.2<br>Referential Integrity and the E-R Model<br>Referential-integrity constraints arise frequently. If we derive our relational-database<br>schema by constructing tables from E-R diagrams, as we did in Chapter 2, then every<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>II. Relational Databases<br>6. Integrity and Security<br>233<br>© The McGraw−Hill <br>Companies, 2001<br>6.2<br>Referential Integrity<br>229<br>R<br>E1<br>E2<br>En<br>En–1<br>.<br>.<br>.<br>Figure 6.1<br>An n-ary relationship set.<br>relation arising from a relationship set has referential-integrity constraints. Figure 6.1<br>shows an n-ary relationship set R, relating entity sets E1, E2, . . . , En. Let Ki denote<br>the primary key of Ei. The attributes of the relation schema for relationship set R<br>include K1 ∪K2 ∪· · · ∪Kn. The following referential integrity constraints are<br>then present: For each i, Ki in the schema for R is a foreign key referencing Ki in the<br>relation schema generated from entity set Ei<br>Another source of referential-integrity constraints is weak entity sets. Recall from<br>Chapter 2 that the relation schema for a weak entity set must include the primary<br>key of the entity set on which the weak entity set depends. Thus, the relation schema<br>for each weak entity set includes a foreign key that leads to a referential-integrity<br>constraint.<br>6.2.3<br>Database Modiﬁcation<br>Database modiﬁcations can cause violations of referential integrity. We list here the<br>test that we must make for each type of database modiﬁcation to preserve the follow-<br>ing referential-integrity constraint:<br>Πα (r2) ⊆ΠK (r1)<br>• Insert. If a tuple t2 is inserted into r2, the system must ensure that there is a<br>tuple t1 in r1 such that t1[K] = t2[α]. That is,<br>t2[α] ∈ΠK (r1)<br>• Delete. If a tuple t1 is deleted from r1, the system must compute the set of<br>tuples in r2 that reference t1:<br>σα = t1[K] (r2)<br>If this set is not empty, either the delete command is rejected as an error, or the<br>tuples that reference t1 must themselves be deleted. The latter solution may<br>lead to cascading deletions, since tuples may reference tuples that reference<br>t1, and so on.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>II. Relational Databases<br>6. Integrity and Security<br>234<br>© The McGraw−Hill <br>Companies, 2001<br>230<br>Chapter 6<br>Integrity and Security<br>• Update. We must consider two cases for update: updates to the referencing<br>relation (r2), and updates to the referenced relation (r1).<br>  If a tuple t2 is updated in relation r2, and the update modiﬁes values for<br>the foreign key α, then a test similar to the insert case is made. Let t2′<br>denote the new value of tuple t2. The system must ensure that<br>t2<br>′[α] ∈ΠK (r1)<br>  If a tuple t1 is updated in r1, and the update modiﬁes values for the pri-<br>mary key (K), then a test similar to the delete case is made. The system<br>must compute<br>σα = t1[K] (r2)<br>using the old value of t1 (the value before the update is applied). If this set<br>is not empty, the update is rejected as an error, or the update is cascaded<br>in a manner similar to delete.<br>6.2.4<br>Referential Integrity in SQL<br>Foreign keys can be speciﬁed as part of the SQL create table statement by using the<br>foreign key clause. We illustrate foreign-key declarations by using the SQL DDL def-<br>inition of part of our bank database, shown in Figure 6.2.<br>By default, a foreign key references the primary key attributes of the referenced<br>table. SQL also supports a version of the references clause where a list of attributes of<br>the referenced relation can be speciﬁed explicitly. The speciﬁed list of attributes must<br>be declared as a candidate key of the referenced relation.<br>We can use the following short form as part of an attribute deﬁnition to declare<br>that the attribute forms a foreign key:<br>branch-name char(15) references branch<br>When a referential-integrity constraint is violated, the normal procedure is to reject<br>the action that caused the violation. However, a foreign key clause can specify that<br>if a delete or update action on the referenced relation violates the constraint, then,<br>instead of rejecting the action, the system must take steps to change the tuple in the<br>referencing relation to restore the constraint. Consider this deﬁnition of an integrity<br>constraint on the relation account:<br>create table account<br>( . . .<br>foreign key (branch-name) references branch<br>on delete cascade<br>on update cascade,<br>. . . )<br>Because of the clause on delete cascade associated with the foreign-key declaration,<br>if a delete of a tuple in branch results in this referential-integrity constraint being vi-<br>olated, the system does not reject the delete. Instead, the delete “cascades” to the<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>II. Relational Databases<br>6. Integrity and Security<br>235<br>© The McGraw−Hill <br>Companies, 2001<br>6.2<br>Referential Integrity<br>231<br>create table customer<br>(customer-name<br>char(20),<br>customer-street<br>char(30),<br>customer-city<br>char(30),<br>primary key (customer-name))<br>create table branch<br>(branch-name<br>char(15),<br>branch-city<br>char(30),<br>assets<br>integer,<br>primary key (branch-name),<br>check (assets &gt;= 0))<br>create table account<br>(account-number char(10),<br>branch-name<br>char(15),<br>balance<br>integer,<br>primary key (account-number),<br>foreign key (branch-name) references branch,<br>check (balance &gt;= 0))<br>create table depositor<br>(customer-name<br>char(20),<br>account-number<br>char(10),<br>primary key (customer-name, account-number),<br>foreign key (customer-name) references customer,<br>foreign key (account-number) references account)<br>Figure 6.2<br>SQL data deﬁnition for part of the bank database.<br>account relation, deleting the tuple that refers to the branch that was deleted. Simi-<br>larly, the system does not reject an update to a ﬁeld referenced by the constraint if it<br>violates the constraint; instead, the system updates the ﬁeld branch-name in the ref-<br>erencing tuples in account to the new value as well. SQL also allows the foreign key<br>clause to specify actions other than cascade, if the constraint is violated: The referenc-<br>ing ﬁeld (here, branch-name) can be set to null (by using set null in place of cascade),<br>or to the default value for the domain (by using set default).<br>If there is a chain of foreign-key dependencies across multiple relations, a deletion<br>or update at one end of the chain can propagate across the entire chain. An interest-<br>ing case where the foreign key constraint on a relation references the same relation<br>appears in Exercise 6.4. If a cascading update or delete causes a constraint violation<br>that cannot be handled by a further cascading operation, the system aborts the trans-<br>action. As a result, all the changes caused by the transaction and its cascading actions<br>are undone.<br>Null values complicate the semantics of referential integrity constraints in SQL.<br>Attributes of foreign keys are allowed to be null, provided that they have not other-<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>II. Relational Databases<br>6. Integrity and Security<br>236<br>© The McGraw−Hill <br>Companies, 2001<br>232<br>Chapter 6<br>Integrity and Security<br>wise been declared to be non-null. If all the columns of a foreign key are non-null in<br>a given tuple, the usual deﬁnition of foreign-key constraints is used for that tuple. If<br>any of the foreign-key columns is null, the tuple is deﬁned automatically to</span><br><br><span style="background-color: #9BF6FF;" title="Chunk 28 | Start: 560056 | End: 580056 | Tokens: 3184"> satisfy<br>the constraint.<br>This deﬁnition may not always be the right choice, so SQL also provides constructs<br>that allow you to change the behavior with null values; we do not discuss the con-<br>structs here. To avoid such complexity, it is best to ensure that all columns of a foreign<br>key speciﬁcation are declared to be non-null.<br>Transactions may consist of several steps, and integrity constraints may be vio-<br>lated temporarily after one step, but a later step may remove the violation. For in-<br>stance, suppose we have a relation marriedperson with primary key name, and an at-<br>tribute spouse, and suppose that spouse is a foreign key on marriedperson. That is, the<br>constraint says that the spouse attribute must contain a name that is present in the per-<br>son table. Suppose we wish to note the fact that John and Mary are married to each<br>other by inserting two tuples, one for John and one for Mary, in the above relation.<br>The insertion of the ﬁrst tuple would violate the foreign key constraint, regardless of<br>which of the two tuples is inserted ﬁrst. After the second tuple is inserted the foreign<br>key constraint would hold again.<br>To handle such situations, integrity constraints are checked at the end of a trans-<br>action, and not at intermediate steps.1<br>6.3<br>Assertions<br>An assertion is a predicate expressing a condition that we wish the database always<br>to satisfy. Domain constraints and referential-integrity constraints are special forms<br>of assertions. We have paid substantial attention to these forms of assertion because<br>they are easily tested and apply to a wide range of database applications. However,<br>there are many constraints that we cannot express by using only these special forms.<br>Two examples of such constraints are:<br>• The sum of all loan amounts for each branch must be less than the sum of all<br>account balances at the branch.<br>• Every loan has at least one customer who maintains an account with a mini-<br>mum balance of $1000.00.<br>An assertion in SQL takes the form<br>create assertion &lt;assertion-name&gt; check &lt;predicate&gt;<br>Here is how the two examples of constraints can be written. Since SQL does not<br>provide a “for all X, P(X)” construct (where P is a predicate), we are forced to im-<br>1.<br>We can work around the problem in the above example in another way, if the spouse attribute can be<br>set to null: We set the spouse attributes to null when inserting the tuples for John and Mary, and we update<br>them later. However, this technique is rather messy, and does not work if the attributes cannot be set to<br>null.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>II. Relational Databases<br>6. Integrity and Security<br>237<br>© The McGraw−Hill <br>Companies, 2001<br>6.4<br>Triggers<br>233<br>plement the construct by the equivalent “not exists X such that not P(X)” construct,<br>which can be written in SQL. We write<br>create assertion sum-constraint check<br>(not exists (select * from branch<br>where (select sum(amount) from loan<br>where loan.branch-name = branch.branch-name)<br>&gt;= (select sum(balance) from account<br>where account.branch-name = branch.branch-name)))<br>create assertion balance-constraint check<br>(not exists (select * from loan<br>where not exists ( select *<br>from borrower, depositor, account<br>where loan.loan-number = borrower.loan-number<br>and borrower.customer-name = depositor.customer-name<br>and depositor.account-number = account.account-number<br>and account.balance &gt;= 1000)))<br>When an assertion is created, the system tests it for validity. If the assertion is valid,<br>then any future modiﬁcation to the database is allowed only if it does not cause that<br>assertion to be violated. This testing may introduce a signiﬁcant amount of overhead<br>if complex assertions have been made. Hence, assertions should be used with great<br>care. The high overhead of testing and maintaining assertions has led some system<br>developers to omit support for general assertions, or to provide specialized forms of<br>assertions that are easier to test.<br>6.4<br>Triggers<br>A trigger is a statement that the system executes automatically as a side effect of<br>a modiﬁcation to the database. To design a trigger mechanism, we must meet two<br>requirements:<br>1. Specify when a trigger is to be executed. This is broken up into an event that<br>causes the trigger to be checked and a condition that must be satisﬁed for trig-<br>ger execution to proceed.<br>2. Specify the actions to be taken when the trigger executes.<br>The above model of triggers is referred to as the event-condition-action model for<br>triggers.<br>The database stores triggers just as if they were regular data, so that they are per-<br>sistent and are accessible to all database operations. Once we enter a trigger into the<br>database, the database system takes on the responsibility of executing it whenever<br>the speciﬁed event occurs and the corresponding condition is satisﬁed.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>II. Relational Databases<br>6. Integrity and Security<br>238<br>© The McGraw−Hill <br>Companies, 2001<br>234<br>Chapter 6<br>Integrity and Security<br>6.4.1<br>Need for Triggers<br>Triggers are useful mechanisms for alerting humans or for starting certain tasks au-<br>tomatically when certain conditions are met. As an illustration, suppose that, instead<br>of allowing negative account balances, the bank deals with overdrafts by setting the<br>account balance to zero, and creating a loan in the amount of the overdraft. The bank<br>gives this loan a loan number identical to the account number of the overdrawn ac-<br>count. For this example, the condition for executing the trigger is an update to the ac-<br>count relation that results in a negative balance value. Suppose that Jones’ withdrawal<br>of some money from an account made the account balance negative. Let t denote the<br>account tuple with a negative balance value. The actions to be taken are:<br>• Insert a new tuple s in the loan relation with<br>s[loan-number] = t[account-number]<br>s[branch-name] = t[branch-name]<br>s[amount] = −t[balance]<br>(Note that, since t[balance] is negative, we negate t[balance] to get the loan<br>amount—a positive number.)<br>• Insert a new tuple u in the borrower relation with<br>u[customer-name] = “Jones”<br>u[loan-number] = t[account-number]<br>• Set t[balance] to 0.<br>As another example of the use of triggers, suppose a warehouse wishes to main-<br>tain a minimum inventory of each item; when the inventory level of an item falls<br>below the minimum level, an order should be placed automatically. This is how the<br>business rule can be implemented by triggers: On an update of the inventory level<br>of an item, the trigger should compare the level with the minimum inventory level<br>for the item, and if the level is at or below the minimum, a new order is added to an<br>orders relation.<br>Note that trigger systems cannot usually perform updates outside the database,<br>and hence in the inventory replenishment example, we cannot use a trigger to di-<br>rectly place an order in the external world. Instead, we add an order to the orders re-<br>lation as in the inventory example. We must create a separate permanently running<br>system process that periodically scans the orders relation and places orders. This sys-<br>tem process would also note which tuples in the orders relation have been processed<br>and when each order was placed. The process would also track deliveries of orders,<br>and alert managers in case of exceptional conditions such as delays in deliveries.<br>6.4.2<br>Triggers in SQL<br>SQL-based database systems use triggers widely, although before SQL:1999 they were<br>not part of the SQL standard. Unfortunately, each database system implemented its<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>II. Relational Databases<br>6. Integrity and Security<br>239<br>© The McGraw−Hill <br>Companies, 2001<br>6.4<br>Triggers<br>235<br>create trigger overdraft-trigger after update on account<br>referencing new row as nrow<br>for each row<br>when nrow.balance &lt; 0<br>begin atomic<br>insert into borrower<br>(select customer-name, account-number<br>from depositor<br>where nrow.account-number = depositor.account-number);<br>insert into loan values<br>(nrow.account-number, nrow.branch-name, −nrow.balance);<br>update account set balance = 0<br>where account.account-number = nrow.account-number<br>end<br>Figure 6.3<br>Example of SQL:1999 syntax for triggers.<br>own syntax for triggers, leading to incompatibilities. We outline in Figure 6.3 the<br>SQL:1999 syntax for triggers (which is similar to the syntax in the IBM DB2 and Oracle<br>database systems).<br>This trigger deﬁnition speciﬁes that the trigger is initiated after any update of the<br>relation account is executed. An SQL update statement could update multiple tuples<br>of the relation, and the for each row clause in the trigger code would then explicitly<br>iterate over each updated row. The referencing new row as clause creates a variable<br>nrow (called a transition variable), which stores the value of an updated row after<br>the update.<br>The when statement speciﬁes a condition, namely nrow.balance &lt; 0. The system<br>executes the rest of the trigger body only for tuples that satisfy the condition. The<br>begin atomic . . . end clause serves to collect multiple SQL statements into a single<br>compound statement. The two insert statements with the begin . . . end structure<br>carry out the speciﬁc tasks of creating new tuples in the borrower and loan relations to<br>represent the new loan. The update statement serves to set the account balance back<br>to 0 from its earlier negative value.<br>The triggering event and actions can take many forms:<br>• The triggering event can be insert or delete, instead of update.<br>For example, the action on delete of an account could be to check if the<br>holders of the account have any remaining accounts, and if they do not, to<br>delete them from the depositor relation. You can deﬁne this trigger as an exer-<br>cise (Exercise 6.7).<br>As another example, if a new depositor is inserted, the triggered action could<br>be to send a welcome letter to the depositor. Obviously a trigger cannot di-<br>rectly cause such an action outside the database, but could instead add a tu-<br>ple to a relation storing addresses to which welcome letters need to be sent. A<br>separate process would go over this table, and print out letters to be sent.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>II. Relational Databases<br>6. Integrity and Security<br>240<br>© The McGraw−Hill <br>Companies, 2001<br>236<br>Chapter 6<br>Integrity and Security<br>• For updates, the trigger can specify columns whose update causes the trigger<br>to execute. For instance if the ﬁrst line of the overdraft trigger were replaced<br>by<br>create trigger overdraft-trigger after update of balance on account<br>then the trigger would be executed only on updates to balance; updates to<br>other attributes would not cause it to be executed.<br>• The referencing old row as clause can be used to create a variable storing the<br>old value of an updated or deleted row. The referencing new row as clause<br>can be used with inserts in addition to updates.<br>• Triggers can be activated before the event (insert/delete/update) instead of<br>after the event.<br>Such triggers can serve as extra constraints that can prevent invalid up-<br>dates. For instance, if we wish not to permit overdrafts, we can create a before<br>trigger that rolls back the transaction if the new balance is negative.<br>As another example, suppose the value in a phone number ﬁeld of an in-<br>serted tuple is blank, which indicates absence of a phone number. We can<br>deﬁne a trigger that replaces the value by the null value. The set statement<br>can be used to carry out such modiﬁcations.<br>create trigger setnull-trigger before update on r<br>referencing new row as nrow<br>for each row<br>when nrow.phone-number = ’ ’<br>set nrow.phone-number = null<br>• Instead of carrying out an action for each affected row, we can carry out a sin-<br>gle action for the entire SQL statement that caused the insert/delete/update.<br>To do so, we use the for each statement clause instead of the for each row<br>clause.<br>The clauses referencing old table as or referencing new table as can then<br>be used to refer to temporary tables (called transition tables) containing all the<br>affected rows. Transition tables cannot be used with before triggers, but can<br>be used with after triggers, regardless of whether they are statement triggers<br>or row triggers.<br>A single SQL statement can then be used to carry out multiple actions on<br>the basis of the transition tables.<br>Returning to our warehouse inventory example, suppose we have the following<br>relations:<br>• inventory(item, level), which notes the current amount (number/weight/vol-<br>ume) of the item in the warehouse<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>II. Relational Databases<br>6. Integrity and Security<br>241<br>© The McGraw−Hill <br>Companies, 2001<br>6.4<br>Triggers<br>237<br>create trigger reorder-trigger after update of amount on inventory<br>referencing old row as orow, new row as nrow<br>for each row<br>when nrow.level &lt;= (select level<br>from minlevel<br>where minlevel.item = orow.item)<br>and orow.level &gt; (select level<br>from minlevel<br>where minlevel.item = orow.item)<br>begin<br>insert into orders<br>(select item, amount<br>from reorder<br>where reorder.item = orow.item)<br>end<br>Figure 6.4<br>Example of trigger for reordering an item.<br>• minlevel(item, level), which notes the minimum amount of the item to be main-<br>tained<br>• reorder(item, amount), which notes the amount of the item to be ordered when<br>its level falls below the minimum<br>• orders(item, amount), which notes the amount of the item to be ordered.<br>We can then use the trigger shown in Figure 6.4 for reordering the item.<br>Note that we have been careful to place an order only when the amount falls from<br>above the minimum level to below the minimum level. If we only check that the<br>new value after an update is below the minimum level, we may place an order erro-<br>neously when the item has already been reordered.<br>Many database systems provide nonstandard trigger implementations, or imple-<br>ment only some of the trigger features. For instance, many database systems do not<br>implement the before clause, and the keyword on is used instead of after. They may<br>not implement the referencing clause. Instead, they may specify transition tables by<br>using the keywords inserted or deleted. Figure 6.5 illustrates how the overdraft trig-<br>ger would be written in MS-SQLServer. Read the user manual for the database system<br>you use for more information about the trigger features it supports.<br>6.4.3<br>When Not to Use Triggers<br>There are many good uses for triggers, such as those we have just seen in Section 6.4.2,<br>but some uses are best handled by alternative techniques. For example, in the past,<br>system designers used triggers to maintain summary data. For instance, they used<br>triggers on insert/delete/update of a employee relation containing salary and dept at-<br>tributes to maintain the total salary of each department. However, many database<br>systems today support materialized views (see Section 3.5.1), which provide a much<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>II. Relational Databases<br>6. Integrity and Security<br>242<br>© The McGraw−Hill <br>Companies, 2001<br>238<br>Chapter 6<br>Integrity and Security<br>create trigger overdraft-trigger on account<br>for update<br>as<br>if nrow.balance &lt; 0<br>begin<br>insert into borrower<br>(select customer-name, account-number<br>from depositor, inserted<br>where inserted.account-number = depositor.account-number)<br>insert into loan values<br>(inserted.account-number, inserted.branch-name, −inserted.balance)<br>update account set balance = 0<br>from account, inserted<br>where account.account-number = inserted.account-number<br>end<br>Figure 6.5<br>Example of trigger in MS-SQL server syntax<br>easier way to maintain summary data. Designers also used triggers extensively for<br>replicating databases; they used triggers on insert/delete/update of each relation to<br>record the changes in relations called change or delta relations. A separate process<br>copied over the changes to the replica (copy) of the database, and the system executed<br>the changes on the replica. Modern database systems, however, provide built-in fa-<br>cilities for database replication, making triggers unnecessary for replication in most<br>cases.<br>In fact, many trigger applications, including our example overdraft trigger, can be<br>substituted by “encapsulation” features being introduced in SQL:1999. Encapsulation<br>can be used to ensure that updates to the balance attribute of account are done only<br>through a special procedure. That procedure would in turn check for negative bal-<br>ance, and carry out the actions of the overdraft trigger. Encapsulations can replace<br>the reorder trigger in a similar manner.<br>Triggers should be written with great care, since a trigger error detected at run<br>time causes the failure of the insert/delete/update statement that set off the trigger.<br>Furthermore, the action of one trigger can set off another trigger. In the worst case,<br>this could even lead to an inﬁnite chain of triggering. For example, suppose an insert<br>trigger on a relation has an action that causes another (new) insert on the same rela-<br>tion. The insert action then triggers yet another insert action, and so on ad inﬁnitum.<br>Database systems typically limit the length of such chains of triggers (for example to<br>16 or 32), and consider longer chains of triggering an error.<br>Triggers are occasionally called rules, or active rules, but should not be confused<br>with Datalog rules (see Section 5.2), which are really view deﬁnitions.<br>6.5<br>Security and Authorization<br>The data stored in the database need protection from unauthorized access and mali-<br>cious destruction or alteration, in addition to the protection against accidental intro-<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>II. Relational Databases<br>6. Integrity and Security<br>243<br>© The McGraw−Hill <br>Companies, 2001<br>6.5<br>Security and Authorization<br>239<br>duction of inconsistency that integrity constraints provide. In this section, we exam-<br>ine the ways in which data may be misused or intentionally made inconsistent. We<br>then present mechanisms to guard against such occurrences.<br>6.5.1<br>Security Violations<br>Among the forms of malicious access are:<br>• Unauthorized reading of data (theft of information)<br>• Unauthorized modiﬁcation of data<br>• Unauthorized destruction of data<br>Database security refers to protection from malicious access. Absolute protection<br>of the database from malicious abuse is not possible, but the cost to the perpetrator<br>can be made high enough to deter most if not all attempts to access the database<br>without proper authority.<br>To protect the database, we must take security measures at several levels:<br>• Database system. Some database-system users may be authorized to access<br>only a limited portion of the database. Other users may be allowed to issue<br>queries, but may be forbidden to modify the data. It is the responsibility of<br>the database system to ensure that these authorization restrictions are not vi-<br>olated.<br>• Operating system. No matter how secure the database system is, weakness in<br>operating-system security may serve as a means of unauthorized access to the<br>database.<br>• Network. Since almost all database systems allow remote access through ter-<br>minals or networks, software-level security within the network software is as<br>important as physical security, both on the Internet and in private networks.<br>• Physical. Sites with computer systems must be physically secured against<br>armed or surreptitious entry by intruders.<br>• Human. Users must be authorized carefully to reduce the chance of any user<br>giving access to an intruder in exchange for a bribe or other favors.<br>Security at all these levels must be maintained if database security is to be ensured.<br>A weakness at a low level of security (physical or human) allows circumvention of<br>strict high-level (database) security measures.<br>In the remainder of this section, we shall address security at the database-system<br>level. Security at the physical and human levels, although important, is beyond the<br>scope of this text.<br>Security within the operating system is implemented at several levels, ranging<br>from passwords for access to the sy</span><br><br><span style="background-color: #A0C4FF;" title="Chunk 29 | Start: 580058 | End: 600058 | Tokens: 3244">stem to the isolation of concurrent processes run-<br>ning within the system. The ﬁle system also provides some degree of protection. The<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>II. Relational Databases<br>6. Integrity and Security<br>244<br>© The McGraw−Hill <br>Companies, 2001<br>240<br>Chapter 6<br>Integrity and Security<br>bibliographical notes reference coverage of these topics in operating-system texts.<br>Finally, network-level security has gained widespread recognition as the Internet<br>has evolved from an academic research platform to the basis of international elec-<br>tronic commerce. The bibliographic notes list textbook coverage of the basic princi-<br>ples of network security. We shall present our discussion of security in terms of the<br>relational-data model, although the concepts of this chapter are equally applicable to<br>all data models.<br>6.5.2<br>Authorization<br>We may assign a user several forms of authorization on parts of the database. For<br>example,<br>• Read authorization allows reading, but not modiﬁcation, of data.<br>• Insert authorization allows insertion of new data, but not modiﬁcation of ex-<br>isting data.<br>• Update authorization allows modiﬁcation, but not deletion, of data.<br>• Delete authorization allows deletion of data.<br>We may assign the user all, none, or a combination of these types of authorization.<br>In addition to these forms of authorization for access to data, we may grant a user<br>authorization to modify the database schema:<br>• Index authorization allows the creation and deletion of indices.<br>• Resource authorization allows the creation of new relations.<br>• Alteration authorization allows the addition or deletion of attributes in a re-<br>lation.<br>• Drop authorization allows the deletion of relations.<br>The drop and delete authorization differ in that delete authorization allows dele-<br>tion of tuples only. If a user deletes all tuples of a relation, the relation still exists, but<br>it is empty. If a relation is dropped, it no longer exists.<br>We regulate the ability to create new relations through resource authorization. A<br>user with resource authorization who creates a new relation is given all privileges on<br>that relation automatically.<br>Index authorization may appear unnecessary, since the creation or deletion of an<br>index does not alter data in relations. Rather, indices are a structure for performance<br>enhancements. However, indices also consume space, and all database modiﬁcations<br>are required to update indices. If index authorization were granted to all users, those<br>who performed updates would be tempted to delete indices, whereas those who is-<br>sued queries would be tempted to create numerous indices. To allow the database<br>administrator to regulate the use of system resources, it is necessary to treat index<br>creation as a privilege.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>II. Relational Databases<br>6. Integrity and Security<br>245<br>© The McGraw−Hill <br>Companies, 2001<br>6.5<br>Security and Authorization<br>241<br>The ultimate form of authority is that given to the database administrator. The<br>database administrator may authorize new users, restructure the database, and so<br>on. This form of authorization is analogous to that of a superuser or operator for an<br>operating system.<br>6.5.3<br>Authorization and Views<br>In Chapter 3, we introduced the concept of views as a means of providing a user<br>with a personalized model of the database. A view can hide data that a user does<br>not need to see. The ability of views to hide data serves both to simplify usage of the<br>system and to enhance security. Views simplify system usage because they restrict<br>the user’s attention to the data of interest. Although a user may be denied direct<br>access to a relation, that user may be allowed to access part of that relation through a<br>view. Thus, a combination of relational-level security and view-level security limits a<br>user’s access to precisely the data that the user needs.<br>In our banking example, consider a clerk who needs to know the names of all<br>customers who have a loan at each branch. This clerk is not authorized to see infor-<br>mation regarding speciﬁc loans that the customer may have. Thus, the clerk must be<br>denied direct access to the loan relation. But, if she is to have access to the information<br>needed, the clerk must be granted access to the view cust-loan, which consists of only<br>the names of customers and the branches at which they have a loan. This view can<br>be deﬁned in SQL as follows:<br>create view cust-loan as<br>(select branch-name, customer-name<br>from borrower, loan<br>where borrower.loan-number = loan.loan-number)<br>Suppose that the clerk issues the following SQL query:<br>select *<br>from cust-loan<br>Clearly, the clerk is authorized to see the result of this query. However, when the<br>query processor translates it into a query on the actual relations in the database, it<br>produces a query on borrower and loan. Thus, the system must check authorization<br>on the clerk’s query before it begins query processing.<br>Creation of a view does not require resource authorization. A user who creates a<br>view does not necessarily receive all privileges on that view. She receives only those<br>privileges that provide no additional authorization beyond those that she already<br>had. For example, a user cannot be given update authorization on a view without<br>having update authorization on the relations used to deﬁne the view. If a user creates<br>a view on which no authorization can be granted, the system will deny the view<br>creation request. In our cust-loan view example, the creator of the view must have<br>read authorization on both the borrower and loan relations.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>II. Relational Databases<br>6. Integrity and Security<br>246<br>© The McGraw−Hill <br>Companies, 2001<br>242<br>Chapter 6<br>Integrity and Security<br>6.5.4<br>Granting of Privileges<br>A user who has been granted some form of authorization may be allowed to pass<br>on this authorization to other users. However, we must be careful how authorization<br>may be passed among users, to ensure that such authorization can be revoked at<br>some future time.<br>Consider, as an example, the granting of update authorization on the loan rela-<br>tion of the bank database. Assume that, initially, the database administrator grants<br>update authorization on loan to users U1, U2, and U3, who may in turn pass on this<br>authorization to other users. The passing of authorization from one user to another<br>can be represented by an authorization graph. The nodes of this graph are the users.<br>The graph includes an edge Ui →Uj if user Ui grants update authorization on loan<br>to Uj. The root of the graph is the database administrator. In the sample graph in<br>Figure 6.6, observe that user U5 is granted authorization by both U1 and U2; U4 is<br>granted authorization by only U1.<br>A user has an authorization if and only if there is a path from the root of the autho-<br>rization graph (namely, the node representing the database administrator) down to<br>the node representing the user.<br>Suppose that the database administrator decides to revoke the authorization of<br>user U1. Since U4 has authorization from U1, that authorization should be revoked as<br>well. However, U5 was granted authorization by both U1 and U2. Since the database<br>administrator did not revoke update authorization on loan from U2, U5 retains update<br>authorization on loan. If U2 eventually revokes authorization from U5, then U5 loses<br>the authorization.<br>A pair of devious users might attempt to defeat the rules for revocation of<br>authorization by granting authorization to each other, as shown in Figure 6.7a. If<br>the database administrator revokes authorization from U2, U2 retains authorization<br>through U3, as in Figure 6.7b. If authorization is revoked subsequently from U3, U3<br>appears to retain authorization through U2, as in Figure 6.7c. However, when the<br>database administrator revokes authorization from U3, the edges from U3 to U2 and<br>from U2 to U3 are no longer part of a path starting with the database administrator.<br>U3<br>DBA<br>U1<br>U5<br>U2<br>U4<br>Figure 6.6<br>Authorization-grant graph.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>II. Relational Databases<br>6. Integrity and Security<br>247<br>© The McGraw−Hill <br>Companies, 2001<br>6.5<br>Security and Authorization<br>243<br>U1<br>U1<br>U<br>U2<br>U2<br>3<br>U3<br>U1<br>U2<br>U3<br>DBA<br>DBA<br>DBA<br>(a)<br>(b)<br>(c)<br>Figure 6.7<br>Attempt to defeat authorization revocation.<br>We require that all edges in an authorization graph be part of some path originating<br>with the database administrator. The edges between U2 and U3 are deleted, and the<br>resulting authorization graph is as in Figure 6.8.<br>6.5.5<br>Notion of Roles<br>Consider a bank where there are many tellers. Each teller must have the same types<br>of authorizations to the same set of relations. Whenever a new teller is appointed, she<br>will have to be given all these authorizations individually.<br>A better scheme would be to specify the authorizations that every teller is to be<br>given, and to separately identify which database users are tellers. The system can use<br>these two pieces of information to determine the authorizations of each person who<br>is a teller. When a new person is hired as a teller, a user identiﬁer must be allocated<br>to him, and he must be identiﬁed as a teller. Individual permissions given to tellers<br>need not be speciﬁed again.<br>The notion of roles captures this scheme. A set of roles is created in the database.<br>Authorizations can be granted to roles, in exactly the same fashion as they are granted<br>to individual users. Each database user is granted a set of roles (which may be empty)<br>that he or she is authorized to perform.<br>U1<br>U3<br>U2<br>DBA<br>Figure 6.8<br>Authorization graph.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>II. Relational Databases<br>6. Integrity and Security<br>248<br>© The McGraw−Hill <br>Companies, 2001<br>244<br>Chapter 6<br>Integrity and Security<br>In our bank database, examples of roles could include teller, branch-manager, audi-<br>tor, and system-administrator.<br>A less preferable alternative would be to create a teller userid, and permit each<br>teller to connect to the database using the teller userid. The problem with this scheme<br>is that it would not be possible to identify exactly which teller carried out a transac-<br>tion, leading to security risks. The use of roles has the beneﬁt of requiring users to<br>connect to the database with their own userid.<br>Any authorization that can be granted to a user can be granted to a role. Roles<br>are granted to users just as authorizations are. And like other authorizations, a user<br>may also be granted authorization to grant a particular role to others. Thus, branch<br>managers may be granted authorization to grant the teller role.<br>6.5.6<br>Audit Trails<br>Many secure database applications require an audit trail be maintained. An audit<br>trail is a log of all changes (inserts/deletes/updates) to the database, along with in-<br>formation such as which user performed the change and when the change was per-<br>formed.<br>The audit trail aids security in several ways. For instance, if the balance on an<br>account is found to be incorrect, the bank may wish to trace all the updates performed<br>on the account, to ﬁnd out incorrect (or fraudulent) updates, as well as the persons<br>who carried out the updates. The bank could then also use the audit trail to trace all<br>the updates performed by these persons, in order to ﬁnd other incorrect or fraudulent<br>updates.<br>It is possible to create an audit trail by deﬁning appropriate triggers on relation<br>updates (using system-deﬁned variables that identify the user name and time). How-<br>ever, many database systems provide built-in mechanisms to create audit trails, which<br>are much more convenient to use. Details of how to create audit trails vary across<br>database systems, and you should refer the database system manuals for details.<br>6.6<br>Authorization in SQL<br>The SQL language offers a fairly powerful mechanism for deﬁning authorizations.<br>We describe these mechanisms, as well as their limitations, in this section.<br>6.6.1<br>Privileges in SQL<br>The SQL standard includes the privileges delete, insert, select, and update. The select<br>privilege corresponds to the read privilege. SQL also includes a references privilege<br>that permits a user/role to declare foreign keys when creating relations. If the relation<br>to be created includes a foreign key that references attributes of another relation,<br>the user/role must have been granted references privilege on those attributes. The<br>reason that the references privilege is a useful feature is somewhat subtle; we explain<br>the reason later in this section.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>II. Relational Databases<br>6. Integrity and Security<br>249<br>© The McGraw−Hill <br>Companies, 2001<br>6.6<br>Authorization in SQL<br>245<br>The SQL data-deﬁnition language includes commands to grant and revoke priv-<br>ileges. The grant statement is used to confer authorization. The basic form of this<br>statement is:<br>grant &lt;privilege list&gt; on &lt;relation name or view name&gt; to &lt;user/role list&gt;<br>The privilege list allows the granting of several privileges in one command.<br>The following grant statement grants users U1, U2, and U3 select authorization on<br>the account relation:<br>grant select on account to U1, U2, U3<br>The update authorization may be given either on all attributes of the relation or<br>on only some. If update authorization is included in a grant statement, the list of at-<br>tributes on which update authorization is to be granted optionally appears in paren-<br>theses immediately after the update keyword. If the list of attributes is omitted, the<br>update privilege will be granted on all attributes of the relation.<br>This grant statement gives users U1, U2, and U3 update authorization on the amount<br>attribute of the loan relation:<br>grant update (amount) on loan to U1, U2, U3<br>The insert privilege may also specify a list of attributes; any inserts to the relation<br>must specify only these attributes, and the system either gives each of the remaining<br>attributes default values (if a default is deﬁned for the attribute) or sets them to null.<br>The SQL references privilege is granted on speciﬁc attributes in a manner like<br>that for the update privilege. The following grant statement allows user U1 to create<br>relations that reference the key branch-name of the branch relation as a foreign key:<br>grant references (branch-name) on branch to U1<br>Initially, it may appear that there is no reason ever to prevent users from creating for-<br>eign keys referencing another relation. However, recall from Section 6.2 that foreign-<br>key constraints restrict deletion and update operations on the referenced relation.<br>In the preceding example, if U1 creates a foreign key in a relation r referencing the<br>branch-name attribute of the branch relation, and then inserts a tuple into r pertaining<br>to the Perryridge branch, it is no longer possible to delete the Perryridge branch from<br>the branch relation without also modifying relation r. Thus, the deﬁnition of a foreign<br>key by U1 restricts future activity by other users; therefore, there is a need for the<br>references privilege.<br>The privilege all privileges can be used as a short form for all the allowable priv-<br>ileges. Similarly, the user name public refers to all current and future users of the<br>system. SQL also includes a usage privilege that authorizes a user to use a speciﬁed<br>domain (recall that a domain corresponds to the programming-language notion of a<br>type, and may be user deﬁned).<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>II. Relational Databases<br>6. Integrity and Security<br>250<br>© The McGraw−Hill <br>Companies, 2001<br>246<br>Chapter 6<br>Integrity and Security<br>6.6.2<br>Roles<br>Roles can be created in SQL:1999 as follows<br>create role teller<br>Roles can then be granted privileges just as the users can, as illustrated in this state-<br>ment:<br>grant select on account<br>to teller<br>Roles can be asigned to the users, as well as to some other roles, as these statements<br>show.<br>grant teller to john<br>create role manager<br>grant teller to manager<br>grant manager to mary<br>Thus the privileges of a user or a role consist of<br>• All privileges directly granted to the user/role<br>• All privileges granted to roles that have been granted to the user/role<br>Note that there can be a chain of roles; for example, the role employee may be granted<br>to all tellers. In turn the role teller is granted to all managers. Thus, the manager role in-<br>herits all privileges granted to the roles employee and to teller in addition to privileges<br>granted directly to manager.<br>6.6.3<br>The Privilege to Grant Privileges<br>By default, a user/role that is granted a privilege is not authorized to grant that priv-<br>ilege to another user/role. If we wish to grant a privilege and to allow the recipient<br>to pass the privilege on to other users, we append the with grant option clause to the<br>appropriate grant command. For example, if we wish to allow U1 the select privilege<br>on branch and allow U1 to grant this privilege to others, we write<br>grant select on branch to U1 with grant option<br>To revoke an authorization, we use the revoke statement. It takes a form almost<br>identical to that of grant:<br>revoke &lt;privilege list&gt; on &lt;relation name or view name&gt;<br>from &lt;user/role list&gt; [restrict | cascade]<br>Thus, to revoke the privileges that we granted previously, we write<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>II. Relational Databases<br>6. Integrity and Security<br>251<br>© The McGraw−Hill <br>Companies, 2001<br>6.6<br>Authorization in SQL<br>247<br>revoke select on branch from U1, U2, U3<br>revoke update (amount) on loan from U1, U2, U3<br>revoke references (branch-name) on branch from U1<br>As we saw in Section 6.5.4, the revocation of a privilege from a user/role may cause<br>other users/roles also to lose that privilege. This behavior is called cascading of the<br>revoke. In most database systems, cascading is the default behavior; the keyword cas-<br>cade can thus be omitted, as we have done in the preceding examples. The revoke<br>statement may alternatively specify restrict:<br>revoke select on branch from U1, U2, U3 restrict<br>In this case, the system returns an error if there are any cascading revokes, and does<br>not carry out the revoke action. The following revoke statement revokes only the<br>grant option, rather than the actual select privilege:<br>revoke grant option for select on branch from U1<br>6.6.4<br>Other Features<br>The creator of an object (relation/view/role) gets all privileges on the object, includ-<br>ing the privilege to grant privileges to others.<br>The SQL standard speciﬁes a primitive authorization mechanism for the database<br>schema: Only the owner of the schema can carry out any modiﬁcation to the schema.<br>Thus, schema modiﬁcations—such as creating or deleting relations, adding or drop-<br>ping attributes of relations, and adding or dropping indices—may be executed by<br>only the owner of the schema. Several database implementations have more power-<br>ful authorization mechanisms for database schemas, similar to those discussed ear-<br>lier, but these mechanisms are nonstandard.<br>6.6.5<br>Limitations of SQL Authorization<br>The current SQL standards for authorization have some shortcomings. For instance,<br>suppose you want all students to be able to see their own grades, but not the grades<br>of anyone else. Authorization must then be at the level of individual tuples, which is<br>not possible in the SQL standards for authorization.<br>Furthermore, with the growth in the Web, database accesses come primarily from<br>Web application servers. The end users may not have individual user identiﬁers on<br>the database, and indeed there may only be a single user identiﬁer in the database<br>corresponding to all users of an application server.<br>The task of authorization then falls on the application server; the entire authoriza-<br>tion scheme of SQL is bypassed. The beneﬁt is that ﬁne-grained authorizations, such<br>as those to individual tuples, can be implemented by the application. The problems<br>are these:<br>• The code for checking authorization becomes intermixed with the rest of the<br>application code.<br>S</span><br><br><span style="background-color: #BDB2FF;" title="Chunk 30 | Start: 600060 | End: 620060 | Tokens: 3209">ilberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>II. Relational Databases<br>6. Integrity and Security<br>252<br>© The McGraw−Hill <br>Companies, 2001<br>248<br>Chapter 6<br>Integrity and Security<br>• Implementing authorization through application code, rather than specifying<br>it declaratively in SQL, makes it hard to ensure the absence of loopholes. Be-<br>cause of an oversight, one of the application programs may not check for au-<br>thorization, allowing unauthorized users access to conﬁdential data. Verifying<br>that all application programs make all required authorization checks involves<br>reading through all the application server code, a formidable task in a large<br>system.<br>6.7<br>Encryption and Authentication<br>The various provisions that a database system may make for authorization may still<br>not provide sufﬁcient protection for highly sensitive data. In such cases, data may<br>be stored in encrypted form. It is not possible for encrypted data to be read unless<br>the reader knows how to decipher (decrypt) them. Encryption also forms the basis of<br>good schemes for authenticating users to a database.<br>6.7.1<br>Encryption Techniques<br>There are a vast number of techniques for the encryption of data. Simple encryption<br>techniques may not provide adequate security, since it may be easy for an unautho-<br>rized user to break the code. As an example of a weak encryption technique, consider<br>the substitution of each character with the next character in the alphabet. Thus,<br>Perryridge<br>becomes<br>Qfsszsjehf<br>If an unauthorized user sees only “Qfsszsjehf,” she probably has insufﬁcient infor-<br>mation to break the code. However, if the intruder sees a large number of encrypted<br>branch names, she could use statistical data regarding the relative frequency of char-<br>acters to guess what substitution is being made (for example, E is the most common<br>letter in English text, followed by T, A, O, N, I and so on).<br>A good encryption technique has the following properties:<br>• It is relatively simple for authorized users to encrypt and decrypt data.<br>• It depends not on the secrecy of the algorithm, but rather on a parameter of<br>the algorithm called the encryption key.<br>• Its encryption key is extremely difﬁcult for an intruder to determine.<br>One approach, the Data Encryption Standard (DES), issued in 1977, does both a<br>substitution of characters and a rearrangement of their order on the basis of an en-<br>cryption key. For this scheme to work, the authorized users must be provided with<br>the encryption key via a secure mechanism. This requirement is a major weakness,<br>since the scheme is no more secure than the security of the mechanism by which<br>the encryption key is transmitted. The DES standard was reafﬁrmed in 1983, 1987,<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>II. Relational Databases<br>6. Integrity and Security<br>253<br>© The McGraw−Hill <br>Companies, 2001<br>6.7<br>Encryption and Authentication<br>249<br>and again in 1993. However, weakness in DES was recongnized in 1993 as reaching a<br>point where a new standard to be called the Advanced Encryption Standard (AES),<br>needed to be selected. In 2000, the Rijndael algorithm (named for the inventors<br>V. Rijmen and J. Daemen), was selected to be the AES. The Rijndael algorithm was<br>chosen for its signiﬁcantly stronger level of security and its relative ease of imple-<br>mentation on current computer systems as well as such devices as smart cards. Like<br>the DES standard, the Rijndael algorithm is a shared-key (or, symmetric key) algo-<br>rithm in which the authorized users share a key.<br>Public-key encryption is an alternative scheme that avoids some of the problems<br>that we face with the DES. It is based on two keys; a public key and a private key. Each<br>user Ui has a public key Ei and a private key Di. All public keys are published: They<br>can be seen by anyone. Each private key is known to only the one user to whom the<br>key belongs. If user U1 wants to store encrypted data, U1 encrypts them using public<br>key E1. Decryption requires the private key D1.<br>Because the encryption key for each user is public, it is possible to exchange infor-<br>mation securely by this scheme. If user U1 wants to share data with U2, U1 encrypts<br>the data using E2, the public key of U2. Since only user U2 knows how to decrypt the<br>data, information is transferred securely.<br>For public-key encryption to work, there must be a scheme for encryption that<br>can be made public without making it easy for people to ﬁgure out the scheme for<br>decryption. In other words, it must be hard to deduce the private key, given the public<br>key. Such a scheme does exist and is based on these conditions:<br>• There is an efﬁcient algorithm for testing whether or not a number is prime.<br>• No efﬁcient algorithm is known for ﬁnding the prime factors of a number.<br>For purposes of this scheme, data are treated as a collection of integers. We create<br>a public key by computing the product of two large prime numbers: P1 and P2. The<br>private key consists of the pair (P1, P2). The decryption algorithm cannot be used<br>successfully if only the product P1P2 is known; it needs the individual values P1 and<br>P2. Since all that is published is the product P1P2, an unauthorized user would need<br>to be able to factor P1P2 to steal data. By choosing P1 and P2 to be sufﬁciently large<br>(over 100 digits), we can make the cost of factoring P1P2 prohibitively high (on the<br>order of years of computation time, on even the fastest computers).<br>The details of public-key encryption and the mathematical justiﬁcation of this tech-<br>nique’s properties are referenced in the bibliographic notes.<br>Although public-key encryption by this scheme is secure, it is also computation-<br>ally expensive. A hybrid scheme used for secure communication is as follows: DES<br>keys are exchanged via a public-key–encryption scheme, and DES encryption is used<br>on the data transmitted subsequently.<br>6.7.2<br>Authentication<br>Authentication refers to the task of verifying the identity of a person/software con-<br>necting to a database. The simplest form of authentication consists of a secret pass-<br>word which must be presented when a connection is opened to a database.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>II. Relational Databases<br>6. Integrity and Security<br>254<br>© The McGraw−Hill <br>Companies, 2001<br>250<br>Chapter 6<br>Integrity and Security<br>Password-based authentication is used widely by operating systems as well as<br>databases. However, the use of passwords has some drawbacks, especially over a<br>network. If an eavesdropper is able to “sniff” the data being sent over the network,<br>she may be able to ﬁnd the password as it is being sent across the network. Once<br>the eavesdropper has a user name and password, she can connect to the database,<br>pretending to be the legitimate user.<br>A more secure scheme involves a challenge-response system. The database sys-<br>tem sends a challenge string to the user. The user encrypts the challenge string using<br>a secret password as encryption key, and then returns the result. The database system<br>can verify the authenticity of the user by decrypting the string with the same secret<br>password, and checking the result with the original challenge string. This scheme<br>ensures that no passwords travel across the network.<br>Public-key systems can be used for encryption in challenge–response systems.<br>The database system encrypts a challenge string using the user’s public key and<br>sends it to the user. The user decrypts the string using her private key, and returns<br>the result to the database system. The database system then checks the response.<br>This scheme has the added beneﬁt of not storing the secret password in the database,<br>where it could potentially be seen by system administrators.<br>Another interesting application of public-key encryption is in digital signatures<br>to verify authenticity of data; digital signatures play the electronic role of physical<br>signatures on documents. The private key is used to sign data, and the signed data<br>can be made public. Anyone can verify them by the public key, but no one could have<br>generated the signed data without having the private key. Thus, we can authenticate<br>the data; that is, we can verify that the data were indeed created by the person who<br>claims to have created them.<br>Furthermore, digital signatures also serve to ensure nonrepudiation. That is, in<br>case the person who created the data later claims she did not create it (the electronic<br>equivalent of claiming not to have signed the check), we can prove that that person<br>must have created the data (unless her private key was leaked to others).<br>6.8<br>Summary<br>• Integrity constraints ensure that changes made to the database by authorized<br>users do not result in a loss of data consistency.<br>• In earlier chapters, we considered several forms of constraints, including key<br>declarations and the declaration of the form of a relationship (many to many,<br>many to one, one to one). In this chapter, we considered several additional<br>forms of constraints, and discussed mechanisms for ensuring the maintenance<br>of these constraints.<br>• Domain constraints specify the set of possible values that may be associated<br>with an attribute. Such constraints may also prohibit the use of null values for<br>particular attributes.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>II. Relational Databases<br>6. Integrity and Security<br>255<br>© The McGraw−Hill <br>Companies, 2001<br>6.8<br>Summary<br>251<br>• Referential-integrity constraints ensure that a value that appears in one rela-<br>tion for a given set of attributes also appears for a certain set of attributes in<br>another relation.<br>• Domain constraints, and referential-integrity constraints are relatively easy to<br>test. Use of more complex constraints may lead to substantial overhead. We<br>saw two ways to express more general constraints. Assertions are declarative<br>expressions that state predicates that we require always to be true.<br>• Triggers deﬁne actions to be executed automatically when certain events oc-<br>cur and corresponding conditions are satisﬁed. Triggers have many uses, such<br>as implementing business rules, audit logging, and even carrying out actions<br>outside the database system. Although triggers were added only lately to the<br>SQL standard as part of SQL:1999, most database systems have long imple-<br>mented triggers.<br>• The data stored in the database need to be protected from unauthorized ac-<br>cess, malicious destruction or alteration, and accidental introduction of incon-<br>sistency.<br>• It is easier to protect against accidental loss of data consistency than to protect<br>against malicious access to the database. Absolute protection of the database<br>from malicious abuse is not possible, but the cost to the perpetrator can be<br>made sufﬁciently high to deter most, if not all, attempts to access the database<br>without proper authority.<br>• A user may have several forms of authorization on parts of the database. Au-<br>thorization is a means by which the database system can be protected against<br>malicious or unauthorized access.<br>• A user who has been granted some form of authority may be allowed to pass<br>on this authority to other users. However, we must be careful about how au-<br>thorization can be passed among users if we are to ensure that such autho-<br>rization can be revoked at some future time.<br>• Roles help to assign a set of privileges to a user according to on the role that<br>the user plays in the organization.<br>• The various authorization provisions in a database system may not provide<br>sufﬁcient protection for highly sensitive data. In such cases, data can be en-<br>crypted. Only a user who knows how to decipher (decrypt) the encrypted data<br>can read them. Encryption also forms the basis for secure authentication of<br>users.<br>Review Terms<br>• Domain constraints<br>• Check clause<br>• Referential integrity<br>• Primary key constraint<br>• Unique constraint<br>• Foreign key constraint<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>II. Relational Databases<br>6. Integrity and Security<br>256<br>© The McGraw−Hill <br>Companies, 2001<br>252<br>Chapter 6<br>Integrity and Security<br>• Cascade<br>• Assertion<br>• Trigger<br>• Event-condition-action model<br>• Before and after triggers<br>• Transition variables and tables<br>• Database security<br>• Levels of security<br>• Authorization<br>• Privileges<br>  Read<br>  Insert<br>  Update<br>  Delete<br>  Index<br>  Resource<br>  Alteration<br>  Drop<br>  Grant<br>  All privileges<br>• Authorization graph<br>• Granting of privileges<br>• Roles<br>• Encryption<br>• Secret-key encryption<br>• Public-key encryption<br>• Authentication<br>• Challenge–response system<br>• Digital signature<br>• Nonrepudiation<br>Exercises<br>6.1 Complete the SQL DDL deﬁnition of the bank database of Figure 6.2 to include<br>the relations loan and borrower.<br>6.2 Consider the following relational database:<br>employee (employee-name, street, city)<br>works (employee-name, company-name, salary)<br>company (company-name, city)<br>manages (employee-name, manager-name)<br>Give an SQL DDL deﬁnition of this database. Identify referential-integrity con-<br>straints that should hold, and include them in the DDL deﬁnition.<br>6.3 Referential-integrity constraints as deﬁned in this chapter involve exactly two<br>relations. Consider a database that includes the following relations:<br>salaried-worker (name, ofﬁce, phone, salary)<br>hourly-worker (name, hourly-wage)<br>address (name, street, city)<br>Suppose that we wish to require that every name that appears in address appear<br>in either salaried-worker or hourly-worker, but not necessarily in both.<br>a. Propose a syntax for expressing such constraints.<br>b. Discuss the actions that the system must take to enforce a constraint of this<br>form.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>II. Relational Databases<br>6. Integrity and Security<br>257<br>© The McGraw−Hill <br>Companies, 2001<br>Exercises<br>253<br>6.4 SQL allows a foreign-key dependency to refer to the same relation, as in the<br>following example:<br>create table manager<br>(employee-name<br>char(20) not null<br>manager-name<br>char(20) not null,<br>primary key employee-name,<br>foreign key (manager-name) references manager<br>on delete cascade )<br>Here, employee-name is a key to the table manager, meaning that each employee<br>has at most one manager. The foreign-key clause requires that every manager<br>also be an employee. Explain exactly what happens when a tuple in the relation<br>manager is deleted.<br>6.5 Suppose there are two relations r and s, such that the foreign key B of r refer-<br>ences the primary key A of s. Describe how the trigger mechanism can be used<br>to implement the on delete cascade option, when a tuple is deleted from s.<br>6.6 Write an assertion for the bank database to ensure that the assets value for the<br>Perryridge branch is equal to the sum of all the amounts lent by the Perryridge<br>branch.<br>6.7 Write an SQL trigger to carry out the following action: On delete of an account,<br>for each owner of the account, check if the owner has any remaining accounts,<br>and if she does not, delete her from the depositor relation.<br>6.8 Consider a view branch-cust deﬁned as follows:<br>create view branch-cust as<br>select branch-name, customer-name<br>from depositor, account<br>where depositor.account-number = account.account-number<br>Suppose that the view is materialized, that is, the view is computed and stored.<br>Write active rules to maintain the view, that is, to keep it up to date on insertions<br>to and deletions from depositor or account. Do not bother about updates.<br>6.9 Make a list of security concerns for a bank. For each item on your list, state<br>whether this concern relates to physical security, human security, operating-<br>system security, or database security.<br>6.10 Using the relations of our sample bank database, write an SQL expression to<br>deﬁne the following views:<br>a. A view containing the account numbers and customer names (but not the<br>balances) for all accounts at the Deer Park branch.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>II. Relational Databases<br>6. Integrity and Security<br>258<br>© The McGraw−Hill <br>Companies, 2001<br>254<br>Chapter 6<br>Integrity and Security<br>b. A view containing the names and addresses of all customers who have an<br>account with the bank, but do not have a loan.<br>c. A view containing the name and average account balance of every customer<br>of the Rock Ridge branch.<br>6.11 For each of the views that you deﬁned in Exercise 6.10, explain how updates<br>would be performed (if they should be allowed at all). Hint: See the discussion<br>of views in Chapter 3.<br>6.12 In Chapter 3, we described the use of views to simplify access to the database<br>by users who need to see only part of the database. In this chapter, we described<br>the use of views as a security mechanism. Do these two purposes for views ever<br>conﬂict? Explain your answer.<br>6.13 What is the purpose of having separate categories for index authorization and<br>resource authorization?<br>6.14 Database systems that store each relation in a separate operating-system ﬁle<br>may use the operating system’s security and authorization scheme, instead of<br>deﬁning a special scheme themselves. Discuss an advantage and a disadvantage<br>of such an approach.<br>6.15 What are two advantages of encrypting data stored in the database?<br>6.16 Perhaps the most important data items in any database system are the pass-<br>words that control access to the database. Suggest a scheme for the secure stor-<br>age of passwords. Be sure that your scheme allows the system to test passwords<br>supplied by users who are attempting to log into the system.<br>Bibliographical Notes<br>Discussions of integrity constraints in the relational model are offered by Hammer<br>and McLeod [1975], Stonebraker [1975], Eswaran and Chamberlin [1975], Schmid and<br>Swenson [1975] and Codd [1979]. The original SQL proposals for assertions and trig-<br>gers are discussed in Astrahan et al. [1976], Chamberlin et al. [1976], and Chamberlin<br>et al. [1981]. See the bibliographic notes of Chapter 4 for references to SQL standards<br>and books on SQL.<br>Discussions of efﬁcient maintenance and checking of semantic-integrity assertions<br>are offered by Hammer and Sarin [1978], Badal and Popek [1979], Bernstein et al.<br>[1980a], Hsu and Imielinski [1985], McCune and Henschen [1989], and Chomicki<br>[1992]. An alternative to using run-time integrity checking is certifying the correct-<br>ness of programs that access the database. Sheard and Stemple [1989] discusses this<br>approach.<br>Active databases are databases that support triggers and other mechanisms that<br>permit the database to take actions on occurrence of events. McCarthy and Dayal<br>[1989] discuss the architecture of an active database system based on the event–<br>condition–action formalism. Widom and Finkelstein [1990] describe the architecture<br>of a rule system based on set-oriented rules; the implementation of the rule system<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>II. Relational Databases<br>6. Integrity and Security<br>259<br>© The McGraw−Hill <br>Companies, 2001<br>Bibliographical Notes<br>255<br>on the Starburst extensible database system is presented in Widom et al. [1991]. Con-<br>sider an execution mechanism that allows a nondeterministic choice of which rule to<br>execute next. A rule system is said to be conﬂuent if, regardless of the rule chosen,<br>the ﬁnal state is the same. Issues of termination, nondeterminism, and conﬂuence of<br>rule systems are discussed in Aiken et al. [1995].<br>Security aspects of computer systems in general are discussed in Bell and La-<br>Padula [1976] and by US Dept. of Defense [1985]. Security aspects of SQL can be<br>found in the SQL standards and textbooks on SQL referenced in the bibliographic<br>notes of Chapter 4.<br>Stonebraker and Wong [1974] discusses the Ingres approach to security, which in-<br>volves modiﬁcation of users’ queries to ensure that users do not access data for which<br>authorization has not been granted. Denning and Denning [1979] survey database se-<br>curity.<br>Database systems that can produce incorrect answers when necessary for security<br>maintenance are discussed in Winslett et al. [1994] and Tendick and Matloff [1994].<br>Work on security in relational databases includes that of Stachour and </span><br><br><span style="background-color: #FFC6FF;" title="Chunk 31 | Start: 620062 | End: 640062 | Tokens: 3227">Thuraising-<br>ham [1990], Jajodia and Sandhu [1990], and Qian and Lunt [1996]. Operating-system<br>security issues are discussed in most operating-system texts, including Silberschatz<br>and Galvin [1998].<br>Stallings [1998] provides a textbook description of cryptography. Daemen and Ri-<br>jmen [2000] present the Rijndael algorithm. The Data Encryption Standard is pre-<br>sented by US Dept. of Commerce [1977]. Public-key encryption is discussed by Rivest<br>et al. [1978]. Other discussions on cryptography include Difﬁe and Hellman [1979],<br>Simmons [1979], Fernandez et al. [1981], and Akl [1983].<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>II. Relational Databases<br>7. Relational−Database <br>Design<br>260<br>© The McGraw−Hill <br>Companies, 2001<br>C<br>H<br>A<br>P<br>T<br>E<br>R<br>7<br>Relational-Database Design<br>This chapter continues our discussion of design issues in relational databases. In gen-<br>eral, the goal of a relational-database design is to generate a set of relation schemas<br>that allows us to store information without unnecessary redundancy, yet also allows<br>us to retrieve information easily. One approach is to design schemas that are in an<br>appropriate normal form. To determine whether a relation schema is in one of the<br>desirable normal forms, we need additional information about the real-world enter-<br>prise that we are modeling with the database. In this chapter, we introduce the notion<br>of functional dependencies. We then deﬁne normal forms in terms of functional de-<br>pendencies and other types of data dependencies.<br>7.1<br>First Normal Form<br>The ﬁrst of the normal forms that we study, ﬁrst normal form, imposes a very basic<br>requirement on relations; unlike the other normal forms, it does not require addi-<br>tional information such as functional dependencies.<br>A domain is atomic if elements of the domain are considered to be indivisible<br>units. We say that a relation schema R is in ﬁrst normal form (1NF) if the domains of<br>all attributes of R are atomic.<br>A set of names is an example of a nonatomic value. For example, if the schema of<br>a relation employee included an attribute children whose domain elements are sets of<br>names, the schema would not be in ﬁrst normal form.<br>Composite attributes, such as an attribute address with component attributes street<br>and city, also have nonatomic domains.<br>Integers are assumed to be atomic, so the set of integers is an atomic domain; the<br>set of all sets of integers is a nonatomic domain. The distinction is that we do not<br>normally consider integers to have subparts, but we consider sets of integers to have<br>subparts—namely, the integers making up the set. But the important issue is not<br>what the domain itself is, but rather how we use domain elements in our database.<br>257<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>II. Relational Databases<br>7. Relational−Database <br>Design<br>261<br>© The McGraw−Hill <br>Companies, 2001<br>258<br>Chapter 7<br>Relational-Database Design<br>The domain of all integers would be nonatomic if we considered each integer to be<br>an ordered list of digits.<br>As a practical illustration of the above point, consider an organization that as-<br>signs employees identiﬁcation numbers of the following form: The ﬁrst two letters<br>specify the department and the remaining four digits are a unique number within<br>the department for the employee. Examples of such numbers would be CS0012 and<br>EE1127. Such identiﬁcation numbers can be divided into smaller units, and are there-<br>fore nonatomic. If a relation schema had an attribute whose domain consists of iden-<br>tiﬁcation numbers encoded as above, the schema would not be in ﬁrst normal form.<br>When such identiﬁcation numbers are used, the department of an employee can<br>be found by writing code that breaks up the structure of an identiﬁcation number.<br>Doing so requires extra programming, and information gets encoded in the applica-<br>tion program rather than in the database. Further problems arise if such identiﬁcation<br>numbers are used as primary keys: When an employee changes department, the em-<br>ployee’s identiﬁcation number must be changed everywhere it occurs, which can be<br>a difﬁcult task, or the code that interprets the number would give a wrong result.<br>The use of set valued attributes can lead to designs with redundant storage of data,<br>which in turn can result in inconsistencies. For instance, instead of the relationship<br>between accounts and customers being represented as a separate relation depositor,<br>a database designer may be tempted to store a set of owners with each account, and<br>a set of accounts with each customer. Whenever an account is created, or the set of<br>owners of an account is updated, the update has to be performed at two places; fail-<br>ure to perform both updates can leave the database in an inconsistent state. Keeping<br>only one of these sets would avoid repeated information, but would complicate some<br>queries. Set valued attributes are also more complicated to write queries with, and<br>more complicated to reason about.<br>In this chapter we consider only atomic domains, and assume that relations are in<br>ﬁrst normal form. Although we have not mentioned ﬁrst normal form earlier, when<br>we introduced the relational model in Chapter 3 we stated that attribute values must<br>be atomic.<br>Some types of nonatomic values can be useful, although they should be used with<br>care. For example, composite valued attributes are often useful, and set valued at-<br>tributes are also useful in many cases, which is why both are supported in the E-R<br>model. In many domains where entities have a complex structure, forcing a ﬁrst nor-<br>mal form representation represents an unnecessary burden on the application pro-<br>grammer, who has to write code to convert data into atomic form. There is also a run-<br>time overhead of converting data back and forth from the atomic form. Support for<br>nonatomic values can thus be very useful in such domains. In fact, modern database<br>systems do support many types of nonatomic values, as we will see in Chapters 8<br>and 9. However, in this chapter we restrict ourselves to relations in ﬁrst normal form.<br>7.2<br>Pitfalls in Relational-Database Design<br>Before we continue our discussion of normal forms, let us look at what can go wrong<br>in a bad database design. Among the undesirable properties that a bad design may<br>have are:<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>II. Relational Databases<br>7. Relational−Database <br>Design<br>262<br>© The McGraw−Hill <br>Companies, 2001<br>7.2<br>Pitfalls in Relational-Database Design<br>259<br>• Repetition of information<br>• Inability to represent certain information<br>We shall discuss these problems with the help of a modiﬁed database design for our<br>banking example: In contrast to the relation schema used in Chapters 3 to 6, sup-<br>pose the information concerning loans is kept in one single relation, lending, which is<br>deﬁned over the relation schema<br>Lending-schema = (branch-name, branch-city, assets, customer-name,<br>loan-number, amount)<br>Figure 7.1 shows an instance of the relation lending (Lending-schema). A tuple t in the<br>lending relation has the following intuitive meaning:<br>• t[assets] is the asset ﬁgure for the branch named t[branch-name].<br>• t[branch-city] is the city in which the branch named t[branch-name] is located.<br>• t[loan-number] is the number assigned to a loan made by the branch named<br>t[branch-name] to the customer named t[customer-name].<br>• t[amount] is the amount of the loan whose number is t[loan-number].<br>Suppose that we wish to add a new loan to our database. Say that the loan is made<br>by the Perryridge branch to Adams in the amount of $1500. Let the loan-number be<br>L-31. In our design, we need a tuple with values on all the attributes of Lending-<br>schema. Thus, we must repeat the asset and city data for the Perryridge branch, and<br>must add the tuple<br>(Perryridge, Horseneck, 1700000, Adams, L-31, 1500)<br>customer-<br>loan-<br>branch-name<br>branch-city<br>assets<br>name<br>number<br>amount<br>Downtown<br>Brooklyn<br>9000000<br>Jones<br>L-17<br>1000<br>Redwood<br>Palo Alto<br>2100000<br>Smith<br>L-23<br>2000<br>Perryridge<br>Horseneck<br>1700000<br>Hayes<br>L-15<br>1500<br>Downtown<br>Brooklyn<br>9000000<br>Jackson<br>L-14<br>1500<br>Mianus<br>Horseneck<br>400000<br>Jones<br>L-93<br>500<br>Round Hill<br>Horseneck<br>8000000<br>Turner<br>L-11<br>900<br>Pownal<br>Bennington<br>300000<br>Williams<br>L-29<br>1200<br>North Town<br>Rye<br>3700000<br>Hayes<br>L-16<br>1300<br>Downtown<br>Brooklyn<br>9000000<br>Johnson<br>L-18<br>2000<br>Perryridge<br>Horseneck<br>1700000<br>Glenn<br>L-25<br>2500<br>Brighton<br>Brooklyn<br>7100000<br>Brooks<br>L-10<br>2200<br>Figure 7.1<br>Sample lending relation.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>II. Relational Databases<br>7. Relational−Database <br>Design<br>263<br>© The McGraw−Hill <br>Companies, 2001<br>260<br>Chapter 7<br>Relational-Database Design<br>to the lending relation. In general, the asset and city data for a branch must appear<br>once for each loan made by that branch.<br>The repetition of information in our alternative design is undesirable. Repeating<br>information wastes space. Furthermore, it complicates updating the database. Sup-<br>pose, for example, that the assets of the Perryridge branch change from 1700000<br>to 1900000. Under our original design, one tuple of the branch relation needs to be<br>changed. Under our alternative design, many tuples of the lending relation need to be<br>changed. Thus, updates are more costly under the alternative design than under the<br>original design. When we perform the update in the alternative database, we must<br>ensure that every tuple pertaining to the Perryridge branch is updated, or else our<br>database will show two different asset values for the Perryridge branch.<br>That observation is central to understanding why the alternative design is bad. We<br>know that a bank branch has a unique value of assets, so given a branch name we can<br>uniquely identify the assets value. On the other hand, we know that a branch may<br>make many loans, so given a branch name, we cannot uniquely determine a loan<br>number. In other words, we say that the functional dependency<br>branch-name →assets<br>holds on Lending-schema, but we do not expect the functional dependency branch-<br>name →loan-number to hold. The fact that a branch has a particular value of assets,<br>and the fact that a branch makes a loan are independent, and, as we have seen, these<br>facts are best represented in separate relations. We shall see that we can use functional<br>dependencies to specify formally when a database design is good.<br>Another problem with the Lending-schema design is that we cannot represent di-<br>rectly the information concerning a branch (branch-name, branch-city, assets) unless<br>there exists at least one loan at the branch. This is because tuples in the lending rela-<br>tion require values for loan-number, amount, and customer-name.<br>One solution to this problem is to introduce null values, as we did to handle up-<br>dates through views. Recall, however, that null values are difﬁcult to handle, as we<br>saw in Section 3.3.4. If we are not willing to deal with null values, then we can create<br>the branch information only when the ﬁrst loan application at that branch is made.<br>Worse, we would have to delete this information when all the loans have been paid.<br>Clearly, this situation is undesirable, since, under our original database design, the<br>branch information would be available regardless of whether or not loans are cur-<br>rently maintained in the branch, and without resorting to null values.<br>7.3<br>Functional Dependencies<br>Functional dependencies play a key role in differentiating good database designs<br>from bad database designs. A functional dependency is a type of constraint that is a<br>generalization of the notion of key, as discussed in Chapters 2 and 3.<br>7.3.1<br>Basic Concepts<br>Functional dependencies are constraints on the set of legal relations. They allow us<br>to express facts about the enterprise that we are modeling with our database.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>II. Relational Databases<br>7. Relational−Database <br>Design<br>264<br>© The McGraw−Hill <br>Companies, 2001<br>7.3<br>Functional Dependencies<br>261<br>In Chapter 2, we deﬁned the notion of a superkey as follows. Let R be a relation<br>schema. A subset K of R is a superkey of R if, in any legal relation r(R), for all pairs<br>t1 and t2 of tuples in r such that t1 ̸= t2, then t1[K] ̸= t2[K]. That is, no two tuples<br>in any legal relation r(R) may have the same value on attribute set K.<br>The notion of functional dependency generalizes the notion of superkey. Consider<br>a relation schema R, and let α ⊆R and β ⊆R. The functional dependency<br>α →β<br>holds on schema R if, in any legal relation r(R), for all pairs of tuples t1 and t2 in r<br>such that t1[α] = t2[α], it is also the case that t1[β] = t2[β].<br>Using the functional-dependency notation, we say that K is a superkey of R if K<br>→R. That is, K is a superkey if, whenever t1[K]<br>=<br>t2[K], it is also the case that<br>t1[R] = t2[R] (that is, t1 = t2).<br>Functional dependencies allow us to express constraints that we cannot express<br>with superkeys. Consider the schema<br>Loan-info-schema = (loan-number, branch-name, customer-name, amount)<br>which is simpliﬁcation of the Lending-schema that we saw earlier. The set of functional<br>dependencies that we expect to hold on this relation schema is<br>loan-number →amount<br>loan-number →branch-name<br>We would not, however, expect the functional dependency<br>loan-number →customer-name<br>to hold, since, in general, a given loan can be made to more than one customer (for<br>example, to both members of a husband–wife pair).<br>We shall use functional dependencies in two ways:<br>1. To test relations to see whether they are legal under a given set of functional<br>dependencies. If a relation r is legal under a set F of functional dependencies,<br>we say that r satisﬁes F.<br>2. To specify constraints on the set of legal relations. We shall thus concern our-<br>selves with only those relations that satisfy a given set of functional dependen-<br>cies. If we wish to constrain ourselves to relations on schema R that satisfy a<br>set F of functional dependencies, we say that F holds on R.<br>Let us consider the relation r of Figure 7.2, to see which functional dependencies<br>are satisﬁed. Observe that A →C is satisﬁed. There are two tuples that have an A<br>value of a1. These tuples have the same C value—namely, c1. Similarly, the two tu-<br>ples with an A value of a2 have the same C value, c2. There are no other pairs of<br>distinct tuples that have the same A value. The functional dependency C →A is not<br>satisﬁed, however. To see that it is not, consider the tuples t1<br>= (a2, b3, c2, d3) and<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>II. Relational Databases<br>7. Relational−Database <br>Design<br>265<br>© The McGraw−Hill <br>Companies, 2001<br>262<br>Chapter 7<br>Relational-Database Design<br>A<br>B<br>C<br>D<br>a1<br>a1<br>a2<br>a2<br>a3<br>b1<br>b2<br>b2<br>b2<br>b3<br>c1<br>c1<br>c2<br>c2<br>c2<br>d1<br>d2<br>d2<br>d3<br>d4<br>Figure 7.2<br>Sample relation r.<br>t2 = (a3, b3, c2, d4). These two tuples have the same C values, c2, but they have dif-<br>ferent A values, a2 and a3, respectively. Thus, we have found a pair of tuples t1 and<br>t2 such that t1[C] = t2[C], but t1[A] ̸= t2[A].<br>Many other functional dependencies are satisﬁed by r, including, for example, the<br>functional dependency AB →D. Note that we use AB as a shorthand for {A,B}, to<br>conform with standard practice. Observe that there is no pair of distinct tuples t1 and<br>t2 such that t1[AB] = t2[AB]. Therefore, if t1[AB] = t2[AB], it must be that t1 = t2<br>and, thus, t1[D] = t2[D]. So, r satisﬁes AB →D.<br>Some functional dependencies are said to be trivial because they are satisﬁed by<br>all relations. For example, A →A is satisﬁed by all relations involving attribute A.<br>Reading the deﬁnition of functional dependency literally, we see that, for all tuples t1<br>and t2 such that t1[A] = t2[A], it is the case that t1[A] = t2[A]. Similarly, AB →A<br>is satisﬁed by all relations involving attribute A. In general, a functional dependency<br>of the form α →β is trivial if β ⊆α.<br>To distinguish between the concepts of a relation satisfying a dependency and a<br>dependency holding on a schema, we return to the banking example. If we consider<br>the customer relation (on Customer-schema) in Figure 7.3, we see that customer-street<br>→customer-city is satisﬁed. However, we believe that, in the real world, two cities<br>customer-name<br>customer-street<br>customer-city<br>Jones<br>Main<br>Harrison<br>Smith<br>North<br>Rye<br>Hayes<br>Main<br>Harrison<br>Curry<br>North<br>Rye<br>Lindsay<br>Park<br>Turner<br>Putnam<br>Stamford<br>Williams<br>Nassau<br>Princeton<br>Adams<br>Spring<br>Pittsfield<br>Pittsfield<br>Johnson<br>Alma<br>Palo Alto<br>Glenn<br>Sand Hill<br>Woodside<br>Brooks<br>Senator<br>Brooklyn<br>Green<br>Walnut<br>Stamford<br>Figure 7.3<br>The customer relation.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>II. Relational Databases<br>7. Relational−Database <br>Design<br>266<br>© The McGraw−Hill <br>Companies, 2001<br>7.3<br>Functional Dependencies<br>263<br>loan-number<br>branch-name<br>amount<br>Downtown<br>L-17<br>1000<br>L-23<br>Redwood<br>2000<br>L-15<br>Perryridge<br>Perryridge<br>1500<br>L-14<br>Downtown<br>Downtown<br>1500<br>L-93<br>Mianus<br>500<br>L-11<br>Round Hill<br>900<br>L-29<br>Pownal<br>1200<br>L-16<br>North Town<br>1300<br>L-18<br>2000<br>L-25<br>2500<br>L-10<br>Brighton<br>2200<br>Figure 7.4<br>The loan relation.<br>can have streets with the same name. Thus, it is possible, at some time, to have an<br>instance of the customer relation in which customer-street →customer-city is not satis-<br>ﬁed. So, we would not include customer-street →customer-city in the set of functional<br>dependencies that hold on Customer-schema.<br>In the loan relation (on Loan-schema) of Figure 7.4, we see that the dependency loan-<br>number →amount is satisﬁed. In contrast to the case of customer-city and customer-<br>street in Customer-schema, we do believe that the real-world enterprise that we are<br>modeling requires each loan to have only one amount. Therefore, we want to require<br>that loan-number →amount be satisﬁed by the loan relation at all times. In other words,<br>we require that the constraint loan-number →amount hold on Loan-schema.<br>In the branch relation of Figure 7.5, we see that branch-name →assets is satisﬁed,<br>as is assets →branch-name. We want to require that branch-name →assets hold on<br>Branch-schema. However, we do not wish to require that assets →branch-name hold,<br>since it is possible to have several branches that have the same asset value.<br>In what follows, we assume that, when we design a relational database, we ﬁrst<br>list those functional dependencies that must always hold. In the banking example,<br>our list of dependencies includes the following:<br>branch-name<br>branch-city<br>assets<br>Downtown<br>Brooklyn<br>9000000<br>Redwood<br>Palo Alto<br>2100000<br>Perryridge<br>Horseneck<br>1700000<br>Mianus<br>Horseneck<br>400000<br>Round Hill<br>Horseneck<br>8000000<br>Pownal<br>Bennington<br>300000<br>North Town<br>Rye<br>3700000<br>Brighton<br>Brooklyn<br>7100000<br>Figure 7.5<br>The branch relation.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>II. Relational Databases<br>7. Relational−Database <br>Design<br>267<br>© The McGraw−Hill <br>Companies, 2001<br>264<br>Chapter 7<br>Relational-Database Design<br>• On Branch-schema:<br>branch-name →branch-city<br>branch-name →assets<br>• On Customer-schema:<br>customer-name →customer-city<br>customer-name →customer-street<br>• On Loan-schema:<br>loan-number →amount<br>loan-number →branch-name<br>• On Borrower-schema:<br>No functional dependencies<br>• On Account-schema:<br>account-number →branch-name<br>account-number →balance<br>• On Depositor-schema:<br>No functional dependencies<br>7.3.2<br>Closure of a Set of Functional Dependencies<br>It is not sufﬁcient to consider the given set of functional dependencies. Rather, we<br>need to consider all functional dependencies that hold. We shall see that, given a set F<br>of functional dependencies, we can prove that certain other functional dependencies<br>hold. We say that such functional dependencies are “logically implied” by F.<br>More formally, given a relational schema R, a functional dependency f on R is log-<br>ically implied by a set of functional dependencies F on R if every relation instance<br>r(R) that satisﬁes F also satisﬁes f.<br>Suppose we are given a relation schema R = (A, B, C, G, H, I) and the set of<br>functional dependencies<br>A →B<br>A →C<br>CG →H<br>CG →I<br>B →H<br>The functional dependency<br>A →H<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>II. Relational Databases<br>7. Relational−Database <br>Design<br>268<br>© The McGraw−Hil</span><br><br><span style="background-color: #FFADAD;" title="Chunk 32 | Start: 640064 | End: 660064 | Tokens: 3527">l <br>Companies, 2001<br>7.3<br>Functional Dependencies<br>265<br>is logically implied. That is, we can show that, whenever our given set of functional<br>dependencies holds on a relation, A →H must also hold on the relation. Suppose that<br>t1 and t2 are tuples such that<br>t1[A] = t2[A]<br>Since we are given that A →B, it follows from the deﬁnition of functional dependency<br>that<br>t1[B] = t2[B]<br>Then, since we are given that B →H, it follows from the deﬁnition of functional<br>dependency that<br>t1[H] = t2[H]<br>Therefore, we have shown that, whenever t1 and t2 are tuples such that t1[A] = t2[A],<br>it must be that t1[H] = t2[H]. But that is exactly the deﬁnition of A →H.<br>Let F be a set of functional dependencies. The closure of F, denoted by F +, is the<br>set of all functional dependencies logically implied by F. Given F, we can compute<br>F + directly from the formal deﬁnition of functional dependency. If F were large, this<br>process would be lengthy and difﬁcult. Such a computation of F + requires argu-<br>ments of the type just used to show that A →H is in the closure of our example set<br>of dependencies.<br>Axioms, or rules of inference, provide a simpler technique for reasoning about<br>functional dependencies. In the rules that follow, we use Greek letters (α, β, γ, . . . )<br>for sets of attributes, and uppercase Roman letters from the beginning of the alphabet<br>for individual attributes. We use αβ to denote α ∪β.<br>We can use the following three rules to ﬁnd logically implied functional dependen-<br>cies. By applying these rules repeatedly, we can ﬁnd all of F +, given F. This collection<br>of rules is called Armstrong’s axioms in honor of the person who ﬁrst proposed it.<br>• Reﬂexivity rule. If α is a set of attributes and β ⊆α, then α →β holds.<br>• Augmentation rule. If α →β holds and γ is a set of attributes, then γα →γβ<br>holds.<br>• Transitivity rule. If α →β holds and β →γ holds, then α →γ holds.<br>Armstrong’s axioms are sound, because they do not generate any incorrect func-<br>tional dependencies. They are complete, because, for a given set F of functional de-<br>pendencies, they allow us to generate all F +. The bibliographical notes provide ref-<br>erences for proofs of soundness and completeness.<br>Although Armstrong’s axioms are complete, it is tiresome to use them directly for<br>the computation of F +. To simplify matters further, we list additional rules. It is pos-<br>sible to use Armstrong’s axioms to prove that these rules are correct (see Exercises 7.8,<br>7.9, and 7.10).<br>• Union rule. If α →β holds and α →γ holds, then α →βγ holds.<br>• Decomposition rule. If α →βγ holds, then α →β holds and α →γ holds.<br>• Pseudotransitivity rule. If α →β holds and γβ →δ holds, then αγ →δ holds.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>II. Relational Databases<br>7. Relational−Database <br>Design<br>269<br>© The McGraw−Hill <br>Companies, 2001<br>266<br>Chapter 7<br>Relational-Database Design<br>Let us apply our rules to the example of schema R = (A, B, C, G, H, I) and the<br>set F of functional dependencies {A →B, A →C, CG →H, CG →I, B →H}. We<br>list several members of F + here:<br>• A →H. Since A →B and B →H hold, we apply the transitivity rule. Observe<br>that it was much easier to use Armstrong’s axioms to show that A →H holds<br>than it was to argue directly from the deﬁnitions, as we did earlier in this<br>section.<br>• CG →HI . Since CG →H and CG →I , the union rule implies that CG →HI .<br>• AG →I. Since A →C and CG →I, the pseudotransitivity rule implies that<br>AG →I holds.<br>Another way of ﬁnding that AG →I holds is as follows. We use the aug-<br>mentation rule on A →C to infer AG →CG. Applying the transitivity rule to<br>this dependency and CG →I, we infer AG →I.<br>Figure 7.6 shows a procedure that demonstrates formally how to use Armstrong’s<br>axioms to compute F +. In this procedure, when a functional dependency is added to<br>F +, it may be already present, and in that case there is no change to F +. We will also<br>see an alternative way of computing F + in Section 7.3.3.<br>The left-hand and right-hand sides of a functional dependency are both subsets<br>of R. Since a set of size n has 2n subsets, there are a total of 2 × 2n = 2n+1 possible<br>functional dependencies, where n is the number of attributes in R. Each iteration of<br>the repeat loop of the procedure, except the last iteration, adds at least one functional<br>dependency to F +. Thus, the procedure is guaranteed to terminate.<br>7.3.3<br>Closure of Attribute Sets<br>To test whether a set α is a superkey, we must devise an algorithm for computing the<br>set of attributes functionally determined by α. One way of doing this is to compute<br>F +, take all functional dependencies with α as the left-hand side, and take the union<br>of the right-hand sides of all such dependencies. However, doing so can be expensive,<br>since F + can be large.<br>F + = F<br>repeat<br>for each functional dependency f in F +<br>apply reﬂexivity and augmentation rules on f<br>add the resulting functional dependencies to F +<br>for each pair of functional dependencies f1 and f2 in F +<br>if f1 and f2 can be combined using transitivity<br>add the resulting functional dependency to F +<br>until F + does not change any further<br>Figure 7.6<br>A procedure to compute F +.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>II. Relational Databases<br>7. Relational−Database <br>Design<br>270<br>© The McGraw−Hill <br>Companies, 2001<br>7.3<br>Functional Dependencies<br>267<br>An efﬁcient algorithm for computing the set of attributes functionally determined<br>by α is useful not only for testing whether α is a superkey, but also for several other<br>tasks, as we will see later in this section.<br>Let α be a set of attributes. We call the set of all attributes functionally determined<br>by α under a set F of functional dependencies the closure of α under F; we denote<br>it by α+. Figure 7.7 shows an algorithm, written in pseudocode, to compute α+. The<br>input is a set F of functional dependencies and the set α of attributes. The output is<br>stored in the variable result.<br>To illustrate how the algorithm works, we shall use it to compute (AG)+ with the<br>functional dependencies deﬁned in Section 7.3.2. We start with result = AG. The ﬁrst<br>time that we execute the while loop to test each functional dependency, we ﬁnd that<br>• A →B causes us to include B in result. To see this fact, we observe that A →B<br>is in F, A ⊆result (which is AG), so result := result ∪B.<br>• A →C causes result to become ABCG.<br>• CG →H causes result to become ABCGH.<br>• CG →I causes result to become ABCGHI.<br>The second time that we execute the while loop, no new attributes are added to result,<br>and the algorithm terminates.<br>Let us see why the algorithm of Figure 7.7 is correct. The ﬁrst step is correct, since<br>α →α always holds (by the reﬂexivity rule). We claim that, for any subset β of result,<br>α →β. Since we start the while loop with α →result being true, we can add γ to result<br>only if β ⊆result and β →γ. But then result →β by the reﬂexivity rule, so α →β by<br>transitivity. Another application of transitivity shows that α →γ (using α →β and<br>β →γ). The union rule implies that α →result ∪γ, so α functionally determines any<br>new result generated in the while loop. Thus, any attribute returned by the algorithm<br>is in α+.<br>It is easy to see that the algorithm ﬁnds all α+. If there is an attribute in α+ that<br>is not yet in result, then there must be a functional dependency β →γ for which β ⊆<br>result, and at least one attribute in γ is not in result.<br>It turns out that, in the worst case, this algorithm may take an amount of time<br>quadratic in the size of F. There is a faster (although slightly more complex) algo-<br>rithm that runs in time linear in the size of F; that algorithm is presented as part of<br>Exercise 7.14.<br>result := α;<br>while (changes to result) do<br>for each functional dependency β →γ in F do<br>begin<br>if β ⊆result then result := result ∪γ;<br>end<br>Figure 7.7<br>An algorithm to compute α+, the closure of α under F.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>II. Relational Databases<br>7. Relational−Database <br>Design<br>271<br>© The McGraw−Hill <br>Companies, 2001<br>268<br>Chapter 7<br>Relational-Database Design<br>There are several uses of the attribute closure algorithm:<br>• To test if α is a superkey, we compute α+, and check if α+ contains all at-<br>tributes of R.<br>• We can check if a functional dependency α →β holds (or, in other words,<br>is in F +), by checking if β ⊆α+. That is, we compute α+ by using attribute<br>closure, and then check if it contains β. This test is particularly useful, as we<br>will see later in this chapter.<br>• It gives us an alternative way to compute F +: For each γ ⊆R, we ﬁnd the<br>closure γ+, and for each S ⊆γ+, we output a functional dependency γ →S.<br>7.3.4<br>Canonical Cover<br>Suppose that we have a set of functional dependencies F on a relation schema. When-<br>ever a user performs an update on the relation, the database system must ensure that<br>the update does not violate any functional dependencies, that is, all the functional<br>dependencies in F are satisﬁed in the new database state.<br>The system must roll back the update if it violates any functional dependencies in<br>the set F.<br>We can reduce the effort spent in checking for violations by testing a simpliﬁed set<br>of functional dependencies that has the same closure as the given set. Any database<br>that satisﬁes the simpliﬁed set of functional dependencies will also satisfy the origi-<br>nal set, and vice versa, since the two sets have the same closure. However, the sim-<br>pliﬁed set is easier to test. We shall see how the simpliﬁed set can be constructed in a<br>moment. First, we need some deﬁnitions.<br>An attribute of a functional dependency is said to be extraneous if we can remove<br>it without changing the closure of the set of functional dependencies. The formal<br>deﬁnition of extraneous attributes is as follows. Consider a set F of functional de-<br>pendencies and the functional dependency α →β in F.<br>• Attribute A is extraneous in α if A ∈α, and F logically implies (F −{α →<br>β}) ∪{(α −A) →β}.<br>• Attribute A is extraneous in β if A ∈β, and the set of functional dependencies<br>(F −{α →β}) ∪{α →(β −A)} logically implies F.<br>For example, suppose we have the functional dependencies AB →C and A →C<br>in F. Then, B is extraneous in AB →C. As another example, suppose we have the<br>functional dependencies AB →CD and A →C in F. Then C would be extraneous<br>in the right-hand side of AB →CD.<br>Beware of the direction of the implications when using the deﬁnition of extraneous<br>attributes: If you exchange the left-hand side with right-hand side, the implication<br>will always hold. That is, (F −{α →β}) ∪{(α −A) →β} always logically implies<br>F, and also F always logically implies (F −{α →β}) ∪{α →(β −A)}<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>II. Relational Databases<br>7. Relational−Database <br>Design<br>272<br>© The McGraw−Hill <br>Companies, 2001<br>7.3<br>Functional Dependencies<br>269<br>Here is how we can test efﬁciently if an attribute is extraneous. Let R be the rela-<br>tion schema, and let F be the given set of functional dependencies that hold on R.<br>Consider an attribute A in a dependency α →β.<br>• If A ∈β, to check if A is extraneous consider the set<br>F ′ = (F −{α →β}) ∪{α →(β −A)}<br>and check if α →A can be inferred from F ′. To do so, compute α+ (the closure<br>of α) under F ′; if α+ includes A, then A is extraneous in β.<br>• If A ∈α, to check if A is extraneous, let γ = α −{A}, and check if γ →β<br>can be inferred from F. To do so, compute γ+ (the closure of γ) under F; if γ+<br>includes all attributes in β, then A is extraneous in α.<br>For example, suppose F contains AB →CD, A →E, and E →C. To check<br>if C is extraneous in AB →CD, we compute the attribute closure of AB under<br>F ′ = {AB →D, A →E, and E →C}. The closure is ABCDE, which includes CD,<br>so we infer that C is extraneous.<br>A canonical cover Fc for F is a set of dependencies such that F logically implies all<br>dependencies in Fc, and Fc logically implies all dependencies in F. Furthermore, Fc<br>must have the following properties:<br>• No functional dependency in Fc contains an extraneous attribute.<br>• Each left side of a functional dependency in Fc is unique. That is, there are no<br>two dependencies α1 →β1 and α2 →β2 in Fc such that α1 = α2.<br>A canonical cover for a set of functional dependencies F can be computed as de-<br>picted in Figure 7.8. It is important to note that when checking if an attribute is extra-<br>neous, the check uses the dependencies in the current value of Fc, and not the depen-<br>dencies in F. If a functional dependency contains only one attribute in its right-hand<br>side, for example A →C, and that attribute is found to be extraneous, we would get a<br>functional dependency with an empty right-hand side. Such functional dependencies<br>should be deleted.<br>The canonical cover of F, Fc, can be shown to have the same closure as F; hence,<br>testing whether Fc is satisﬁed is equivalent to testing whether F is satisﬁed. However,<br>Fc is minimal in a certain sense—it does not contain extraneous attributes, and it<br>Fc = F<br>repeat<br>Use the union rule to replace any dependencies in Fc of the form<br>α1 →β1 and α1 →β2 with α1 →β1 β2.<br>Find a functional dependency α →β in Fc with an extraneous<br>attribute either in α or in β.<br>/* Note: the test for extraneous attributes is done using Fc, not F */<br>If an extraneous attribute is found, delete it from α →β.<br>until Fc does not change.<br>Figure 7.8<br>Computing canonical cover<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>II. Relational Databases<br>7. Relational−Database <br>Design<br>273<br>© The McGraw−Hill <br>Companies, 2001<br>270<br>Chapter 7<br>Relational-Database Design<br>combines functional dependencies with the same left side. It is cheaper to test Fc<br>than it is to test F itself.<br>Consider the following set F of functional dependencies on schema (A, B, C):<br>A →BC<br>B →C<br>A →B<br>AB →C<br>Let us compute the canonical cover for F.<br>• There are two functional dependencies with the same set of attributes on the<br>left side of the arrow:<br>A →BC<br>A →B<br>We combine these functional dependencies into A →BC.<br>• A is extraneous in AB →C because F logically implies (F −{AB →C}) ∪<br>{B →C}. This assertion is true because B →C is already in our set of func-<br>tional dependencies.<br>• C is extraneous in A →BC, since A →BC is logically implied by A →B and<br>B →C.<br>Thus, our canonical cover is<br>A →B<br>B →C<br>Given a set F of functional dependencies, it may be that an entire functional de-<br>pendency in the set is extraneous, in the sense that dropping it does not change the<br>closure of F. We can show that a canonical cover Fc of F contains no such extraneous<br>functional dependency. Suppose that, to the contrary, there were such an extraneous<br>functional dependency in Fc. The right-side attributes of the dependency would then<br>be extraneous, which is not possible by the deﬁnition of canonical covers.<br>A canonical cover might not be unique. For instance, consider the set of functional<br>dependencies F = {A →BC, B →AC, and C →AB}. If we apply the extraneity<br>test to A →BC, we ﬁnd that both B and C are extraneous under F. However, it is<br>incorrect to delete both! The algorithm for ﬁnding the canonical cover picks one of<br>the two, and deletes it. Then,<br>1. If C is deleted, we get the set F ′ = {A →B, B →AC, and C →AB}. Now,<br>B is not extraneous in the righthand side of A →B under F ′. Continuing the<br>algorithm, we ﬁnd A and B are extraneous in the right-hand side of C →AB,<br>leading to two canonical covers<br>Fc = {A →B, B →C, and C →A}, and<br>Fc = {A →B, B →AC, and C →B}.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>II. Relational Databases<br>7. Relational−Database <br>Design<br>274<br>© The McGraw−Hill <br>Companies, 2001<br>7.4<br>Decomposition<br>271<br>2. If B is deleted, we get the set {A →C, B →AC, and C →AB}. This case is<br>symmetrical to the previous case, leading to the canonical covers<br>Fc = {A →C, C →B, and B →A}, and<br>Fc = {A →C, B →C, and C →AB}.<br>As an exercise, can you ﬁnd one more canonical cover for F?<br>7.4<br>Decomposition<br>The bad design of Section 7.2 suggests that we should decompose a relation schema<br>that has many attributes into several schemas with fewer attributes. Careless decom-<br>position, however, may lead to another form of bad design.<br>Consider an alternative design in which we decompose Lending-schema into the<br>following two schemas:<br>Branch-customer-schema = (branch-name, branch-city, assets, customer-name)<br>Customer-loan-schema = (customer-name, loan-number, amount)<br>Using the lending relation of Figure 7.1, we construct our new relations branch-customer<br>(Branch-customer) and customer-loan (Customer-loan-schema):<br>branch-customer = Πbranch-name, branch-city, assets, customer-name (lending)<br>customer-loan = Πcustomer-name, loan-number, amount (lending)<br>Figures 7.9 and 7.10, respectively, show the resulting branch-customer and customer-<br>name relations.<br>Of course, there are cases in which we need to reconstruct the loan relation. For<br>example, suppose that we wish to ﬁnd all branches that have loans with amounts<br>less than $1000. No relation in our alternative database contains these data. We need<br>to reconstruct the lending relation. It appears that we can do so by writing<br>branch-customer<br> customer-loan<br>branch-name<br>branch-city<br>assets<br>customer-name<br>Downtown<br>Brooklyn<br>9000000<br>Jones<br>Redwood<br>Palo Alto<br>2100000<br>Smith<br>Perryridge<br>Horseneck<br>1700000<br>Hayes<br>Downtown<br>Brooklyn<br>9000000<br>Jackson<br>Mianus<br>Horseneck<br>400000<br>Jones<br>Round Hill<br>Horseneck<br>8000000<br>Turner<br>Pownal<br>Bennington<br>300000<br>Williams<br>North Town<br>Rye<br>3700000<br>Hayes<br>Downtown<br>Brooklyn<br>9000000<br>Johnson<br>Perryridge<br>Horseneck<br>1700000<br>Glenn<br>Brighton<br>Brooklyn<br>7100000<br>Brooks<br>Figure 7.9<br>The relation branch-customer.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>II. Relational Databases<br>7. Relational−Database <br>Design<br>275<br>© The McGraw−Hill <br>Companies, 2001<br>272<br>Chapter 7<br>Relational-Database Design<br>customer-name<br>loan-number<br>amount<br>Jones<br>L-17<br>1000<br>Smith<br>L-23<br>2000<br>Hayes<br>L-15<br>1500<br>Jackson<br>L-14<br>1500<br>Jones<br>L-93<br>500<br>Turner<br>L-11<br>900<br>Williams<br>L-29<br>1200<br>Hayes<br>L-16<br>1300<br>Johnson<br>L-18<br>2000<br>Glenn<br>L-25<br>2500<br>Brooks<br>L-10<br>2200<br>Figure 7.10<br>The relation customer-loan.<br>Figure 7.11 shows the result of computing branch-customer<br> customer-loan. When<br>we compare this relation and the lending relation with which we started (Figure 7.1),<br>we notice a difference: Although every tuple that appears in the lending relation ap-<br>pears in branch-customer<br> customer-loan, there are tuples in branch-customer<br><br>customer-loan that are not in lending. In our example, branch-customer<br> customer-loan<br>has the following additional tuples:<br>(Downtown, Brooklyn, 9000000, Jones, L-93, 500)<br>(Perryridge, Horseneck, 1700000, Hayes, L-16, 1300)<br>(Mianus, Horseneck, 400000, Jones, L-17, 1000)<br>(North Town, Rye, 3700000, Hayes, L-15, 1500)<br>Consider the query, “Find all bank branches that have made a loan in an amount less<br>than $1000.” If we look back at Figure 7.1, we see that the only branches with loan<br>amounts less than $1000 are Mianus and Round Hill. However, when we apply the<br>expression<br>Πbranch-name (σamount &lt; 1000 (branch-customer<br> customer-loan))<br>we obtain three branch names: Mianus, Round Hill, and Downtown.<br>A closer examination of this example shows why. If a customer happens to have<br>several loans from different branches, we cannot tell which loan belongs to which<br>branch. Thus, when we join branch-customer and customer-loan, we obtain not only<br>the tuples we had originally in lending, but also several additional tuples. Although<br>we have more tuples in branch-customer<br> customer-loan, we actually have less in-<br>formation. We are no longer able, in general, to represent in the database information<br>about which customers are borrowers from which branch. Because of this loss of in-<br>formation, we call the decomposition of Lending-schema into Branch-customer-schema<br>and customer-loan-schema a lossy decomposition, or a lossy-join decomposition. A<br>decomposition that is not a lossy-join decomposition is a lossless-join decomposi-<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>II. Relational Databases<br>7. Relational−Database <br>Design<br>276<br>© The McGraw−Hill <br>Companies, 2001<br>7.4<br>Decomposition<br>273<br>customer-<br>loan-</span><br><br><span style="background-color: #FFD6A5;" title="Chunk 33 | Start: 660066 | End: 680066 | Tokens: 3261"><br>branch-name<br>branch-city<br>assets<br>name<br>number<br>amount<br>Downtown<br>Brooklyn<br>9000000<br>Jones<br>L-17<br>1000<br>Downtown<br>Brooklyn<br>9000000<br>Jones<br>L-93<br>500<br>Redwood<br>Palo Alto<br>2100000<br>Smith<br>L-23<br>2000<br>Perryridge<br>Horseneck<br>1700000<br>Hayes<br>L-15<br>1500<br>Perryridge<br>Horseneck<br>1700000<br>Hayes<br>L-16<br>1300<br>Downtown<br>Brooklyn<br>9000000<br>Jackson<br>L-14<br>1500<br>Mianus<br>Horseneck<br>400000<br>Jones<br>L-17<br>1000<br>Mianus<br>Horseneck<br>400000<br>Jones<br>L-93<br>500<br>Round Hill<br>Horseneck<br>8000000<br>Turner<br>L-11<br>900<br>Pownal<br>Bennington<br>300000<br>Williams<br>L-29<br>1200<br>North Town<br>Rye<br>3700000<br>Hayes<br>L-15<br>1500<br>North Town<br>Rye<br>3700000<br>Hayes<br>L-16<br>1300<br>Downtown<br>Brooklyn<br>9000000<br>Johnson<br>L-18<br>2000<br>Perryridge<br>Horseneck<br>1700000<br>Glenn<br>L-25<br>2500<br>Brighton<br>Brooklyn<br>7100000<br>Brooks<br>L-10<br>2200<br>Figure 7.11<br>The relation branch-customer<br> customer-loan.<br>tion. It should be clear from our example that a lossy-join decomposition is, in gen-<br>eral, a bad database design.<br>Why is the decomposition lossy? There is one attribute in common between Branch-<br>customer-schema and Customer-loan-schema:<br>Branch-customer-schema ∩Customer-loan-schema = {customer-name}<br>The only way that we can represent a relationship between, for example, loan-number<br>and branch-name is through customer-name. This representation is not adequate be-<br>cause a customer may have several loans, yet these loans are not necessarily obtained<br>from the same branch.<br>Let us consider another alternative design, in which we decompose Lending-schema<br>into the following two schemas:<br>Branch-schema = (branch-name, branch-city, assets)<br>Loan-info-schema = (branch-name, customer-name, loan-number, amount)<br>There is one attribute in common between these two schemas:<br>Branch-loan-schema ∩Customer-loan-schema = {branch-name}<br>Thus, the only way that we can represent a relationship between, for example,<br>customer-name and assets is through branch-name. The difference between this exam-<br>ple and the preceding one is that the assets of a branch are the same, regardless<br>of the customer to which we are referring, whereas the lending branch associated<br>with a certain loan amount does depend on the customer to which we are referring.<br>For a given branch-name, there is exactly one assets value and exactly one branch-city;<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>II. Relational Databases<br>7. Relational−Database <br>Design<br>277<br>© The McGraw−Hill <br>Companies, 2001<br>274<br>Chapter 7<br>Relational-Database Design<br>whereas a similar statement cannot be made for customer-name. That is, the functional<br>dependency<br>branch-name →assets branch-city<br>holds, but customer-name does not functionally determine loan-number.<br>The notion of lossless joins is central to much of relational-database design. There-<br>fore, we restate the preceding examples more concisely and more formally. Let R be<br>a relation schema. A set of relation schemas {R1, R2, . . . , Rn} is a decomposition of<br>R if<br>R = R1 ∪R2 ∪· · · ∪Rn<br>That is, {R1, R2, . . . , Rn} is a decomposition of R if, for i = 1, 2, . . . , n, each Ri is a<br>subset of R, and every attribute in R appears in at least one Ri.<br>Let r be a relation on schema R, and let ri = ΠRi(r) for i = 1, 2, . . . , n. That is,<br>{r1, r2, . . . , rn} is the database that results from decomposing R into {R1, R2, . . . , Rn}.<br>It is always the case that<br>r ⊆r1<br> r2<br> · · ·<br> rn<br>To see that this assertion is true, consider a tuple t in relation r. When we compute the<br>relations r1, r2, . . . , rn, the tuple t gives rise to one tuple ti in each ri, i = 1, 2, . . . , n.<br>These n tuples combine to regenerate t when we compute r1<br> r2<br> · · ·<br> rn. The<br>details are left for you to complete as an exercise. Therefore, every tuple in r appears<br>in r1<br> r2<br> · · ·<br> rn.<br>In general, r ̸= r1<br> r2<br> · · ·<br> rn. As an illustration, consider our earlier<br>example, in which<br>• n = 2.<br>• R = Lending-schema.<br>• R1 = Branch-customer-schema.<br>• R2 = Customer-loan-schema.<br>• r = the relation shown in Figure 7.1.<br>• r1 = the relation shown in Figure 7.9.<br>• r2 = the relation shown in Figure 7.10.<br>• r1<br> r2 = the relation shown in Figure 7.11.<br>Note that the relations in Figures 7.1 and 7.11 are not the same.<br>To have a lossless-join decomposition, we need to impose constraints on the set of<br>possible relations. We found that the decomposition of Lending-schema into Branch-<br>schema and Loan-info-schema is lossless because the functional dependency<br>branch-name →branch-city assets<br>holds on Branch-schema.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>II. Relational Databases<br>7. Relational−Database <br>Design<br>278<br>© The McGraw−Hill <br>Companies, 2001<br>7.5<br>Desirable Properties of Decomposition<br>275<br>Later in this chapter, we shall introduce constraints other than functional depen-<br>dencies. We say that a relation is legal if it satisﬁes all rules, or constraints, that we<br>impose on our database.<br>Let C represent a set of constraints on the database, and let R be a relation schema.<br>A decomposition {R1, R2, . . . , Rn} of R is a lossless-join decomposition if, for all<br>relations r on schema R that are legal under C,<br>r = ΠR1 (r)<br> ΠR2 (r)<br> · · ·<br> ΠRn (r)<br>We shall show how to test whether a decomposition is a lossless-join decomposi-<br>tion in the next few sections. A major part of this chapter deals with the questions of<br>how to specify constraints on the database, and how to obtain lossless-join decom-<br>positions that avoid the pitfalls represented by the examples of bad database designs<br>that we have seen in this section.<br>7.5<br>Desirable Properties of Decomposition<br>We can use a given set of functional dependencies in designing a relational database<br>in which most of the undesirable properties discussed in Section 7.2 do not occur.<br>When we design such systems, it may become necessary to decompose a relation<br>into several smaller relations.<br>In this section, we outline the desirable properties of a decomposition of a rela-<br>tional schema. In later sections, we outline speciﬁc ways of decomposing a relational<br>schema to get the properties we desire. We illustrate our concepts with the Lending-<br>schema schema of Section 7.2:<br>Lending-schema = (branch-name, branch-city, assets, customer-name,<br>loan-number, amount)<br>The set F of functional dependencies that we require to hold on Lending-schema are<br>branch-name →branch-city assets<br>loan-number →amount branch-name<br>As we discussed in Section 7.2, Lending-schema is an example of a bad database<br>design. Assume that we decompose it to the following three relations:<br>Branch-schema = (branch-name, branch-city, assets)<br>Loan-schema = (loan-number, branch-name, amount)<br>Borrower-schema = (customer-name, loan-number)<br>We claim that this decomposition has several desirable properties, which we discuss<br>next. Note that these three relation schemas are precisely the ones that we used pre-<br>viously, in Chapters 3 through 5.<br>7.5.1<br>Lossless-Join Decomposition<br>In Section 7.2, we argued that, when we decompose a relation into a number of<br>smaller relations, it is crucial that the decomposition be lossless. We claim that the<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>II. Relational Databases<br>7. Relational−Database <br>Design<br>279<br>© The McGraw−Hill <br>Companies, 2001<br>276<br>Chapter 7<br>Relational-Database Design<br>decomposition in Section 7.5 is indeed lossless. To demonstrate our claim, we must<br>ﬁrst present a criterion for determining whether a decomposition is lossy.<br>Let R be a relation schema, and let F be a set of functional dependencies on R. Let<br>R1 and R2 form a decomposition of R. This decomposition is a lossless-join decom-<br>position of R if at least one of the following functional dependencies is in F +:<br>• R1 ∩R2 →R1<br>• R1 ∩R2 →R2<br>In other words, if R1 ∩R2 forms a superkey of either R1 or R2, the decomposition of<br>R is a lossless-join decomposition. We can use attribute closure to efﬁciently test for<br>superkeys, as we have seen earlier.<br>We now demonstrate that our decomposition of Lending-schema is a lossless-join<br>decomposition by showing a sequence of steps that generate the decomposition. We<br>begin by decomposing Lending-schema into two schemas:<br>Branch-schema = (branch-name, branch-city, assets)<br>Loan-info-schema = (branch-name, customer-name, loan-number, amount)<br>Since branch-name →branch-city assets, the augmentation rule for functional depen-<br>dencies (Section 7.3.2) implies that<br>branch-name →branch-name branch-city assets<br>Since Branch-schema ∩Loan-info-schema = {branch-name}, it follows that our initial<br>decomposition is a lossless-join decomposition.<br>Next, we decompose Loan-info-schema into<br>Loan-schema = (loan-number, branch-name, amount)<br>Borrower-schema = (customer-name, loan-number)<br>This step results in a lossless-join decomposition, since loan-number is a common at-<br>tribute and loan-number →amount branch-name.<br>For the general case of decomposition of a relation into multiple parts at once, the<br>test for lossless join decomposition is more complicated. See the bibliographical notes<br>for references on the topic.<br>While the test for binary decomposition is clearly a sufﬁcient condition for lossless<br>join, it is a necessary condition only if all constraints are functional dependencies.<br>We shall see other types of constraints later (in particular, a type of constraint called<br>multivalued dependencies), that can ensure that a decomposition is lossless join even<br>if no functional dependencies are present.<br>7.5.2<br>Dependency Preservation<br>There is another goal in relational-database design: dependency preservation. When an<br>update is made to the database, the system should be able to check that the update<br>will not create an illegal relation—that is, one that does not satisfy all the given<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>II. Relational Databases<br>7. Relational−Database <br>Design<br>280<br>© The McGraw−Hill <br>Companies, 2001<br>7.5<br>Desirable Properties of Decomposition<br>277<br>functional dependencies. If we are to check updates efﬁciently, we should design<br>relational-database schemas that allow update validation without the computation<br>of joins.<br>To decide whether joins must be computed to check an update, we need to deter-<br>mine what functional dependencies can be tested by checking each relation individ-<br>ually. Let F be a set of functional dependencies on a schema R, and let R1, R2, . . . , Rn<br>be a decomposition of R. The restriction of F to Ri is the set Fi of all functional depen-<br>dencies in F + that include only attributes of Ri. Since all functional dependencies in<br>a restriction involve attributes of only one relation schema, it is possible to test such<br>a dependency for satisfaction by checking only one relation.<br>Note that the deﬁnition of restriction uses all dependencies in F +, not just those in<br>F. For instance, suppose F = {A →B, B →C}, and we have a decomposition into<br>AC and AB. The restriction of F to AC is then A →C, since A →C is in F +, even<br>though it is not in F.<br>The set of restrictions F1, F2, . . . , Fn is the set of dependencies that can be checked<br>efﬁciently. We now must ask whether testing only the restrictions is sufﬁcient. Let<br>F ′ = F1 ∪F2 ∪· · · ∪Fn. F ′ is a set of functional dependencies on schema R, but,<br>in general, F ′ ̸= F. However, even if F ′ ̸= F, it may be that F ′+ = F +. If the latter is<br>true, then every dependency in F is logically implied by F ′, and, if we verify that F ′<br>is satisﬁed, we have veriﬁed that F is satisﬁed. We say that a decomposition having<br>the property F ′+ = F + is a dependency-preserving decomposition.<br>Figure 7.12 shows an algorithm for testing dependency preservation. The input<br>is a set D = {R1, R2, . . . , Rn} of decomposed relation schemas, and a set F of func-<br>tional dependencies. This algorithm is expensive since it requires computation of F +;<br>we will describe another algorithm that is more efﬁcient after giving an example of<br>testing for dependency preservation.<br>We can now show that our decomposition of Lending-schema is dependency pre-<br>serving. Instead of applying the algorithm of Figure 7.12, we consider an easier al-<br>ternative: We consider each member of the set F of functional dependencies that we<br>compute F +;<br>for each schema Ri in D do<br>begin<br>Fi : = the restriction of F + to Ri;<br>end<br>F ′ := ∅<br>for each restriction Fi do<br>begin<br>F ′ = F ′ ∪Fi<br>end<br>compute F ′+;<br>if (F ′+ = F +) then return (true)<br>else return (false);<br>Figure 7.12<br>Testing for dependency preservation.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>II. Relational Databases<br>7. Relational−Database <br>Design<br>281<br>© The McGraw−Hill <br>Companies, 2001<br>278<br>Chapter 7<br>Relational-Database Design<br>require to hold on Lending-schema, and show that each one can be tested in at least<br>one relation in the decomposition.<br>• We can test the functional dependency: branch-name →branch-city assets using<br>Branch-schema = (branch-name, branch-city, assets).<br>• We can test the functional dependency: loan-number →amount branch-name<br>using Loan-schema = (branch-name, loan-number, amount).<br>If each member of F can be tested on one of the relations of the decomposition, then<br>the decomposition is dependency preserving. However, there are cases where, even<br>though the decomposition is dependency preserving, there is a dependency in F that<br>cannot be tested in any one relation in the decomposition. The alternative test can<br>therefore be used as a sufﬁcient condition that is easy to check; if it fails we cannot<br>conclude that the decomposition is not dependency preserving, instead we will have<br>to apply the general test.<br>We now give a more efﬁcient test for dependency preservation, which avoids com-<br>puting F +. The idea is to test each functional dependency α →β in F by using a<br>modiﬁed form of attribute closure to see if it is preserved by the decomposition. We<br>apply the following procedure to each α →β in F.<br>result = α<br>while (changes to result) do<br>for each Ri in the decomposition<br>t = (result ∩Ri)+ ∩Ri<br>result = result ∪t<br>The attribute closure is with respect to the functional dependencies in F. If result<br>contains all attributes in β, then the functional dependency α →β is preserved. The<br>decomposition is dependency preserving if and only if all the dependencies in F are<br>preserved.<br>Note that instead of precomputing the restriction of F on Ri and using it for com-<br>puting the attribute closure of result, we use attribute closure on (result ∩Ri) with<br>respect to F, and then intersect it with Ri, to get an equivalent result. This procedure<br>takes polynomial time, instead of the exponential time required to compute F +.<br>7.5.3<br>Repetition of Information<br>The decomposition of Lending-schema does not suffer from the problem of repetition<br>of information that we discussed in Section 7.2. In Lending-schema, it was necessary<br>to repeat the city and assets of a branch for each loan. The decomposition separates<br>branch and loan data into distinct relations, thereby eliminating this redundancy.<br>Similarly, observe that, if a single loan is made to several customers, we must repeat<br>the amount of the loan once for each customer (as well as the city and assets of the<br>branch) in lending-schema. In the decomposition, the relation on schema Borrower-<br>schema contains the loan-number, customer-name relationship, and no other schema<br>does. Therefore, we have one tuple for each customer for a loan in only the relation<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>II. Relational Databases<br>7. Relational−Database <br>Design<br>282<br>© The McGraw−Hill <br>Companies, 2001<br>7.6<br>Boyce–Codd Normal Form<br>279<br>on Borrower-schema. In the other relations involving loan-number (those on schemas<br>Loan-schema and Borrower-schema), only one tuple per loan needs to appear.<br>Clearly, the lack of redundancy in our decomposition is desirable. The degree to<br>which we can achieve this lack of redundancy is represented by several normal forms,<br>which we shall discuss in the remainder of this chapter.<br>7.6<br>Boyce–Codd Normal Form<br>Using functional dependencies, we can deﬁne several normal forms that represent<br>“good” database designs. In this section we cover BCNF (deﬁned below), and later, in<br>Section 7.7, we cover 3NF.<br>7.6.1<br>Deﬁnition<br>One of the more desirable normal forms that we can obtain is Boyce–Codd normal<br>form (BCNF). A relation schema R is in BCNF with respect to a set F of functional<br>dependencies if, for all functional dependencies in F + of the form α →β, where α ⊆<br>R and β ⊆R, at least one of the following holds:<br>• α →β is a trivial functional dependency (that is, β ⊆α).<br>• α is a superkey for schema R.<br>A database design is in BCNF if each member of the set of relation schemas that con-<br>stitutes the design is in BCNF.<br>As an illustration, consider the following relation schemas and their respective<br>functional dependencies:<br>• Customer-schema = (customer-name, customer-street, customer-city)<br>customer-name →customer-street customer-city<br>• Branch-schema = (branch-name, assets, branch-city)<br>branch-name →assets branch-city<br>• Loan-info-schema = (branch-name, customer-name, loan-number, amount)<br>loan-number →amount branch-name<br>We claim that Customer-schema is in BCNF. We note that a candidate key for the<br>schema is customer-name. The only nontrivial functional dependencies that hold on<br>Customer-schema have customer-name on the left side of the arrow. Since customer-name<br>is a candidate key, functional dependencies with customer-name on the left side do<br>not violate the deﬁnition of BCNF. Similarly, it can be shown easily that the relation<br>schema Branch-schema is in BCNF.<br>The schema Loan-info-schema, however, is not in BCNF. First, note that loan-number<br>is not a superkey for Loan-info-schema, since we could have a pair of tuples represent-<br>ing a single loan made to two people—for example,<br>(Downtown, John Bell, L-44, 1000)<br>(Downtown, Jane Bell, L-44, 1000)<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>II. Relational Databases<br>7. Relational−Database <br>Design<br>283<br>© The McGraw−Hill <br>Companies, 2001<br>280<br>Chapter 7<br>Relational-Database Design<br>Because we did not list functional dependencies that rule out the preceding case, loan-<br>number is not a candidate key. However, the functional dependency loan-number →<br>amount is nontrivial. Therefore, Loan-info-schema does not satisfy the deﬁnition of<br>BCNF.<br>We claim that Loan-info-schema is not in a desirable form, since it suffers from the<br>problem of repetition of information that we described in Section 7.2. We observe that,<br>if there are several customer names associated with a loan, in a relation on Loan-info-<br>schema, then we are forced to repeat the branch name and the amount once for each<br>customer. We can eliminate this redundancy by redesigning our database such that<br>all schemas are in BCNF. One approach to this problem is to take the existing non-<br>BCNF design as a starting point, and to decompose those schemas that are not in<br>BCNF. Consider the decomposition of Loan-info-schema into two schemas:<br>Loan-schema = (loan-number, branch-name, amount)<br>Borrower-schema = (customer-name, loan-number)<br>This decomposition is a lossless-join decomposition.<br>To determine whether these schemas are in BCNF, we need to determine what<br>functional dependencies apply to them. In this example, it is easy to see that<br>loan-number →amount branch-name<br>applies to the Loan-schema, and that only trivial functional dependencies apply to<br>Borrower-schema. Although loan-number is not a superkey for Loan-info-schema, it is a<br>candidate key for Loan-schema. Thus, both schemas of our decomposition are in BCNF.<br>It is now possible to avoid redundancy in the case where there are several cus-<br>tomers associated with a loan. There is exactly one tuple for each loan in the rela-<br>tion on Loan-schema, and one tuple for each customer of each loan in the relation on<br>Borrower-schema. Thus, we do not have to repeat the branch name and the amount<br>once for each customer associated with a loan.<br>Often testing of a relation to see if it satisﬁes BCNF can be simpliﬁed:<br>• To check if a nontrivial dependency α →β causes a violation of BCNF, com-<br>pute α+ (the attribute closure of α), and verify that it includes all attributes of<br>R; that is, it is a superk</span><br><br><span style="background-color: #FDFFB6;" title="Chunk 34 | Start: 680068 | End: 700068 | Tokens: 3276">ey of R.<br>• To check if a relation schema R is in BCNF, it sufﬁces to check only the depen-<br>dencies in the given set F for violation of BCNF, rather than check all depen-<br>dencies in F +.<br>We can show that if none of the dependencies in F causes a violation of<br>BCNF, then none of the dependencies in F + will cause a violation of BCNF<br>either.<br>Unfortunately, the latter procedure does not work when a relation is decomposed.<br>That is, it does not sufﬁce to use F when we test a relation Ri, in a decomposition<br>of R, for violation of BCNF. For example, consider relation schema R (A, B, C, D, E),<br>with functional dependencies F containing A →B and BC →D. Suppose this were<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>II. Relational Databases<br>7. Relational−Database <br>Design<br>284<br>© The McGraw−Hill <br>Companies, 2001<br>7.6<br>Boyce–Codd Normal Form<br>281<br>decomposed into R1(A, B) and R2(A, C, D, E). Now, neither of the dependencies in<br>F contains only attributes from (A, C, D, E) so we might be misled into thinking R2<br>satisﬁes BCNF. In fact, there is a dependency AC →D in F + (which can be inferred<br>using the pseudotransitivity rule from the two dependencies in F), which shows that<br>R2 is not in BCNF. Thus, we may need a dependency that is in F +, but is not in F, to<br>show that a decomposed relation is not in BCNF.<br>An alternative BCNF test is sometimes easier than computing every dependency<br>in F +. To check if a relation Ri in a decomposition of R is in BCNF, we apply this test:<br>• For every subset α of attributes in Ri, check that α+ (the attribute closure of α<br>under F) either includes no attribute of Ri −α, or includes all attributes of Ri.<br>If the condition is violated by some set of attributes α in Ri, consider the following<br>functional dependency, which can be shown to be present in F +:<br>α →(α+ −α) ∩Ri.<br>The above dependency shows that Ri violates BCNF, and is a “witness” for the viola-<br>tion. The BCNF decomposition algorithm, which we shall see in Section 7.6.2, makes<br>use of the witness.<br>7.6.2<br>Decomposition Algorithm<br>We are now able to state a general method to decompose a relation schema so as to<br>satisfy BCNF. Figure 7.13 shows an algorithm for this task. If R is not in BCNF, we<br>can decompose R into a collection of BCNF schemas R1, R2, . . . , Rn by the algorithm.<br>The algorithm uses dependencies (“witnesses”) that demonstrate violation of BCNF<br>to perform the decomposition.<br>The decomposition that the algorithm generates is not only in BCNF, but is also<br>a lossless-join decomposition. To see why our algorithm generates only lossless-join<br>decompositions, we note that, when we replace a schema Ri with (Ri −β) and (α, β),<br>the dependency α →β holds, and (Ri −β) ∩(α, β) = α.<br>result := {R};<br>done := false;<br>compute F +;<br>while (not done) do<br>if (there is a schema Ri in result that is not in BCNF)<br>then begin<br>let α →β be a nontrivial functional dependency that holds<br>on Ri such that α →Ri is not in F +, and α ∩β = ∅;<br>result := (result −Ri) ∪(Ri −β) ∪( α, β);<br>end<br>else done := true;<br>Figure 7.13<br>BCNF decomposition algorithm.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>II. Relational Databases<br>7. Relational−Database <br>Design<br>285<br>© The McGraw−Hill <br>Companies, 2001<br>282<br>Chapter 7<br>Relational-Database Design<br>We apply the BCNF decomposition algorithm to the Lending-schema schema that<br>we used in Section 7.2 as an example of a poor database design:<br>Lending-schema = (branch-name, branch-city, assets, customer-name,<br>loan-number, amount)<br>The set of functional dependencies that we require to hold on Lending-schema are<br>branch-name →assets branch-city<br>loan-number →amount branch-name<br>A candidate key for this schema is {loan-number, customer-name}.<br>We can apply the algorithm of Figure 7.13 to the Lending-schema example as fol-<br>lows:<br>• The functional dependency<br>branch-name →assets branch-city<br>holds on Lending-schema, but branch-name is not a superkey. Thus, Lending-<br>schema is not in BCNF. We replace Lending-schema by<br>Branch-schema = (branch-name, branch-city, assets)<br>Loan-info-schema = (branch-name, customer-name, loan-number, amount)<br>• The only nontrivial functional dependencies that hold on Branch-schema in-<br>clude branch-name on the left side of the arrow. Since branch-name is a key for<br>Branch-schema, the relation Branch-schema is in BCNF.<br>• The functional dependency<br>loan-number →amount branch-name<br>holds on Loan-info-schema, but loan-number is not a key for Loan-info-schema.<br>We replace Loan-info-schema by<br>Loan-schema = (loan-number, branch-name, amount)<br>Borrower-schema = (customer-name, loan-number)<br>• Loan-schema and Borrower-schema are in BCNF.<br>Thus, the decomposition of Lending-schema results in the three relation schemas Branch-<br>schema, Loan-schema, and Borrower-schema, each of which is in BCNF. These relation<br>schemas are the same as those in Section 7.5, where we demonstrated that the result-<br>ing decomposition is both a lossless-join decomposition and a dependency-preserving<br>decomposition.<br>The BCNF decomposition algorithm takes time exponential in the size of the initial<br>schema, since the algorithm for checking if a relation in the decomposition satisﬁes<br>BCNF can take exponential time. The bibliographical notes provide references to an<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>II. Relational Databases<br>7. Relational−Database <br>Design<br>286<br>© The McGraw−Hill <br>Companies, 2001<br>7.6<br>Boyce–Codd Normal Form<br>283<br>algorithm that can compute a BCNF decomposition in polynomial time. However, the<br>algorithm may “overnormalize,” that is, decompose a relation unnecessarily.<br>7.6.3<br>Dependency Preservation<br>Not every BCNF decomposition is dependency preserving. As an illustration, con-<br>sider the relation schema<br>Banker-schema = (branch-name, customer-name, banker-name)<br>which indicates that a customer has a “personal banker” in a particular branch. The<br>set F of functional dependencies that we require to hold on the Banker-schema is<br>banker-name →branch-name<br>branch-name customer-name →banker-name<br>Clearly, Banker-schema is not in BCNF since banker-name is not a superkey.<br>If we apply the algorithm of Figure 7.13, we obtain the following BCNF decompo-<br>sition:<br>Banker-branch-schema = (banker-name, branch-name)<br>Customer-banker-schema = (customer-name, banker-name)<br>The decomposed schemas preserve only banker-name →<br>branch-name (and trivial<br>dependencies), but the closure of {banker-name →branch-name} does not include<br>customer-name branch-name →banker-name. The violation of this dependency cannot<br>be detected unless a join is computed.<br>To see why the decomposition of Banker-schema into the schemas Banker-branch-<br>schema and Customer-banker-schema is not dependency preserving, we apply the al-<br>gorithm of Figure 7.12. We ﬁnd that the restrictions F1 and F2 of F to each schema<br>are:<br>F1 = {banker-name →branch-name}<br>F2 = ∅(only trivial dependencies hold on Customer-banker-schema)<br>(For brevity, we do not show trivial functional dependencies.) It is easy to see that<br>the dependency customer-name branch-name →banker-name is not in (F1 ∪F2)+ even<br>though it is in F +. Therefore, (F1 ∪F2)+ ̸= F +, and the decomposition is not depen-<br>dency preserving.<br>This example demonstrates that not every BCNF decomposition is dependency<br>preserving. Moreover, it is easy to see that any BCNF decomposition of Banker-schema<br>must fail to preserve customer-name branch-name →banker-name. Thus, the example<br>shows that we cannot always satisfy all three design goals:<br>1. Lossless join<br>2. BCNF<br>3. Dependency preservation<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>II. Relational Databases<br>7. Relational−Database <br>Design<br>287<br>© The McGraw−Hill <br>Companies, 2001<br>284<br>Chapter 7<br>Relational-Database Design<br>Recall that lossless join is an essential condition for a decomposition, to avoid loss<br>of information. We are therefore forced to give up either BCNF or dependency preser-<br>vation. In Section 7.7 we present an alternative normal form, called third normal<br>form, which is a small relaxation of BCNF; the motivation for using third normal<br>form is that there is always a dependency preserving decomposition into third nor-<br>mal form.<br>There are situations where there is more than one way to decompose a schema<br>into BCNF. Some of these decompositions may be dependency preserving, while oth-<br>ers may not. For instance, suppose we have a relation schema R(A, B, C) with the<br>functional dependencies A →B and B →C. From this set we can derive the further<br>dependency A →C. If we used the dependency A →B (or equivalently, A →C)<br>to decompose R, we would end up with two relations R1(A, B) and R2(A, C); the<br>dependency B →C would not be preserved.<br>If instead we used the dependency B →C to decompose R, we would end up with<br>two relations R1(A, B) and R2(B, C), which are in BCNF, and the decomposition is<br>also dependency preserving. Clearly the decomposition into R1(A, B) and R2(B, C)<br>is preferable. In general, the database designer should therefore look at alternative<br>decompositions, and pick a dependency preserving decomposition where possible.<br>7.7<br>Third Normal Form<br>As we saw earlier, there are relational schemas where a BCNF decomposition cannot<br>be dependency preserving. For such schemas, we have two alternatives if we wish to<br>check if an update violates any functional dependencies:<br>• Pay the extra cost of computing joins to test for violations.<br>• Use an alternative decomposition, third normal form (3NF), which we present<br>below, which makes testing of updates cheaper. Unlike BCNF, 3NF decompo-<br>sitions may contain some redundancy in the decomposed schema.<br>We shall see that it is always possible to ﬁnd a lossless-join, dependency-preserving<br>decomposition that is in 3NF. Which of the two alternatives to choose is a design<br>decision to be made by the database designer on the basis of the application require-<br>ments.<br>7.7.1<br>Deﬁnition<br>BCNF requires that all nontrivial dependencies be of the form α →β, where α is a<br>superkey. 3NF relaxes this constraint slightly by allowing nontrivial functional de-<br>pendencies whose left side is not a superkey.<br>A relation schema R is in third normal form (3NF) with respect to a set F of func-<br>tional dependencies if, for all functional dependencies in F + of the form α →β,<br>where α ⊆R and β ⊆R, at least one of the following holds:<br>• α →β is a trivial functional dependency.<br>• α is a superkey for R.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>II. Relational Databases<br>7. Relational−Database <br>Design<br>288<br>© The McGraw−Hill <br>Companies, 2001<br>7.7<br>Third Normal Form<br>285<br>• Each attribute A in β −α is contained in a candidate key for R.<br>Note that the third condition above does not say that a single candidate key should<br>contain all the attributes in β −α; each attribute A in β −α may be contained in a<br>different candidate key.<br>The ﬁrst two alternatives are the same as the two alternatives in the deﬁnition of<br>BCNF. The third alternative of the 3NF deﬁnition seems rather unintuitive, and it is<br>not obvious why it is useful. It represents, in some sense, a minimal relaxation of the<br>BCNF conditions that helps ensure that every schema has a dependency-preserving<br>decomposition into 3NF. Its purpose will become more clear later, when we study<br>decomposition into 3NF.<br>Observe that any schema that satisﬁes BCNF also satisﬁes 3NF, since each of its<br>functional dependencies would satisfy one of the ﬁrst two alternatives. BCNF is there-<br>fore a more restrictive constraint than is 3NF.<br>The deﬁnition of 3NF allows certain functional dependencies that are not allowed<br>in BCNF. A dependency α →β that satisﬁes only the third alternative of the 3NF<br>deﬁnition is not allowed in BCNF, but is allowed in 3NF.1<br>Let us return to our Banker-schema example (Section 7.6). We have shown that this<br>relation schema does not have a dependency-preserving, lossless-join decomposition<br>into BCNF. This schema, however, turns out to be in 3NF. To see that it is, we note<br>that {customer-name, branch-name} is a candidate key for Banker-schema, so the only<br>attribute not contained in a candidate key for Banker-schema is banker-name. The only<br>nontrivial functional dependencies of the form<br>α →banker-name<br>include {customer-name, branch-name} as part of α. Since {customer-name, branch-name}<br>is a candidate key, these dependencies do not violate the deﬁnition of 3NF.<br>As an optimization when testing for 3NF, we can consider only functional depen-<br>dencies in the given set F, rather than in F +. Also, we can decompose the dependen-<br>cies in F so that their right-hand side consists of only single attributes, and use the<br>resultant set in place of F.<br>Given a dependency α →β, we can use the same attribute-closure–based tech-<br>nique that we used for BCNF to check if α is a superkey. If α is not a superkey, we<br>have to verify whether each attribute in β is contained in a candidate key of R; this<br>test is rather more expensive, since it involves ﬁnding candidate keys. In fact, test-<br>ing for 3NF has been shown to be NP-hard; thus, it is very unlikely that there is a<br>polynomial time complexity algorithm for the task.<br>7.7.2<br>Decomposition Algorithm<br>Figure 7.14 shows an algorithm for ﬁnding a dependency-preserving, lossless-join<br>decomposition into 3NF. The set of dependencies Fc used in the algorithm is a canoni-<br>1.<br>These dependencies are examples of transitive dependencies (see Exercise 7.25). The original deﬁ-<br>nition of 3NF was in terms of transitive dependencies. The deﬁnition we use is equivalent but easier to<br>understand.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>II. Relational Databases<br>7. Relational−Database <br>Design<br>289<br>© The McGraw−Hill <br>Companies, 2001<br>286<br>Chapter 7<br>Relational-Database Design<br>let Fc be a canonical cover for F;<br>i := 0;<br>for each functional dependency α →β in Fc do<br>if none of the schemas Rj, j = 1, 2, . . . , i contains α β<br>then begin<br>i := i + 1;<br>Ri := α β;<br>end<br>if none of the schemas Rj, j = 1, 2, . . . , i contains a candidate key for R<br>then begin<br>i := i + 1;<br>Ri := any candidate key for R;<br>end<br>return (R1, R2, . . . , Ri)<br>Figure 7.14<br>Dependency-preserving, lossless-join decomposition into 3NF.<br>cal cover for F. Note that the algorithm considers the set of schemas Rj, j = 1, 2, . . . , i;<br>initially i = 0, and in this case the set is empty.<br>To illustrate the algorithm of Figure 7.14, consider the following extension to the<br>Banker-schema in Section 7.6:<br>Banker-info-schema = (branch-name, customer-name, banker-name,<br>ofﬁce-number)<br>The main difference here is that we include the banker’s ofﬁce number as part of the<br>information. The functional dependencies for this relation schema are<br>banker-name →branch-name ofﬁce-number<br>customer-name branch-name →banker-name<br>The for loop in the algorithm causes us to include the following schemas in our<br>decomposition:<br>Banker-ofﬁce-schema = (banker-name, branch-name, ofﬁce-number)<br>Banker-schema = (customer-name, branch-name, banker-name)<br>Since Banker-schema contains a candidate key for Banker-info-schema, we are ﬁnished<br>with the decomposition process.<br>The algorithm ensures the preservation of dependencies by explicitly building a<br>schema for each dependency in a canonical cover. It ensures that the decomposition<br>is a lossless-join decomposition by guaranteeing that at least one schema contains a<br>candidate key for the schema being decomposed. Exercise 7.19 provides some insight<br>into the proof that this sufﬁces to guarantee a lossless join.<br>This algorithm is also called the 3NF synthesis algorithm, since it takes a set of de-<br>pendencies and adds one schema at a time, instead of decomposing the initial schema<br>repeatedly. The result is not uniquely deﬁned, since a set of functional dependencies<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>II. Relational Databases<br>7. Relational−Database <br>Design<br>290<br>© The McGraw−Hill <br>Companies, 2001<br>7.7<br>Third Normal Form<br>287<br>can have more than one canonical cover, and, further, in some cases the result of the<br>algorithm depends on the order in which it considers the dependencies in Fc.<br>If a relation Ri is in the decomposition generated by the synthesis algorithm, then<br>Ri is in 3NF. Recall that when we test for 3NF, it sufﬁces to consider functional de-<br>pendencies whose right-hand side is a single attribute. Therefore, to see that Ri is in<br>3NF, you must convince yourself that any functional dependency γ →B that holds<br>on Ri satisﬁes the deﬁnition of 3NF. Assume that the dependency that generated Ri<br>in the synthesis algorithm is α →β. Now, B must be in α or β, since B is in Ri and<br>α →β generated Ri. Let us consider the three possible cases:<br>• B is in both α and β. In this case, the dependency α →β would not have been<br>in Fc since B would be extraneous in β. Thus, this case cannot hold.<br>• B is in β but not α. Consider two cases:<br>  γ is a superkey. The second condition of 3NF is satisﬁed.<br>  γ is not a superkey. Then α must contain some attribute not in γ. Now,<br>since γ →B is in F +, it must be derivable from Fc by using the attribute<br>closure algorithm on γ. The derivation could not have used α →β —<br>if it had been used, α must be contained in the attribute closure of γ,<br>which is not possible, since we assumed γ is not a superkey. Now, us-<br>ing α →(β −{B}) and γ →B, we can derive α →B (since γ ⊆αβ, and γ<br>cannot contain B because γ →B is nontrivial). This would imply that B<br>is extraneous in the right-hand side of α →β, which is not possible since<br>α →β is in the canonical cover Fc. Thus, if B is in β, then γ must be a<br>superkey, and the second condition of 3NF must be satisﬁed.<br>• B is in α but not β.<br>Since α is a candidate key, the third alternative in the deﬁnition of 3NF is<br>satisﬁed.<br>Interestingly, the algorithm we described for decomposition into 3NF can be imple-<br>mented in polynomial time, even though testing a given relation to see if it satisﬁes<br>3NF is NP-hard.<br>7.7.3<br>Comparison of BCNF and 3NF<br>Of the two normal forms for relational-database schemas, 3NF and BCNF, there are<br>advantages to 3NF in that we know that it is always possible to obtain a 3NF design<br>without sacriﬁcing a lossless join or dependency preservation. Nevertheless, there are<br>disadvantages to 3NF: If we do not eliminate all transitive relations schema depen-<br>dencies, we may have to use null values to represent some of the possible meaningful<br>relationships among data items, and there is the problem of repetition of information.<br>As an illustration of the null value problem, consider again the Banker-schema and<br>its associated functional dependencies. Since banker-name →branch-name, we may<br>want to represent relationships between values for banker-name and values for branch-<br>name in our database. If we are to do so, however, either there must be a correspond-<br>ing value for customer-name, or we must use a null value for the attribute customer-<br>name.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>II. Relational Databases<br>7. Relational−Database <br>Design<br>291<br>© The McGraw−Hill <br>Companies, 2001<br>288<br>Chapter 7<br>Relational-Database Design<br>customer-name<br>banker-name<br>branch-name<br>Jones<br>Johnson<br>Perryridge<br>Smith<br>Johnson<br>Perryridge<br>Hayes<br>Johnson<br>Perryridge<br>Jackson<br>Johnson<br>Perryridge<br>Curry<br>Johnson<br>Perryridge<br>Turner<br>Johnson<br>Perryridge<br>Figure 7.15<br>An instance of Banker-schema.<br>As an illustration of the repetition of information problem, consider the instance<br>of Banker-schema in Figure 7.15. Notice that the information indicating that Johnson<br>is working at the Perryridge branch is repeated.<br>Recall that our goals of database design with functional dependencies are:<br>1. BCNF<br>2. Lossless join<br>3. Dependency preservation<br>Since it is not always possible to satisfy all three, we may be forced to choose between<br>BCNF and dependency preservation with 3NF.<br>It is worth noting that SQL does not provide a way of specifying functional depen-<br>dencies, except for the special case of declaring superkeys by using the primary key<br>or unique constraints. It is possible, although a little complicated, to write assertions<br>that enforce a functiona</span><br><br><span style="background-color: #CAFFBF;" title="Chunk 35 | Start: 700070 | End: 720070 | Tokens: 3300">l dependency (see Exercise 7.15); unfortunately, testing the<br>assertions would be very expensive in most database systems. Thus even if we had<br>a dependency-preserving decomposition, if we use standard SQL we would not be<br>able to efﬁciently test a functional dependency whose left-hand side is not a key.<br>Although testing functional dependencies may involve a join if the decomposition<br>is not dependency preserving, we can reduce the cost by using materialized views,<br>which many database systems support. Given a BCNF decomposition that is not de-<br>pendency preserving, we consider each dependency in a minimum cover Fc that is<br>not preserved in the decomposition. For each such dependency α →β, we deﬁne<br>a materialized view that computes a join of all relations in the decomposition, and<br>projects the result on αβ. The functional dependency can be easily tested on the ma-<br>terialized view, by means of a constraint unique (α). On the negative side, there is a<br>space and time overhead due to the materialized view, but on the positive side, the<br>application programmer need not worry about writing code to keep redundant data<br>consistent on updates; it is the job of the database system to maintain the material-<br>ized view, that is, keep up up to date when the database is updated. (Later in the<br>book, in Section 14.5, we outline how a database system can perform materialized<br>view maintenance efﬁciently.)<br>Thus, in case we are not able to get a dependency-preserving BCNF decomposition,<br>it is generally preferable to opt for BCNF, and use techniques such as materialized<br>views to reduce the cost of checking functional dependencies.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>II. Relational Databases<br>7. Relational−Database <br>Design<br>292<br>© The McGraw−Hill <br>Companies, 2001<br>7.8<br>Fourth Normal Form<br>289<br>7.8<br>Fourth Normal Form<br>Some relation schemas, even though they are in BCNF, do not seem to be sufﬁciently<br>normalized, in the sense that they still suffer from the problem of repetition of infor-<br>mation. Consider again our banking example. Assume that, in an alternative design<br>for the bank database schema, we have the schema<br>BC-schema = (loan-number, customer-name, customer-street, customer-city)<br>The astute reader will recognize this schema as a non-BCNF schema because of the<br>functional dependency<br>customer-name →customer-street customer-city<br>that we asserted earlier, and because customer-name is not a key for BC-schema. How-<br>ever, assume that our bank is attracting wealthy customers who have several ad-<br>dresses (say, a winter home and a summer home). Then, we no longer wish to en-<br>force the functional dependency customer-name →customer-street customer-city. If we<br>remove this functional dependency, we ﬁnd BC-schema to be in BCNF with respect to<br>our modiﬁed set of functional dependencies. Yet, even though BC-schema is now in<br>BCNF, we still have the problem of repetition of information that we had earlier.<br>To deal with this problem, we must deﬁne a new form of constraint, called a mul-<br>tivalued dependency. As we did for functional dependencies, we shall use multivalued<br>dependencies to deﬁne a normal form for relation schemas. This normal form, called<br>fourth normal form (4NF), is more restrictive than BCNF. We shall see that every 4NF<br>schema is also in BCNF, but there are BCNF schemas that are not in 4NF.<br>7.8.1<br>Multivalued Dependencies<br>Functional dependencies rule out certain tuples from being in a relation. If A →B,<br>then we cannot have two tuples with the same A value but different B values. Mul-<br>tivalued dependencies, on the other hand, do not rule out the existence of certain<br>tuples. Instead, they require that other tuples of a certain form be present in the rela-<br>tion. For this reason, functional dependencies sometimes are referred to as equality-<br>generating dependencies, and multivalued dependencies are referred to as tuple-<br>generating dependencies.<br>Let R be a relation schema and let α ⊆R and β ⊆R. The multivalued dependency<br>α →→β<br>holds on R if, in any legal relation r(R), for all pairs of tuples t1 and t2 in r such that<br>t1[α] = t2[α], there exist tuples t3 and t4 in r such that<br>t1[α] = t2[α] = t3[α] = t4[α]<br>t3[β] = t1[β]<br>t3[R −β] = t2[R −β]<br>t4[β] = t2[β]<br>t4[R −β] = t1[R −β]<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>II. Relational Databases<br>7. Relational−Database <br>Design<br>293<br>© The McGraw−Hill <br>Companies, 2001<br>290<br>Chapter 7<br>Relational-Database Design<br>α<br>α<br>β<br>β<br>–<br>R –<br>t1<br>t2<br>t3<br>t4<br>a1 ... ai<br>a1 ... ai<br>a1 ... ai<br>a1 ... ai<br>ai + 1 ... aj<br>bi + 1 ... bj<br>ai + 1 ... aj<br>bi + 1 ... bj<br>aj + 1 ... an<br>bj + 1 ... bn<br>bj + 1 ... bn<br>aj + 1 ... an<br>Figure 7.16<br>Tabular representation of α →→β.<br>This deﬁnition is less complicated than it appears to be. Figure 7.16 gives a tabular<br>picture of t1, t2, t3, and t4. Intuitively, the multivalued dependency α →→β says that<br>the relationship between α and β is independent of the relationship between α and<br>R −β. If the multivalued dependency α →→β is satisﬁed by all relations on schema<br>R, then α →→β is a trivial multivalued dependency on schema R. Thus, α →→β is<br>trivial if β ⊆α or β ∪α = R.<br>To illustrate the difference between functional and multivalued dependencies, we<br>consider the BC-schema again, and the relation bc (BC-schema) of Figure 7.17. We must<br>repeat the loan number once for each address a customer has, and we must repeat<br>the address for each loan a customer has. This repetition is unnecessary, since the<br>relationship between a customer and his address is independent of the relationship<br>between that customer and a loan. If a customer (say, Smith) has a loan (say, loan<br>number L-23), we want that loan to be associated with all Smith’s addresses. Thus,<br>the relation of Figure 7.18 is illegal. To make this relation legal, we need to add the<br>tuples (L-23, Smith, Main, Manchester) and (L-27, Smith, North, Rye) to the bc relation<br>of Figure 7.18.<br>Comparing the preceding example with our deﬁnition of multivalued dependency,<br>we see that we want the multivalued dependency<br>customer-name →→customer-street customer-city<br>to hold. (The multivalued dependency customer-name →→loan-number will do as well.<br>We shall soon see that they are equivalent.)<br>As with functional dependencies, we shall use multivalued dependencies in two<br>ways:<br>1. To test relations to determine whether they are legal under a given set of func-<br>tional and multivalued dependencies<br>2. To specify constraints on the set of legal relations; we shall thus concern our-<br>selves with only those relations that satisfy a given set of functional and mul-<br>tivalued dependencies<br>loan-number<br>customer-name<br>customer-street<br>customer-city<br>L-23<br>Smith<br>North<br>Rye<br>L-23<br>Smith<br>Main<br>Manchester<br>L-93<br>Curry<br>Lake<br>Horseneck<br>Figure 7.17<br>Relation bc: An example of redundancy in a BCNF relation.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>II. Relational Databases<br>7. Relational−Database <br>Design<br>294<br>© The McGraw−Hill <br>Companies, 2001<br>7.8<br>Fourth Normal Form<br>291<br>loan-number<br>customer-name<br>customer-street<br>customer-city<br>L-23<br>Smith<br>North<br>Rye<br>L-27<br>Smith<br>Main<br>Manchester<br>Figure 7.18<br>An illegal bc relation.<br>Note that, if a relation r fails to satisfy a given multivalued dependency, we can con-<br>struct a relation r′ that does satisfy the multivalued dependency by adding tuples<br>to r.<br>Let D denote a set of functional and multivalued dependencies. The closure D+<br>of D is the set of all functional and multivalued dependencies logically implied by D.<br>As we did for functional dependencies, we can compute D+ from D, using the formal<br>deﬁnitions of functional dependencies and multivalued dependencies. We can man-<br>age with such reasoning for very simple multivalued dependencies. Luckily, multi-<br>valued dependencies that occur in practice appear to be quite simple. For complex<br>dependencies, it is better to reason about sets of dependencies by using a system of<br>inference rules. (Section C.1.1 of the appendix outlines a system of inference rules for<br>multivalued dependencies.)<br>From the deﬁnition of multivalued dependency, we can derive the following rule:<br>• If α →β, then α →→β.<br>In other words, every functional dependency is also a multivalued dependency.<br>7.8.2<br>Deﬁnition of Fourth Normal Form<br>Consider again our BC-schema example in which the multivalued dependency<br>customer-name →→customer-street customer-city holds, but no nontrivial functional de-<br>pendencies hold. We saw in the opening paragraphs of Section 7.8 that, although BC-<br>schema is in BCNF, the design is not ideal, since we must repeat a customer’s address<br>information for each loan. We shall see that we can use the given multivalued de-<br>pendency to improve the database design, by decomposing BC-schema into a fourth<br>normal form decomposition.<br>A relation schema R is in fourth normal form (4NF) with respect to a set D of<br>functional and multivalued dependencies if, for all multivalued dependencies in D+<br>of the form α →→β, where α ⊆R and β ⊆R, at least one of the following holds<br>• α →→β is a trivial multivalued dependency.<br>• α is a superkey for schema R.<br>A database design is in 4NF if each member of the set of relation schemas that consti-<br>tutes the design is in 4NF.<br>Note that the deﬁnition of 4NF differs from the deﬁnition of BCNF in only the use<br>of multivalued dependencies instead of functional dependencies. Every 4NF schema<br>is in BCNF. To see this fact, we note that, if a schema R is not in BCNF, then there is<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>II. Relational Databases<br>7. Relational−Database <br>Design<br>295<br>© The McGraw−Hill <br>Companies, 2001<br>292<br>Chapter 7<br>Relational-Database Design<br>result := {R};<br>done := false;<br>compute D+; Given schema Ri, let Di denote the restriction of D+ to Ri<br>while (not done) do<br>if (there is a schema Ri in result that is not in 4NF w.r.t. Di)<br>then begin<br>let α →→β be a nontrivial multivalued dependency that holds<br>on Ri such that α →Ri is not in Di, and α ∩β = ∅;<br>result := (result −Ri) ∪(Ri −β) ∪(α, β);<br>end<br>else done := true;<br>Figure 7.19<br>4NF decomposition algorithm.<br>a nontrivial functional dependency α →β holding on R, where α is not a superkey.<br>Since α →β implies α →→β, R cannot be in 4NF.<br>Let R be a relation schema, and let R1, R2, . . . , Rn be a decomposition of R. To<br>check if each relation schema Ri in the decomposition is in 4NF, we need to ﬁnd<br>what multivalued dependencies hold on each Ri. Recall that, for a set F of functional<br>dependencies, the restriction Fi of F to Ri is all functional dependencies in F + that<br>include only attributes of Ri. Now consider a set D of both functional and multivalued<br>dependencies. The restriction of D to Ri is the set Di consisting of<br>1. All functional dependencies in D+ that include only attributes of Ri<br>2. All multivalued dependencies of the form<br>α →→β ∩Ri<br>where α ⊆Ri and α →→β is in D+.<br>7.8.3<br>Decomposition Algorithm<br>The analogy between 4NF and BCNF applies to the algorithm for decomposing a<br>schema into 4NF. Figure 7.19 shows the 4NF decomposition algorithm. It is identical<br>to the BCNF decomposition algorithm of Figure 7.13, except that it uses multivalued,<br>instead of functional, dependencies and uses the restriction of D+ to Ri.<br>If we apply the algorithm of Figure 7.19 to BC-schema, we ﬁnd that customer-name<br>→→loan-number is a nontrivial multivalued dependency, and customer-name is not<br>a superkey for BC-schema. Following the algorithm, we replace BC-schema by two<br>schemas:<br>Borrower-schema = (customer-name, loan-number)<br>Customer-schema = (customer-name, customer-street, customer-city).<br>This pair of schemas, which is in 4NF, eliminates the problem we encountered earlier<br>with the redundancy of BC-schema.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>II. Relational Databases<br>7. Relational−Database <br>Design<br>296<br>© The McGraw−Hill <br>Companies, 2001<br>7.10<br>Overall Database Design Process<br>293<br>As was the case when we were dealing solely with functional dependencies, we<br>are interested in decompositions that are lossless-join decompositions and that pre-<br>serve dependencies. The following fact about multivalued dependencies and lossless<br>joins shows that the algorithm of Figure 7.19 generates only lossless-join decomposi-<br>tions:<br>• Let R be a relation schema, and let D be a set of functional and multivalued<br>dependencies on R. Let R1 and R2 form a decomposition of R. This decom-<br>position is a lossless-join decomposition of R if and only if at least one of the<br>following multivalued dependencies is in D+:<br>R1 ∩R2 →→R1<br>R1 ∩R2 →→R2<br>Recall that we stated in Section 7.5.1 that, if R1 ∩R2 →R1 or R1 ∩R2 →R2, then<br>R1 and R2 are a lossless-join decomposition of R. The preceding fact about multival-<br>ued dependencies is a more general statement about lossless joins. It says that, for<br>every lossless-join decomposition of R into two schemas R1 and R2, one of the two<br>dependencies R1 ∩R2 →→R1 or R1 ∩R2 →→R2 must hold.<br>The issue of dependency preservation when we decompose a relation becomes<br>more complicated in the presence of multivalued dependencies. Section C.1.2 of the<br>appendix pursues this topic.<br>7.9<br>More Normal Forms<br>The fourth normal form is by no means the “ultimate” normal form. As we saw ear-<br>lier, multivalued dependencies help us understand and tackle some forms of rep-<br>etition of information that cannot be understood in terms of functional dependen-<br>cies. There are types of constraints called join dependencies that generalize multi-<br>valued dependencies, and lead to another normal form called project-join normal<br>form (PJNF) (PJNF is called ﬁfth normal form in some books). There is a class of even<br>more general constraints, which leads to a normal form called domain-key normal<br>form.<br>A practical problem with the use of these generalized constraints is that they are<br>not only hard to reason with, but there is also no set of sound and complete inference<br>rules for reasoning about the constraints. Hence PJNF and domain-key normal form<br>are used quite rarely. Appendix C provides more details about these normal forms.<br>Conspicuous by its absence from our discussion of normal forms is second nor-<br>mal form (2NF). We have not discussed it, because it is of historical interest only. We<br>simply deﬁne it, and let you experiment with it in Exercise 7.26.<br>7.10<br>Overall Database Design Process<br>So far we have looked at detailed issues about normal forms and normalization. In<br>this section we study how normalization ﬁts into the overall database design process.<br>Earlier in the chapter, starting in Section 7.4, we assumed that a relation schema<br>R is given, and proceeded to normalize it. There are several ways in which we could<br>have come up with the schema R:<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>II. Relational Databases<br>7. Relational−Database <br>Design<br>297<br>© The McGraw−Hill <br>Companies, 2001<br>294<br>Chapter 7<br>Relational-Database Design<br>1. R could have been generated when converting a E-R diagram to a set of tables.<br>2. R could have been a single relation containing all attributes that are of interest.<br>The normalization process then breaks up R into smaller relations.<br>3. R could have been the result of some ad hoc design of relations, which we<br>then test to verify that it satisﬁes a desired normal form.<br>In the rest of this section we examine the implications of these approaches. We also<br>examine some practical issues in database design, including denormalization for per-<br>formance and examples of bad design that are not detected by normalization.<br>7.10.1<br>E-R Model and Normalization<br>When we carefully deﬁne an E-R diagram, identifying all entities correctly, the tables<br>generated from the E-R diagram should not need further normalization. However,<br>there can be functional dependencies between attributes of an entity. For instance,<br>suppose an employee entity had attributes department-number and department-address,<br>and there is a functional dependency department-number →department-address. We<br>would then need to normalize the relation generated from employee.<br>Most examples of such dependencies arise out of poor E-R diagram design. In the<br>above example, if we did the E-R diagram correctly, we would have created a depart-<br>ment entity with attribute department-address and a relationship between employee and<br>department. Similarly, a relationship involving more than two entities may not be in a<br>desirable normal form. Since most relationships are binary, such cases are relatively<br>rare. (In fact, some E-R diagram variants actually make it difﬁcult or impossible to<br>specify nonbinary relations.)<br>Functional dependencies can help us detect poor E-R design. If the generated re-<br>lations are not in desired normal form, the problem can be ﬁxed in the E-R diagram.<br>That is, normalization can be done formally as part of data modeling. Alternatively,<br>normalization can be left to the designer’s intuition during E-R modeling, and can be<br>done formally on the relations generated from the E-R model.<br>7.10.2<br>The Universal Relation Approach<br>The second approach to database design is to start with a single relation schema<br>containing all attributes of interest, and decompose it. One of our goals in choosing a<br>decomposition was that it be a lossless-join decomposition. To consider losslessness,<br>we assumed that it is valid to talk about the join of all the relations of the decomposed<br>database.<br>Consider the database of Figure 7.20, showing a decomposition of the loan-info re-<br>lation. The ﬁgure depicts a situation in which we have not yet determined the amount<br>of loan L-58, but wish to record the remainder of the data on the loan. If we compute<br>the natural join of these relations, we discover that all tuples referring to loan L-58<br>disappear. In other words, there is no loan-info relation corresponding to the relations<br>of Figure 7.20. Tuples that disappear when we compute the join are dangling tuples<br>(see Section 6.2.1). Formally, let r1(R1), r2(R2), . . . , rn(Rn) be a set of relations. A<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>II. Relational Databases<br>7. Relational−Database <br>Design<br>298<br>© The McGraw−Hill <br>Companies, 2001<br>7.10<br>Overall Database Design Process<br>295<br>branch-name<br>loan-number<br>Round Hill<br>L-58<br>loan-number<br>amount<br>loan-number<br>customer-name<br>L-58<br>Johnson<br>Figure 7.20<br>Decomposition of loan-info.<br>tuple t of relation ri is a dangling tuple if t is not in the relation<br>ΠRi (r1<br> r2<br> · · ·<br> rn)<br>Dangling tuples may occur in practical database applications. They represent in-<br>complete information, as they do in our example, where we wish to store data about a<br>loan that is still in the process of being negotiated. The relation r1<br> r2<br> · · ·<br> rn is<br>called a universal relation, since it involves all the attributes in the universe deﬁned<br>by R1 ∪R2 ∪· · · ∪Rn.<br>The only way that we can write a universal relation for the example of Figure 7.20<br>is to include null values in the universal relation. We saw in Chapter 3 that null values<br>present several difﬁculties. Because of them, it may be better to view the relations<br>of the decomposed design as representing the database, rather than as the univer-<br>sal relation whose schema we decomposed during the normalization process. (The<br>bibliographical notes discuss research on null values and universal relations.)<br>Note that we cannot enter all incomplete information into the database of Fig-<br>ure 7.20 without resorting to null values. For example, we cannot enter a loan number<br>unless we know at least one of the following:<br>• The customer name<br>• The branch name<br>• The amount of the loan<br>Thus, a particular decomposition deﬁnes a restricted form of incomplete information<br>that is acceptable in our database.<br>The normal forms that we have deﬁned generate good database designs from the<br>point of view of representation of incomplete information. Returning again to the<br>example of Figure 7.20, we would not want to allow storage of the following fact:<br>“There is a loan (whose number is unknown) to Jones in the amount of $100.” This is<br>because<br>loan-number →customer-name amount<br>and therefore the only way that we c</span><br><br><span style="background-color: #9BF6FF;" title="Chunk 36 | Start: 720072 | End: 740072 | Tokens: 3226">an relate customer-name and amount is through<br>loan-number. If we do not know the loan number, we cannot distinguish this loan<br>from other loans with unknown numbers.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>II. Relational Databases<br>7. Relational−Database <br>Design<br>299<br>© The McGraw−Hill <br>Companies, 2001<br>296<br>Chapter 7<br>Relational-Database Design<br>In other words, we do not want to store data for which the key attributes are un-<br>known. Observe that the normal forms that we have deﬁned do not allow us to store<br>that type of information unless we use null values. Thus, our normal forms allow<br>representation of acceptable incomplete information via dangling tuples, while pro-<br>hibiting the storage of undesirable incomplete information.<br>Another consequence of the universal relation approach to database design is that<br>attribute names must be unique in the universal relation. We cannot use name to refer<br>to both customer-name and to branch-name. It is generally preferable to use unique<br>names, as we have done. Nevertheless, if we deﬁned our relation schemas directly,<br>rather than in terms of a universal relation, we could obtain relations on schemas<br>such as the following for our banking example:<br>branch-loan (name, number)<br>loan-customer (number, name)<br>amt (number, amount)<br>Observe that, with the preceding relations, expressions such as branch-loan<br> loan-<br>customer are meaningless. Indeed, the expression branch-loan<br> loan-customer ﬁnds<br>loans made by branches to customers who have the same name as the name of the<br>branch.<br>In a language such as SQL, however, a query involving branch-loan and loan-custom-<br>er must remove ambiguity in references to name by preﬁxing the relation name. In<br>such environments, the multiple roles for name (as branch name and as customer<br>name) are less troublesome and may be simpler to use.<br>We believe that using the unique-role assumption—that each attribute name has<br>a unique meaning in the database—is generally preferable to reusing of the same<br>name in multiple roles. When the unique-role assumption is not made, the database<br>designer must be especially careful when constructing a normalized relational-data-<br>base design.<br>7.10.3<br>Denormalization for Performance<br>Occasionally database designers choose a schema that has redundant information;<br>that is, it is not normalized. They use the redundancy to improve performance for<br>speciﬁc applications. The penalty paid for not using a normalized schema is the extra<br>work (in terms of coding time and execution time) to keep redundant data consistent.<br>For instance, suppose that the name of an account holder has to be displayed along<br>with the account number and balance, every time the account is accessed. In our<br>normalized schema, this requires a join of account with depositor.<br>One alternative to computing the join on the ﬂy is to store a relation containing all<br>the attributes of account and depositor. This makes displaying the account information<br>faster. However, the balance information for an account is repeated for every person<br>who owns the account, and all copies must be updated by the application, when-<br>ever the account balance is updated. The process of taking a normalized schema and<br>making it non-normalized is called denormalization, and designers use it to tune<br>performance of systems to support time-critical operations.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>II. Relational Databases<br>7. Relational−Database <br>Design<br>300<br>© The McGraw−Hill <br>Companies, 2001<br>7.11<br>Summary<br>297<br>A better alternative, supported by many database systems today, is to use the nor-<br>malized schema, and additionally store the join or account and depositor as a materi-<br>alized view. (Recall that a materialized view is a view whose result is stored in the<br>database, and brought up to date when the relations used in the view are updated.)<br>Like denormalization, using materialized view does have space and time overheads;<br>however, it has the advantage that keeping the view up to date is the job of the<br>database system, not the application programmer.<br>7.10.4<br>Other Design Issues<br>There are some aspects of database design that are not addressed by normalization,<br>and can thus lead to bad database design. We give examples here; obviously, such<br>designs should be avoided.<br>Consider a company database, where we want to store earnings of companies in<br>different years. A relation earnings(company-id, year, amount) could be used to store the<br>earnings information. The only functional dependency on this relation is company-id,<br>year →amount, and the relation is in BCNF.<br>An alternative design is to use multiple relations, each storing the earnings for a<br>different year. Let us say the years of interest are 2000, 2001, and 2002; we would then<br>have relations of the form earnings-2000, earnings-2001, earnings-2002, all of which are<br>on the schema (company-id, earnings). The only functional dependency here on each<br>relation would be company-id →earnings, so these relations are also in BCNF.<br>However, this alternative design is clearly a bad idea—we would have to create<br>a new relation every year, and would also have to write new queries every year, to<br>take each new relation into account. Queries would also be more complicated since<br>they may have to refer to many relations.<br>Yet another way of representing the same data is to have a single relation company-<br>year(company-id, earnings-2000, earnings-2001, earnings-2002). Here the only functional<br>dependencies are from company-id to the other attributes, and again the relation is<br>in BCNF. This design is also a bad idea since it has problems similar to the previous<br>design—namely we would have to modify the relation schema and write new queries,<br>every year. Queries would also be more complicated, since they may have to refer to<br>many attributes.<br>Representations such as those in the company-year relation, with one column for<br>each value of an attribute, are called crosstabs; they are widely used in spreadsheets<br>and reports and in data analysis tools. While such representations are useful for dis-<br>play to users, for the reasons just given, they are not desirable in a database design.<br>SQL extensions have been proposed to convert data from a normal relational repre-<br>sentation to a crosstab, for display.<br>7.11<br>Summary<br>• We showed pitfalls in database design, and how to systematically design a<br>database schema that avoids the pitfalls. The pitfalls included repeated infor-<br>mation and inability to represent some information.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>II. Relational Databases<br>7. Relational−Database <br>Design<br>301<br>© The McGraw−Hill <br>Companies, 2001<br>298<br>Chapter 7<br>Relational-Database Design<br>• We introduced the concept of functional dependencies, and showed how to<br>reason with functional dependencies. We laid special emphasis on what de-<br>pendencies are logically implied by a set of dependencies. We also deﬁned the<br>notion of a canonical cover, which is a minimal set of functional dependencies<br>equivalent to a given set of functional dependencies.<br>• We introduced the concept of decomposition, and showed that decomposi-<br>tions must be lossless-join decompositions, and should preferably be depen-<br>dency preserving.<br>• If the decomposition is dependency preserving, given a database update, all<br>functional dependencies can be veriﬁable from individual relations, without<br>computing a join of relations in the decomposition.<br>• We then presented Boyce–Codd Normal Form (BCNF); relations in BCNF are<br>free from the pitfalls outlined earlier. We outlined an algorithm for decompos-<br>ing relations into BCNF. There are relations for which there is no dependency-<br>preserving BCNF decomposition.<br>• We used the canonical covers to decompose a relation into 3NF, which is a<br>small relaxation of the BCNF condition. Relations in 3NF may have some re-<br>dundancy, but there is always a dependency-preserving decomposition into<br>3NF.<br>• We presented the notion of multivalued dependencies, which specify con-<br>straints that cannot be speciﬁed with functional dependencies alone. We de-<br>ﬁned fourth normal form (4NF) with multivalued dependencies. Section C.1.1<br>of the appendix gives details on reasoning about multivalued dependencies.<br>• Other normal forms, such as PJNF and DKNF, eliminate more subtle forms<br>of redundancy. However, these are hard to work with and are rarely used.<br>Appendix C gives details on these normal forms.<br>• In reviewing the issues in this chapter, note that the reason we could deﬁne<br>rigorous approaches to relational-database design is that the relational data<br>model rests on a ﬁrm mathematical foundation. That is one of the primary<br>advantages of the relational model compared with the other data models that<br>we have studied.<br>Review Terms<br>• Atomic domains<br>• First normal form<br>• Pitfalls in relational-database<br>design<br>• Functional dependencies<br>• Superkey<br>• F holds on R<br>• R satisﬁes F<br>• Trivial functional dependencies<br>• Closure of a set of functional<br>dependencies<br>• Armstrong’s axioms<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>II. Relational Databases<br>7. Relational−Database <br>Design<br>302<br>© The McGraw−Hill <br>Companies, 2001<br>Exercises<br>299<br>• Closure of attribute sets<br>• Decomposition<br>• Lossless-join decomposition<br>• Legal relations<br>• Dependency preservation<br>• Restriction of F to Ri<br>• Boyce–Codd normal form<br>(BCNF)<br>• BCNF decomposition algorithm<br>• Canonical cover<br>• Extraneous attributes<br>• Third normal form<br>• 3NF decomposition algorithm<br>• Multivalued dependencies<br>• Fourth normal form<br>• restriction of a multivalued<br>dependency<br>• Project-join normal form (PJNF)<br>• Domain-key normal form<br>• E-R model and normalization<br>• Universal relation<br>• Unique-role assumption<br>• Denormalization<br>Exercises<br>7.1 Explain what is meant by repetition of information and inability to represent in-<br>formation. Explain why each of these properties may indicate a bad relational-<br>database design.<br>7.2 Suppose that we decompose the schema R = (A, B, C, D, E) into<br>(A, B, C)<br>(A, D, E)<br>Show that this decomposition is a lossless-join decomposition if the following<br>set F of functional dependencies holds:<br>A →BC<br>CD →E<br>B →D<br>E →A<br>7.3 Why are certain functional dependencies called trivial functional dependencies?<br>7.4 List all functional dependencies satisﬁed by the relation of Figure 7.21.<br>A<br>B<br>C<br>a1<br>a1<br>a2<br>a2<br>b1<br>b1<br>b1<br>b1<br>c1<br>c2<br>c1<br>c3<br>Figure 7.21<br>Relation of Exercise 7.4.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>II. Relational Databases<br>7. Relational−Database <br>Design<br>303<br>© The McGraw−Hill <br>Companies, 2001<br>300<br>Chapter 7<br>Relational-Database Design<br>7.5 Use the deﬁnition of functional dependency to argue that each of Armstrong’s<br>axioms (reﬂexivity, augmentation, and transitivity) is sound.<br>7.6 Explain how functional dependencies can be used to indicate the following:<br>• A one-to-one relationship set exists between entity sets account and customer.<br>• A many-to-one relationship set exists between entity sets account and cus-<br>tomer.<br>7.7 Consider the following proposed rule for functional dependencies: If α →β and<br>γ →β, then α →γ. Prove that this rule is not sound by showing a relation r that<br>satisﬁes α →β and γ →β, but does not satisfy α →γ.<br>7.8 Use Armstrong’s axioms to prove the soundness of the union rule. (Hint: Use the<br>augmentation rule to show that, if α →β, then α →αβ. Apply the augmentation<br>rule again, using α →γ, and then apply the transitivity rule.)<br>7.9 Use Armstrong’s axioms to prove the soundness of the decomposition rule.<br>7.10 Use Armstrong’s axioms to prove the soundness of the pseudotransitivity rule.<br>7.11 Compute the closure of the following set F of functional dependencies for rela-<br>tion schema R = (A, B, C, D, E).<br>A →BC<br>CD →E<br>B →D<br>E →A<br>List the candidate keys for R.<br>7.12 Using the functional dependencies of Exercise 7.11, compute B+.<br>7.13 Using the functional dependencies of Exercise 7.11, compute the canonical<br>cover Fc.<br>7.14 Consider the algorithm in Figure 7.22 to compute α+. Show that this algorithm<br>is more efﬁcient than the one presented in Figure 7.7 (Section 7.3.3) and that it<br>computes α+ correctly.<br>7.15 Given the database schema R(a, b, c), and a relation r on the schema R, write an<br>SQL query to test whether the functional dependency b →c holds on relation<br>r. Also write an SQL assertion that enforces the functional dependency. Assume<br>that no null values are present.<br>7.16 Show that the following decomposition of the schema R of Exercise 7.2 is not a<br>lossless-join decomposition:<br>(A, B, C)<br>(C, D, E)<br>Hint: Give an example of a relation r on schema R such that<br>ΠA, B, C (r)<br> ΠC, D, E (r) ̸= r<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>II. Relational Databases<br>7. Relational−Database <br>Design<br>304<br>© The McGraw−Hill <br>Companies, 2001<br>Exercises<br>301<br>result := ∅;<br>/* fdcount is an array whose ith element contains the number<br>of attributes on the left side of the ith FD that are<br>not yet known to be in α+ */<br>for i := 1 to |F| do<br>begin<br>let β →γ denote the ith FD;<br>fdcount [i] := |β|;<br>end<br>/* appears is an array with one entry for each attribute. The<br>entry for attribute A is a list of integers. Each integer<br>i on the list indicates that A appears on the left side<br>of the ith FD */<br>for each attribute A do<br>begin<br>appears [A] := NIL;<br>for i := 1 to |F| do<br>begin<br>let β →γ denote the ith FD;<br>if A ∈β then add i to appears [A];<br>end<br>end<br>addin (α);<br>return (result);<br>procedure addin (α);<br>for each attribute A in α do<br>begin<br>if A ̸∈result then<br>begin<br>result := result ∪{A};<br>for each element i of appears[A] do<br>begin<br>fdcount [i] := fdcount [i] −1;<br>if fdcount [i] := 0 then<br>begin<br>let β →γ denote the ith FD;<br>addin (γ);<br>end<br>end<br>end<br>end<br>Figure 7.22<br>An algorithm to compute α+.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>II. Relational Databases<br>7. Relational−Database <br>Design<br>305<br>© The McGraw−Hill <br>Companies, 2001<br>302<br>Chapter 7<br>Relational-Database Design<br>7.17 Let R1, R2, . . . , Rn be a decomposition of schema U. Let u(U) be a relation, and<br>let ri = ΠRI(u). Show that<br>u ⊆r1<br> r2<br> · · ·<br> rn<br>7.18 Show that the decomposition in Exercise 7.2 is not a dependency-preserving<br>decomposition.<br>7.19 Show that it is possible to ensure that a dependency-preserving decomposi-<br>tion into 3NF is a lossless-join decomposition by guaranteeing that at least one<br>schema contains a candidate key for the schema being decomposed. (Hint: Show<br>that the join of all the projections onto the schemas of the decomposition cannot<br>have more tuples than the original relation.)<br>7.20 List the three design goals for relational databases, and explain why each is<br>desirable.<br>7.21 Give a lossless-join decomposition into BCNF of schema R of Exercise 7.2.<br>7.22 Give an example of a relation schema R′ and set F ′ of functional dependencies<br>such that there are at least three distinct lossless-join decompositions of R′ into<br>BCNF.<br>7.23 In designing a relational database, why might we choose a non-BCNF design?<br>7.24 Give a lossless-join, dependency-preserving decomposition into 3NF of schema<br>R of Exercise 7.2.<br>7.25 Let a prime attribute be one that appears in at least one candidate key. Let α and<br>β be sets of attributes such that α →β holds, but β →α does not hold. Let A be<br>an attribute that is not in α, is not in β, and for which β →A holds. We say that<br>A is transitively dependent on α. We can restate our deﬁnition of 3NF as follows:<br>A relation schema R is in 3NF with respect to a set F of functional dependencies<br>if there are no nonprime attributes A in R for which A is transitively dependent<br>on a key for R.<br>Show that this new deﬁnition is equivalent to the original one.<br>7.26 A functional dependency α →β is called a partial dependency if there is a<br>proper subset γ of α such that γ →β. We say that β is partially dependent on α. A<br>relation schema R is in second normal form (2NF) if each attribute A in R meets<br>one of the following criteria:<br>• It appears in a candidate key.<br>• It is not partially dependent on a candidate key.<br>Show that every 3NF schema is in 2NF. (Hint: Show that every partial depen-<br>dency is a transitive dependency.)<br>7.27 Given the three goals of relational-database design, is there any reason to design<br>a database schema that is in 2NF, but is in no higher-order normal form? (See<br>Exercise 7.26 for the deﬁnition of 2NF.)<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>II. Relational Databases<br>7. Relational−Database <br>Design<br>306<br>© The McGraw−Hill <br>Companies, 2001<br>Bibliographical Notes<br>303<br>7.28 Give an example of a relation schema R and a set of dependencies such that R is<br>in BCNF, but is not in 4NF.<br>7.29 Explain why 4NF is a normal form more desirable than BCNF.<br>7.30 Explain how dangling tuples may arise. Explain problems that they may cause.<br>Bibliographical Notes<br>The ﬁrst discussion of relational-database design theory appeared in an early paper<br>by Codd [1970]. In that paper, Codd also introduced functional dependencies, and<br>ﬁrst, second, and third normal forms.<br>Armstrong’s axioms were introduced in Armstrong [1974]. Ullman [1988] is an<br>easily accessible source of proofs of soundness and completeness of Armstrong’s ax-<br>ioms. Ullman [1988] also provides an algorithm for testing for lossless-join decompo-<br>sition for general (nonbinary) decompositions, and many other algorithms, theorems,<br>and proofs concerning dependency theory. Maier [1983] discusses the theory of func-<br>tional dependencies.Graham et al. [1986] discusses formal aspects of the concept of a<br>legal relation.<br>BCNF was introduced in Codd [1972]. The desirability of BCNF is discussed in<br>Bernstein et al. [1980a]. A polynomial-time algorithm for BCNF decomposition ap-<br>pears in Tsou and Fischer [1982], and can also be found in Ullman [1988]. Biskup et al.<br>[1979] gives the algorithm we used to ﬁnd a lossless-join dependency-preserving de-<br>composition into 3NF. Fundamental results on the lossless-join property appear in<br>Aho et al. [1979a].<br>Multivalued dependencies are discussed in Zaniolo [1976]. Beeri et al. [1977] gives<br>a set of axioms for multivalued dependencies, and proves that the authors axioms<br>are sound and complete. Our axiomatization is based on theirs. The notions of 4NF,<br>PJNF, and DKNF are from Fagin [1977], Fagin [1979], and Fagin [1981], respectively.<br>Maier [1983] presents the design theory of relational databases in detail. Ullman<br>[1988] and Abiteboul et al. [1995] present a more theoretic coverage of many of the<br>dependencies and normal forms presented here. See the bibliographical notes of Ap-<br>pendix C for further references to literature on normalization.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>III. Object−Based <br>Databases and XML<br>Introduction<br>307<br>© The McGraw−Hill <br>Companies, 2001<br>P<br>A<br>R<br>T<br>3<br>Object-based Databases<br>and XML<br>Several application areas for database systems are limited by the restrictions of the<br>relational data model. As a result, researchers have developed several data models to<br>deal with these application domains. In this part, we study the object-oriented data<br>model and the object-relational data model. In addition, we study XML, a language<br>that can represent data that is less structured than that of the other data models.<br>The object-oriented data model, described in Chapter 8, is based on the object-<br>oriented-programming language paradigm, which is now in wide use. Inheritance,<br>object-identity, and encapsulation (information hiding), with methods to provide an<br>interface to objects, are among the key concepts of object-oriented programming that<br>have found applications in data modeling. The object-oriented data model also sup-<br>ports a rich type system, including structured and collection types. While inheritance<br>and, to some extent, complex types are also present in the E-R model, encapsulation<br>and object-identity distinguish the object-oriented data model from the E-R model.<br>The object-relational model, described in Chapter 9, combines features of the re-<br>lational and object-oriented models. This model provides the rich </span><br><br><span style="background-color: #A0C4FF;" title="Chunk 37 | Start: 740074 | End: 760074 | Tokens: 3160">type system of<br>object-oriented databases, combined with relations as the basis for storage of data.<br>It applies inheritance to relations, not just to types. The object-relational data model<br>provides a smooth migration path from relational databases, which is attractive to<br>relational database vendors. As a result, the SQL:1999 standard includes a number<br>of object-oriented features in its type system, while continuing to use the relational<br>model as the underlying model.<br>The XML language was initially designed as a way of adding markup informa-<br>tion to text documents, but has become important because of its applications in data<br>exchange. XML provides a way to represent data that have nested structure, and fur-<br>thermore allows a great deal of ﬂexibility in structuring of data, which is important<br>for certain kinds of nontraditional data. Chapter 10 describes the XML language, and<br>then presents different ways of expressing queries on data represented in XML, and<br>transforming XML data from one form to another.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>III. Object−Based <br>Databases and XML<br>8. Object−Oriented <br>Databases<br>308<br>© The McGraw−Hill <br>Companies, 2001<br>P<br>A<br>R<br>T<br>8<br>Case Studies<br>This part describes how different database systems integrate the various concepts<br>described earlier in the book. Speciﬁcally, three widely used database systems—IBM<br>DB2, Oracle, and Microsoft SQL Server—are covered in Chapters 25, 26, and 27. These<br>three represent three of the most widely used database systems.<br>Each of these chapters highlights unique features of each database system: tools,<br>SQL variations and extensions, and system architecture, including storage organiza-<br>tion, query processing, concurrency control and recovery, and replication.<br>The chapters cover only key aspects of the database products they describe, and<br>therefore should not be regarded as a comprehensive coverage of the product. Fur-<br>thermore, since products are enhanced regularly, details of the product may change.<br>When using a particular product version, be sure to consult the user manuals for<br>speciﬁc details.<br>Keep in mind that the chapters in this part use industrial rather than academic<br>terminology. For instance, they use table instead of relation, row instead of tuple,<br>and column instead of attribute.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>III. Object−Based <br>Databases and XML<br>8. Object−Oriented <br>Databases<br>309<br>© The McGraw−Hill <br>Companies, 2001<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>III. Object−Based <br>Databases and XML<br>8. Object−Oriented <br>Databases<br>310<br>© The McGraw−Hill <br>Companies, 2001<br>C<br>H<br>A<br>P<br>T<br>E<br>R<br>2<br>5<br>Oracle<br>Hakan Jakobsson<br>Oracle Corporation<br>When Oracle was founded in 1977 as Software Development Laboratories by Larry<br>Ellison, Bob Miner, and Ed Oates, there were no commercial relational database prod-<br>ucts. The company, which was later renamed Oracle, set out to build a relational<br>database management system as a commercial product, and was the ﬁrst to reach the<br>market. Since then, Oracle has held a leading position in the relational database mar-<br>ket, but over the years its product and service offerings have grown beyond the rela-<br>tional database server. In addition to tools directly related to database development<br>and management, Oracle sells business intelligence tools, including a multidimen-<br>sional database management system (Oracle Express), query and analysis tools, data-<br>mining products, and an application server with close integration to the database<br>server.<br>In addition to database-related servers and tools, the company also offers appli-<br>cation software for enterprise resource planning and customer-relationship manage-<br>ment, including areas such as ﬁnancials, human resources, manufacturing, market-<br>ing, sales, and supply chain management. Oracle’s Business OnLine unit offers ser-<br>vices in these areas as an application service provider.<br>This chapter surveys a subset of the features, options, and functionality of Oracle<br>products. New versions of the products are being developed continually, so all prod-<br>uct descriptions are subject to change. The feature set described here is based on the<br>ﬁrst release of Oracle9i.<br>25.1<br>Database Design and Querying Tools<br>Oracle provides a variety of tools for database design, querying, report generation<br>and data analysis, including OLAP.<br>921<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>III. Object−Based <br>Databases and XML<br>8. Object−Oriented <br>Databases<br>311<br>© The McGraw−Hill <br>Companies, 2001<br>922<br>Chapter 25<br>Oracle<br>25.1.1<br>Database Design Tools<br>Most of Oracle’s design tools are included in the Oracle Internet Development Suite.<br>This is a suite of tools for various aspects of application development, including tools<br>for forms development, data modeling, reporting, and querying. The suite supports<br>the UML standard (see Section 2.10) for development modeling. It provides class<br>modeling to generate code for the business components for Java framework as well<br>as activity modeling for general-purpose control ﬂow modeling. The suite also sup-<br>ports XML for data exchange with other UML tools.<br>The major database design tool in the suite is Oracle Designer, which translates<br>business logic and data ﬂows into a schema deﬁnitions and procedural scripts for<br>application logic. It supports such modeling techniques as E-R diagrams, information<br>engineering, and object analysis and design. Oracle Designer stores the design in<br>Oracle Repository, which serves as a single point of metadata for the application.<br>The metadata can then be used to generate forms and reports. Oracle Repository<br>provides conﬁguration management for database objects, forms applications, Java<br>classes, XML ﬁles, and other types of ﬁles.<br>The suite also contains application development tools for generating forms, re-<br>ports, and tools for various aspects of Java and XML-based development. The busi-<br>ness intelligence component provides JavaBeans for analytic functionality such as<br>data visualization, querying, and analytic calculations.<br>Oracle also has an application development tool for data warehousing, Oracle<br>Warehouse Builder. Warehouse Builder is a tool for design and deployment of all as-<br>pects of a data warehouse, including schema design, data mapping and transforma-<br>tions, data load processing, and metadata management. Oracle Warehouse Builder<br>supports both 3NF and star schemas and can also import designs from Oracle De-<br>signer.<br>25.1.2<br>Querying Tools<br>Oracle provides tools for ad-hoc querying, report generation and data analysis, in-<br>cluding OLAP.<br>Oracle Discoverer is a Web-based, ad hoc query, reporting, analysis and Web pub-<br>lishing tool for end users and data analysts. It allows users to drill up and down on<br>result sets, pivot data, and store calculations as reports that can be published in a<br>variety of formats such as spreadsheets or HTML. Discoverer has wizards to help end<br>users visualize data as graphs. Oracle9i has supports a rich set of analytical func-<br>tions, such as ranking and moving aggregation in SQL. Discoverer’s ad hoc query<br>interface can generate SQL that takes advantage of this functionality and can pro-<br>vide end users with rich analytical functionality. Since the processing takes place in<br>the relational database management system, Discoverer does not require a complex<br>client-side calculation engine and there is a version of Discoverer that is browser<br>based.<br>Oracle Express Server is a multidimensional database server. It supports a wide<br>variety of analytical queries as well as forecasting, modeling, and scenario manage-<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>III. Object−Based <br>Databases and XML<br>8. Object−Oriented <br>Databases<br>312<br>© The McGraw−Hill <br>Companies, 2001<br>25.2<br>SQL Variations and Extensions<br>923<br>ment. It can use the relational database management system as a back end for storage<br>or use its own multidimensional storage of the data.<br>With the introduction of OLAP services in Oracle9i, Oracle is moving away from<br>supporting a separate storage engine and moving most of the calculations into SQL.<br>The result is a model where all the data reside in the relational database management<br>system and where any remaining calculations that cannot be performed in SQL are<br>done in a calculation engine running on the database server. The model also provides<br>a Java OLAP application programmer interface.<br>There are many reasons for moving away from a separate multidimensional stor-<br>age engine:<br>• A relational engine can scale to much larger data sets.<br>• A common security model can be used for the analytical applications and the<br>data warehouse.<br>• Multidimensional modeling can be integrated with data warehouse modeling.<br>• The relational database management system has a larger set of features and<br>functionality in many areas such as high availability, backup and recovery,<br>and third-party tool support.<br>• There is no need to train database administrators for two database engines.<br>The main challenge with moving away from a separate multidimensional database<br>engine is to provide the same performance. A multidimensional database manage-<br>ment system that materializes all or large parts of a data cube can offer very fast<br>response times for many calculations. Oracle has approached this problem in two<br>ways.<br>• Oracle has added SQL support for a wide range of analytical functions, in-<br>cluding cube, rollup, grouping sets, ranks, moving aggregation, lead and lag<br>functions, histogram buckets, linear regression, and standard deviation, along<br>with the ability to optimize the execution of such functions in the database en-<br>gine.<br>• Oracle has extended materialized views to permit analytical functions, in par-<br>ticular grouping sets. The ability to materialize parts or all of the cube is key<br>to the performance of a multidimensional database management system and<br>materialized views give a relational database management system the ability<br>to do the same thing.<br>25.2<br>SQL Variations and Extensions<br>Oracle9i supports all core SQL:1999 features fully or partially, with some minor ex-<br>ceptions such as distinct data types. In addition, Oracle supports a large number of<br>other language constructs, some of which conform with SQL:1999, while others are<br>Oracle-speciﬁc in syntax or functionality. For example, Oracle supports the OLAP<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>III. Object−Based <br>Databases and XML<br>8. Object−Oriented <br>Databases<br>313<br>© The McGraw−Hill <br>Companies, 2001<br>924<br>Chapter 25<br>Oracle<br>operations described in Section 22.2, including ranking, moving aggregation, cube,<br>and rollup.<br>A few examples of Oracle SQL extensions are:<br>• connect by, which is a form of tree traversal that allows transitive closure-<br>style calculations in a single SQL statement. It is an Oracle-speciﬁc syntax for<br>a feature that Oracle has had since the 1980s.<br>• Upsert and multitable inserts. The upsert operation combines update and in-<br>sert, and is useful for merging new data with old data in data warehousing<br>applications. If a new row has the same key value as an old row, the old row is<br>updated (for example by adding the measure values from the new row), oth-<br>erwise the new row is inserted into the table. Multitable inserts allow multiple<br>tables to be updated based on a single scan of new data.<br>• with clause, which is described in Section 4.8.2.<br>25.2.1<br>Object-Relational Features<br>Oracle has extensive support for object-relational constructs, including:<br>• Object types. A single-inheritance model is supported for type hierarchies.<br>• Collection types. Oracle supports varrays which are variable length arrays,<br>and nested tables.<br>• Object tables. These are used to store objects while providing a relational<br>view of the attributes of the objects.<br>• Table functions. These are functions that produce sets of rows as output, and<br>can be used in the from clause of a query. Table functions in Oracle can be<br>nested. If a table function is used to express some form of data transformation,<br>nesting multiple functions allows multiple transformations to be expressed in<br>a single statement.<br>• Object views. These provide a virtual object table view of data stored in a<br>regular relational table. They allow data to be accessed or viewed in an object-<br>oriented style even if the data are really stored in a traditional relational for-<br>mat.<br>• Methods. These can be written in PL/SQL, Java, or C.<br>• User-deﬁned aggregate functions. These can be used in SQL statements in the<br>same way as built-in functions such as sum and count.<br>• XML data types. These can be used to store and index XML documents.<br>Oracle has two main procedural languages, PL/SQL and Java. PL/SQL was Oracle’s<br>original language for stored procedures and it has syntax similar to that used in the<br>Ada language. Java is supported through a Java virtual machine inside the database<br>engine. Oracle provides a package to encapsulate related procedures, functions, and<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>III. Object−Based <br>Databases and XML<br>8. Object−Oriented <br>Databases<br>314<br>© The McGraw−Hill <br>Companies, 2001<br>25.3<br>Storage and Indexing<br>925<br>variables into single units. Oracle supports SQLJ (SQL embedded in Java) and JDBC,<br>and provides a tool to generate Java class deﬁnitions corresponding to user-deﬁned<br>database types.<br>25.2.2<br>Triggers<br>Oracle provides several types of triggers and several options for when and how they<br>are invoked. (See Section 6.4 for an introduction to triggers in SQL.) Triggers can be<br>written in PL/SQL or Java or as C callouts.<br>For triggers that execute on DML statements such as insert, update, and delete,<br>Oracle supports row triggers and statement triggers. Row triggers execute once for<br>every row that is affected (updated or deleted, for example) by the DML operation.<br>A statement trigger is executed just once per statement. In each case, the trigger can<br>be deﬁned as either a before or after trigger, depending on whether it is to be invoked<br>before or after the DML operation is carried out.<br>Oracle allows the creation of instead of triggers for views that cannot be subject<br>to DML operations. Depending on the view deﬁnition, it may not be possible for Or-<br>acle to translate a DML statement on a view to modiﬁcations of the underlying base<br>tables unambiguously. Hence, DML operations on views are subject to numerous re-<br>strictions. A user can create an instead of trigger on a view to specify manually what<br>operations on the base tables are to occur in response to the DML operation on the<br>view. Oracle executes the trigger instead of the DML operation and therefore pro-<br>vides a mechanism to circumvent the restrictions on DML operations against views.<br>Oracle also has triggers that execute on a variety of other events, like database<br>startup or shutdown, server error messages, user logon or logoff, and DDL statements<br>such as create, alter and drop statements.<br>25.3<br>Storage and Indexing<br>In Oracle parlance, a database consists of information stored in ﬁles and is accessed<br>through an instance, which is a shared memory area and a set of processes that inter-<br>act with the data in the ﬁles.<br>25.3.1<br>Table Spaces<br>A database consists of one or more logical storage units called table spaces. Each<br>table space, in turn, consists of one or more physical structures called data ﬁles. These<br>may be either ﬁles managed by the operating system or raw devices.<br>Usually, an Oracle database will have the following table spaces:<br>• The system table space, which is always created. It contains the data dictio-<br>nary tables and storage for triggers and stored procedures.<br>• Table spaces created to store user data. While user data can be stored in the<br>system table space, it is often desirable to separate the user data from the sys-<br>tem data. Usually, the decision about what other table spaces should be cre-<br>ated is based on performance, availability, maintainability, and ease of admin-<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>III. Object−Based <br>Databases and XML<br>8. Object−Oriented <br>Databases<br>315<br>© The McGraw−Hill <br>Companies, 2001<br>926<br>Chapter 25<br>Oracle<br>istration. For example, having multiple table spaces can be useful for partial<br>backup and recovery operations.<br>• Temporary table spaces. Many database operations require sorting the data,<br>and the sort routine may have to store data temporarily on disk if the sort<br>cannot be done in memory. Temporary table spaces are allocated for sorting,<br>to make the space management operations involved in spilling to disk more<br>efﬁcient.<br>Table spaces can also be used as a means of moving data between databases. For<br>example, it is common to move data from a transactional system to a data warehouse<br>at regular intervals. Oracle allows moving all the data in a table space from one sys-<br>tem to the other by simply copying the ﬁles and exporting and importing a small<br>amount of data dictionary metadata. These operations can be much faster than un-<br>loading the data from one database and then using a loader to insert it into the other.<br>A requirement for this feature is that both systems use the same operating system.<br>25.3.2<br>Segments<br>The space in a table space is divided into units, called segments, that each contain<br>data for a speciﬁc data structure. There are four types of segments.<br>• Data segments. Each table in a table space has its own data segment where<br>the table data are stored unless the table is partitioned; if so, there is one data<br>segment per partition. (Partitioning in Oracle is described in Section 25.3.10.)<br>• Index segments. Each index in a table space has its own index segment, except<br>for partitioned indices, which have one index segment per partition.<br>• Temporary segments. These are segments used when a sort operation needs<br>to write data to disk or when data are inserted into a temporary table.<br>• Rollback segments. These segments contain undo information so that an un-<br>committed transaction can be rolled back. They also play an important roll in<br>Oracle’s concurrency control model and for database recovery, described in<br>Sections 25.5.1 and 25.5.2.<br>Below the level of segment, space is allocated at a level of granularity called extent.<br>Each extent consists of a set of contiguous database blocks. A database block is the<br>lowest level of granularity at which Oracle performs disk I/O. A database block does<br>not have to be the same as an operating system block in size, but should be a multiple<br>thereof.<br>Oracle provides storage parameters that allow for detailed control of how space is<br>allocated and managed, parameters such as:<br>• The size of a new extent that is to be allocated to provide room for rows that<br>are inserted into a table.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>III. Object−Based <br>Databases and XML<br>8. Object−Oriented <br>Databases<br>316<br>© The McGraw−Hill <br>Companies, 2001<br>25.3<br>Storage and Indexing<br>927<br>• The percentage of space utilization at which a database block is considered full<br>and at which no more rows will be inserted into that block. (Leaving some free<br>space in a block can allow the existing rows to grow in size through updates,<br>without running out of space in the block.)<br>25.3.3<br>Tables<br>A standard table in Oracle is heap organized; that is, the storage location of a row in<br>a table is not based on the values contained in the row, and is ﬁxed when the row<br>is inserted. However, if the table is partitioned, the content of the row affects the<br>partition in which it is stored. There are several features and variations.<br>Oracle supports nested tables; that is, a table can have a column whose data type<br>is another table. The nested table is not stored in line in the parent table, but is stored<br>in a separate table.<br>Oracle supports temporary tables where the duration of the data is either the trans-<br>action in which the data are inserted, or the user session. The data are private to the<br>session and are automatically removed at the end of its du</span><br><br><span style="background-color: #BDB2FF;" title="Chunk 38 | Start: 760076 | End: 780076 | Tokens: 3388">ration.<br>A cluster is another form of organization for table data (see Section 11.7). The<br>concept, in this context, should not be confused with other meanings of the word<br>cluster, such as those relating to hardware architecture. In a cluster, rows from dif-<br>ferent tables are stored together in the same block on the basis of some common<br>columns. For example, a department table and an employee table could be clustered<br>so that each row in the department table is stored together with all the employee<br>rows for those employees who work in that department. The primary key/foreign<br>key values are used to determine the storage location. This organization gives per-<br>formance beneﬁts when the two tables are joined, but without the space penalty of a<br>denormalized schema, since the values in the department table are not repeated for<br>each employee. As a tradeoff, a query involving only the department table may have<br>to involve a substantially larger number of blocks than if that table had been stored<br>on its own.<br>The cluster organization implies that a row belongs in a speciﬁc place; for example,<br>a new employee row must be inserted with the other rows for the same department.<br>Therefore, an index on the clustering column is mandatory. An alternative organiza-<br>tion is a hash cluster. Here, Oracle computes the location of a row by applying a hash<br>function to the value for the cluster column. The hash function maps the row to a<br>speciﬁc block in the hash cluster. Since no index traversal is needed to access a row<br>according to its cluster column value, this organization can save signiﬁcant amounts<br>of disk I/O. However, the number of hash buckets and other storage parameters must<br>be set carefully to avoid performance problems due to too many collisions or space<br>wastage due to empty hash buckets.<br>Both the hash cluster and regular cluster organization can be applied to a single<br>table. Storing a table as a hash cluster with the primary key column as the cluster key<br>can allow an access based on a primary key value with a single disk I/O provided<br>that there is no overﬂow for that data block.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>III. Object−Based <br>Databases and XML<br>8. Object−Oriented <br>Databases<br>317<br>© The McGraw−Hill <br>Companies, 2001<br>928<br>Chapter 25<br>Oracle<br>25.3.4<br>Index-Organized Tables<br>In an index organized table, records are stored in an Oracle B-tree index instead of in a<br>heap. An index-organized table requires that a unique key be identiﬁed for use as the<br>index key. While an entry in a regular index contains the key value and row-id of the<br>indexed row, an index-organized table replaces the row-id with the column values<br>for the remaining columns of the row. Compared to storing the data in a regular heap<br>table and creating an index on the key columns, index-organized table can improve<br>both performance and space utilization. Consider looking up all the column values<br>of a row, given its primary key value. For a heap table, that would require an index<br>probe followed by a table access by row-id. For an index-organized table, only the<br>index probe is necessary.<br>Secondary indices on nonkey columns of an index-organized table are different<br>from indices on a regular heap table. In a heap table, each row has a ﬁxed row-id<br>that does not change. However, a B-tree is reorganized as it grows or shrinks when<br>entries are inserted or deleted, and there is no guarantee that a row will stay in a<br>ﬁxed place inside an index-organized table. Hence, a secondary index on an index-<br>organized table contains not normal row-ids, but logical row-ids instead. A logical<br>row-id consists of two parts: a physical row-id corresponding to where the row was<br>when the index was created or last rebuilt and a value for the unique key. The phys-<br>ical row-id is referred to as a “guess” since it could be incorrect if the row has been<br>moved. If so, the other part of a logical row-id, the key value for the row, is used to<br>access the row; however, this access is slower than if the guess had been correct, since<br>it involves a traversal of the B-tree for the index-organized table from the root all the<br>way to the leaf nodes, potentially incurring several disk I/Os. However, if a table is<br>highly volatile and a large percentage of the guesses are likely to be wrong, it can be<br>better to create the secondary index with only key values, since using an incorrect<br>guess may result in a wasted disk I/O.<br>25.3.5<br>Indices<br>Oracle supports several different types of indices. The most commonly used type is a<br>B-tree index, created on one or multiple columns. (Note: in the terminology of Oracle<br>(as also in several other database systems) a B-tree index is what is referred to as a<br>B+-tree index in Chapter 12.) Index entries have the following format: For an index<br>on columns col1, col2, and col3, each row in the table where at least one of the columns<br>has a nonnull value would result in the index entry<br>&lt; col1 &gt;&lt; col2 &gt;&lt; col3 &gt;&lt; row-id &gt;<br>where &lt; coli &gt; denotes the value for column i and &lt; row-id &gt; is the row-id for<br>the row. Oracle can optionally compress the preﬁx of the entry to save space. For<br>example, if there are many repeated combinations of &lt; col1 &gt;&lt; col2 &gt; values, the<br>representation of each distinct &lt; col1 &gt;&lt; col2 &gt; preﬁx can be shared between the<br>entries that have that combination of values, rather than stored explicitly for each<br>such entry. Preﬁx compression can lead to substantial space savings.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>III. Object−Based <br>Databases and XML<br>8. Object−Oriented <br>Databases<br>318<br>© The McGraw−Hill <br>Companies, 2001<br>25.3<br>Storage and Indexing<br>929<br>25.3.6<br>Bitmap Indices<br>Bitmap indices (described in Section 12.9.4) use a bitmap representation for index<br>entries, which can lead to substantial space saving (and therefore disk I/O savings),<br>when the indexed column has a moderate number of distinct values. Bitmap indices<br>in Oracle use the same kind of B-tree structure to store the entries as a regular in-<br>dex. However, where a regular index on a column would have entries of the form<br>&lt; col1 &gt;&lt; row-id &gt;, a bitmap index entry has the form<br>&lt; col1 &gt;&lt; startrow-id &gt;&lt; endrow-id &gt;&lt; compressedbitmap &gt;<br>The bitmap conceptually represents the space of all possible rows in the table be-<br>tween the start and end row-id. The number of such possible rows in a block depends<br>on how many rows can ﬁt into a block, which is a function of the number of columns<br>in the table and their data types. Each bit in the bitmap represents one such possible<br>row in a block. If the column value of that row is that of the index entry, the bit is set<br>to 1. If the row has some other value, or the row does not actually exist in the table,<br>the bit is set to 0. (It is possible that the row does not actually exist because a table<br>block may well have a smaller number of rows than the number that was calculated<br>as the maximum possible.) If the difference is large, the result may be long strings<br>of consecutive zeros in the bitmap, but the compression algorithm deals with such<br>strings of zeros, so the negative effect is limited.<br>The compression algorithm is a variation of a compression technique called Byte-<br>Aligned Bitmap Compression (BBC). Essentially, a section of the bitmap where the<br>distance between two consecutive ones is small enough is stored as verbatim bitmaps.<br>If the distance between two ones is sufﬁciently large—that is, there is a sufﬁcient<br>number of adjacent zeros between them—a runlength of zeros, that is the number of<br>zeros, is stored.<br>Bitmap indices allow multiple indices on the same table to be combined in the<br>same access path if there are multiple conditions on indexed columns in the where<br>clause of a query. For example, for the condition<br>(col1 = 1 or col1 = 2) and col2 &gt; 5 and col3 &lt;&gt; 10<br>Oracle would be able to calculate which rows match the condition by performing<br>Boolean operations on bitmaps from indices on the three columns. In this case, these<br>operations would take place for each index:<br>• For the index on col1, the bitmaps for key values 1 and 2 would be ored.<br>• For the index on col2, all the bitmaps for key values &gt; 5 would be merged in<br>an operation that corresponds to a logical or.<br>• For the index on col3, the bitmaps for key values 10 and null would be re-<br>trieved. Then, a Boolean and would be performed on the results from the ﬁrst<br>two indices, followed by two Boolean minuses of the bitmaps for values 10<br>and null for col3.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>III. Object−Based <br>Databases and XML<br>8. Object−Oriented <br>Databases<br>319<br>© The McGraw−Hill <br>Companies, 2001<br>930<br>Chapter 25<br>Oracle<br>All operations are performed directly on the compressed representation of the bit-<br>maps—no decompression is necessary—and the resulting (compressed) bitmap rep-<br>resents those rows that match all the logical conditions.<br>The ability to use the Boolean operations to combine multiple indices is not lim-<br>ited to bitmap indices. Oracle can convert row-ids to the compressed bitmap repre-<br>sentation, so it can use a regular B-tree index anywhere in a Boolean tree of bitmap<br>operation simply by putting a row-id-to-bitmap operator on top of the index access<br>in the execution plan.<br>As a rule of thumb, bitmap indices tend to be more space efﬁcient than regular<br>B-tree indices if the number of distinct key values is less than half the number of<br>rows in the table. For example, in a table with 1 million rows, an index on a column<br>with less than 500,000 distinct values would probably be smaller if it were created as<br>a bitmap index. For columns with a very small number of distinct values—for ex-<br>ample, columns referring to properties such as country, state, gender, marital status,<br>and various status ﬂags—a bitmap index might require only a small fraction of the<br>space of a regular B-tree index. Any such space advantage can also give rise to corre-<br>sponding performance advantages in the form of fewer disk I/Os when the index is<br>scanned.<br>25.3.7<br>Function-Based Indices<br>In addition to creating indices on one or multiple columns of a table, Oracle allows<br>indices to be created on expressions that involve one or more columns, such as col1 +<br>col2 ∗5. For example, by creating an index on the expression upper(name), where upper<br>is a function that returns the uppercase version of a string, and name is a column, it is<br>possible to do case-insensitive searches on the name column. In order to ﬁnd all rows<br>with name “van Gogh” efﬁciently, the condition<br>upper(name) = ’VAN GOGH’<br>would be used in the where clause of the query. Oracle then matches the condition<br>with the index deﬁnition and concludes that the index can be used to retrieve all the<br>rows matching “van Gogh” regardless of how the name was capitalized when it was<br>stored in the database. A function-based index can be created as either a bitmap or a<br>B-tree index.<br>25.3.8<br>Join Indices<br>A join index is an index where the key columns are not in the table that is referenced<br>by the row-ids in the index. Oracle supports bitmap join indices primarily for use<br>with star schemas (see Section 22.4.2). For example, if there is a column for product<br>names in a product dimension table, a bitmap join index on the fact table with this key<br>column could be used to retrieve the fact table rows that correspond to a product with<br>a speciﬁc name, although the name is not stored in the fact table. How the rows in<br>the fact and dimension tables correspond is based on a join condition that is speciﬁed<br>when the index is created, and becomes part of the index metadata. When a query is<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>III. Object−Based <br>Databases and XML<br>8. Object−Oriented <br>Databases<br>320<br>© The McGraw−Hill <br>Companies, 2001<br>25.3<br>Storage and Indexing<br>931<br>processed, the optimizer will look for the same join condition in the where clause of<br>the query in order to determine if the join index is applicable.<br>Oracle allows bitmap join indices to have more than one key column and these<br>columns can be in different tables. In all cases, the join conditions between the fact<br>table on which the index is built and the dimension tables must refer to unique keys<br>in the dimension tables; that is, an indexed row in the fact table must correspond to<br>a unique row in each of the dimension tables.<br>Oracle can combine a bitmap join index on a fact table with other indices on the<br>same table—whether join indices or not—by using the operators for Boolean bitmap<br>operations. For example, consider a schema with a fact table for sales, and dimension<br>tables for customers, products, and time. Suppose a query requests information about<br>sales to customers in a certain zip code who bought products in a certain product cat-<br>egory during a certain time period. If a multicolumn bitmap join index exists where<br>the key columns are the constrained dimension table columns (zip code, product cat-<br>egory and time), Oracle can use the join index to ﬁnd rows in the fact table that match<br>the constraining conditions. However, if individual, single-column indices exist for<br>the key columns (or a subset of them), Oracle can retrieve bitmaps for fact table rows<br>that match each individual condition, and use the Boolean and operation to generate<br>a fact table bitmap for those rows that satisfy all the conditions. If the query contains<br>conditions on some columns of the fact table, indices on those columns could be in-<br>cluded in the same access path, even if they were regular B-tree indices or domain<br>indices (domain indices are described below in Section 25.3.9).<br>25.3.9<br>Domain Indices<br>Oracle allows tables to be indexed by index structures that are not native to Oracle.<br>This extensibility feature of the Oracle server allows software vendors to develop<br>so-called cartridges with functionality for speciﬁc application domains, such as text,<br>spatial data, and images, with indexing functionality beyond that provided by the<br>standard Oracle index types. In implementing the logic for creating, maintaining,<br>and searching the index, the index designer must ensure that it adheres to a speciﬁc<br>protocol in its interaction with the Oracle server.<br>A domain index must be registered in the data dictionary, together with the oper-<br>ators it supports. Oracle’s optimizer considers domain indices as one of the possible<br>access paths for a table. Oracle allows cost functions to be registered with the opera-<br>tors so that the optimizer can compare the cost of using the domain index to those of<br>other access paths.<br>For example, a domain index for advanced text searches may support an operator<br>contains. Once this operator has been registered, the domain index will be considered<br>as an access path for a query like<br>select *<br>from employees<br>where contains(resume, ’LINUX’)<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>III. Object−Based <br>Databases and XML<br>8. Object−Oriented <br>Databases<br>321<br>© The McGraw−Hill <br>Companies, 2001<br>932<br>Chapter 25<br>Oracle<br>where resume is a text column in the employee table. The domain index can be stored<br>in either an external data ﬁle or inside an Oracle index-organized table.<br>A domain index can be combined with other (bitmap or B-tree) indices in the same<br>access path by converting between the row-id and bitmap representation and using<br>Boolean bitmap operations.<br>25.3.10<br>Partitioning<br>Oracle supports various kinds of horizontal partitioning of tables and indices, and<br>this feature plays a major role in Oracle’s ability to support very large databases. The<br>ability to partition a table or index has advantages in many areas.<br>• Backup and recovery are easier and faster, since they can be done on individ-<br>ual partitions rather than on the table as a whole.<br>• Loading operations in a data warehousing environment are less intrusive:<br>data can be added to a partition, and then the partition added to a table, which<br>is an instantaneous operation. Likewise, dropping a partition with obsolete<br>data from a table is very easy in a data warehouse that maintains a rolling<br>window of historical data.<br>• Query performance beneﬁts substantially, since the optimizer can recognize<br>that only a subset of the partitions of a table need to be accessed in order to<br>resolve a query (partition pruning). Also, the optimizer can recognize that in<br>a join, it is not necessary to try to match all rows in one table with all rows in<br>the other, but that the joins need to be done only between matching pairs of<br>partitions (partitionwise join).<br>Each row in a partitioned table is associated with a speciﬁc partition. This associa-<br>tion is based on the partitioning column or columns that are part of the deﬁnition of a<br>partitioned table. There are several ways to map column values to partitions, giving<br>rise to several types of partitioning, each with different characteristics: range, hash,<br>composite, and list partitioning.<br>25.3.10.1<br>Range Partitioning<br>In range partitioning, the partitioning criteria are ranges of values. This type of par-<br>titioning is especially well suited to date columns, in which case all rows in the same<br>date range, say a day or a month, belong in the same partition. In a data warehouse<br>where data are loaded from the transactional systems at regular intervals, range par-<br>titioning can be used to implement a rolling window of historical data efﬁciently.<br>Each data load gets its own new partition, making the loading process faster and<br>more efﬁcient. The system actually loads the data into a separate table with the same<br>column deﬁnition as the partitioned table. It can then check the data for consistency,<br>cleanse them, and index them. After that, the system can make the separate table a<br>new partition of the partitioned table, by a simple change to the metadata in the data<br>dictionary—a nearly instantaneous operation.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>III. Object−Based <br>Databases and XML<br>8. Object−Oriented <br>Databases<br>322<br>© The McGraw−Hill <br>Companies, 2001<br>25.3<br>Storage and Indexing<br>933<br>Up until the metadata change, the loading process does not affect the existing<br>data in the partitioned table in any way. There is no need to do any maintenance<br>of existing indices as part of the loading. Old data can be removed from a table by<br>simply dropping its partition; this operation does not affect the other partitions.<br>In addition, queries in a data warehousing environment often contain conditions<br>that restrict them to a certain time period, such as a quarter or month. If date range<br>partitioning is used, the query optimizer can restrict the data access to those parti-<br>tions that are relevant to the query, and avoid a scan of the entire table.<br>25.3.10.2<br>Hash Partitioning<br>In hash partitioning, a hash function maps rows to partitions according to the values<br>in the partitioning columns. This type of partitioning is primarily useful when it is<br>important to distribute the rows evenly among partitions or when partitionwise joins<br>are important for query performance.<br>25.3.10.3<br>Composite Partitioning<br>In composite partitioning, the table is range partitioned, but each partition is subpar-<br>titioned by using hash partitioning. This type of partitioning combines the advan-<br>tages of range partitioning and hash partitioning.<br>25.3.10.4<br>List Partitioning<br>In list partitioning, the values associated with a particular partition are stated in a<br>list. This type of partitioning is useful if the data in the partitioning column have a<br>relatively small set of discrete values. For instance, a table with a state column can be<br>implicitly partitioned by geographical region if each partition list has the states that<br>belong in the same region.<br>25.3.11<br>Materialized Views<br>The materialized view feature (see Section 3.5.1) allows the result of an SQL query to<br>be stored in a table and used for later query processing. In addition, Oracle maintains<br>the materialized result, updating it when the tables that were referenced in the query<br>are updated. Materialized views are used in data warehousing to speed up</span><br><br><span style="background-color: #FFC6FF;" title="Chunk 39 | Start: 780078 | End: 800078 | Tokens: 3272"> query<br>processing, but the technology is also used for replication in distributed and mobile<br>environments.<br>In data warehousing, a common usage for materialized views is to summarize<br>data. For example, a common type of query asks for “the sum of sales for each quarter<br>during the last 2 years.” Precomputing the result, or some partial result, of such a<br>query can speed up query processing dramatically compared to computing it from<br>scratch by aggregating all detail-level sales records.<br>Oracle supports automatic query rewrites that take advantage of any useful mate-<br>rialized view when resolving a query. The rewrite consists of changing the query to<br>use the materialized view instead of the original tables in the query. In addition, the<br>rewrite may add additional joins or aggregate processing as may be required to get<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>III. Object−Based <br>Databases and XML<br>8. Object−Oriented <br>Databases<br>323<br>© The McGraw−Hill <br>Companies, 2001<br>934<br>Chapter 25<br>Oracle<br>the correct result. For example, if a query needs sales by quarter, the rewrite can take<br>advantage of a view that materializes sales by month, by adding additional aggre-<br>gation to roll up the months to quarters. Oracle has a type of metadata object called<br>dimension that allows hierarchical relationships in tables to be deﬁned. For example,<br>for a time dimension table in a star schema, Oracle can deﬁne a dimension metadata<br>object to specify how days roll up to months, months to quarters, quarters to years,<br>and so forth. Likewise, hierarchical properties relating to geography can be speciﬁed<br>—for example, how sales districts roll up to regions. The query rewrite logic looks at<br>these relationships since they allow a materialized view to be used for wider classes<br>of queries.<br>The container object for a materialized view is a table, which means that a mate-<br>rialized view can be indexed, partitioned, or subjected to other controls, to improve<br>query performance.<br>When there are changes to the data in the tables referenced in the query that de-<br>ﬁnes a materialized view, the materialized view must be refreshed to reﬂect those<br>changes. Oracle supports both full refresh of a materialized view and fast, incremen-<br>tal refresh. In a full refresh, Oracle recomputes the materialized view from scratch,<br>which may be the best option if the underlying tables have had signiﬁcant changes,<br>for example, changes due to a bulk load. In an incremental refresh, Oracle updates<br>the view using the records that were changed in the underlying tables; the refresh to<br>the view is immediate, that is, it is executed as part of the transaction that changed<br>the underlying tables. Incremental refresh may be better if the number of rows that<br>were changed is low. There are some restrictions on the classes of queries for which<br>a materialized view can be incrementally refreshed (and others for when a material-<br>ized view can be created at all).<br>A materialized view is similar to an index in the sense that, while it can improve<br>query performance, it uses up space, and creating and maintaining it consumes re-<br>sources. To help resolve this tradeoff, Oracle provides a package that can advise a<br>user of the most cost-effective materialized views, given a particular query workload<br>as input.<br>25.4<br>Query Processing and Optimization<br>Oracle supports a large variety of processing techniques in its query processing en-<br>gine. Some of the more important ones are described here brieﬂy.<br>25.4.1<br>Execution Methods<br>Data can be accessed through a variety of access methods:<br>• Full table scan. The query processor scans the entire table by getting infor-<br>mation about the blocks that make up the table from the extent map, and<br>scanning those blocks.<br>• Index scan. The processor creates a start and/or stop key from conditions<br>in the query and uses it to scan to a relevant part of the index. If there are<br>columns that need to be retrieved, that are not part of the index, the index<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>III. Object−Based <br>Databases and XML<br>8. Object−Oriented <br>Databases<br>324<br>© The McGraw−Hill <br>Companies, 2001<br>25.4<br>Query Processing and Optimization<br>935<br>scan would be followed by a table access by index row-id. If no start or stop<br>key is available, the scan would be a full index scan.<br>• Index fast full scan. The processor scans the extents the same way as the table<br>extent in a full table scan. If the index contains all the columns that are needed<br>in the index, and there are no good start/stop keys that would signiﬁcantly<br>reduce that portion of the index that would be scanned in a regular index scan,<br>this method may be the fastest way to access the data. This is because the fast<br>full scan can take full advantage of multiblock disk I/O. However, unlike a<br>regular full scan, which traverses the index leaf blocks in order, a fast full scan<br>does not guarantee that the output preserves the sort order of the index.<br>• Index join. If a query needs only a small subset of the columns of a wide<br>table, but no single index contains all those columns, the processor can use an<br>index join to generate the relevant information without accessing the table, by<br>joining several indices that together contain the needed columns. It performs<br>the joins as hash joins on the row-ids from the different indices.<br>• Cluster and hash cluster access. The processor accesses the data by using the<br>cluster key.<br>Oracle has several ways to combine information from multiple indices in a single<br>access path. This ability allows multiple where-clause conditions to be used together<br>to compute the result set as efﬁciently as possible. The functionality includes the<br>ability to perform Boolean operations and, or, and minus on bitmaps representing<br>row-ids. There are also operators that map a list of row-ids into bitmaps and vice<br>versa, which allows regular B-tree indices and bitmap indices to be used together in<br>the same access path. In addition, for many queries involving count(*) on selections<br>on a table, the result can be computed by just counting the bits that are set in the<br>bitmap generated by applying the where clause conditions, without accessing the<br>table.<br>Oracle supports several types of joins in the execution engine: inner joins, outer<br>joins, semijoins, and antijoins. (An antijoin in Oracle returns rows from the left-hand<br>side input that do not match any row in the right-hand side input; this operation is<br>called anti-semijoin in other literature.) It evaluates each type of join by one of three<br>methods: hash join, sort–merge join, or nested-loop join.<br>25.4.2<br>Optimization<br>In Chapter 14, we discussed the general topic of query optimization. Here, we discuss<br>optimization in the context of Oracle.<br>25.4.2.1<br>Query Transformations<br>Oracle does query optimization in several stages. Most of the techniques relating to<br>query transformations and rewrites take place before access path selection, but Or-<br>acle also supports several types of cost-based query transformations that generate a<br>complete plan and return a cost estimate for both a standard version of the query and<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>III. Object−Based <br>Databases and XML<br>8. Object−Oriented <br>Databases<br>325<br>© The McGraw−Hill <br>Companies, 2001<br>936<br>Chapter 25<br>Oracle<br>one that has been subjected to advanced transformations. Not all query transforma-<br>tion techniques are guaranteed to be beneﬁcial for every query, but by generating a<br>cost estimate for the best plan with and without the transformation applied, Oracle<br>is able to make an intelligent decision.<br>Some of the major types of transformations and rewrites supported by Oracle are<br>as follows:<br>• View merging. A view reference in a query is replaced by the view deﬁnition.<br>This transformation is not applicable to all views.<br>• Complex view merging. Oracle offers this feature for certain classes of views<br>that are not subject to regular view merging because they have a group by or<br>select distinct in the view deﬁnition. If such a view is joined to other tables,<br>Oracle can commute the joins and the sort operation used for the group by or<br>distinct.<br>• Subquery ﬂattening. Oracle has a variety of transformations that convert var-<br>ious classes of subqueries into joins, semijoins, or antijoins.<br>• Materialized view rewrite. Oracle has the ability to rewrite a query automati-<br>cally to take advantage of materialized views. If some part of the query can be<br>matched up with an existing materialized view, Oracle can replace that part<br>of the query with a reference to the table in which the view is materialized.<br>If need be, Oracle adds join conditions or group by operations to preserve<br>the semantics of the query. If multiple materialized views are applicable, Ora-<br>cle picks the one that gives the greatest advantage in reducing the amount of<br>data that has to be processed. In addition, Oracle subjects both the rewritten<br>query and the original version to the full optimization process producing an<br>execution plan and an associated cost estimate for each. Oracle then decides<br>whether to execute the rewritten or the original version of the query on the<br>basis of the cost estimates.<br>• Star transformation. Oracle supports a technique for evaluating queries against<br>star schemas, known as the star transformation. When a query contains a join<br>of a fact table with dimension tables, and selections on attributes from the<br>dimension tables, the query is transformed by deleting the join condition be-<br>tween the fact table and the dimension tables, and replacing the selection con-<br>dition on each dimension table by a subquery of the form:<br>fact table.fki in<br>(select pk from dimension tablei<br>where &lt;conditions on dimension tablei &gt;)<br>One such subquery is generated for each dimension that has some constrain-<br>ing predicate. If the dimension has a snow-ﬂake schema (see Section 22.4), the<br>subquery will contain a join of the applicable tables that make up the dimen-<br>sion.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>III. Object−Based <br>Databases and XML<br>8. Object−Oriented <br>Databases<br>326<br>© The McGraw−Hill <br>Companies, 2001<br>25.4<br>Query Processing and Optimization<br>937<br>Oracle uses the values that are returned from each subquery to probe an<br>index on the corresponding fact table column, getting a bitmap as a result.<br>The bitmaps generated from different subqueries are combined by a bitmap<br>and operation. The resultant bitmap can be used to access matching fact table<br>rows. Hence, only those rows in the fact table that simultaneously match the<br>conditions on the constrained dimensions will be accessed.<br>Both the decision on whether the use of a subquery for a particular dimen-<br>sion is cost-effective, and the decision on whether the rewritten query is better<br>than the original, are based on the optimizer’s cost estimates.<br>25.4.2.2<br>Access Path Selection<br>Oracle has a cost-based optimizer that determines join order, join methods, and ac-<br>cess paths. Each operation that the optimizer considers has an associated cost func-<br>tion, and the optimizer tries to generate the combination of operations that has the<br>lowest overall cost.<br>In estimating the cost of an operation, the optimizer relies on statistics that have<br>been computed for schema objects such as tables and indices. The statistics contain<br>information about the size of the object, the cardinality, data distribution of table<br>columns, and so forth. For column statistics, Oracle supports height-balanced and<br>frequency histograms. To facilitate the collection of optimizer statistics, Oracle can<br>monitor modiﬁcation activity on tables and keep track of those tables that have been<br>subject to enough changes that recalculating the statistics may be appropriate. Oracle<br>also tracks what columns are used in where clauses of queries, which make them po-<br>tential candidates for histogram creation. With a single command, a user can tell Or-<br>acle to refresh the statistics for those tables that were marked as sufﬁciently changed.<br>Oracle uses sampling to speed up the process of gathering the new statistics and<br>automatically chooses the smallest adequate sample percentage. It also determines<br>whether the distribution of the marked columns merit the creation of histograms; if<br>the distribution is close to uniform, Oracle uses a simpler representation of the col-<br>umn statistics.<br>Oracle uses both CPU cost and disk I/Os in the optimizer cost model. To balance<br>the two components, it stores measures about CPU speed and disk I/O performance<br>as part of the optimizer statistics. Oracle’s package for gathering optimizer statistics<br>computes these measures.<br>For queries involving a nontrivial number of joins, the search space is an issue for a<br>query optimizer. Oracle addresses this issue in several ways. The optimizer generates<br>an initial join order and then decides on the best join methods and access paths for<br>that join order. It then changes the order of the tables and determines the best join<br>methods and access paths for the new join order and so forth, while keeping the best<br>plan that has been found so far. Oracle cuts the optimization short if the number of<br>different join orders that have been considered becomes so large that the time spent<br>in the optimizer may be noticeable compared to the time it would take to execute<br>the best plan found so far. Since this cutoff depends on the cost estimate for the best<br>plan found so far, ﬁnding a good plan early is important so that the optimization can<br>be stopped after a smaller number of join orders, resulting in better response time.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>III. Object−Based <br>Databases and XML<br>8. Object−Oriented <br>Databases<br>327<br>© The McGraw−Hill <br>Companies, 2001<br>938<br>Chapter 25<br>Oracle<br>Oracle uses several initial ordering heuristics to increase the likelihood that the ﬁrst<br>join order considered is a good one.<br>For each join order that is considered, the optimizer may make additional passes<br>over the tables to decide join methods and access paths. Such additional passes would<br>target speciﬁc global side effects of the access path selection. For instance, a speciﬁc<br>combination of join methods and access paths may eliminate the need to perform an<br>order by sort. Since such a global side effect may not be obvious when the costs of<br>the different join methods and access paths are considered locally, a separate pass<br>targeting a speciﬁc side effect is used to ﬁnd a possible execution plan with a better<br>overall cost.<br>25.4.2.3<br>Partition Pruning<br>For partitioned tables, the optimizer tries to match conditions in the where clause<br>of a query with the partitioning criteria for the table, in order to avoid accessing<br>partitions that are not needed for the result. For example, if a table is partitioned<br>by date range and the query is constrained to data between two speciﬁc dates, the<br>optimizer determines which partitions contain data between the speciﬁed dates and<br>ensures that only those partitions are accessed. This scenario is very common, and<br>the speedup can be dramatic if only a small subset of the partitions are needed.<br>25.4.3<br>Parallel Execution<br>Oracle allows the execution of a single SQL statement to be parallelized by dividing<br>the work between multiple processes on a multiprocessor computer. This feature is<br>especially useful for computationally intensive operations that would otherwise take<br>an unacceptably long time to perform. Representative examples are decision support<br>queries that need to process large amounts of data, data loads in a data warehouse,<br>and index creation or rebuild.<br>In order to achieve good speedup through parallelism, it is important that the<br>work involved in executing the statement be divided into granules that can be pro-<br>cessed independently by the different parallel processors. Depending on the type of<br>operation, Oracle has several ways to split up the work.<br>For operations that access base objects (tables and indices), Oracle can divide the<br>work by horizontal slices of the data. For some operations, such as a full table scan,<br>each such slice can be a range of blocks—each parallel query process scans the table<br>from the block at the start of the range to the block at the end. For other operations on<br>a partitioned table, like update and delete, the slice would be a partition. For inserts<br>into a nonpartitioned table, the data to be inserted are randomly divided across the<br>parallel processes.<br>Joins can be parallelized in several different ways. One way is to divide one of the<br>inputs to the join between parallel processes and let each process join its slice with<br>the other input to the join; this is the asymmetric fragment-and-replicate method<br>of Section 20.5.2.2. For example, if a large table is joined to a small one by a hash<br>join, Oracle divides the large table among the processes and broadcasts a copy of the<br>small table to each process, which then joins its slice with the smaller table. If both<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>III. Object−Based <br>Databases and XML<br>8. Object−Oriented <br>Databases<br>328<br>© The McGraw−Hill <br>Companies, 2001<br>25.4<br>Query Processing and Optimization<br>939<br>tables are large, it would be prohibitively expensive to broadcast one of them to all<br>processes. In that case, Oracle achieves parallelism by partitioning the data among<br>processes by hashing on the values of the join columns (the partitioned hash-join<br>method of Section 20.5.2.1). Each table is scanned in parallel by a set of processes and<br>each row in the output is passed on to one of a set of processes that are to perform<br>the join. Which one of these processes gets the row is determined by a hash function<br>on the values of the join column. Hence, each join process gets only rows that could<br>potentially match, and no rows that could match could end up in different processes.<br>Oracle parallelizes sort operations by value ranges of the column on which the<br>sort is performed (that is, using the range-partitioning sort of Section 20.5.1). Each<br>process participating in the sort is sent rows with values in its range, and it sorts the<br>rows in its range. To maximize the beneﬁts of parallelism, the rows need to be divided<br>as evenly as possible among the parallel processes, and the problem of determining<br>range boundaries that generates a good distribution then arises. Oracle solves the<br>problem by dynamically sampling a subset of the rows in the input to the sort before<br>deciding on the range boundaries.<br>25.4.3.1<br>Process Structure<br>The processes involved in the parallel execution of an SQL statement consist of a<br>coordinator process and a number of parallel server processes. The coordinator is<br>responsible for assigning work to the parallel servers and for collecting and returning<br>data to the user process that issued the statement. The degree of parallelism is the<br>number of parallel server processes that are assigned to execute a primitive operation<br>as part of the statement. The degree of parallelism is determined by the optimizer, but<br>can be throttled back dynamically if the load on the system increases.<br>The parallel servers operate on a producer/consumer model. When a sequence of<br>operations is needed to process a statement, the producer set of servers performs the<br>ﬁrst operation and passes the resulting data to the consumer set. For example, if a full<br>table scan is followed by a sort and the degree of parallelism is 12, there would be<br>12 producer servers performing the table scan and passing the result to 12 consumer<br>servers that perform the sort. If a subsequent operation is needed, like another sort,<br>the roles of the two sets of servers switch. The servers that originally performed the<br>table scan take on the role of consumers of the output produced by the the ﬁrst sort<br>and use it to perform the second sort. Hence, a sequence of operations proceeds by<br>passing data back and forth between two sets of servers that alternate in their roles as<br>producers and consumers. The servers communicate with </span><br><br><span style="background-color: #FFADAD;" title="Chunk 40 | Start: 800080 | End: 820080 | Tokens: 3176">each other through mem-<br>ory buffers on shared-memory hardware and through high-speed network connec-<br>tions on MPP (shared nothing) conﬁgurations and clustered (shared disk) systems.<br>For shared nothing systems, the cost of accessing data on disk is not uniform<br>among processes. A process running on a node that has direct access to a device<br>is able to process data on that device faster than a process that has to retrieve the<br>data over a network. Oracle uses knowledge about device-to-node and device-to-<br>process afﬁnity—that is, the ability to access devices directly—when distributing<br>work among parallel execution servers.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>III. Object−Based <br>Databases and XML<br>8. Object−Oriented <br>Databases<br>329<br>© The McGraw−Hill <br>Companies, 2001<br>940<br>Chapter 25<br>Oracle<br>25.5<br>Concurrency Control and Recovery<br>Oracle supports concurrency control and recovery techniques that provide a number<br>of useful features.<br>25.5.1<br>Concurrency Control<br>Oracle’s multiversion concurrency control differs from the concurrency mechanisms<br>used by most other database vendors. Read-only queries are given a read-consistent<br>snapshot, which is a view of the database as it existed at a speciﬁc point in time,<br>containing all updates that were committed by that point in time, and not containing<br>any updates that were not committed at that point in time. Thus, read locks are not<br>used and read-only queries do not interfere with other database activity in terms of<br>locking. (This is basically the multiversion two-phase locking protocol described in<br>Section 16.5.2.)<br>Oracle supports both statement and transaction level read consistency: At the be-<br>ginning of the execution of either a statement or a transaction (depending on what<br>level of consistency is used), Oracle determines the current system change number<br>(SCN). The SCN essentially acts as a timestamp, where the time is measured in terms<br>of transaction commits instead of wall-clock time.<br>If in the course of a query a data block is found that has a higher SCN than the one<br>being associated with the query, it is evident that the data block has been modiﬁed<br>after the time of the original query’s SCN by some other transaction that may or may<br>not have committed. Hence, the data in the block cannot be included in a consistent<br>view of the database as it existed at the time of the query’s SCN. Instead, an older<br>version of the data in the block must be used; speciﬁcally, the one that has the highest<br>SCN that does not exceed the SCN of the query. Oracle retrieves that version of the<br>data from the rollback segment (rollback segments are described in Section 25.5.2).<br>Hence, provided that the rollback segment is sufﬁciently large, Oracle can return<br>a consistent result of the query even if the data items have been modiﬁed several<br>times since the query started execution. Should the block with the desired SCN no<br>longer exist in the rollback segment, the query will return an error. It would be an<br>indication that the rollback segment has not been properly sized, given the activity<br>on the system.<br>In the Oracle concurrency model, read operations do not block write operations<br>and write operations do not block read operations, a property that allows a high<br>degree of concurrency. In particular, the scheme allows for long-running queries (for<br>example, reporting queries) to run on a system with a large amount of transactional<br>activity. This kind of scenario is often problematic for database systems where queries<br>use read locks, since the query may either fail to acquire them or lock large amounts<br>of data for a long time, thereby preventing transactional activity against that data<br>and reducing concurrency. (An alternative that is used in some systems is to use a<br>lower degree of consistency, such as degree-two consistency, but that could result in<br>inconsistent query results.)<br>Oracle’s concurrency model is used as a basis for the Flashback Query feature. This<br>feature allows a user to set a certain SCN number or wall-clock time in his session and<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>III. Object−Based <br>Databases and XML<br>8. Object−Oriented <br>Databases<br>330<br>© The McGraw−Hill <br>Companies, 2001<br>25.5<br>Concurrency Control and Recovery<br>941<br>perform queries on the data that existed at that point in time (provided that the data<br>still exist in the rollback segment). Normally in a database system, once a change<br>has been committed, there is no way to get back to the previous state of the data<br>other than performing point-in-time recovery from backups. However, recovery of a<br>very large database can be very costly, especially if the goal is just to retrieve some<br>data item that had been inadvertently deleted by a user. The Flashback Query feature<br>provides a much simpler mechanism to deal with user errors.<br>Oracle supports two ANSI/ISO isolation levels, “read committed” and “serializ-<br>able”. There is no support for dirty reads since it is not needed. The two isolation<br>levels correspond to whether statement-level or transaction-level read consistency is<br>used. The level can be set for a session or an individual transaction. Statement-level<br>read consistency is the default.<br>Oracle uses row-level locking. Updates to different rows do not conﬂict. If two<br>writers attempt to modify the same row, one waits until the other either commits or<br>is rolled back, and then it can either return a write-conﬂict error or go ahead and<br>modify the row. Locks are held for the duration of a transaction.<br>In addition to row-level locks that prevent inconsistencies due to DML activity,<br>Oracle uses table locks that prevent inconsistencies due to DDL activity. These locks<br>prevent one user from, say, dropping a table while another user has an uncommitted<br>transaction that is accessing that table. Oracle does not use lock escalation to convert<br>row locks to table locks for the purpose of its regular concurrency control.<br>Oracle detects deadlocks automatically and resolves them by rolling back one of<br>the transactions involved in the deadlock.<br>Oracle supports autonomous transactions, which are independent transactions<br>generated within other transactions. When Oracle invokes an autonomous transac-<br>tion, it generates a new transaction in a separate context. The new transaction can<br>be either committed or rolled back before control returns to the calling transaction.<br>Oracle supports multiple levels of nesting of autonomous transactions.<br>25.5.2<br>Basic Structures for Recovery<br>In order to understand how Oracle recovers from a failure, such as a disk crash, it<br>is important to understand the basic structures that are involved. In addition to the<br>data ﬁles that contain tables and indices, there are control ﬁles, redo logs, archived<br>redo logs, and rollback segments.<br>The control ﬁle contains various metadata that are needed to operate the database,<br>including information about backups.<br>Oracle records any transactional modiﬁcation of a database buffer in the redo log,<br>which consists of two or more ﬁles. It logs the modiﬁcation as part of the operation<br>that causes it and regardless of whether the transaction eventually commits. It logs<br>changes to indices and rollback segments as well as changes to table data. As the redo<br>logs ﬁll up, they are archived by one or several background processes (if the database<br>is running in archivelog mode).<br>The rollback segment contains information about older versions of the data (that<br>is, undo information). In addition to its role in Oracle’s consistency model, the infor-<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>III. Object−Based <br>Databases and XML<br>8. Object−Oriented <br>Databases<br>331<br>© The McGraw−Hill <br>Companies, 2001<br>942<br>Chapter 25<br>Oracle<br>mation is used to restore the old version of data items when a transaction that has<br>modiﬁed the data items is rolled back.<br>To be able to recover from a storage failure, the data ﬁles and control ﬁles should be<br>backed up regularly. The frequency of the backup determines the worst-case recovery<br>time, since it takes longer to recover if the backup is old. Oracle supports hot backups<br>—backups performed on an online database that is subject to transactional activity.<br>During recovery from a backup, Oracle performs two steps to reach a consistent<br>state of the database as it existed just prior to the failure. First, Oracle rolls forward<br>by applying the (archived) redo logs to the backup. This action takes the database<br>to a state that existed at the time of the failure, but not necessarily a consistent state<br>since the redo logs include uncommitted data. Second, Oracle rolls back uncommit-<br>ted transactions by using the rollback segment. The database is now in a consistent<br>state.<br>Recovery on a database that has been subject to heavy transactional activity since<br>the last backup can be time consuming. Oracle supports parallel recovery in which<br>several processes are used to apply redo information simultaneously. Oracle provides<br>a GUI tool, Recovery Manager, which automates most tasks associated with backup<br>and recovery.<br>25.5.3<br>Managed Standby Databases<br>To ensure high availability, Oracle provides a managed standby database feature.<br>(This feature is the same as remote backups, described in Section 17.10.) A standby<br>database is a copy of the regular database that is installed on a separate system. If<br>a catastrophic failure occurs on the primary system, the standby system is activated<br>and takes over, thereby minimizing the effect of the failure on availability. Oracle<br>keeps the standby database up to date by constantly applying archived redo logs<br>that are shipped from the primary database. The backup database can be brought<br>online in read-only mode and used for reporting and decision support queries.<br>25.6<br>System Architecture<br>Whenever an database application executes an SQL statement, there is an operating<br>system process that executes code in the database server. Oracle can be conﬁgured<br>so that the operating system process is dedicated exclusively to the statement it is<br>processing or so that the process can be shared among multiple statements. The latter<br>conﬁguration, known as the multithreaded server, has somewhat different properties<br>with regard to the process and memory architecture. We shall discuss the dedicated<br>server architecture ﬁrst and the multithreaded server architecture later.<br>25.6.1<br>Dedicated Server: Memory Structures<br>The memory used by Oracle falls mainly into three categories: software code areas,<br>the system global area (SGA), and the program global area (PGA).<br>The system code areas are the parts of the memory where the Oracle server code<br>resides. A PGA is allocated for each process to hold its local data and control informa-<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>III. Object−Based <br>Databases and XML<br>8. Object−Oriented <br>Databases<br>332<br>© The McGraw−Hill <br>Companies, 2001<br>25.6<br>System Architecture<br>943<br>tion. This area contains stack space for various session data and the private memory<br>for the SQL statement that it is executing. It also contains memory for sorting and<br>hashing operations that may occur during the evaluation of the statement.<br>The SGA is a memory area for structures that are shared among users. It is made<br>up by several major structures, including:<br>• The buffer cache. This cache keeps frequently accessed data blocks (from ta-<br>bles as well as indices) in memory to reduce the need to perform physical<br>disk I/O. A least recently used replacement policy is used except for blocks ac-<br>cessed during a full table scan. However, Oracle allows multiple buffer pools<br>to be created that have different criteria for aging out data. Some Oracle oper-<br>ations bypass the buffer cache and read data directly from disk.<br>• The redo log buffer. This buffer contains the part of the redo log that has not<br>yet been written to disk.<br>• The shared pool. Oracle seeks to maximize the number of users that can<br>use the database concurrently by minimizing the amount of memory that is<br>needed for each user. One important concept in this context is the ability to<br>share the internal representation of SQL statements and procedural code writ-<br>ten in PL/SQL. When multiple users execute the same SQL statement, they can<br>share most data structures that represent the execution plan for the statement.<br>Only data that is local to each speciﬁc invocation of the statement needs to be<br>kept in private memory.<br>The sharable parts of the data structures representing the SQL statement are<br>stored in the shared pool, including the text of the statement. The caching of<br>SQL statements in the shared pool also saves compilation time, since a new in-<br>vocation of a statement that is already cached does not have to go through the<br>complete compilation process. The determination of whether an SQL state-<br>ment is the same as one existing in the shared pool is based on exact text<br>matching and the setting of certain session parameters. Oracle can automati-<br>cally replace constants in an SQL statement with bind variables; future queries<br>that are the same except for the values of constants will then match the earlier<br>query in the shared pool. The shared pool also contains caches for dictionary<br>information and various control structures.<br>25.6.2<br>Dedicated Server: Process Structures<br>There are two types of processes that execute Oracle server code: server processes<br>that process SQL statements and background processes that perform various admin-<br>istrative and performance-related tasks. Some of these processes are optional, and in<br>some cases, multiple processes of the same type can be used for performance reasons.<br>Some of the most important types of background processes are:<br>• Database writer. When a buffer is removed from the buffer cache, it must be<br>written back to disk if it has been modiﬁed since it entered the cache. This task<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>III. Object−Based <br>Databases and XML<br>8. Object−Oriented <br>Databases<br>333<br>© The McGraw−Hill <br>Companies, 2001<br>944<br>Chapter 25<br>Oracle<br>is performed by the database writer processes, which help the performance of<br>the system by freeing up space in the buffer cache.<br>• Log writer. The log writer process writes entries in the redo log buffer to the<br>redo log ﬁle on disk. It also writes a commit record to disk whenever a trans-<br>action commits.<br>• Checkpoint. The checkpoint process updates the headers of the data ﬁle when<br>a checkpoint occurs.<br>• System monitor. This process performs crash recovery if needed. It is also<br>performs some space management to reclaim unused space in temporary seg-<br>ments.<br>• Process monitor. This process performs process recovery for server processes<br>that fail, releasing resources and performing various cleanup operations.<br>• Recoverer. The recoverer process resolves failures and conducts cleanup for<br>distributed transactions.<br>• Archiver. The archiver copies the online redo log ﬁle to an archived redo log<br>every time the online log ﬁle ﬁlls up.<br>25.6.3<br>Multithreaded Server<br>The multithreaded server conﬁguration increases the number of users that a given<br>number of server processes can support by sharing server processes among state-<br>ments. It differs from the dedicated server architecture in these major aspects:<br>• A background dispatch process routes user requests to the next available ser-<br>ver process. In doing so, it uses a request queue and a response queue in the<br>SGA. The dispatcher puts a new request in the request queue where it will<br>be picked up by a server process. As a server process completes a request, it<br>puts the result in the response queue to be picked up by the dispatcher and<br>returned to the user.<br>• Since a server process is shared among multiple SQL statements, Oracle does<br>not keep private data in the PGA. Instead, it stores the session-speciﬁc data in<br>the SGA.<br>25.6.4<br>Oracle9i Real Application Clusters<br>Oracle9i Real Application Clusters is a feature that allows multiple instances of Ora-<br>cle to run against the same database. (Recall that, in Oracle terminology, an instance<br>is the combination of background processes and memory areas.) This feature enables<br>Oracle to run on clustered and MPP (shared disk and shared nothing) hardware ar-<br>chitectures. This feature was called Oracle Parallel Server in earlier versions of Or-<br>acle. The ability to cluster multiple nodes has important beneﬁts for scalability and<br>availability that are useful in both OLTP and data warehousing environments.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>III. Object−Based <br>Databases and XML<br>8. Object−Oriented <br>Databases<br>334<br>© The McGraw−Hill <br>Companies, 2001<br>25.7<br>Replication, Distribution, and External Data<br>945<br>The scalability beneﬁts of the feature are obvious, since more nodes mean more<br>processing power. Oracle further optimizes the use of the hardware through features<br>such as afﬁnity and partitionwise joins.<br>Oracle9i Real Application Clusters can also be used to achieve high availability. If<br>one node fails, the remaining ones are still available to the application accessing the<br>database. The remaining instances will automatically roll back uncommitted trans-<br>actions that were being processed on the failed node in order to prevent them from<br>blocking activity on the remaining nodes.<br>Having multiple instances run against the same database gives rise to some tech-<br>nical issues that do not exist on a single instance. While it is sometimes possible to<br>partition an application among nodes so that nodes rarely access the same data, there<br>is always the possibility of overlaps, which affects locking and cache management.<br>To address this, Oracle supports a distributed lock manager and the cache fusion fea-<br>ture, which allows data blocks to ﬂow directly among caches on different instances<br>using the interconnect, without being written to disk.<br>25.7<br>Replication, Distribution, and External Data<br>Oracle provides support for replication and distributed transactions with two-phase<br>commit.<br>25.7.1<br>Replication<br>Oracle supports several types of replication. (See Section 19.2.1 for an introduction<br>to replication.) In its simplest form, data in a master site are replicated to other sites<br>in the form of snapshots. (The term “snapshot” in this context should not be con-<br>fused with the concept of a read-consistent snapshot in the context of the concurrency<br>model.) A snapshot does not have to contain all the master data—it can, for example,<br>exclude certain columns from a table for security reasons. Oracle supports two types<br>of snapshots: read-only and updatable. An updatable snapshot can be modiﬁed at a<br>slave site and the modiﬁcations propagated to the master table. However, read-only<br>snapshots allow for a wider range of snapshot deﬁnitions. For instance, a read-only<br>snapshot can be deﬁned in terms of set operations on tables at the master site.<br>Oracle also supports multiple master sites for the same data, where all master<br>sites act as peers. A replicated table can be updated at any of the master sites and<br>the update is propagated to the other sites. The updates can be propagated either<br>asynchronously or synchronously.<br>For asynchronous replication, the update information is sent in batches to the other<br>master sites and applied. Since the same data could be subject to conﬂicting modi-<br>ﬁcations at different sites, conﬂict resolution based on some business rules might be<br>needed. Oracle provides a number of of built-in conﬂict resolution methods and al-<br>lows users to write their own if need be.<br>With synchronous replication, an update to one master site is propagated imme-<br>diately to all other sites. If the update transaction fails at any master site, the update<br>is rolled back at all sites.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>III. Object−Based <br>Databases and XML<br>8. Object−Oriented <br>Databases<br>335<br>© The McGraw−Hill <br>Companies, 2001<br>946<br>Chapter 25<br>Oracle<br>25.7.2<br>Distributed Databases<br>Oracle supports queries and transactions spanning multiple databases on dif</span><br><br><span style="background-color: #FFD6A5;" title="Chunk 41 | Start: 820082 | End: 840082 | Tokens: 3086">ferent<br>systems. With the use of gateways, the remote systems can include non-Oracle data-<br>bases. Oracle has built-in capability to optimize a query that includes tables at differ-<br>ent sites, retrieve the relevant data, and return the result as if it had been a normal,<br>local query. Oracle also transparently supports transactions spanning multiple sites<br>by a built-in two-phase-commit protocol.<br>25.7.3<br>External Data Sources<br>Oracle has several mechanisms for supporting external data sources. The most com-<br>mon usage is in data warehousing when large amounts of data are regularly loaded<br>from a transactional system.<br>25.7.3.1<br>SQL*Loader<br>Oracle has a direct load utility, SQL*Loader, that supports fast parallel loads of large<br>amounts of data from external ﬁles. It supports a variety of data formats and it can<br>perform various ﬁltering operations on the data being loaded.<br>25.7.3.2<br>External Tables<br>Oracle allows external data sources, such as ﬂat ﬁles, to be referenced in the from<br>clause of a query as if they were regular tables. An external table is deﬁned by meta-<br>data that describe the Oracle column types and the mapping of the external data into<br>those columns. An access driver is also needed to access the external data. Oracle<br>provides a default driver for ﬂat ﬁles.<br>The external table feature is primarily intended for extraction, transformation, and<br>loading (ETL) operations in a data warehousing environment. Data can be loaded into<br>the data warehouse from a ﬂat ﬁle using<br>create table table as<br>select ... from &lt; external table &gt;<br>where ...<br>By adding operations on the data in either the select list or where clause, trans-<br>formations and ﬁltering can be done as part of the same SQL statement. Since these<br>operations can be expressed either in native SQL or in functions written in PL/SQL or<br>Java, the external table feature provides a very powerful mechanism for expressing<br>all kinds of data transformation and ﬁltering operations. For scalability, the access to<br>the external table can be parallelized by Oracle’s parallel execution feature.<br>25.8<br>Database Administration Tools<br>Oracle provides users a number of tools for system management and application<br>development.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>III. Object−Based <br>Databases and XML<br>8. Object−Oriented <br>Databases<br>336<br>© The McGraw−Hill <br>Companies, 2001<br>Bibliographical Notes<br>947<br>25.8.1<br>Oracle Enterprise Manager<br>Oracle Enterprise Manager is Oracle’s main tool for database systems management.<br>It provides an easy-to-use graphical user interface (GUI) and a variety of wizards for<br>schema management, security management, instance management, storage manage-<br>ment, and job scheduling. It also provides performance monitoring and tools to help<br>an administrator tune application SQL, access paths, and instance and data storage<br>parameters. For example, it includes a wizard that can suggest what indices are the<br>most cost-effective to create under a given workload.<br>25.8.2<br>Database Resource Management<br>A database administrator needs to be able to control how the processing power of<br>the hardware is divided among users or groups of users. Some groups may execute<br>interactive queries where response time is critical; others may execute long-running<br>reports that can be run as batch jobs in the background when the system load is low.<br>It is also important to be able to prevent a user from inadvertently submitting an<br>extremely expensive ad hoc query that will unduly delay other users.<br>Oracle’s Database Resource Management feature allows the database administra-<br>tor to divide users into resource consumer groups with different priorities and prop-<br>erties. For example, a group of high-priority, interactive users may be guaranteed at<br>least 60 percent of the CPU. The remainder, plus any part of the 60 percent not used<br>up by the high-priority group, would be allocated among resource consumer groups<br>with lower priority. A really low-priority group could get assigned 0 percent, which<br>would mean that queries issued by this group would run only when there are spare<br>CPU cycles available. Limits for the degree of parallelism for parallel execution can<br>be set for each group. The database administrator can also set time limits for how<br>long an SQL statement is allowed to run for each group. When a users submits a<br>statement, the Resource Manager estimates how long it would take to execute it and<br>returns an error if the statement violates the limit. The resource manager can also<br>limit the number of user sessions that can be active concurrently for each resource<br>consumer group.<br>Bibliographical Notes<br>Up-to-date product information, including documentation, on Oracle products can<br>be found at the Web sites http://www.oracle.com and http://technet.oracle.com.<br>Extensible indexing in Oracle8i is described by Srinivasan et al. [2000b], while<br>Srinivasan et al. [2000a] describe index organized tables in Oracle8i. Banerjee et al.<br>[2000] describe XML support in Oracle8i. Bello et al. [1998] describe materialized<br>views in Oracle. Antoshenkov [1995] describes the byte-aligned bitmap compression<br>technique used in Oracle; see also Johnson [1999b].<br>The Oracle Parallel Server is described by Bamford et al. [1998]. Recovery in Oracle<br>is described by Joshi et al. [1998] and Lahiri et al. [2001]. Messaging and queuing in<br>Oracle are described by Gawlick [1998].<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>III. Object−Based <br>Databases and XML<br>9. Object−Relational <br>Databases<br>337<br>© The McGraw−Hill <br>Companies, 2001<br>C<br>H<br>A<br>P<br>T<br>E<br>R<br>9<br>Object-Relational Databases<br>Persistent programming languages add persistence and other database features to ex-<br>isting programming languages by using an existing object-oriented type system. In<br>contrast, object-relational data models extend the relational data model by providing a<br>richer type system including complex data types and object orientation. Relational<br>query languages, in particular SQL, need to be correspondingly extended to deal<br>with the richer type system. Such extensions attempt to preserve the relational foun-<br>dations—in particular, the declarative access to data—while extending the model-<br>ing power. Object-relational database systems (that is, database systems based on<br>the object-relation model) provide a convenient migration path for users of relational<br>databases who wish to use object-oriented features.<br>We ﬁrst present the motivation for the nested relational model, which allows rela-<br>tions that are not in ﬁrst normal form, and allows direct representation of hierarchical<br>structures. We then show how to extend SQL by adding a variety of object-relational<br>features. Our discussion is based on the SQL:1999 standard.<br>Finally, we discuss differences between persistent programming languages and<br>object-relational systems, and mention criteria for choosing between them.<br>9.1<br>Nested Relations<br>In Chapter 7, we deﬁned ﬁrst normal form (1NF), which requires that all attributes<br>have atomic domains. Recall that a domain is atomic if elements of the domain are<br>considered to be indivisible units.<br>The assumption of 1NF is a natural one in the bank examples we have considered.<br>However, not all applications are best modeled by 1NF relations. For example, rather<br>than view a database as a set of records, users of certain applications view it as a set of<br>objects (or entities). These objects may require several records for their representation.<br>We shall see that a simple, easy-to-use interface requires a one-to-one correspondence<br>335<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>III. Object−Based <br>Databases and XML<br>9. Object−Relational <br>Databases<br>338<br>© The McGraw−Hill <br>Companies, 2001<br>336<br>Chapter 9<br>Object-Relational Databases<br>title<br>author-set<br>publisher<br>keyword-set<br>(name, branch)<br>Compilers<br>{Smith, Jones}<br>(McGraw-Hill, New York)<br>{parsing, analysis}<br>Networks<br>{Jones, Frick}<br>(Oxford, London)<br>{Internet, Web}<br>Figure 9.1<br>Non-1NF books relation, books.<br>between the user’s intuitive notion of an object and the database system’s notion of<br>a data item.<br>The nested relational model is an extension of the relational model in which do-<br>mains may be either atomic or relation valued. Thus, the value of a tuple on an at-<br>tribute may be a relation, and relations may be contained within relations. A complex<br>object thus can be represented by a single tuple of a nested relation. If we view a tu-<br>ple of a nested relation as a data item, we have a one-to-one correspondence between<br>data items and objects in the user’s view of the database.<br>We illustrate nested relations by an example from a library. Suppose we store for<br>each book the following information:<br>• Book title<br>• Set of authors<br>• Publisher<br>• Set of keywords<br>We can see that, if we deﬁne a relation for the preceding information, several domains<br>will be nonatomic.<br>• Authors. A book may have a set of authors. Nevertheless, we may want to<br>ﬁnd all books of which Jones was one of the authors. Thus, we are interested<br>in a subpart of the domain element “set of authors.”<br>• Keywords. If we store a set of keywords for a book, we expect to be able to<br>retrieve all books whose keywords include one or more keywords. Thus, we<br>view the domain of the set of keywords as nonatomic.<br>• Publisher. Unlike keywords and authors, publisher does not have a set-valued<br>domain. However, we may view publisher as consisting of the subﬁelds name<br>and branch. This view makes the domain of publisher nonatomic.<br>Figure 9.1 shows an example relation, books. The books relation can be represented<br>in 1NF, as in Figure 9.2. Since we must have atomic domains in 1NF, yet want ac-<br>cess to individual authors and to individual keywords, we need one tuple for each<br>(keyword, author) pair. The publisher attribute is replaced in the 1NF version by two<br>attributes: one for each subﬁeld of publisher.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>III. Object−Based <br>Databases and XML<br>9. Object−Relational <br>Databases<br>339<br>© The McGraw−Hill <br>Companies, 2001<br>9.2<br>Complex Types<br>337<br>title<br>author<br>pub-name<br>pub-branch<br>keyword<br>Compilers<br>Smith<br>McGraw-Hill<br>New York<br>parsing<br>Compilers<br>Jones<br>McGraw-Hill<br>New York<br>parsing<br>Compilers<br>Smith<br>McGraw-Hill<br>New York<br>analysis<br>Compilers<br>Jones<br>McGraw-Hill<br>New York<br>analysis<br>Networks<br>Jones<br>Oxford<br>London<br>Internet<br>Networks<br>Frick<br>Oxford<br>London<br>Internet<br>Networks<br>Jones<br>Oxford<br>London<br>Web<br>Networks<br>Frick<br>Oxford<br>London<br>Web<br>Figure 9.2<br>ﬂat-books, a 1NF version of non-1NF relation books.<br>Much of the awkwardness of the ﬂat-books relation in Figure 9.2 disappears if we<br>assume that the following multivalued dependencies hold:<br>• title →→author<br>• title →→keyword<br>• title →pub-name, pub-branch<br>Then, we can decompose the relation into 4NF using the schemas:<br>• authors(title, author)<br>• keywords(title, keyword)<br>• books4(title, pub-name, pub-branch)<br>Figure 9.3 shows the projection of the relation ﬂat-books of Figure 9.2 onto the preced-<br>ing decomposition.<br>Although our example book database can be adequately expressed without using<br>nested relations, the use of nested relations leads to an easier-to-understand model:<br>The typical user of an information-retrieval system thinks of the database in terms of<br>books having sets of authors, as the non-1NF design models. The 4NF design would<br>require users to include joins in their queries, thereby complicating interaction with<br>the system.<br>We could deﬁne a non-nested relational view (whose contents are identical to ﬂat-<br>books) that eliminates the need for users to write joins in their query. In such a view,<br>however, we lose the one-to-one correspondence between tuples and books.<br>9.2<br>Complex Types<br>Nested relations are just one example of extensions to the basic relational model;<br>other nonatomic data types, such as nested records, have also proved useful. The<br>object-oriented data model has caused a need for features such as inheritance and<br>references to objects. With complex type systems and object orientation, we can rep-<br>resent E-R model concepts, such as identity of entities, multivalued attributes, and<br>generalization and specialization directly, without a complex translation to the rela-<br>tional model.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>III. Object−Based <br>Databases and XML<br>9. Object−Relational <br>Databases<br>340<br>© The McGraw−Hill <br>Companies, 2001<br>338<br>Chapter 9<br>Object-Relational Databases<br>title<br>author<br>Compilers<br>Smith<br>Compilers<br>Jones<br>Networks<br>Jones<br>Networks<br>Frick<br>authors<br>title<br>keyword<br>Compilers<br>parsing<br>Compilers<br>analysis<br>Networks<br>Internet<br>Networks<br>Web<br>keywords<br>title<br>pub-name<br>pub-branch<br>Compilers<br>McGraw-Hill<br>New York<br>Networks<br>Oxford<br>London<br>books4<br>Figure 9.3<br>4NF version of the relation ﬂat-books of Figure 9.2.<br>In this section, we describe extensions to SQL to allow complex types, includ-<br>ing nested relations, and object-oriented features. Our presentation is based on the<br>SQL:1999 standard, but we also outline features that are not currently in the standard<br>but may be introduced in future versions of SQL standards.<br>9.2.1<br>Collection and Large Object Types<br>Consider this fragment of code.<br>create table books (<br>. . .<br>keyword-set setof(varchar(20))<br>. . .<br>)<br>This table deﬁnition differs from table deﬁnitions in ordinary relational databases,<br>since it allows attributes that are sets, thereby permitting multivalued attributes of<br>E-R diagrams to be represented directly.<br>Sets are an instance of collection types. Other instances of collection types include<br>arrays and multisets (that is, unordered collections, where an element may occur<br>multiple times). The following attribute deﬁnitions illustrate the declaration of an<br>array:<br>author-array varchar(20) array [10]<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>III. Object−Based <br>Databases and XML<br>9. Object−Relational <br>Databases<br>341<br>© The McGraw−Hill <br>Companies, 2001<br>9.2<br>Complex Types<br>339<br>Here, author-array is an array of up to 10 author names. We can access elements of an<br>array by specifying the array index, for example author-array[1].<br>Arrays are the only collection type supported by SQL:1999; the syntax used is as<br>in the preceding declaration. SQL:1999 does not support unordered sets or multisets,<br>although they may appear in future versions of SQL.1<br>Many current-generation database applications need to store attributes that can<br>be large (of the order of many kilobytes), such as a photograph of a person, or very<br>large (of the order of many megabytes or even gigabytes), such as a high-resolution<br>medical image or video clip. SQL:1999 therefore provides new large-object data types<br>for character data (clob) and binary data (blob). The letters “lob” in these data types<br>stand for “Large OBject”. For example, we may declare attributes<br>book-review clob(10KB)<br>image blob(10MB)<br>movie blob(2GB))<br>Large objects are typically used in external applications, and it makes little sense to<br>retrieve them in their entirety by SQL. Instead, an application would usually retrieve<br>a “locator” for a large object and then use the locator to manipulate the object from<br>the host language. For instance, JDBC permits the programmer to fetch a large object<br>in small pieces, rather than all at once, much like fetching data from an operating<br>system ﬁle.<br>9.2.2<br>Structured Types<br>Structured types can be declared and used in SQL:1999 as in the following example:<br>create type Publisher as<br>(name varchar(20),<br>branch varchar(20))<br>create type Book as<br>(title varchar(20),<br>author-array varchar(20) array [10],<br>pub-date date,<br>publisher Publisher,<br>keyword-set setof(varchar(20)))<br>create table books of Book<br>The ﬁrst statement deﬁnes a type called Publisher, which has two components: a name<br>and a branch. The second statement deﬁnes a structured type Book, which contains<br>a title, an author-array, which is an array of authors, a publication date, a publisher<br>(of type Publisher), and a set of keywords. (The declaration of keyword-set as a set<br>uses our extended syntax, and is not supported by the SQL:1999 standard.) The types<br>illustrated above are called structured types in SQL:1999.<br>1.<br>The Oracle 8 database system supports nested relations, but uses a syntax different from that in this<br>chapter.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>III. Object−Based <br>Databases and XML<br>9. Object−Relational <br>Databases<br>342<br>© The McGraw−Hill <br>Companies, 2001<br>340<br>Chapter 9<br>Object-Relational Databases<br>Finally, a table books containing tuples of type Book is created. The table is similar<br>to the nested relation books in Figure 9.1, except we have decided to create an array<br>of author names instead of a set of author names. The array permits us to record the<br>order of author names.<br>Structured types allow composite attributes of E-R diagrams to be represented<br>directly. Unnamed row types can also be used in SQL:1999 to deﬁne composite at-<br>tributes. For instance, we could have deﬁned an attribute publisher1 as<br>publisher1 row (name varchar(20),<br>branch varchar(20))<br>instead of creating a named type Publisher.<br>We can of course create tables without creating an intermediate type for the table.<br>For example, the table books could also be deﬁned as follows:<br>create table books<br>(title varchar(20),<br>author-array varchar(20) array[10],<br>pub-date date,<br>publisher Publisher,<br>keyword-set setof(varchar(20)))<br>With the above declaration, there is no explicit type for rows of the table. 2<br>A structured type can have methods deﬁned on it. We declare methods as part of<br>the type deﬁnition of a structured type:<br>create type Employee as (<br>name varchar(20),<br>salary integer )<br>method giveraise (percent integer)<br>We create the method body separately:<br>create method giveraise (percent integer) for Employee<br>begin<br>set self.salary = self.salary + (self.salary * percent) / 100;<br>end<br>The variable self refers to the structured type instance on which the method is in-<br>voked. The body of the method can contain procedural statements, which we shall<br>study in Section 9.6.<br>2.<br>In Oracle PL/SQL, given a table t, t%rowtype denotes the type of the rows of the table. Similarly,<br>t.a%type denotes the type of attribute a of table t.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>III. Object−Based <br>Databases and XML<br>9. Object−Relational <br>Databases<br>343<br>© The McGraw−Hill <br>Companies, 2001<br>9.2<br>Complex Types<br>341<br>9.2.3<br>Creation of Values of Complex Types<br>In SQL:1999 constructor functions are used to create values of structured types. A<br>function with the same name as a structured type is a constructor function for the<br>structured type. For instance, we could declare a constructor for the type Publisher<br>like this:<br>create function Publisher (n varchar(20), b varchar(20))<br>returns Publisher<br>begin<br>set name = n;<br>set branch = b;<br>end<br>We can then use Publisher(’McGraw-Hill’, ’New York’) to create a value of the type<br>Publisher.<br>SQL:1999 also supports functions other than constructors, as we shall see in Sec-<br>tion 9.6; the names of such functions must be different from the name of any struc-<br>tured type.<br>Note that in SQL:1999, unlike in object-oriented databases, a constructor creates a<br>value of the type, not an object of the type. That is, the value the constructor creates<br>has no object identity. In SQL:1999 objects correspond to tuples of a relation, and are<br>created by inserting a tuple in a relation.<br>By default every structured type has a constructor with no arguments, which sets<br>the attributes to their default values. Any other constructors have to be created explic-<br>itly. There can be more than one constructor for the same structured type; although<br>they have the same name, they must be distinguishable by the number of arguments<br>and types of their arguments.<br>An array of values can be created in SQL:1999 in this way:<br>array[’Silberschatz’, ’Korth’, ’Sudarshan’]<br>We can construct a row value by listing its attributes within parentheses. For instance,<br>if we declare an attribute publisher1 as a row type (as in Section 9.2.2), we can con-<br>struct this value for it:<br>(’McGraw-Hill’, ’New York’)<br>without using a constructor.<br>We creat</span><br><br><span style="background-color: #FDFFB6;" title="Chunk 42 | Start: 840084 | End: 860084 | Tokens: 3179">e set-valued attributes, such as keyword-set, by enumerating their elements<br>within parentheses following the keyword set. We can create multiset values just like<br>set values, by replacing set by multiset.3<br>Thus, we can create a tuple of the type deﬁned by the books relation as:<br>(’Compilers’, array[’Smith’, ’Jones’], Publisher(’McGraw-Hill’, ’New York’),<br>set(’parsing’, ’analysis’))<br>3.<br>Although sets and multisets are not part of the SQL:1999 standard, the other constructs shown in this<br>section are part of the standard. Future versions of SQL are likely to support sets and multisets.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>III. Object−Based <br>Databases and XML<br>9. Object−Relational <br>Databases<br>344<br>© The McGraw−Hill <br>Companies, 2001<br>342<br>Chapter 9<br>Object-Relational Databases<br>Here we have created a value for the attribute Publisher by invoking a constructor<br>function for Publisher with appropriate arguments.<br>If we want to insert the preceding tuple into the relation books, we could execute<br>the statement<br>insert into books<br>values<br>(’Compilers’, array[’Smith’, ’Jones’], Publisher(’McGraw-Hill’, ’New York’),<br>set(’parsing’, ’analysis’))<br>9.3<br>Inheritance<br>Inheritance can be at the level of types, or at the level of tables. We ﬁrst consider<br>inheritance of types, then inheritance at the level of tables.<br>9.3.1<br>Type Inheritance<br>Suppose that we have the following type deﬁnition for people:<br>create type Person<br>(name varchar(20),<br>address varchar(20))<br>We may want to store extra information in the database about people who are stu-<br>dents, and about people who are teachers. Since students and teachers are also peo-<br>ple, we can use inheritance to deﬁne the student and teacher types in SQL:1999:<br>create type Student<br>under Person<br>(degree varchar(20),<br>department varchar(20))<br>create type Teacher<br>under Person<br>(salary integer,<br>department varchar(20))<br>Both Student and Teacher inherit the attributes of Person—namely, name and address.<br>Student and Teacher are said to be subtypes of Person, and Person is a supertype of<br>Student, as well as of Teacher.<br>Methods of a structured type are inherited by its subtypes, just as attributes are.<br>However, a subtype can redeﬁne the effect of a method by declaring the method<br>again, using overriding method in place of method in the method declaration.<br>Now suppose that we want to store information about teaching assistants, who<br>are simultaneously students and teachers, perhaps even in different departments.<br>We can do this by using multiple inheritance, which we studied in Chapter 8. The<br>SQL:1999 standard does not support multiple inheritance. However, draft versions<br>of the SQL:1999 standard provided for multiple inheritance, and although the ﬁnal<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>III. Object−Based <br>Databases and XML<br>9. Object−Relational <br>Databases<br>345<br>© The McGraw−Hill <br>Companies, 2001<br>9.3<br>Inheritance<br>343<br>SQL:1999 omitted it, future versions of the SQL standard may introduce it. We base<br>our discussion on the draft versions of the SQL:1999 standard.<br>For instance, if our type system supports multiple inheritance, we can deﬁne a<br>type for teaching assistant as follows:<br>create type TeachingAssistant<br>under Student, Teacher<br>TeachingAssistant would inherit all the attributes of Student and Teacher. There is a<br>problem, however, since the attributes name, address, and department are present in<br>Student, as well as in Teacher.<br>The attributes name and address are actually inherited from a common source, Per-<br>son. So there is no conﬂict caused by inheriting them from Student as well as Teacher.<br>However, the attribute department is deﬁned separately in Student and Teacher. In fact,<br>a teaching assistant may be a student of one department and a teacher in another<br>department. To avoid a conﬂict between the two occurrences of department, we can<br>rename them by using an as clause, as in this deﬁnition of the type TeachingAssistant:<br>create type TeachingAssistant<br>under Student with (department as student-dept),<br>Teacher with (department as teacher-dept)<br>We note that SQL:1999 supports only single inheritance— that is, a type can inherit<br>from only a single type; the syntax used is as in our earlier examples. Multiple inher-<br>itance as in the TeachingAssistant example is not supported in SQL:1999. The SQL:1999<br>standard also requires an extra ﬁeld at the end of the type deﬁnition, whose value<br>is either ﬁnal or not ﬁnal. The keyword ﬁnal says that subtypes may not be created<br>from the given type, while not ﬁnal says that subtypes may be created.<br>In SQL as in most other languages, a value of a structured type must have exactly<br>one “most-speciﬁc type.” That is, each value must be associated with one speciﬁc<br>type, called its most-speciﬁc type, when it is created. By means of inheritance, it<br>is also associated with each of the supertypes of its most speciﬁc type. For example,<br>suppose that an entity has the type Person, as well as the type Student. Then, the most-<br>speciﬁc type of the entity is Student, since Student is a subtype of Person. However, an<br>entity cannot have the type Student, as well as the type Teacher, unless it has a type,<br>such as TeachingAssistant, that is a subtype of Teacher, as well as of Student.<br>9.3.2<br>Table Inheritance<br>Subtables in SQL:1999 correspond to the E-R notion of specialization/generalization.<br>For instance, suppose we deﬁne the people table as follows:<br>create table people of Person<br>We can then deﬁne tables students and teachers as subtables of people, as follows:<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>III. Object−Based <br>Databases and XML<br>9. Object−Relational <br>Databases<br>346<br>© The McGraw−Hill <br>Companies, 2001<br>344<br>Chapter 9<br>Object-Relational Databases<br>create table students of Student<br>under people<br>create table teachers of Teacher<br>under people<br>The types of the subtables must be subtypes of the type of the parent table. Thereby,<br>every attribute present in people is also present in the subtables.<br>Further, when we declare students and teachers as subtables of people, every tuple<br>present in students or teachers becomes also implicitly present in people. Thus, if a<br>query uses the table people, it will ﬁnd not only tuples directly inserted into that table,<br>but also tuples inserted into its subtables, namely students and teachers. However,<br>only those attributes that are present in people can be accessed.<br>Multiple inheritance is possible with tables, just as it is possible with types. (We<br>note, however, that multiple inheritance of tables is not supported by SQL:1999.) For<br>example, we can create a table of type TeachingAssistant:<br>create table teaching-assistants<br>of TeachingAssistant<br>under students, teachers<br>As a result of the declaration, every tuple present in the teaching-assistants table is<br>also implicitly present in the teachers and in the students table, and in turn in the<br>people table.<br>SQL:1999 permits us to ﬁnd tuples that are in people but not in its subtables by using<br>“only people” in place of people in a query.<br>There are some consistency requirements for subtables. Before we state the con-<br>straints, we need a deﬁnition: We say that tuples in a subtable corresponds to tuples<br>in a parent table if they have the same values for all inherited attributes. Thus, corre-<br>sponding tuples represent the same entity.<br>The consistency requirements for subtables are:<br>1. Each tuple of the supertable can correspond to at most one tuple in each of its<br>immediate subtables.<br>2. SQL:1999 has an additional constraint that all the tuples corresponding to each<br>other must be derived from one tuple (inserted into one table).<br>For example, without the ﬁrst condition, we could have two tuples in students (or<br>teachers) that correspond to the same person.<br>The second condition rules out a tuple in people corresponding to both a tuple in<br>students and a tuple in teachers, unless all these tuples are implicitly present because<br>a tuple was inserted in a table teaching-assistants, which is a subtable of both teachers<br>and students.<br>Since SQL:1999 does not support multiple inheritance, the second condition actu-<br>ally prevents a person from being both a teacher and a student. The same problem<br>would arise if the subtable teaching-assistants is absent, even if multiple inheritance<br>were supported. Obviously it would be useful to model a situation where a person<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>III. Object−Based <br>Databases and XML<br>9. Object−Relational <br>Databases<br>347<br>© The McGraw−Hill <br>Companies, 2001<br>9.3<br>Inheritance<br>345<br>can be a teacher and a student, even if a common subtable teaching-assistants is not<br>present. Thus, it can be useful to remove the second consistency constraint. We return<br>to this issue in Section 9.3.3.<br>Subtables can be stored in an efﬁcient manner without replication of all inherited<br>ﬁelds, in one of two ways:<br>• Each table stores the primary key (which may be inherited from a parent table)<br>and the attributes deﬁned locally. Inherited attributes (other than the primary<br>key) do not need to be stored, and can be derived by means of a join with the<br>supertable, based on the primary key.<br>• Each table stores all inherited and locally deﬁned attributes. When a tuple is<br>inserted, it is stored only in the table in which it is inserted, and its presence is<br>inferred in each of the supertables. Access to all attributes of a tuple is faster,<br>since a join is not required. However, in case the second consistency constraint<br>is absent—that is, an entity can be represented in two subtables without be-<br>ing present in a common subtable of both—this representation can result in<br>replication of information.<br>9.3.3<br>Overlapping Subtables<br>Inheritance of types should be used with care. A university database may have many<br>subtypes of Person, such as Student, Teacher, FootballPlayer, ForeignCitizen, and so on.<br>Student may itself have subtypes such as UndergraduateStudent, GraduateStudent, and<br>PartTimeStudent. Clearly, a person can belong to several of these categories at once.<br>As Chapter 8 mentions, each of these categories is sometimes called a role.<br>For each entity to have exactly one most-speciﬁc type, we would have to create<br>a subtype for every possible combination of the supertypes. In the preceding exam-<br>ple, we would have subtypes such as ForeignUndergraduateStudent, ForeignGraduate-<br>StudentFootballPlayer, and so on. Unfortunately, we would end up with an enormous<br>number of subtypes of Person.<br>A better approach in the context of database systems is to allow an object to have<br>multiple types, without having a most-speciﬁc type. Object-relational systems can<br>model such a feature by using inheritance at the level of tables, rather than of types,<br>and allowing an entity to exist in more than one table at once.<br>For example, suppose we again have the type Person, with subtypes Student and<br>Teacher, and the corresponding table people, with subtables teachers and students. We<br>can then have a tuple in teachers and a tuple in students corresponding to the same<br>tuple in people.<br>There is no need to have a type TeachingAssistant that is a subtype of both Student<br>and Teacher. We need not create a type TeachingAssistant unless we wish to store extra<br>attributes or redeﬁne methods in a manner speciﬁc to people who are both students<br>and teachers.<br>We note, however, that SQL:1999 prohibits such a situation, because of consistency<br>requirement 2 from Section 9.3.2. Since SQL:1999 also does not support multiple in-<br>heritance, we cannot use inheritance to model a situation where a person can be<br>both a student and a teacher. We can of course create separate tables to represent the<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>III. Object−Based <br>Databases and XML<br>9. Object−Relational <br>Databases<br>348<br>© The McGraw−Hill <br>Companies, 2001<br>346<br>Chapter 9<br>Object-Relational Databases<br>information without using inheritance. We would have to add appropriate referen-<br>tial integrity constraints to ensure that students and teachers are also represented in<br>the people table.<br>9.4<br>Reference Types<br>Object-oriented languages provide the ability to refer to objects. An attribute of a type<br>can be a reference to an object of a speciﬁed type. For example, in SQL:1999 we can<br>deﬁne a type Department with a ﬁeld name and a ﬁeld head which is a reference to the<br>type Person, and a table departments of type Department, as follows:<br>create type Department (<br>name varchar(20),<br>head ref(Person) scope people<br>)<br>create table departments of Department<br>Here, the reference is restricted to tuples of the table people. The restriction of the<br>scope of a reference to tuples of a table is mandatory in SQL:1999, and it makes refer-<br>ences behave like foreign keys.<br>We can omit the declaration scope people from the type declaration and instead<br>make an addition to the create table statement:<br>create table departments of Department<br>(head with options scope people)<br>In order to initialize a reference attribute, we need to get the identiﬁer of the tuple<br>that is to be referenced. We can get the identiﬁer value of a tuple by means of a query.<br>Thus, to create a tuple with the reference value, we may ﬁrst create the tuple with a<br>null reference and then set the reference separately:<br>insert into departments<br>values (’CS’, null)<br>update departments<br>set head = (select ref(p)<br>from people as p<br>where name = ’John’)<br>where name = ’CS’<br>This syntax for accessing the identiﬁer of a tuple is based on the Oracle syntax.<br>SQL:1999 adopts a different approach, one where the referenced table must have an<br>attribute that stores the identiﬁer of the tuple. We declare this attribute, called the<br>self-referential attribute, by adding a ref is clause to the create table statement:<br>create table people of Person<br>ref is oid system generated<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>III. Object−Based <br>Databases and XML<br>9. Object−Relational <br>Databases<br>349<br>© The McGraw−Hill <br>Companies, 2001<br>9.4<br>Reference Types<br>347<br>Here, oid is an attribute name, not a keyword. The subquery above would then use<br>select p.oid<br>instead of select ref(p).<br>An alternative to system-generated identiﬁers is to allow users to generate iden-<br>tiﬁers. The type of the self-referential attribute must be speciﬁed as part of the type<br>deﬁnition of the referenced table, and the table deﬁnition must specify that the refer-<br>ence is user generated:<br>create type Person<br>(name varchar(20),<br>address varchar(20))<br>ref using varchar(20)<br>create table people of Person<br>ref is oid user generated<br>When inserting a tuple in people, we must provide a value for the identiﬁer:<br>insert into people values<br>(’01284567’, ’John’, ’23 Coyote Run’)<br>No other tuple for people or its supertables or subtables can have the same identiﬁer.<br>We can then use the identiﬁer value when inserting a tuple into departments, without<br>the need for a separate query to retrieve the identiﬁer:<br>insert into departments<br>values (’CS’, ’01284567’)<br>It is even possible to use an existing primary key value as the identiﬁer, by includ-<br>ing the ref from clause in the type deﬁnition:<br>create type Person<br>(name varchar(20) primary key,<br>address varchar(20))<br>ref from(name)<br>create table people of Person<br>ref is oid derived<br>Note that the table deﬁnition must specify that the reference is derived, and must still<br>specify a self-referential attribute name. When inserting a tuple for departments, we<br>can then use<br>insert into departments<br>values (’CS’, ’John’)<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>III. Object−Based <br>Databases and XML<br>9. Object−Relational <br>Databases<br>350<br>© The McGraw−Hill <br>Companies, 2001<br>348<br>Chapter 9<br>Object-Relational Databases<br>9.5<br>Querying with Complex Types<br>In this section, we present extensions of the SQL query language to deal with complex<br>types. Let us start with a simple example: Find the title and the name of the publisher<br>of each book. This query carries out the task:<br>select title, publisher.name<br>from books<br>Notice that the ﬁeld name of the composite attribute publisher is referred to by a dot<br>notation.<br>9.5.1<br>Path Expressions<br>References are dereferenced in SQL:1999 by the −&gt; symbol. Consider the departments<br>table deﬁned earlier. We can use this query to ﬁnd the names and addresses of the<br>heads of all departments:<br>select head−&gt;name, head−&gt;address<br>from departments<br>An expression such as “head−&gt;name” is called a path expression.<br>Since head is a reference to a tuple in the people table, the attribute name in the<br>preceding query is the name attribute of the tuple from the people table. References can<br>be used to hide join operations; in the preceding example, without the references, the<br>head ﬁeld of department would be declared a foreign key of the table people. To ﬁnd<br>the name and address of the head of a department, we would require an explicit<br>join of the relations departments and people. The use of references simpliﬁes the query<br>considerably.<br>9.5.2<br>Collection-Valued Attributes<br>We now consider how to handle collection-valued attributes. Arrays are the only<br>collection type supported by SQL:1999, but we use the same syntax for relation-valued<br>attributes also. An expression evaluating to a collection can appear anywhere that a<br>relation name may appear, such as in a from clause, as the following paragraphs<br>illustrate. We use the table books which we deﬁned earlier.<br>If we want to ﬁnd all books that have the word “database” as one of their key-<br>words, we can use this query:<br>select title<br>from books<br>where ’database’ in (unnest(keyword-set))<br>Note that we have used unnest(keyword-set) in a position where SQL without nested<br>relations would have required a select-from-where subexpression.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>III. Object−Based <br>Databases and XML<br>9. Object−Relational <br>Databases<br>351<br>© The McGraw−Hill <br>Companies, 2001<br>9.5<br>Querying with Complex Types<br>349<br>If we know that a particular book has three authors, we could write:<br>select author-array[1], author-array[2], author-array[3]<br>from books<br>where title = ’Database System Concepts’<br>Now, suppose that we want a relation containing pairs of the form “title, author-<br>name” for each book and each author of the book. We can use this query:<br>select B.title, A.name<br>from books as B, unnest(B.author-array) as A<br>Since the author-array attribute of books is a collection-valued ﬁeld, it can be used in a<br>from clause, where a relation is expected.<br>9.5.3<br>Nesting and Unnesting<br>The transformation of a nested relation into a form with fewer (or no) relation-valued<br>attributes is called unnesting. The books relation has two attributes, author-array and<br>keyword-set, that are collections, and two attributes, title and publisher, that are not.<br>Suppose that we want to convert the relation into a single ﬂat relation, with no nested<br>relations or structured types as attributes. We can use the following query to carry out<br>the task:<br>select title, A as author, publisher.name as pub-name, publisher.branch<br>as pub-branch, K as keyword<br>from books as B, unnest(B.author-array) as A, unnest (B.keyword-set) as K<br>The variable B in the from clause is declared to range over books. The variable A is<br>declared to range over the authors in author-array for the book B, and K is declared to<br>range over the keywords in the keyword-set of the book B. Figure 9.1 (in Section 9.1)<br>shows an instance books relation, and Figure 9.2 shows the 1NF relation that is the<br>result of the preceding query.<br>The reverse process of transforming a 1NF relation into a nested relation is called<br>nesting. Nesting can be carried out by an extension of grouping in SQL. In the normal<br>use of grouping in SQL, a temporary multiset relation is (logically) created for each<br>group, and an aggregate function is applied on the temporary relation. By return-<br>ing the multiset instead of applying the aggregate function, we can create a nested<br>relation. Suppose that we are given a 1NF relation ﬂat-books, as in Figure 9.2. The<br>following query nests the relation on the attrib</span><br><br><span style="background-color: #CAFFBF;" title="Chunk 43 | Start: 860086 | End: 880086 | Tokens: 3095">ute keyword:<br>select title, author, Publisher(pub-name, pub-branch) as publisher,<br>set(keyword) as keyword-set<br>from ﬂat-books<br>groupby title, author, publisher<br>The result of the query on the books relation from Figure 9.2 appears in Figure 9.4.<br>If we want to nest the author attribute as well, and thereby to convert the 1NF table<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>III. Object−Based <br>Databases and XML<br>9. Object−Relational <br>Databases<br>352<br>© The McGraw−Hill <br>Companies, 2001<br>350<br>Chapter 9<br>Object-Relational Databases<br>title<br>author<br>publisher<br>keyword-set<br>(pub-name, pub-branch)<br>Compilers<br>Smith<br>(McGraw-Hill, New York)<br>{parsing, analysis}<br>{parsing, analysis}<br>Compilers<br>Jones<br>(McGraw-Hill, New York)<br>Networks<br>Jones<br>(Oxford, London)<br>{Internet, Web}<br>{Internet, Web}<br>Networks<br>Frick<br>(Oxford, London)<br>Figure 9.4<br>A partially nested version of the ﬂat-books relation.<br>ﬂat-books in Figure 9.2 back to the nested table books in Figure 9.1, we can use the<br>query:<br>select title, set(author) as author-set, Publisher(pub-name, pub-branch) as publisher,<br>set(keyword) as keyword-set<br>from ﬂat-books<br>groupby title, publisher<br>Another approach to creating nested relations is to use subqueries in the select<br>clause. The following query, which performs the same task as the previous query,<br>illustrates this approach.<br>select title,<br>( select author<br>from ﬂat-books as M<br>where M.title = O.title) as author-set,<br>Publisher(pub-name, pub-branch) as publisher,<br>( select keyword<br>from ﬂat-books as N<br>where N.title = O.title) as keyword-set,<br>from ﬂat-books as O<br>The system executes the nested subqueries in the select clause for each tuple gen-<br>erated by the from and where clauses of the outer query. Observe that the attribute<br>O.title from the outer query is used in the nested queries, to ensure that only the<br>correct sets of authors and keywords are generated for each title. An advantage of<br>this approach is that an orderby clause can be used in the nested query, to generate<br>results in a desired order. An array or a list could be constructed from the result of<br>the nested query. Without such an ordering, arrays and lists would not be uniquely<br>determined.<br>We note that while unnesting of array-valued attributes can be carried out in<br>SQL:1999 as shown above, the reverse process of nesting is not supported in SQL:1999.<br>The extensions we have shown for nesting illustrate features from some proposals<br>for extending SQL, but are not part of any standard currently.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>III. Object−Based <br>Databases and XML<br>9. Object−Relational <br>Databases<br>353<br>© The McGraw−Hill <br>Companies, 2001<br>9.6<br>Functions and Procedures<br>351<br>9.6<br>Functions and Procedures<br>SQL:1999 allows the deﬁnition of functions, procedures, and methods. These can be<br>deﬁned either by the procedural component of SQL:1999, or by an external program-<br>ming language such as Java, C, or C++. We look at deﬁnitions in SQL:1999 ﬁrst, and<br>then see how to use deﬁnitions in external languages. Several database systems sup-<br>port their own procedural languages, such as PL/SQL in Oracle and TransactSQL in<br>Microsoft SQLServer. These resemble the procedural part of SQL:1999, but there are<br>differences in syntax and semantics; see the respective system manuals for further<br>details.<br>9.6.1<br>SQL Functions and Procedures<br>Suppose that we want a function that, given the title of a book, returns the count of<br>the number of authors, using the 4NF schema. We can deﬁne the function this way:<br>create function author-count(title varchar(20))<br>returns integer<br>begin<br>declare a-count integer;<br>select count(author) into a-count<br>from authors<br>where authors.title = title<br>return a-count;<br>end<br>This function can be used in a query that returns the titles of all books that have<br>more than one author:<br>select title<br>from books4<br>where author-count(title) &gt; 1<br>Functions are particularly useful with specialized data types such as images and<br>geometric objects. For instance, a polygon data type used in a map database may<br>have an associated function that checks if two polygons overlap, and an image data<br>type may have associated functions to compare two images for similarity. Functions<br>may be written in an external language such as C, as we see in Section 9.6.2. Some<br>database systems also support functions that return relations, that is, multisets of<br>tuples, although such functions are not supported by SQL:1999.<br>Methods, which we saw in Section 9.2.2, can be viewed as functions associated<br>with structured types. They have an implicit ﬁrst parameter called self, which is set<br>to the structured type value on which the method is invoked. Thus, the body of the<br>method can refer to an attribute a of the value by using self.a. These attributes can<br>also be updated by the method.<br>SQL:1999 also supports procedures. The author-count function could instead be writ-<br>ten as a procedure:<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>III. Object−Based <br>Databases and XML<br>9. Object−Relational <br>Databases<br>354<br>© The McGraw−Hill <br>Companies, 2001<br>352<br>Chapter 9<br>Object-Relational Databases<br>create procedure author-count-proc(in title varchar(20), out a-count integer)<br>begin<br>select count(author) into a-count<br>from authors<br>where authors.title = title<br>end<br>Procedures can be invoked either from an SQL procedure or from embedded SQL<br>by the call statement:<br>declare a-count integer;<br>call author-count-proc(’Database Systems Concepts’, a-count);<br>SQL:1999 permits more than one procedure of the same name, so long as the<br>number of arguments of the procedures with the same name is different. The name,<br>along with the number of arguments, is used to identify the procedure. SQL:1999 also<br>permits more than one function with the same name, so long as the different func-<br>tions with the same name either have different numbers of arguments, or for func-<br>tions with the same number of arguments, differ in the type of at least one argument.<br>9.6.2<br>External Language Routines<br>SQL:1999 allows us to deﬁne functions in a programming language such as C or C++.<br>Functions deﬁned in this fashion can be more efﬁcient than functions deﬁned in SQL,<br>and computations that cannot be carried out in SQL can be executed by these func-<br>tions. An example of the use of such functions would be to perform a complex arith-<br>metic computation on the data in a tuple.<br>External procedures and functions can be speciﬁed in this way:<br>create procedure author-count-proc( in title varchar(20), out count integer)<br>language C<br>external name ’/usr/avi/bin/author-count-proc’<br>create function author-count (title varchar(20))<br>returns integer<br>language C<br>external name ’/usr/avi/bin/author-count’<br>The external language procedures need to deal with null values and exceptions.<br>They must therefore have several extra parameters: an sqlstate value to indicate fail-<br>ure/success status, a parameter to store the return value of the function, and indi-<br>cator variables for each parameter/function result to indicate if the value is null. An<br>extra line parameter style general added to the declaration above indicates that the<br>external procedures/functions take only the arguments shown and do not deal with<br>null values or exceptions.<br>Functions deﬁned in a programming language and compiled outside the database<br>system may be loaded and executed with the database system code. However, do-<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>III. Object−Based <br>Databases and XML<br>9. Object−Relational <br>Databases<br>355<br>© The McGraw−Hill <br>Companies, 2001<br>9.6<br>Functions and Procedures<br>353<br>ing so carries the risk that a bug in the program can corrupt the database internal<br>structures, and can bypass the access-control functionality of the database system.<br>Database systems that are concerned more about efﬁcient performance than about<br>security may execute procedures in such a fashion.<br>Database systems that are concerned about security would typically execute such<br>code as part of a separate process, communicate the parameter values to it, and fetch<br>results back, via interprocess communication.<br>If the code is written in a language such as Java, there is a third possibility: execut-<br>ing the code in a “sandbox” within the database process itself. The sandbox prevents<br>the Java code from carrying out any reads or updates directly on the database.<br>9.6.3<br>Procedural Constructs<br>SQL:1999 supports a variety of procedural constructs, which gives it almost all the<br>power of a general purpose programming language. The part of the SQL:1999<br>standard that deals with these constructs is called the Persistent Storage Module<br>(PSM).<br>A compound statement is of the form begin . . . end, and it may contain multi-<br>ple SQL statements between the begin and the end. Local variables can be declared<br>within a compound statement, as we have seen in Section 9.6.1.<br>SQL:1999 supports while statements and repeat statements by this syntax:<br>declare n integer default 0;<br>while n &lt; 10 do<br>set n = n + 1;<br>end while<br>repeat<br>set n = n −1;<br>until n = 0<br>end repeat<br>This code does not do anything useful; it is simply meant to show the syntax of while<br>and repeat loops. We will see more meaningful uses later.<br>There is also a for loop, which permits iteration over all results of a query:<br>declare n integer default 0;<br>for r as<br>select balance from account<br>where branch-name = ‘Perryridge‘<br>do<br>set n = n+ r.balance<br>end for<br>The program implicitly opens a cursor when the for loop begins execution and uses<br>it to fetch the values one row at a time into the for loop variable (r, in the above exam-<br>ple). It is possible to give a name to the cursor, by inserting the text cn cursor for just<br>after the keyword as, where cn is the name we wish to give to the cursor. The cursor<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>III. Object−Based <br>Databases and XML<br>9. Object−Relational <br>Databases<br>356<br>© The McGraw−Hill <br>Companies, 2001<br>354<br>Chapter 9<br>Object-Relational Databases<br>name can be used to perform update/delete operations on the tuple being pointed to<br>by the cursor. The statement leave can be used to exit the loop, while iterate starts on<br>the next tuple, from the beginning of the loop, skipping the remaining statements.<br>The conditional statements supported by SQL:1999 include if-then-else statements<br>statements by using this syntax:<br>if r.balance &lt; 1000<br>then set l = l+ r.balance<br>elseif r.balance &lt; 5000<br>then set m = m+ r.balance<br>else set h = h+ r.balance<br>end if<br>This code assumes that l, m, and h are integer variables, and r is a row variable. If we<br>replace the line “set n = n+ r.balance” in the for loop of the preceding paragraph by<br>the if-then-else code, the loop would compute the total balances of accounts that fall<br>under the low, medium, and high balance categories respectively.<br>SQL:1999 also supports a case statement similar to the C/C++ language case state-<br>ment (in addition to case expressions, which we saw in Chapter 4).<br>Finally, SQL:1999 includes the concept of signaling exception conditions, and dec-<br>laring handlers that can handle the exception, as in this code:<br>declare out-of-stock condition<br>declare exit handler for out-of-stock<br>begin<br>. . .<br>end<br>The statements between the begin and the end can raise an exception by executing<br>signal out-of-stock. The handler says that if the condition arises, the action to be taken<br>is to exit the enclosing begin end statement. Alternative actions would be continue,<br>which continues execution from the next statement following the one that raised the<br>exception. In addition to explicitly deﬁned conditions, there are also predeﬁned con-<br>ditions such as sqlexception, sqlwarning, and not found.<br>Figure 9.5 provides a larger example of the use of SQL:1999 procedural constructs.<br>The procedure ﬁndEmpl computes the set of all direct and indirect employees of a<br>given manager (speciﬁed by the parameter mgr), and stores the resulting employee<br>names in a relation called empl, which is assumed to exist already. The relation man-<br>ager(empname, mgrname), specifying who works directly for which manager, is as-<br>sumed to be available. The set of all direct/indirect employees is basically the transi-<br>tive closure of the relation manager. We saw how to express such a query by recursion<br>in Chapter 5 (Section 5.2.6).<br>The procedure uses two temporary tables, newemp and temp. The procedure inserts<br>all employees who directly work for mgr into newemp before the repeat loop. The<br>repeat loop ﬁrst adds all employees in newemp to empl. Next, it computes employees<br>who work for those in newemp, except those who have already been found to be<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>III. Object−Based <br>Databases and XML<br>9. Object−Relational <br>Databases<br>357<br>© The McGraw−Hill <br>Companies, 2001<br>9.6<br>Functions and Procedures<br>355<br>create procedure ﬁndEmpl(in mgr char(10))<br>– – Finds all employees who work directly or indirectly for mgr<br>– – and adds them to the relation empl(name).<br>– – The relation manager(empname, mgrname) speciﬁes who directly<br>– – works for whom.<br>begin<br>create temporary table newemp (name char(10));<br>create temporary table temp (name char(10));<br>insert into newemp<br>select empname<br>from manager<br>where mgrname = mgr<br>repeat<br>insert into empl<br>select name<br>from newemp;<br>insert into temp<br>(select manager.empname<br>from newemp, manager<br>where newemp.empname = manager.mgrname;<br>)<br>except (<br>select empname<br>from empl<br>);<br>delete from newemp;<br>insert into newemp<br>select *<br>from temp;<br>delete from temp;<br>until not exists (select * from newemp)<br>end repeat;<br>end<br>Figure 9.5<br>Finding all employees of a manager.<br>employees of mgr, and stores them in the temporary table temp. Finally, it replaces<br>the contents of newemp by the contents of temp. The repeat loop terminates when it<br>ﬁnds no new (indirect) employees.<br>We note that the use of the except clause in the procedure ensures that the proce-<br>dure works even in the (abnormal) case where there is a cycle of management. For<br>example, if a works for b, b works for c, and c works for a, there is a cycle.<br>While cycles may be unrealistic in management control, cycles are possible in other<br>applications. For instance, suppose we have a relation ﬂights(to, from) that says which<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>III. Object−Based <br>Databases and XML<br>9. Object−Relational <br>Databases<br>358<br>© The McGraw−Hill <br>Companies, 2001<br>356<br>Chapter 9<br>Object-Relational Databases<br>cities can be reached from which other cities by a direct ﬂight. We can modify the<br>ﬁndEmpl procedure to ﬁnd all cities that are reachable by a sequence of one or more<br>ﬂights from a given city. All we have to do is to replace manager by ﬂight and replace<br>attribute names correspondingly. In this situation there can be cycles of reachability,<br>but the procedure would work correctly since it would eliminate cities that have<br>already been seen.<br>9.7<br>Object-Oriented versus Object-Relational<br>We have now studied object-oriented databases built around persistent program-<br>ming languages, as well as object-relational databases, which are object-oriented data-<br>bases built on top of the relation model. Database systems of both types are on the<br>market, and a database designer needs to choose the kind of system that is appropri-<br>ate to the needs of the application.<br>Persistent extensions to programming languages and object-relational systems<br>target different markets. The declarative nature and limited power (compared to a<br>programming language) of the SQL language provides good protection of data from<br>programming errors, and makes high-level optimizations, such as reducing I/O, rel-<br>atively easy. (We cover optimization of relational expressions in Chapter 13.) Object-<br>relational systems aim at making data modeling and querying easier by using com-<br>plex data types. Typical applications include storage and querying of complex data,<br>including multimedia data.<br>A declarative language such as SQL, however, imposes a signiﬁcant performance<br>penalty for certain kinds of applications that run primarily in main memory, and<br>that perform a large number of accesses to the database. Persistent programming<br>languages target such applications that have high performance requirements. They<br>provide low-overhead access to persistent data, and eliminate the need for data trans-<br>lation if the data are to be manipulated by a programming language. However, they<br>are more susceptible to data corruption by programming errors, and usually do not<br>have a powerful querying capability. Typical applications include CAD databases.<br>We can summarize the strengths of the various kinds of database systems in this<br>way:<br>• Relational systems: simple data types, powerful query languages, high pro-<br>tection<br>• Persistent-programming-language–based OODBs: complex data types, in-<br>tegration with programming language, high performance<br>• Object-relational systems: complex data types, powerful query languages,<br>high protection<br>These descriptions hold in general, but keep in mind that some database systems blur<br>the boundaries. For example, some object-oriented database systems built around a<br>persistent programming language are implemented on top of a relational database<br>system. Such systems may provide lower performance than object-oriented database<br>systems built directly on a storage system, but provide some of the stronger protec-<br>tion guarantees of relational systems.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>III. Object−Based <br>Databases and XML<br>9. Object−Relational <br>Databases<br>359<br>© The McGraw−Hill <br>Companies, 2001<br>9.8<br>Summary<br>357<br>Many object-relational database systems are built on top of existing relational<br>database systems. To do so, the complex data types supported by object-relational<br>systems need to be translated to the simpler type system of relational databases.<br>To understand how the translation is done, we need only look at how some fea-<br>tures of the E-R model are translated into relations. For instance, multivalued at-<br>tributes in the E-R model correspond to set-valued attributes in the object-relational<br>model. Composite attributes roughly correspond to structured types. ISA hierarchies<br>in the E-R model correspond to table inheritance in the object-relational model. The<br>techniques for converting E-R model features to tables, which we saw in Section 2.9,<br>can be used, with some extensions, to translate object-relational data to relational<br>data.<br>9.8<br>Summary<br>• The object-relational data model extends the relational data model by provid-<br>ing a richer type system including collection types, and object orientation.<br>• Object orientation provides inheritance with subtypes and subtables, as well<br>as object (tuple) references.<br>• Collection types include nested relations, sets, multisets, and arrays, and the<br>object-relational model permits attributes of a table to be collections.<br>• The SQL:1999 standard extends the SQL data deﬁnition and query language to<br>deal with the new data types and with object orientation.<br>• We saw a variety of features of the extended data-deﬁnition language, as<br>well as the query language, and in particular support for collection-valued<br>attributes, inheritance, and tuple references. Such extensions attempt to pre-<br>serve the relational foundations—in particular, the declarative access to data<br>—while extending the modeling power.<br>• Object-relational database systems (that is, database systems based on the<br>object-relation model) provide a convenient migration path for users of re-<br>lational databases who wish to use object-oriented features.<br>• We have also outlined the procedural extensions provided by SQL:1999.<br>• We discussed differences between persistent programming languages and<br>object-relational systems, and mention criteria for choosing between them.<br>Review Terms<br>• Nested relations<br>• Nested relational model<br>• Complex types<br>• Collection types<br>• Large object types<br>• Sets<br>• Arrays<br>• Multisets<br>• Character large object (clob)<br>• Binary large object (blob)<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br></span><br><br><span style="background-color: #9BF6FF;" title="Chunk 44 | Start: 880088 | End: 900088 | Tokens: 2999">III. Object−Based <br>Databases and XML<br>9. Object−Relational <br>Databases<br>360<br>© The McGraw−Hill <br>Companies, 2001<br>358<br>Chapter 9<br>Object-Relational Databases<br>• Structured types<br>• Methods<br>• Row types<br>• Constructors<br>• Inheritance<br>  Single inheritance<br>  Multiple inheritance<br>• Type inheritance<br>• Most-speciﬁc type<br>• Table inheritance<br>• Subtable<br>• Overlapping subtables<br>• Reference types<br>• Scope of a reference<br>• Self-referential attribute<br>• Path expressions<br>• Nesting and unnesting<br>• SQL functions and procedures<br>• Procedural constructs<br>• Exceptions<br>• Handlers<br>• External language routines<br>Exercises<br>9.1 Consider the database schema<br>Emp = (ename, setof(Children), setof(Skills))<br>Children = (name, Birthday)<br>Birthday = (day, month, year)<br>Skills = (type, setof(Exams))<br>Exams = (year, city)<br>Assume that attributes of type setof(Children), setof(Skills), and setof(Exams),<br>have attribute names ChildrenSet, SkillsSet, and ExamsSet, respectively. Suppose<br>that the database contains a relation emp (Emp). Write the following queries in<br>SQL:1999 (with the extensions described in this chapter).<br>a. Find the names of all employees who have a child who has a birthday in<br>March.<br>b. Find those employees who took an examination for the skill type “typing”<br>in the city “Dayton”.<br>c. List all skill types in the relation emp.<br>9.2 Redesign the database of Exercise 9.1 into ﬁrst normal form and fourth normal<br>form. List any functional or multivalued dependencies that you assume. Also<br>list all referential-integrity constraints that should be present in the ﬁrst- and<br>fourth-normal-form schemas.<br>9.3 Consider the schemas for the table people, and the tables students and teachers,<br>which were created under people, in Section 9.3. Give a relational schema in third<br>normal form that represents the same information. Recall the constraints on sub-<br>tables, and give all constraints that must be imposed on the relational schema<br>so that every database instance of the relational schema can also be represented<br>by an instance of the schema with inheritance.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>III. Object−Based <br>Databases and XML<br>9. Object−Relational <br>Databases<br>361<br>© The McGraw−Hill <br>Companies, 2001<br>Exercises<br>359<br>9.4 A car-rental company maintains a vehicle database for all vehicles in its current<br>ﬂeet. For all vehicles, it includes the vehicle identiﬁcation number, license num-<br>ber, manufacturer, model, date of purchase, and color. Special data are included<br>for certain types of vehicles:<br>• Trucks: cargo capacity<br>• Sports cars: horsepower, renter age requirement<br>• Vans: number of passengers<br>• Off-road vehicles: ground clearance, drivetrain (four- or two-wheel drive)<br>Construct an SQL:1999 schema deﬁnition for this database. Use inheritance where<br>appropriate.<br>9.5 Explain the distinction between a type x and a reference type ref(x). Under what<br>circumstances would you choose to use a reference type?<br>9.6 Consider the E-R diagram in Figure 2.11, which contains composite, multivalued<br>and derived attributes.<br>a. Give an SQL:1999 schema deﬁnition corresponding to the E-R diagram. Use<br>an array to represent the multivalued attribute, and appropriate SQL:1999<br>constructs to represent the other attribute types.<br>b. Give constructors for each of the structured types deﬁned above.<br>9.7 Give an SQL:1999 schema deﬁnition of the E-R diagram in Figure 2.17, which<br>contains specializations.<br>9.8 Consider the relational schema shown in Figure 3.39.<br>a. Give a schema deﬁnition in SQL:1999 corresponding to the relational schema,<br>but using references to express foreign-key relationships.<br>b. Write each of the queries given in Exercise 3.10 on the above schema, using<br>SQL:1999.<br>9.9 Consider an employee database with two relations<br>employee (employee-name, street, city)<br>works (employee-name, company-name, salary)<br>where the primary keys are underlined. Write a query to ﬁnd companies<br>whose employees earn a higher salary, on average, than the average salary at<br>First Bank Corporation.<br>a. Using SQL:1999 functions as appropriate.<br>b. Without using SQL:1999 functions.<br>9.10 Rewrite the query in Section 9.6.1 that returns the titles of all books that have<br>more than one author, using the with clause in place of the function.<br>9.11 Compare the use of embedded SQL with the use in SQL of functions deﬁned in<br>a general-purpose programming language. Under what circumstances would<br>you use each of these features?<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>III. Object−Based <br>Databases and XML<br>9. Object−Relational <br>Databases<br>362<br>© The McGraw−Hill <br>Companies, 2001<br>360<br>Chapter 9<br>Object-Relational Databases<br>9.12 Suppose that you have been hired as a consultant to choose a database system<br>for your client’s application. For each of the following applications, state what<br>type of database system (relational, persistent-programming-language–based<br>OODB, object relational; do not specify a commercial product) you would rec-<br>ommend. Justify your recommendation.<br>a. A computer-aided design system for a manufacturer of airplanes<br>b. A system to track contributions made to candidates for public ofﬁce<br>c. An information system to support the making of movies<br>Bibliographical Notes<br>The nested relational model was introduced in Makinouchi [1977] and Jaeschke and<br>Schek [1982]. Various algebraic query languages are presented in Fischer and Thomas<br>[1983], Zaniolo [1983], Ozsoyoglu et al. [1987], Gucht [1987], and Roth et al. [1988].<br>The management of null values in nested relations is discussed in Roth et al. [1989].<br>Design and normalization issues are discussed in Ozsoyoglu and Yuan [1987], Roth<br>and Korth [1987], and Mok et al. [1996]. A collection of papers on nested relations<br>appears in<br>Several object-oriented extensions to SQL have been proposed. POSTGRES (Stone-<br>braker and Rowe [1986] and Stonebraker [1986a]) was an early implementation of an<br>object-relational system. Illustra was the commercial object-relational system that is<br>the successor of POSTGRES (Illustra was later acquired by Informix, which itself was<br>recently acquired by IBM). The Iris database system from Hewlett-Packard (Fishman<br>et al. [1990] and Wilkinson et al. [1990]) provides object-oriented extensions on top<br>of a relational database system. The O2 query language described in Bancilhon et al.<br>[1989] is an object-oriented extension of SQL implemented in the O2 object-oriented<br>database system (Deux [1991]). UniSQL is described in UniSQL [1991]. XSQL is an<br>object-oriented extension of SQL proposed by Kifer et al. [1992].<br>SQL:1999 was the product of an extensive (and long-delayed) standardization ef-<br>fort, which originally started off as adding object-oriented features to SQL and ended<br>up adding many more features, such as control ﬂow, as we have seen. The ofﬁcial<br>standard documents are available (for a fee) from http://webstore.ansi.org. However,<br>standards documents are very hard to read, and are best left to SQL:1999 imple-<br>menters. Books on SQL:1999 were still in press at the time of writing this book, see<br>the Web site of the book for current information.<br>Tools<br>The Informix database system provides support for many object-relational features.<br>Oracle introduced several object-relational features in Oracle 8.0. Both these systems<br>provided object-relational features before the SQL:1999 standard was ﬁnalized, and<br>have some features that are not part of SQL:1999. IBM DB2 supports many of the<br>SQL:1999 features.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>III. Object−Based <br>Databases and XML<br>10. XML<br>363<br>© The McGraw−Hill <br>Companies, 2001<br>C<br>H<br>A<br>P<br>T<br>E<br>R<br>1<br>0<br>XML<br>Unlike most of the technologies presented in the preceding chapters, the Extensible<br>Markup Language (XML) was not originally conceived as a database technology. In<br>fact, like the Hyper-Text Markup Language (HTML) on which the World Wide Web is<br>based, XML has its roots in document management, and is derived from a language<br>for structuring large documents known as the Standard Generalized Markup Language<br>(SGML). However, unlike SGML and HTML, XML can represent database data, as well<br>as many other kinds of structured data used in business applications. It is particularly<br>useful as a data format when an application must communicate with another appli-<br>cation, or integrate information from several other applications. When XML is used in<br>these contexts, many database issues arise, including how to organize, manipulate,<br>and query the XML data. In this chapter, we introduce XML and discuss both the man-<br>agement of XML data with database techniques and the exchange of data formatted<br>as XML documents.<br>10.1<br>Background<br>To understand XML, it is important to understand its roots as a document markup<br>language. The term markup refers to anything in a document that is not intended to<br>be part of the printed output. For example, a writer creating text that will eventually<br>be typeset in a magazine may want to make notes about how the typesetting should<br>be done. It would be important to type these notes in a way so that they could be<br>distinguished from the actual content, so that a note like “do not break this para-<br>graph” does not end up printed in the magazine. In electronic document processing,<br>a markup language is a formal description of what part of the document is content,<br>what part is markup, and what the markup means.<br>Just as database systems evolved from physical ﬁle processing to provide a sepa-<br>rate logical view, markup languages evolved from specifying instructions for how to<br>361<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>III. Object−Based <br>Databases and XML<br>10. XML<br>364<br>© The McGraw−Hill <br>Companies, 2001<br>362<br>Chapter 10<br>XML<br>print parts of the document to specify the function of the content. For instance, with<br>functional markup, text representing section headings (for this section, the words<br>“Background”) would be marked up as being a section heading, instead of being<br>marked up as text to be printed in large size, bold font. Such functional markup al-<br>lowed the document to be formatted differently in different situations. It also helps<br>different parts of a large document, or different pages in a large Web site to be for-<br>matted in a uniform manner. Functional markup also helps automate extraction of<br>key parts of documents.<br>For the family of markup languages that includes HTML, SGML, and XML the<br>markup takes the form of tags enclosed in angle-brackets, &lt;&gt;. Tags are used in pairs,<br>with &lt;tag&gt; and &lt;/tag&gt; delimiting the beginning and the end of the portion of the<br>document to which the tag refers. For example, the title of a document might be<br>marked up as follows.<br>&lt;title&gt;Database System Concepts&lt;/title&gt;<br>Unlike HTML, XML does not prescribe the set of tags allowed, and the set may be<br>specialized as needed. This feature is the key to XML’s major role in data representa-<br>tion and exchange, whereas HTML is used primarily for document formatting.<br>For example, in our running banking application, account and customer informa-<br>tion can be represented as part of an XML document as in Figure 10.1. Observe the<br>use of tags such as account and account-number. These tags provide context for each<br>value and allow the semantics of the value to be identiﬁed.<br>Compared to storage of data in a database, the XML representation may be inefﬁ-<br>cient, since tag names are repeated throughout the document. However, in spite of<br>this disadvantage, an XML representation has signiﬁcant advantages when it is used<br>to exchange data, for example, as part of a message:<br>• First, the presence of the tags makes the message self-documenting; that is, a<br>schema need not be consulted to understand the meaning of the text. We can<br>readily read the fragment above, for example.<br>• Second, the format of the document is not rigid. For example, if some sender<br>adds additional information, such as a tag last-accessed noting the last date<br>on which an account was accessed, the recipient of the XML data may simply<br>ignore the tag. The ability to recognize and ignore unexpected tags allows the<br>format of the data to evolve over time, without invalidating existing applica-<br>tions.<br>• Finally, since the XML format is widely accepted, a wide variety of tools are<br>available to assist in its processing, including browser software and database<br>tools.<br>Just as SQL is the dominant language for querying relational data, XML is becoming<br>the dominant format for data exchange.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>III. Object−Based <br>Databases and XML<br>10. XML<br>365<br>© The McGraw−Hill <br>Companies, 2001<br>10.1<br>Background<br>363<br>&lt;bank&gt;<br>&lt;account&gt;<br>&lt;account-number&gt; A-101 &lt;/account-number&gt;<br>&lt;branch-name&gt; Downtown &lt;/branch-name&gt;<br>&lt;balance&gt; 500 &lt;/balance&gt;<br>&lt;/account&gt;<br>&lt;account&gt;<br>&lt;account-number&gt; A-102 &lt;/account-number&gt;<br>&lt;branch-name&gt; Perryridge &lt;/branch-name&gt;<br>&lt;balance&gt; 400 &lt;/balance&gt;<br>&lt;/account&gt;<br>&lt;account&gt;<br>&lt;account-number&gt; A-201 &lt;/account-number&gt;<br>&lt;branch-name&gt; Brighton &lt;/branch-name&gt;<br>&lt;balance&gt; 900 &lt;/balance&gt;<br>&lt;/account&gt;<br>&lt;customer&gt;<br>&lt;customer-name&gt; Johnson &lt;/customer-name&gt;<br>&lt;customer-street&gt; Alma &lt;/customer-street&gt;<br>&lt;customer-city&gt; Palo Alto &lt;/customer-city&gt;<br>&lt;/customer&gt;<br>&lt;customer&gt;<br>&lt;customer-name&gt; Hayes &lt;/customer-name&gt;<br>&lt;customer-street&gt; Main &lt;/customer-street&gt;<br>&lt;customer-city&gt; Harrison &lt;/customer-city&gt;<br>&lt;/customer&gt;<br>&lt;depositor&gt;<br>&lt;account-number&gt; A-101 &lt;/account-number&gt;<br>&lt;customer-name&gt; Johnson &lt;/customer-name&gt;<br>&lt;/depositor&gt;<br>&lt;depositor&gt;<br>&lt;account-number&gt; A-201 &lt;/account-number&gt;<br>&lt;customer-name&gt; Johnson &lt;/customer-name&gt;<br>&lt;/depositor&gt;<br>&lt;depositor&gt;<br>&lt;account-number&gt; A-102 &lt;/account-number&gt;<br>&lt;customer-name&gt; Hayes &lt;/customer-name&gt;<br>&lt;/depositor&gt;<br>&lt;/bank&gt;<br>Figure 10.1<br>XML representation of bank information.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>III. Object−Based <br>Databases and XML<br>10. XML<br>366<br>© The McGraw−Hill <br>Companies, 2001<br>364<br>Chapter 10<br>XML<br>10.2<br>Structure of XML Data<br>The fundamental construct in an XML document is the element. An element is simply<br>a pair of matching start- and end-tags, and all the text that appears between them.<br>XML documents must have a single root element that encompasses all other ele-<br>ments in the document. In the example in Figure 10.1, the &lt;bank&gt; element forms<br>the root element. Further, elements in an XML document must nest properly. For in-<br>stance,<br>&lt;account&gt; . . . &lt;balance&gt; . . . &lt;/balance&gt; . . . &lt;/account&gt;<br>is properly nested, whereas<br>&lt;account&gt; . . . &lt;balance&gt; . . . &lt;/account&gt; . . . &lt;/balance&gt;<br>is not properly nested.<br>While proper nesting is an intuitive property, we may deﬁne it more formally.<br>Text is said to appear in the context of an element if it appears between the start-tag<br>and end-tag of that element. Tags are properly nested if every start-tag has a unique<br>matching end-tag that is in the context of the same parent element.<br>Note that text may be mixed with the subelements of an element, as in Figure 10.2.<br>As with several other features of XML, this freedom makes more sense in a document-<br>processing context than in a data-processing context, and is not particularly useful for<br>representing more structured data such as database content in XML.<br>The ability to nest elements within other elements provides an alternative way to<br>represent information. Figure 10.3 shows a representation of the bank information<br>from Figure 10.1, but with account elements nested within customer elements. The<br>nested representation makes it easy to ﬁnd all accounts of a customer, although it<br>would store account elements redundantly if they are owned by multiple customers.<br>Nested representations are widely used in XML data interchange applications to<br>avoid joins. For instance, a shipping application would store the full address of sender<br>and receiver redundantly on a shipping document associated with each shipment,<br>whereas a normalized representation may require a join of shipping records with a<br>company-address relation to get address information.<br>In addition to elements, XML speciﬁes the notion of an attribute. For instance, the<br>type of an account can represented as an attribute, as in Figure 10.4. The attributes of<br>. . .<br>&lt;account&gt;<br>This account is seldom used any more.<br>&lt;account-number&gt; A-102 &lt;/account-number&gt;<br>&lt;branch-name&gt; Perryridge &lt;/branch-name&gt;<br>&lt;balance&gt; 400 &lt;/balance&gt;<br>&lt;/account&gt;<br>. . .<br>Figure 10.2<br>Mixture of text with subelements.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>III. Object−Based <br>Databases and XML<br>10. XML<br>367<br>© The McGraw−Hill <br>Companies, 2001<br>10.2<br>Structure of XML Data<br>365<br>&lt;bank-1&gt;<br>&lt;customer&gt;<br>&lt;customer-name&gt; Johnson &lt;/customer-name&gt;<br>&lt;customer-street&gt; Alma &lt;/customer-street&gt;<br>&lt;customer-city&gt; Palo Alto &lt;/customer-city&gt;<br>&lt;account&gt;<br>&lt;account-number&gt; A-101 &lt;/account-number&gt;<br>&lt;branch-name&gt; Downtown &lt;/branch-name&gt;<br>&lt;balance&gt; 500 &lt;/balance&gt;<br>&lt;/account&gt;<br>&lt;account&gt;<br>&lt;account-number&gt; A-201 &lt;/account-number&gt;<br>&lt;branch-name&gt; Brighton &lt;/branch-name&gt;<br>&lt;balance&gt; 900 &lt;/balance&gt;<br>&lt;/account&gt;<br>&lt;/customer&gt;<br>&lt;customer&gt;<br>&lt;customer-name&gt; Hayes &lt;/customer-name&gt;<br>&lt;customer-street&gt; Main &lt;/customer-street&gt;<br>&lt;customer-city&gt; Harrison &lt;/customer-city&gt;<br>&lt;account&gt;<br>&lt;account-number&gt; A-102 &lt;/account-number&gt;<br>&lt;branch-name&gt; Perryridge &lt;/branch-name&gt;<br>&lt;balance&gt; 400 &lt;/balance&gt;<br>&lt;/account&gt;<br>&lt;/customer&gt;<br>&lt;/bank-1&gt;<br>Figure 10.3<br>Nested XML representation of bank information.<br>an element appear as name=value pairs before the closing “&gt;” of a tag. Attributes are<br>strings, and do not contain markup. Furthermore, attributes can appear only once in<br>a given tag, unlike subelements, which may be repeated.<br>Note that in a document construction context, the distinction between subelement<br>and attribute is important—an attribute is implicitly text that does not appear in the<br>printed or displayed document. However, in database and data exchange applica-<br>tions of XML, this distinction is less relevant, and the choice of representing data as<br>an attribute or a subelement is frequently arbitrary.<br>One ﬁnal syntactic note is that an element of the form &lt;element&gt;&lt;/element&gt;,<br>which contains no subelements or text, can be abbreviated as &lt;element/&gt;; abbrevi-<br>ated elements may, however, contain attributes.<br>Since XML documents are designed to be exchanged between applications, a name-<br>space mechanism has been introduced to allow organizations to specify globally<br>unique names to be used as element tags in documents. The idea of a namespace<br>is to prepend each tag or attribute with a universal resource identiﬁer (for example, a<br>Web address) Thus, for example, if First Bank wanted to ensure that XML documents<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>III. Object−Based <br>Databases and XML<br>10. XML<br>368<br>© The McGraw−Hill <br>Companies, 2001<br>366<br>Chapter 10<br>XML<br>. . .<br>&lt;account acct-type= “checking”&gt;<br>&lt;account-number&gt; A-102 &lt;/account-number&gt;<br>&lt;branch-name&gt; Perryridge &lt;/branch-name&gt;<br>&lt;balance&gt; 400 &lt;/balance&gt;<br>&lt;/account&gt;<br>. . .<br>Figure 10.4<br>Use of attributes.<br>it created would not duplicate tags used by any business partner’s XML documents,<br>it can prepend a unique identiﬁer with a colon to each tag name. The bank may use<br>a Web URL such as<br>http://www.FirstBank.com<br>as a unique identiﬁer. Using long unique identiﬁers in every tag would be rather<br>inconvenient, so the namespace standard provides a way to deﬁne an abbreviation<br>for identiﬁers.<br>In Figure 10.5, the root element (bank) has an attribute xmlns:FB, which declares<br>that FB is deﬁned as an abbreviation for the URL given above. The abbreviation can<br>then be used in various element tags, as illustrated in the ﬁgure.<br>A document can have more than one namespace, declared as part of the root ele-<br>ment. Different elements can then be associated with different namespaces. A default<br>namespace can be deﬁned, by using the attribute xmlns instead of xmlns:FB in the<br>root element. Elements without an explicit namespace preﬁx would then belong to<br>the default namespace.<br>Sometimes we need to store values containing tags without having the tags inter-<br>preted as XML tags. So that we can do so, XML allows this construct:<br>&lt;![CDATA[&lt;account&gt; · · ·&lt;/account&gt;]]&gt;<br>Because it is enclosed within CDATA, the t</span><br><br><span style="background-color: #A0C4FF;" title="Chunk 45 | Start: 900090 | End: 920090 | Tokens: 3010">ext &lt;account&gt; is treated as normal text<br>data, not as a tag. The term CDATA stands for character data.<br>&lt;bank xmlns:FB=“http://www.FirstBank.com”&gt;<br>. . .<br>&lt;FB:branch&gt;<br>&lt;FB:branchname&gt; Downtown &lt;/FB:branchname&gt;<br>&lt;FB:branchcity&gt; Brooklyn &lt;/FB:branchcity&gt;<br>&lt;/FB:branch&gt;<br>. . .<br>&lt;/bank&gt;<br>Figure 10.5<br>Unique tag names through the use of namespaces.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>III. Object−Based <br>Databases and XML<br>10. XML<br>369<br>© The McGraw−Hill <br>Companies, 2001<br>10.3<br>XML Document Schema<br>367<br>10.3<br>XML Document Schema<br>Databases have schemas, which are used to constrain what information can be stored<br>in the database and to constrain the data types of the stored information. In contrast,<br>by default, XML documents can be created without any associated schema: An el-<br>ement may then have any subelement or attribute. While such freedom may occa-<br>sionally be acceptable given the self-describing nature of the data format, it is not<br>generally useful when XML documents must be processesed automatically as part of<br>an application, or even when large amounts of related data are to be formatted in<br>XML.<br>Here, we describe the document-oriented schema mechanism included as part of<br>the XML standard, the Document Type Deﬁnition, as well as the more recently deﬁned<br>XMLSchema.<br>10.3.1<br>Document Type Deﬁnition<br>The document type deﬁnition (DTD) is an optional part of an XML document. The<br>main purpose of a DTD is much like that of a schema: to constrain and type the infor-<br>mation present in the document. However, the DTD does not in fact constrain types<br>in the sense of basic types like integer or string. Instead, it only constrains the appear-<br>ance of subelements and attributes within an element. The DTD is primarily a list of<br>rules for what pattern of subelements appear within an element. Figure 10.6 shows<br>a part of an example DTD for a bank information document; the XML document in<br>Figure 10.1 conforms to this DTD.<br>Each declaration is in the form of a regular expression for the subelements of an<br>element. Thus, in the DTD in Figure 10.6, a bank element consists of one or more<br>account, customer, or depositor elements; the | operator speciﬁes “or” while the +<br>operator speciﬁes “one or more.” Although not shown here, the ∗operator is used to<br>specify “zero or more,” while the ? operator is used to specify an optional element<br>(that is, “zero or one”).<br>&lt;!DOCTYPE bank [<br>&lt;!ELEMENT bank ( (account—customer—depositor)+)&gt;<br>&lt;!ELEMENT account ( account-number branch-name balance )&gt;<br>&lt;!ELEMENT customer ( customer-name customer-street customer-city )&gt;<br>&lt;!ELEMENT depositor ( customer-name account-number )&gt;<br>&lt;!ELEMENT account-number ( #PCDATA )&gt;<br>&lt;!ELEMENT branch-name ( #PCDATA )&gt;<br>&lt;!ELEMENT balance( #PCDATA )&gt;<br>&lt;!ELEMENT customer-name( #PCDATA )&gt;<br>&lt;!ELEMENT customer-street( #PCDATA )&gt;<br>&lt;!ELEMENT customer-city( #PCDATA )&gt;<br>] &gt;<br>Figure 10.6<br>Example of a DTD.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>III. Object−Based <br>Databases and XML<br>10. XML<br>370<br>© The McGraw−Hill <br>Companies, 2001<br>368<br>Chapter 10<br>XML<br>The account element is deﬁned to contain subelements account-number, branch-<br>name and balance (in that order). Similarly, customer and depositor have the at-<br>tributes in their schema deﬁned as subelements.<br>Finally, the elements account-number, branch-name, balance, customer-name, cu-<br>stomer-street, and customer-city are all declared to be of type #PCDATA. The keyword<br>#PCDATA indicates text data; it derives its name, historically, from “parsed character<br>data.” Two other special type declarations are empty, which says that the element has<br>no contents, and any, which says that there is no constraint on the subelements of the<br>element; that is, any elements, even those not mentioned in the DTD, can occur as<br>subelements of the element. The absence of a declaration for an element is equivalent<br>to explicitly declaring the type as any.<br>The allowable attributes for each element are also declared in the DTD. Unlike<br>subelements, no order is imposed on attributes. Attributes may speciﬁed to be of<br>type CDATA, ID, IDREF, or IDREFS; the type CDATA simply says that the attribute con-<br>tains character data, while the other three are not so simple; they are explained in<br>more detail shortly. For instance, the following line from a DTD speciﬁes that element<br>account has an attribute of type acct-type, with default value checking.<br>&lt;!ATTLIST account acct-type CDATA “checking” &gt;<br>Attributes must have a type declaration and a default declaration. The default<br>declaration can consist of a default value for the attribute or #REQUIRED, meaning<br>that a value must be speciﬁed for the attribute in each element, or #IMPLIED, meaning<br>that no default value has been provided. If an attribute has a default value, for every<br>element that does not specify a value for the attribute, the default value is ﬁlled in<br>automatically when the XML document is read<br>An attribute of type ID provides a unique identiﬁer for the element; a value that<br>occurs in an ID attribute of an element must not occur in any other element in the<br>same document. At most one attribute of an element is permitted to be of type ID.<br>&lt;!DOCTYPE bank-2 [<br>&lt;!ELEMENT account ( branch, balance )&gt;<br>&lt;!ATTLIST account<br>account-number ID #REQUIRED<br>owners IDREFS #REQUIRED &gt;<br>&lt;!ELEMENT customer ( customer-name, customer-street, customer-city )&gt;<br>&lt;!ATTLIST customer<br>customer-id ID #REQUIRED<br>accounts IDREFS #REQUIRED &gt;<br>· · · declarations for branch, balance, customer-name,<br>customer-street and customer-city · · ·<br>] &gt;<br>Figure 10.7<br>DTD with ID and IDREF attribute types.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>III. Object−Based <br>Databases and XML<br>10. XML<br>371<br>© The McGraw−Hill <br>Companies, 2001<br>10.3<br>XML Document Schema<br>369<br>An attribute of type IDREF is a reference to an element; the attribute must contain<br>a value that appears in the ID attribute of some element in the document. The type<br>IDREFS allows a list of references, separated by spaces.<br>Figure 10.7 shows an example DTD in which customer account relationships are<br>represented by ID and IDREFS attributes, instead of depositor records. The account<br>elements use account-number as their identiﬁer attribute; to do so, account-number<br>has been made an attribute of account instead of a subelement. The customer ele-<br>ments have a new identiﬁer attribute called customer-id. Additionally, each customer<br>element contains an attribute accounts, of type IDREFS, which is a list of identiﬁers<br>of accounts that are owned by the customer. Each account element has an attribute<br>owners, of type IDREFS, which is a list of owners of the account.<br>Figure 10.8 shows an example XML document based on the DTD in Figure 10.7.<br>Note that we use a different set of accounts and customers from our earlier example,<br>in order to illustrate the IDREFS feature better.<br>The ID and IDREF attributes serve the same role as reference mechanisms in object-<br>oriented and object-relational databases, permitting the construction of complex data<br>relationships.<br>&lt;bank-2&gt;<br>&lt;account account-number=“A-401” owners=“C100 C102”&gt;<br>&lt;branch-name&gt; Downtown &lt;/branch-name&gt;<br>&lt;balance&gt; 500 &lt;/balance&gt;<br>&lt;/account&gt;<br>&lt;account account-number=“A-402” owners=“C102 C101”&gt;<br>&lt;branch-name&gt; Perryridge &lt;/branch-name&gt;<br>&lt;balance&gt; 900 &lt;/balance&gt;<br>&lt;/account&gt;<br>&lt;customer customer-id=“C100” accounts=“A-401”&gt;<br>&lt;customer-name&gt;Joe&lt;/customer-name&gt;<br>&lt;customer-street&gt; Monroe &lt;/customer-street&gt;<br>&lt;customer-city&gt; Madison &lt;/customer-city&gt;<br>&lt;/customer&gt;<br>&lt;customer customer-id=“C101” accounts=“A-402”&gt;<br>&lt;customer-name&gt;Lisa&lt;/customer-name&gt;<br>&lt;customer-street&gt; Mountain &lt;/customer-street&gt;<br>&lt;customer-city&gt; Murray Hill &lt;/customer-city&gt;<br>&lt;/customer&gt;<br>&lt;customer customer-id=“C102” accounts=“A-401 A-402”&gt;<br>&lt;customer-name&gt;Mary&lt;/customer-name&gt;<br>&lt;customer-street&gt; Erin &lt;/customer-street&gt;<br>&lt;customer-city&gt; Newark &lt;/customer-city&gt;<br>&lt;/customer&gt;<br>&lt;/bank-2&gt;<br>Figure 10.8<br>XML data with ID and IDREF attributes.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>III. Object−Based <br>Databases and XML<br>10. XML<br>372<br>© The McGraw−Hill <br>Companies, 2001<br>370<br>Chapter 10<br>XML<br>Document type deﬁnitions are strongly connected to the document formatting her-<br>itage of XML. Because of this, they are unsuitable in many ways for serving as the type<br>structure of XML for data processing applications. Nevertheless, a tremendous num-<br>ber of data exchange formats are being deﬁned in terms of DTDs, since they were<br>part of the original standard. Here are some of the limitations of DTDs as a schema<br>mechanism.<br>• Individual text elements and attributes cannot be further typed. For instance,<br>the element balance cannot be constrained to be a positive number. The lack of<br>such constraints is problematic for data processing and exchange applications,<br>which must then contain code to verify the types of elements and attributes.<br>• It is difﬁcult to use the DTD mechanism to specify unordered sets of subele-<br>ments. Order is seldom important for data exchange (unlike document layout,<br>where it is crucial). While the combination of alternation (the | operation) and<br>the ∗operation as in Figure 10.6 permits the speciﬁcation of unordered collec-<br>tions of tags, it is much more difﬁcult to specify that each tag may only appear<br>once.<br>• There is a lack of typing in IDs and IDREFs. Thus, there is no way to specify<br>the type of element to which an IDREF or IDREFS attribute should refer. As a<br>result, the DTD in Figure 10.7 does not prevent the “owners” attribute of an<br>account element from referring to other accounts, even though this makes no<br>sense.<br>10.3.2<br>XML Schema<br>An effort to redress many of these DTD deﬁciencies resulted in a more sophisticated<br>schema language, XMLSchema. We present here an example of XMLSchema, and list<br>some areas in which it improves DTDs, without giving full details of XMLSchema’s<br>syntax.<br>Figure 10.9 shows how the DTD in Figure 10.6 can be represented by XMLSchema.<br>The ﬁrst element is the root element bank, whose type is declared later. The example<br>then deﬁnes the types of elements account, customer, and depositor. Observe the use<br>of types xsd:string and xsd:decimal to constrain the types of data elements. Finally<br>the example deﬁnes the type BankType as containing zero or more occurrences of<br>each of account, customer and depositor. XMLSchema can deﬁne the minimum and<br>maximum number of occurrences of subelements by using minOccurs and maxOc-<br>curs. The default for both minimum and maximum occurrences is 1, so these have to<br>be explicity speciﬁed to allow zero or more accounts, deposits, and customers.<br>Among the beneﬁts that XMLSchema offers over DTDs are these:<br>• It allows user-deﬁned types to be created.<br>• It allows the text that appears in elements to be constrained to speciﬁc types,<br>such as numeric types in speciﬁc formats or even more complicated types such<br>as lists or union.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>III. Object−Based <br>Databases and XML<br>10. XML<br>373<br>© The McGraw−Hill <br>Companies, 2001<br>10.3<br>XML Document Schema<br>371<br>&lt;xsd:schema xmlns:xsd=“http://www.w3.org/2001/XMLSchema”&gt;<br>&lt;xsd:element name=“bank” type=“BankType” /&gt;<br>&lt;xsd:element name=“account”&gt;<br>&lt;xsd:complexType&gt;<br>&lt;xsd:sequence&gt;<br>&lt;xsd:element name=“account-number” type=“xsd:string”/&gt;<br>&lt;xsd:element name=“branch-name” type=“xsd:string”/&gt;<br>&lt;xsd:element name=“balance” type=“xsd:decimal”/&gt;<br>&lt;/xsd:sequence&gt;<br>&lt;/xsd:complexType&gt;<br>&lt;/xsd:element&gt;<br>&lt;xsd:element name=“customer”&gt;<br>&lt;xsd:element name=“customer-number” type=“xsd:string”/&gt;<br>&lt;xsd:element name=“customer-street” type=“xsd:string”/&gt;<br>&lt;xsd:element name=“customer-city” type=“xsd:string”/&gt;<br>&lt;/xsd:element&gt;<br>&lt;xsd:element name=“depositor”&gt;<br>&lt;xsd:complexType&gt;<br>&lt;xsd:sequence&gt;<br>&lt;xsd:element name=“customer-name” type=“xsd:string”/&gt;<br>&lt;xsd:element name=“account-number” type=“xsd:string”/&gt;<br>&lt;/xsd:sequence&gt;<br>&lt;/xsd:complexType&gt;<br>&lt;/xsd:element&gt;<br>&lt;xsd:complexType name=“BankType”&gt;<br>&lt;xsd:sequence&gt;<br>&lt;xsd:element ref=“account” minOccurs=“0” maxOccurs=“unbounded”/&gt;<br>&lt;xsd:element ref=“customer” minOccurs=“0” maxOccurs=“unbounded”/&gt;<br>&lt;xsd:element ref=“depositor” minOccurs=“0” maxOccurs=“unbounded”/&gt;<br>&lt;/xsd:sequence&gt;<br>&lt;/xsd:complexType&gt;<br>&lt;/xsd:schema&gt;<br>Figure 10.9<br>XMLSchema version of DTD from Figure 10.6.<br>• It allows types to be restricted to create specialized types, for instance by spec-<br>ifying minimum and maximum values.<br>• It allows complex types to be extended by using a form of inheritance.<br>• It is a superset of DTDs.<br>• It allows uniqueness and foreign key constraints.<br>• It is integrated with namespaces to allow different parts of a document to<br>conform to different schema.<br>• It is itself speciﬁed by XML syntax, as Figure 10.9 shows.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>III. Object−Based <br>Databases and XML<br>10. XML<br>374<br>© The McGraw−Hill <br>Companies, 2001<br>372<br>Chapter 10<br>XML<br>However, the price paid for these features is that XMLSchema is signiﬁcantly more<br>complicated than DTDs.<br>10.4<br>Querying and Transformation<br>Given the increasing number of applications that use XML to exchange, mediate, and<br>store data, tools for effective management of XML data are becoming increasingly im-<br>portant. In particular, tools for querying and transformation of XML data are essential<br>to extract information from large bodies of XML data, and to convert data between<br>different representations (schemas) in XML. Just as the output of a relational query is<br>a relation, the output of an XML query can be an XML document. As a result, querying<br>and transformation can be combined into a single tool.<br>Several languages provide increasing degrees of querying and transformation ca-<br>pabilities:<br>• XPath is a language for path expressions, and is actually a building block for<br>the remaining two query languages.<br>• XSLT was designed to be a transformation language, as part of the XSL style<br>sheet system, which is used to control the formatting of XML data into HTML<br>or other print or display languages. Although designed for formatting, XSLT<br>can generate XML as output, and can express many interesting queries. Fur-<br>thermore, it is currently the most widely available language for manipulating<br>XML data.<br>• XQuery has been proposed as a standard for querying of XML data. XQuery<br>combines features from many of the earlier proposals for querying XML, in<br>particular the language Quilt.<br>A tree model of XML data is used in all these languages. An XML document is mod-<br>eled as a tree, with nodes corresponding to elements and attributes. Element nodes<br>can have children nodes, which can be subelements or attributes of the element. Cor-<br>respondingly, each node (whether attribute or element), other than the root element,<br>has a parent node, which is an element. The order of elements and attributes in the<br>XML document is modeled by the ordering of children of nodes of the tree. The terms<br>parent, child, ancestor, descendant, and siblings are interpreted in the tree model of<br>XML data.<br>The text content of an element can be modeled as a text node child of the element.<br>Elements containing text broken up by intervening subelements can have multiple<br>text node children. For instance, an element containing “this is a &lt;bold&gt; wonderful<br>&lt;/bold&gt; book” would have a subelement child corresponding to the element bold<br>and two text node children corresponding to “this is a” and “book”. Since such struc-<br>tures are not commonly used in database data, we shall assume that elements do not<br>contain both text and subelements.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>III. Object−Based <br>Databases and XML<br>10. XML<br>375<br>© The McGraw−Hill <br>Companies, 2001<br>10.4<br>Querying and Transformation<br>373<br>10.4.1<br>XPath<br>XPath addresses parts of an XML document by means of path expressions. The lan-<br>guage can be viewed as an extension of the simple path expressions in object-oriented<br>and object-relational databases (See Section 9.5.1).<br>A path expression in XPath is a sequence of location steps separated by “/” (in-<br>stead of the “.” operator that separates steps in SQL:1999). The result of a path ex-<br>pression is a set of values. For instance, on the document in Figure 10.8, the XPath<br>expression<br>/bank-2/customer/name<br>would return these elements:<br>&lt;name&gt;Joe&lt;/name&gt;<br>&lt;name&gt;Lisa&lt;/name&gt;<br>&lt;name&gt;Mary&lt;/name&gt;<br>The expression<br>/bank-2/customer/name/text()<br>would return the same names, but without the enclosing tags.<br>Like a directory hierarchy, the initial ’/’ indicates the root of the document. (Note<br>that this is an abstract root “above” &lt;bank-2&gt; that is the document tag.) Path expres-<br>sions are evaluated from left to right. As a path expression is evaluated, the result of<br>the path at any point consists of a set of nodes from the document.<br>When an element name, such as customer, appears before the next ’/’, it refers to<br>all elements of the speciﬁed name that are children of elements in the current element<br>set. Since multiple children can have the same name, the number of nodes in the node<br>set can increase or decrease with each step. Attribute values may also be accessed,<br>using the “@” symbol. For instance, /bank-2/account/@account-number returns a set<br>of all values of account-number attributes of account elements. By default, IDREF<br>links are not followed; we shall see how to deal with IDREFs later.<br>XPath supports a number of other features:<br>• Selection predicates may follow any step in a path, and are contained in square<br>brackets. For example,<br>/bank-2/account[balance &gt; 400]<br>returns account elements with a balance value greater than 400, while<br>/bank-2/account[balance &gt; 400]/@account-number<br>returns the account numbers of those accounts.<br>We can test the existence of a subelement by listing it without any compar-<br>ison operation; for instance, if we removed just “&gt; 400” from the above, the<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>III. Object−Based <br>Databases and XML<br>10. XML<br>376<br>© The McGraw−Hill <br>Companies, 2001<br>374<br>Chapter 10<br>XML<br>expression would return account numbers of all accounts that have a balance<br>subelement, regardless of its value.<br>• XPath provides several functions that can be used as part of predicates, includ-<br>ing testing the position of the current node in the sibling order and counting<br>the number of nodes matched. For example, the path expression<br>/bank-2/account/[customer/count()&gt; 2]<br>returns accounts with more than 2 customers. Boolean connectives and and or<br>can be used in predicates, while the function not(. . .) can be used for negation.<br>• The function id(“foo”) returns the node (if any) with an attribute of type ID and<br>value “foo”. The function id can even be applied on sets of references, or even<br>strings containing multiple references separated by blanks, such as IDREFS.<br>For instance, the path<br>/bank-2/account/id(@owner)<br>returns all customers referred to from the owners attribute of account ele-<br>ments.<br>• The | operator allows expression results to be unioned. For example, if the<br>DTD of bank-2 also contained elements for loans, with attribute borrower of<br>type IDREFS identifying loan borrower, the expression<br>/bank-2/account/id(@owner) | /bank-2/loan/id(@borrower)<br>gives customers with either accounts or loans. However, the | operator cannot<br>be nested inside other operators.<br>• An XPath expression can skip multiple levels of nodes by using “//”. For in-<br>stance, the expression /bank-2//name ﬁnds any name element anywhere under<br>the /bank-2 element, regardless of the element in which it is contained. This<br>example illustrates the ability to ﬁnd required data without full knowledge of<br>the schema.<br>• Each step in the path need not select from the children of the nodes in the<br>current node set. In fact, this is just one of several directions along which a<br>step in the path may proceed, such as parents, siblings, ancestors and descen-<br>dants. We omit details, but note that “//”, described above, is a short form for<br>specifying “all descendants,” while “..” speciﬁes the parent.<br>10.4.2<br>XSLT<br>A style sheet is a representation of formatting options for a document, usually stored<br>outside the document itself, so that formatting is separate from content. For</span><br><br><span style="background-color: #BDB2FF;" title="Chunk 46 | Start: 920092 | End: 940092 | Tokens: 3049"> example,<br>a style sheet for HTML might specify the font to be used on all headers, and thus<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>III. Object−Based <br>Databases and XML<br>10. XML<br>377<br>© The McGraw−Hill <br>Companies, 2001<br>10.4<br>Querying and Transformation<br>375<br>&lt;xsl:template match=“/bank-2/customer”&gt;<br>&lt;customer&gt;<br>&lt;xsl:value-of select=“customer-name”/&gt;<br>&lt;/customer&gt;<br>&lt;/xsl:template&gt;<br>&lt;xsl:template match=“.”/&gt;<br>Figure 10.10<br>Using XSLT to wrap results in new XML elements.<br>replace a large number of font declarations in the HTML page. The XML Stylesheet<br>Language (XSL) was originally designed for generating HTML from XML, and is thus<br>a logical extension of HTML style sheets. The language includes a general-purpose<br>transformation mechanism, called XSL Transformations (XSLT), which can be used<br>to transform one XML document into another XML document, or to other formats<br>such as HTML.1 XSLT transformations are quite powerful, and in fact XSLT can even<br>act as a query language.<br>XSLT transformations are expressed as a series of recursive rules, called templates.<br>In their basic form, templates allow selection of nodes in an XML tree by an XPath<br>expression. However, templates can also generate new XML content, so that selection<br>and content generation can be mixed in natural and powerful ways. While XSLT can<br>be used as a query language, its syntax and semantics are quite dissimilar from those<br>of SQL.<br>A simple template for XSLT consists of a match part and a select part. Consider<br>this XSLT code:<br>&lt;xsl:template match=“/bank-2/customer”&gt;<br>&lt;xsl:value-of select=“customer-name”/&gt;<br>&lt;/xsl:template&gt;<br>&lt;xsl:template match=“.”/&gt;<br>The xsl:template match statement contains an XPath expression that selects one or<br>more nodes. The ﬁrst template matches customer elements that occur as children of<br>the bank-2 root element. The xsl:value-of statement enclosed in the match statement<br>outputs values from the nodes in the result of the XPath expression. The ﬁrst template<br>outputs the value of the customer-name subelement; note that the value does not<br>contain the element tag.<br>Note that the second template matches all nodes. This is required because the de-<br>fault behavior of XSLT on subtrees of the input document that do not match any<br>template is to copy the subtrees to the output document.<br>XSLT copies any tag that is not in the xsl namespace unchanged to the output. Fig-<br>ure 10.10 shows how to use this feature to make each customer name from our exam-<br>ple appear as a subelement of a “&lt;customer&gt;” element, by placing the xsl:value-of<br>statement between &lt;customer&gt; and &lt;/customer&gt;.<br>1.<br>The XSL standard now consists of XSLT and a standard for specifying formatting features such as<br>fonts, page margins, and tables. Formatting is not relevant from a database perspective, so we do not<br>cover it here.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>III. Object−Based <br>Databases and XML<br>10. XML<br>378<br>© The McGraw−Hill <br>Companies, 2001<br>376<br>Chapter 10<br>XML<br>&lt;xsl:template match=“/bank”&gt;<br>&lt;customers&gt;<br>&lt;xsl:apply-templates/&gt;<br>&lt;/customers&gt;<br>&lt;/xsl:template&gt;<br>&lt;xsl:template match=“/customer”&gt;<br>&lt;customer&gt;<br>&lt;xsl:value-of select=“customer-name”/&gt;<br>&lt;/customer&gt;<br>&lt;/xsl:template&gt;<br>&lt;xsl:template match=“.”/&gt;<br>Figure 10.11<br>Applying rules recursively.<br>Structural recursion is a key part of XSLT. Recall that elements and subelements<br>naturally form a tree structure. The idea of structural recursion is this: When a tem-<br>plate matches an element in the tree structure, XSLT can use structural recursion to<br>apply template rules recursively on subtrees, instead of just outputting a value. It<br>applies rules recursively by the xsl:apply-templates directive, which appears inside<br>other templates.<br>For example, the results of our previous query can be placed in a surrounding<br>&lt;customers&gt; element by the addition of a rule using xsl:apply-templates, as in Fig-<br>ure 10.11 The new rule matches the outer “bank” tag, and constructs a result doc-<br>ument by applying all other templates to the subtrees appearing within the bank<br>element, but wrapping the results in the given &lt;customers&gt; &lt;/customers&gt; ele-<br>ment. Without recursion forced by the &lt;xsl:apply-templates/&gt; clause, the template<br>would output &lt;customers&gt; &lt;/customers&gt;, and then apply the other templates on<br>the subelements.<br>In fact, the structural recursion is critical to constructing well-formed XML doc-<br>uments, since XML documents must have a single top-level element containing all<br>other elements in the document.<br>XSLT provides a feature called keys, which permit lookup of elements by using<br>values of subelements or attributes; the goals are similar to that of the id() function in<br>XPath, but permits attributes other than the ID attributes to be used. Keys are deﬁned<br>by an xsl:key directive, which has three parts, for example:<br>&lt;xsl:key name=“acctno” match=“account” use=“account-number”/&gt;<br>The name attribute is used to distinguish different keys. The match attribute speciﬁes<br>which nodes the key applies to. Finally, the use attribute speciﬁes the expression<br>to be used as the value of the key. Note that the expression need not be unique to<br>an element; that is, more than one element may have the same expression value. In<br>the example, the key named acctno speciﬁes that the account-number subelement of<br>account should be used as a key for that account.<br>Keys can be subsequently used in templates as part of any pattern through the<br>key function. This function takes the name of the key and a value, and returns the<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>III. Object−Based <br>Databases and XML<br>10. XML<br>379<br>© The McGraw−Hill <br>Companies, 2001<br>10.4<br>Querying and Transformation<br>377<br>&lt;xsl:key name=“acctno” match=“account”use=“account-number”/&gt;<br>&lt;xsl:key name=“custno” match=“customer” use=“customer-name”/&gt;<br>&lt;xsl:template match=“depositor”&gt;<br>&lt;cust-acct&gt;<br>&lt;xsl:value-of select=key(“custno”, “customer-name”)/&gt;<br>&lt;xsl:value-of select=key(“acctno”, “account-number”)/&gt;<br>&lt;/cust-acct&gt;<br>&lt;/xsl:template&gt;<br>&lt;xsl:template match=“.”/&gt;<br>Figure 10.12<br>Joins in XSLT.<br>set of nodes that match that value. Thus, the XML node for account “A-401” can be<br>referenced as key(“acctno”, “A-401”).<br>Keys can be used to implement some types of joins, as in Figure 10.12. The code<br>in the ﬁgure can be applied to XML data in the format in Figure 10.1. Here, the key<br>function joins the depositor elements with matching customer and account elements.<br>The result of the query consists of pairs of customer and account elements enclosed<br>within cust-acct elements.<br>XSLT allows nodes to be sorted. A simple example shows how xsl:sort would be<br>used in our style sheet to return customer elements sorted by name:<br>&lt;xsl:template match=“/bank”&gt;<br>&lt;xsl:apply-templates select=“customer”&gt;<br>&lt;xsl:sort select=“customer-name”/&gt;<br>&lt;/xsl:apply-templates&gt;<br>&lt;/xsl:template&gt;<br>&lt;xsl:template match=“customer”&gt;<br>&lt;customer&gt;<br>&lt;xsl:value-of select=“customer-name”/&gt;<br>&lt;xsl:value-of select=“customer-street”/&gt;<br>&lt;xsl:value-of select=“customer-city”/&gt;<br>&lt;/customer&gt;<br>&lt;/xsl:template&gt;<br>&lt;xsl:template match=“.”/&gt;<br>Here, the xsl:apply-template has a select attribute, which constrains it to be applied<br>only on customer subelements. The xsl:sort directive within the xsl:apply-template el-<br>ement causes nodes to be sorted before they are processed by the next set of templates.<br>Options exist to allow sorting on multiple subelements/attributes, by numeric value,<br>and in descending order.<br>10.4.3<br>XQuery<br>The World Wide Web Consortium (W3C) is developing XQuery, a query language<br>for XML. Our discusssion here is based on a draft of the language standard, so the<br>ﬁnal standard may differ; however we expect the main features we cover here will<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>III. Object−Based <br>Databases and XML<br>10. XML<br>380<br>© The McGraw−Hill <br>Companies, 2001<br>378<br>Chapter 10<br>XML<br>not change substantially. The XQuery language derives from an XML query language<br>called Quilt; most of the XQuery features we outline here are part of Quilt. Quilt itself<br>includes features from earlier languages such as XPath, discussed in Section 10.4.1,<br>and two other XML query languages, XQL and XML-QL.<br>Unlike XSLT, XQuery does not represent queries in XML. Instead, they appear more<br>like SQL queries, and are organized into “FLWR” (pronounced “ﬂower”) expressions<br>comprising four sections: for, let, where, and return. The for section gives a series<br>of variables that range over the results of XPath expressions. When more than one<br>variable is speciﬁed, the results include the Cartesian product of the possible values<br>the variables can take, making the for clause similar in spirit to the from clause of<br>an SQL query. The let clause simply allows complicated expressions to be assigned<br>to variable names for simplicity of representation. The where section, like the SQL<br>where clause, performs additional tests on the joined tuples from the for section.<br>Finally, the return section allows the construction of results in XML.<br>A simple FLWR expression that returns the account numbers for checking accounts<br>is based on the XML document of Figure 10.8, which uses ID and IDREFS:<br>for $x in /bank-2/account<br>let $acctno := $x/@account-number<br>where $x/balance &gt; 400<br>return &lt;account-number&gt; $acctno &lt;/account-number&gt;<br>Since this query is simple, the let clause is not essential, and the variable $acctno<br>in the return clause could be replaced with $x/@account-number. Note further that,<br>since the for clause uses XPath expressions, selections may occur within the XPath<br>expression. Thus, an equivalent query may have only for and return clauses:<br>for $x in /bank-2/account[balance &gt; 400]<br>return &lt;account-number&gt; $x/@account-number &lt;/account-number&gt;<br>However, the let clause simpliﬁes complex queries.<br>Path expressions in XQuery may return a multiset, with repeated nodes. The func-<br>tion distinct applied on a multiset, returns a set without duplication. The distinct func-<br>tion can be used even within a for clause. XQuery also provides aggregate functions<br>such as sum and count that can be applied on collections such as sets and multi-<br>sets. While XQuery does not provide a group by construct, aggregate queries can<br>be written by using nested FLWR constructs in place of grouping; we leave details<br>as an exercise for you. Note also that variables assigned by let clauses may be set- or<br>multiset-valued, if the path expression on the right-hand side returns a set or multiset<br>value.<br>Joins are speciﬁed in XQuery much as they are in SQL. The join of depositor, ac-<br>count and customer elements in Figure 10.1, which we wrote in XSLT in Section 10.4.2,<br>can be written in XQuery this way:<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>III. Object−Based <br>Databases and XML<br>10. XML<br>381<br>© The McGraw−Hill <br>Companies, 2001<br>10.4<br>Querying and Transformation<br>379<br>for $b in /bank/account,<br>$c in /bank/customer,<br>$d in /bank/depositor<br>where $a/account-number = $d/account-number<br>and $c/customer-name = $d/customer-name<br>return &lt;cust-acct&gt; $c $a &lt;/cust-acct&gt;<br>The same query can be expressed with the selections speciﬁed as XPath selections:<br>for $a in /bank/account,<br>$c in /bank/customer,<br>$d in /bank/depositor[account-number = $a/account-number<br>and customer-name = $c/customer-name]<br>return &lt;cust-acct&gt; $c $a&lt;/cust-acct&gt;<br>XQuery FLWR expressions can be nested in the return clause, in order to generate<br>element nestings that do not appear in the source document. This feature is similar<br>to nested subqueries in the from clause of SQL queries in Section 9.5.3.<br>For instance, the XML structure shown in Figure 10.3, with account elements nested<br>within customer elements, can be generated from the structure in Figure 10.1 by this<br>query:<br>&lt;bank-1&gt;<br>for $c in /bank/customer<br>return<br>&lt;customer&gt;<br>$c/*<br>for $d in /bank/depositor[customer-name = $c/customer-name],<br>$a in /bank/account[account-number=$d/account-number]<br>return $a<br>&lt;/customer&gt;<br>&lt;/bank-1&gt;<br>The query also introduces the syntax $c/*, which refers to all the children of the node,<br>which is bound to the variable $c. Similarly, $c/text() gives the text content of an<br>element, without the tags.<br>Path expressions in XQuery are based on path expressions in XPath, but XQuery<br>provides some extensions (which may eventually be added to XPath itself). One of<br>the useful syntax extensions is the operator -&gt;, which can be used to dereference<br>IDREFs, just like the function id(). The operator can be applied on a value of type<br>IDREFS to get a set of elements. It can be used, for example, to ﬁnd all the accounts<br>associated with a customer, with the ID/IDREFS representation of bank information.<br>We leave details to the reader.<br>Results can be sorted in XQuery if a sortby clause is included at the end of any ex-<br>pression; the clause speciﬁes how the instances of that expression should be sorted.<br>For instance, this query outputs all customer elements sorted by the name subele-<br>ment:<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>III. Object−Based <br>Databases and XML<br>10. XML<br>382<br>© The McGraw−Hill <br>Companies, 2001<br>380<br>Chapter 10<br>XML<br>for $c in /bank/customer,<br>return &lt;customer&gt; $c/* &lt;/customer&gt; sortby(name)<br>To sort in descending order, we can use sortby(name descending).<br>Sorting can be done at multiple levels of nesting. For instance, we can get a nested<br>representation of bank information sorted in customer name order, with accounts of<br>each customer sorted by account number, as follows.<br>&lt;bank-1&gt;<br>for $c in /bank/customer<br>return<br>&lt;customer&gt;<br>$c/*<br>for $d in /bank/depositor[customer-name = $c/customer-name],<br>$a in /bank/account[account-number=$d/account-number]<br>return &lt;account&gt; $a/* &lt;/account&gt; sortby(account-number)<br>&lt;/customer&gt; sortby(customer-name)<br>&lt;/bank-1&gt;<br>XQuery provides a variety of built-in functions, and supports user-deﬁned func-<br>tions. For instance, the built-in function document(name) returns the root of a named<br>document; the root can then be used in a path expression to access the contents of the<br>document. Users can deﬁne functions as illustrated by this function, which returns a<br>list of all balances of a customer with a speciﬁed name:<br>function balances(xsd:string $c) returns list(xsd:numeric) {<br>for $d in /bank/depositor[customer-name = $c],<br>$a in /bank/account[account-number=$d/account-number]<br>return $a/balance<br>}<br>XQuery uses the type system of XMLSchema. XQuery also provides functions to con-<br>vert between types. For instance, number(x) converts a string to a number.<br>XQuery offers a variety of other features, such as if-then-else clauses, which can be<br>used within return clauses, and existential and universal quantiﬁcation, which can<br>be used in predicates in where clauses. For example, existential quantiﬁcation can be<br>expressed using some $e in path satisﬁes P where path is a path expression, and P<br>is a predicate which can use $e. Universal quantiﬁcation can be expressed by using<br>every in place of some.<br>10.5<br>The Application Program Interface<br>With the wide acceptance of XML as a data representation and exchange format, soft-<br>ware tools are widely available for manipulation of XML data. In fact, there are two<br>standard models for programmatic manipulation of XML, each available for use with<br>a wide variety of popular programming languages.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>III. Object−Based <br>Databases and XML<br>10. XML<br>383<br>© The McGraw−Hill <br>Companies, 2001<br>10.6<br>Storage of XML Data<br>381<br>One of the standard APIs for manipulating XML is the document object model (DOM),<br>which treats XML content as a tree, with each element represented by a node, called<br>a DOMNode. Programs may access parts of the document in a navigational fashion,<br>beginning with the root.<br>DOM libraries are available for most common programming langauges and are<br>even present in Web browsers, where it may be used to manipulate the document<br>displayed to the user. We outline here some of the interfaces and methods in the Java<br>API for DOM, to give a ﬂavor of DOM. The Java DOM API provides an interface called<br>Node, and interfaces Element and Attribute, which inherit from the Node interface.<br>The Node interface provides methods such as getParentNode(), getFirstChild(), and<br>getNextSibling(), to navigate the DOM tree, starting with the root node. Subelements<br>of an element can be accessed by name getElementsByTagName(name), which re-<br>turns a list of all child elements with a speciﬁed tag name; individual members of<br>the list can be accessed by the method item(i), which returns the ith element in the<br>list. Attribute values of an element can be accessed by name, using the method getAt-<br>tribute(name). The text value of an element is modeled as a Text node, which is a child<br>of the element node; an element node with no subelements has only one such child<br>node. The method getData() on the Text node returns the text contents. DOM also<br>provides a variety of functions for updating the document by adding and deleting<br>attribute and element children of a node, setting node values, and so on.<br>Many more details are required for writing an actual DOM program; see the biblio-<br>graphical notes for references to further information.<br>DOM can be used to access XML data stored in databases, and an XML database<br>can be built using DOM as its primary interface for accessing and modifying data.<br>However, the DOM interface does not support any form of declarative querying.<br>The second programming interface we discuss, the Simple API for XML (SAX) is an<br>event model, designed to provide a common interface between parsers and applica-<br>tions. This API is built on the notion of event handlers, which consists of user-speciﬁed<br>functions associated with parsing events. Parsing events correspond to the recogni-<br>tion of parts of a document; for example, an event is generated when the start-tag is<br>found for an element, and another event is generated when the end-tag is found. The<br>pieces of a document are always encountered in order from start to ﬁnish. SAX is not<br>appropriate for database applications.<br>10.6<br>Storage of XML Data<br>Many applications require storage of XML data. One way to store XML data is to<br>convert it to relational representation, and store it in a relational database. There are<br>several alternatives for storing XML data, brieﬂy outlined here.<br>10.6.1<br>Relational Databases<br>Since relational databases are widely used in existing applications, there is a great<br>beneﬁt to be had in storing XML data in relational databases, so that the data can be<br>accessed from existing applications.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>III. Object−Based <br>Databases and XML<br>10. XML<br>384<br>© The McGraw−Hill <br>Companies, 2001<br>382<br>Chapter 10<br>XML<br>Converting XML data to relational form is usually straightforward if the data were<br>generated from a relational schema in the ﬁrst place, and XML was used merely as<br>a data exchange format for relational data. However, there are many applications<br>where the XML data is not generated from a relational schema, and translating the<br>data to relational form for storage may not be straightforward. In particular, nested<br>elements and elements that recur (corresponding to set valued attributes) complicate<br>storage of XML data in relational format. Several alternative approaches are available:<br>• Store as string. A simple way to store XML data in a relational database is to<br>store each child element of the top-level element as a string in a separate tuple<br>in the database. For instance, the XML data in Figure 10.1 could be stored as<br>a set of tuples in a relation elements(data), with the attribute data of each tuple<br>storing one XML element (account, customer, or depositor) in string form.<br>While the above representation is easy to use, the database system does<br>not know the schema of the stored elements. As a result, it is not possible<br>to query the data directly. In fact, it is not even possible to implement simple<br>selections such as ﬁnding all account elements, or ﬁnding the account element<br>with account number A-401, without scanning all tuples of the relation and<br>examining the contents of the strin</span><br><br><span style="background-color: #FFC6FF;" title="Chunk 47 | Start: 940094 | End: 960094 | Tokens: 3194">g stored in the tuple.<br>A partial solution to this problem is to store different types of elements<br>in different relations, and also store the values of some critical elements as<br>attributes of the relation to enable indexing. For instance, in our example, the<br>relations would be account-elements, customer-elements, and depositor-elements,<br>each with an attribute data. Each relation may have extra attributes to store the<br>values of some subelements, such as account-number or customer-name. Thus, a<br>query that requires account elements with a speciﬁed account number can be<br>answered efﬁciently with this representation. Such an approach depends on<br>type information about XML data, such as the DTD of the data.<br>Some database systems, such as Oracle 9, support function indices, which<br>can help avoid replication of attributes between the XML string and relation<br>attributes. Unlike normal indices, which are on attribute values, function in-<br>dices can be built on the result of applying user-deﬁned functions on tuples.<br>For instance, a function index can be built on a user-deﬁned function that re-<br>turns the value of the account-number subelement of the XML string in a tuple.<br>The index can then be used in the same way as an index on a account-number<br>attribute.<br>The above approaches have the drawback that a large part of the XML in-<br>formation is stored within strings. It is possible to store all the information in<br>relations in one of several ways which we examine next.<br>• Tree representation. Arbitrary XML data can be modeled as a tree and stored<br>using a pair of relations:<br>nodes(id, type, label, value)<br>child(child-id, parent-id)<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>III. Object−Based <br>Databases and XML<br>10. XML<br>385<br>© The McGraw−Hill <br>Companies, 2001<br>10.6<br>Storage of XML Data<br>383<br>Each element and attribute in the XML data is given a unique identiﬁer. A tu-<br>ple inserted in the nodes relation for each element and attribute with its iden-<br>tiﬁer (id), its type (attribute or element), the name of the element or attribute<br>(label), and the text value of the element or attribute (value). The relation child<br>is used to record the parent element of each element and attribute. If order<br>information of elements and attributes must be preserved, an extra attribute<br>position can be added to the child relation to indicate the relative position of<br>the child among the children of the parent. As an exercise, you can represent<br>the XML data of Figure 10.1 by using this technique.<br>This representation has the advantage that all XML information can be rep-<br>resented directly in relational form, and many XML queries can be translated<br>into relational queries and executed inside the database system. However, it<br>has the drawback that each element gets broken up into many pieces, and a<br>large number of joins are required to reassemble elements.<br>• Map to relations. In this approach, XML elements whose schema is known are<br>mapped to relations and attributes. Elements whose schema is unknown are<br>stored as strings, or as a tree representation.<br>A relation is created for each element type whose schema is known. All<br>attributes of these elements are stored as attributes of the relation. All subele-<br>ments that occur at most once inside these element (as speciﬁed in the DTD)<br>can also be represented as attributes of the relation; if the subelement can con-<br>tain only text, the attribute stores the text value. Otherwise, the relation corre-<br>sponding to the subelement stores the contents of the subelement, along with<br>an identiﬁer for the parent type and the attribute stores the identiﬁer of the<br>subelement. If the subelement has further nested subelements, the same pro-<br>cedure is applied to the subelement.<br>If a subelement can occur multiple times in an element, the map-to-relations<br>approach stores the contents of the subelements in the relation corresponding<br>to the subelement. It gives both parent and subelement unique identiﬁers, and<br>creates a separate relation, similar to the child relation we saw earlier in the<br>tree representation, to identify which subelement occurs under which parent.<br>Note that when we apply this appoach to the DTD of the data in Figure 10.1,<br>we get back the original relational schema that we have used in earlier chap-<br>ters. The bibliographical notes provide references to such hybrid approaches.<br>10.6.2<br>Nonrelational Data Stores<br>There are several alternatives for storing XML data in nonrelational data storage sys-<br>tems:<br>• Store in ﬂat ﬁles. Since XML is primarily a ﬁle format, a natural storage mech-<br>anism is simply a ﬂat ﬁle. This approach has many of the drawbacks, outlined<br>in Chapter 1, of using ﬁle systems as the basis for database applications. In<br>particular, it lacks data isolation, integrity checks, atomicity, concurrent ac-<br>cess, and security. However, the wide availability of XML tools that work on<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>III. Object−Based <br>Databases and XML<br>10. XML<br>386<br>© The McGraw−Hill <br>Companies, 2001<br>384<br>Chapter 10<br>XML<br>ﬁle data makes it relatively easy to access and query XML data stored in ﬁles.<br>Thus, this storage format may be sufﬁcient for some applications.<br>• Store in an XML Database. XML databases are databases that use XML as<br>their basic data model. Early XML databases implemented the Document Ob-<br>ject Model on a C++-based object-oriented database. This allows much of the<br>object-oriented database infrastucture to be reused, while using a standard<br>XML interface. The addition of an XML query language provides declarative<br>querying. It is also possible to build XML databases as a layer on top of rela-<br>tional databases.<br>10.7<br>XML Applications<br>A central design goal for XML is to make it easier to communicate information, on the<br>Web and between applications, by allowing the semantics of the data to be described<br>with the data itself. Thus, while the large amount of XML data and its use in business<br>applications will undoubtably require and beneﬁt from database technologies, XML<br>is foremost a means of communication. Two applications of XML for communication<br>—exchange of data, and mediation of Web information resources—illustrate how<br>XML achieves its goal of supporting data exchange and demonstrate how database<br>technology and interaction are key in supporting exchange-based applications.<br>10.7.1<br>Exchange of Data<br>Standards are being developed for XML representation of data for a variety of special-<br>ized applications ranging from business applications such as banking and shipping<br>to scientiﬁc applications such as chemistry and molecular biology. Some examples:<br>• The chemical industry needs information about chemicals, such as their molec-<br>ular structure, and a variety of important properties such as boiling and melt-<br>ing points, caloriﬁc values, solubility in various solvents, and so on. ChemML<br>is a standard for representing such information.<br>• In shipping, carriers of goods and customs and tax ofﬁcials need shipment<br>records containing detailed information about the goods being shipped, from<br>whom and to where they were sent, to whom and to where they are being<br>shipped, the monetary value of the goods, and so on.<br>• An online marketplace in which business can buy and sell goods (a so-called<br>business-to-business B2B market) requires information such as product cata-<br>logs, including detailed product descriptions and price information, product<br>inventories, offers to buy, and quotes for a proposed sale.<br>Using normalized relational schemas to model such complex data requirements<br>results in a large number of relations, which is often hard for users to manage. The<br>relations often have large numbers of attributes; explicit representation of attribute/-<br>element names along with values in XML helps avoid confusion between attributes.<br>Nested element representations help reduce the number of relations that must be<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>III. Object−Based <br>Databases and XML<br>10. XML<br>387<br>© The McGraw−Hill <br>Companies, 2001<br>10.7<br>XML Applications<br>385<br>represented, as well as the number of joins required to get required information, at<br>the possible cost of redundancy. For instance, in our bank example, listing customers<br>with account elements nested within account elements, as in Figure 10.3, results in a<br>format that is more natural for some applications, in particular for humans to read,<br>than is the normalized representation in Figure 10.1.<br>When XML is used to exchange data between business applications, the data most<br>often originate in relational databases. Data in relational databases must be published,<br>that is, converted to XML form, for export to other applications. Incoming data must<br>be shredded, that is, converted back from XML to normalized relation form and stored<br>in a relational database. While application code can perform the publishing and<br>shredding operations, the operations are so common that the conversions should<br>be done automatically, without writing application code, where possible. Database<br>vendors are therefore working to XML-enable their database products.<br>An XML-enabled database supports an automatic mapping from its internal model<br>(relational, object-relational or object-oriented) to XML. These mappings may be sim-<br>ple or complex. A simple mapping might assign an element to every row of a table,<br>and make each column in that row either an attribute or a subelement of the row’s<br>element. Such a mapping is straightforward to generate automatically. A more com-<br>plicated mapping would allow nested structures to be created. Extensions of SQL<br>with nested queries in the select clause have been developed to allow easy creation<br>of nested XML output. Some database products also allow XML queries to access re-<br>lational data by treating the XML form of relational data as a virtual XML document.<br>10.7.1.1<br>Data Mediation<br>Comparison shopping is an example of a mediation application, in which data about<br>items, inventory, pricing, and shipping costs are extracted from a variety of Web sites<br>offering a particular item for sale. The resulting aggregated information is signiﬁ-<br>cantly more valuable than the individual information offered by a single site.<br>A personal ﬁnancial manager is a similar application in the context of banking.<br>Consider a consumer with a variety of accounts to manage, such as bank accounts,<br>savings accounts, and retirement accounts. Suppose that these accounts may be held<br>at different institutions. Providing centralized management for all accounts of a cus-<br>tomer is a major challenge. XML-based mediation addresses the problem by extract-<br>ing an XML representation of account information from the respective Web sites of<br>the ﬁnancial institutions where the individual holds accounts. This information may<br>be extracted easily if the institution exports it in a standard XML format, and un-<br>doubtedly some will. For those that do not, wrapper software is used to generate XML<br>data from HTML Web pages returned by the Web site. Wrapper applications need<br>constant maintenance, since they depend on formatting details of Web pages, which<br>change often. Nevertheless, the value provided by mediation often justiﬁes the effort<br>required to develop and maintain wrappers.<br>Once the basic tools are available to extract information from each source, a medi-<br>ator application is used to combine the extracted information under a single schema.<br>This may require further transformation of the XML data from each site, since dif-<br>ferent sites may structure the same information differently. For instance, one of the<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>III. Object−Based <br>Databases and XML<br>10. XML<br>388<br>© The McGraw−Hill <br>Companies, 2001<br>386<br>Chapter 10<br>XML<br>banks may export information in the format in Figure 10.1, while another may use the<br>nested format in Figure 10.3. They may also use different names for the same informa-<br>tion (for instance, acct-number and account-id), or may even use the same name for<br>different information. The mediator must decide on a single schema that represents<br>all required information, and must provide code to transform data between different<br>representations. Such issues are discussed in more detail in Section 19.8, in the con-<br>text of distributed databases. XML query languages such as XSLT and XQuery play an<br>important role in the task of transformation between different XML representations.<br>10.8<br>Summary<br>• Like the Hyper-Text Markup Language, HTML, on which the Web is based, the<br>Extensible Markup Language, XML, is a descendant of the Standard General-<br>ized Markup Language (SGML). XML was originally intended for providing<br>functional markup for Web documents, but has now become the defacto stan-<br>dard data format for data exchange between applications.<br>• XML documents contain elements, with matching starting and ending tags<br>indicating the beginning and end of an element. Elements may have subele-<br>ments nested within them, to any level of nesting. Elements may also have<br>attributes. The choice between representing information as attributes and sub-<br>elements is often arbitrary in the context of data representation.<br>• Elements may have an attribute of type ID that stores a unique identiﬁer for the<br>element. Elements may also store references to other elements using attributes<br>of type IDREF. Attributes of type IDREFS can store a list of references.<br>• Documents may optionally have their schema speciﬁed by a Document Type<br>Declaration, DTD. The DTD of a document speciﬁes what elements may occur,<br>how they may be nested, and what attributes each element may have.<br>• Although DTDs are widely used, they have several limitations. For instance,<br>they do not provide a type system. XMLSchema is a new standard for spec-<br>ifying the schema of a document. While it provides more expressive power,<br>including a powerful type system, it is also more complicated.<br>• XML data can be represented as tree structures, with nodes corresponding to<br>elements and attributes. Nesting of elements is reﬂected by the parent-child<br>structure of the tree representation.<br>• Path expressions can be used to traverse the XML tree structure, to locate re-<br>quired data. XPath is a standard language for path expressions, and allows<br>required elements to be speciﬁed by a ﬁle-system-like path, and additionally<br>allows selections and other features. XPath also forms part of other XML query<br>languages.<br>• The XSLT language was originally designed as the transformation language<br>for a style sheet facility, in other words, to apply formatting information to<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>III. Object−Based <br>Databases and XML<br>10. XML<br>389<br>© The McGraw−Hill <br>Companies, 2001<br>10.8<br>Summary<br>387<br>XML documents. However, XSLT offers quite powerful querying and transfor-<br>mation features and is widely available, so it is used for quering XML data.<br>• XSLT programs contain a series of templates, each with a match part and a<br>select part. Each element in the input XML data is matched against available<br>templates, and the select part of the ﬁrst matching template is applied to the<br>element.<br>Templates can be applied recursively, from within the body of another tem-<br>plate, a procedure known as structural recursion. XSLT supports keys, which<br>can be used to implement some types of joins. It also supports sorting and<br>other querying facilities.<br>• The XQuery language, which is currently being standardized, is based on the<br>Quilt query language. The XQuery language is similar to SQL, with for, let,<br>where, and return clauses.<br>However, it supports many extensions to deal with the tree nature of XML<br>and to allow for the transformation of XML documents into other documents<br>with a signiﬁcantly different structure.<br>• XML data can be stored in any of several different ways. For example, XML<br>data can be stored as strings in a relational database. Alternatively, relations<br>can represent XML data as trees. As another alternative, XML data can be<br>mapped to relations in the same way that E-R schemas are mapped to rela-<br>tional schemas.<br>XML data may also be stored in ﬁle systems, or in XML-databases, which<br>use XML as their internal representation.<br>• The ability to transform documents in languages such as XSLT and XQuery<br>is a key to the use of XML in mediation applications, such as electronic busi-<br>ness exchanges and the extraction and combination of Web data for use by a<br>personal ﬁnance manager or comparison shopper.<br>Review Terms<br>• Extensible Markup Language<br>(XML)<br>• Hyper-Text Markup Language<br>(HTML)<br>• Standard Generalized Markup<br>Language<br>• Markup language<br>• Tags<br>• Self-documenting<br>• Element<br>• Root element<br>• Nested elements<br>• Attribute<br>• Namespace<br>• Default namespace<br>• Schema deﬁnition<br>  Document Type Deﬁnition<br>(DTD)<br>  XMLSchema<br>• ID<br>• IDREF and IDREFS<br>• Tree model of XML data<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>III. Object−Based <br>Databases and XML<br>10. XML<br>390<br>© The McGraw−Hill <br>Companies, 2001<br>388<br>Chapter 10<br>XML<br>• Nodes<br>• Querying and transformation<br>• Path expressions<br>• XPath<br>• Style sheet<br>• XML Style sheet Language (XSL)<br>• XSL Transformations (XSLT)<br>  Templates<br>–– Match<br>–– Select<br>  Structural recursion<br>  Keys<br>  Sorting<br>• XQuery<br>  FLWR expressions<br>–– for<br>–– let<br>–– where<br>–– return<br>  Joins<br>  Nested FLWR expression<br>  Sorting<br>• XML API<br>• Document Object Model (DOM)<br>• Simple API for XML (SAX)<br>• Storage of XML data<br>  In relational databases<br>–– Store as string<br>–– Tree representation<br>–– Map to relations<br>  In nonrelational data stores<br>–– Files<br>–– XML-databases<br>• XML Applications<br>  Exchange of data<br>–– Publish and shred<br>  Data mediation<br>–– Wrapper software<br>• XML-Enabled database<br>Exercises<br>10.1 Give an alternative representation of bank information containing the same<br>data as in Figure 10.1, but using attributes instead of subelements. Also give<br>the DTD for this representation.<br>10.2 Show, by giving a DTD, how to represent the books nested-relation from Sec-<br>tion 9.1, using XML.<br>10.3 Give the DTD for an XML representation of the following nested-relational<br>schema<br>Emp = (ename, ChildrenSet setof(Children), SkillsSet setof(Skills))<br>Children = (name, Birthday)<br>Birthday = (day, month, year)<br>Skills = (type, ExamsSet setof(Exams))<br>Exams = (year, city)<br>10.4 Write the following queries in XQuery, assuming the DTD from Exercise 10.3.<br>a. Find the names of all employees who have a child who has a birthday in<br>March.<br>b. Find those employees who took an examination for the skill type “typing”<br>in the city “Dayton”.<br>c. List all skill types in Emp.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>III. Object−Based <br>Databases and XML<br>10. XML<br>391<br>© The McGraw−Hill <br>Companies, 2001<br>Exercises<br>389<br>&lt;!DOCTYPE bibliography [<br>&lt;!ELEMENT book (title, author+, year, publisher, place?)&gt;<br>&lt;!ELEMENT article (title, author+, journal, year, number, volume, pages?)&gt;<br>&lt;!ELEMENT author ( last-name, ﬁrst-name) &gt;<br>&lt;!ELEMENT title ( #PCDATA )&gt;<br>· · · similar PCDATA declarations for year, publisher, place, journal, year,<br>number, volume, pages, last-name and ﬁrst-name<br>] &gt;<br>Figure 10.13<br>DTD for bibliographical data.<br>10.5 Write queries in XSLT and in XPath on the DTD of Exercise 10.3 to list all skill<br>types in Emp.<br>10.6 Write a query in XQuery on the XML representation in Figure 10.1 to ﬁnd the<br>total balance, across all accounts, at each branch. (Hint: Use a nested query to<br>get the effect of an SQL group by.)<br>10.7 Write a query in XQuery on the XML representation in Figure 10.1 to compute<br>the left outer join of customer elements with account elements. (Hint: Use uni-<br>versal quantiﬁcation.)<br>10.8 Give a query in XQuery to ﬂip the nesting of data from Exercise 10.2. That is, at<br>the outermost level of nesting the output must have elements corresponding to<br>authors, and each such element must have nested within it items correspond-<br>ing to all the books written by the author.<br>10.9 Give the DTD for an XML representation of the information in Figure 2.29. Cre-<br>ate a separate element type to </span><br><br><span style="background-color: #FFADAD;" title="Chunk 48 | Start: 960096 | End: 980096 | Tokens: 3268">represent each relationship, but use ID and IDREF<br>to implement primary and foreign keys.<br>10.10 Write queries in XSLT and XQuery to output customer elements with associ-<br>ated account elements nested within the customer elements, given the bank<br>information representation using ID and IDREFS in Figure 10.8.<br>10.11 Give a relational schema to represent bibliographical information speciﬁed as<br>per the DTD fragment in Figure 10.13. The relational schema must keep track<br>of the order of author elements. You can assume that only books and articles<br>appear as top level elements in XML documents.<br>10.12 Consider Exercise 10.11, and suppose that authors could also appear as top<br>level elements. What change would have to be done to the relational schema.<br>10.13 Write queries in XQuery on the bibliography DTD fragment in Figure 10.13, to<br>do the following.<br>a. Find all authors who have authored a book and an article in the same year.<br>b. Display books and articles sorted by year.<br>c. Display books with more than one author.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>III. Object−Based <br>Databases and XML<br>10. XML<br>392<br>© The McGraw−Hill <br>Companies, 2001<br>390<br>Chapter 10<br>XML<br>10.14 Show the tree representation of the XML data in Figure 10.1, and the represen-<br>tation of the tree using nodes and child relations described in Section 10.6.1.<br>10.15 Consider the following recursive DTD.<br>&lt;!DOCTYPE parts [<br>&lt;!ELEMENT part (name, subpartinfo*)&gt;<br>&lt;!ELEMENT subpartinfo (part, quantity)&gt;<br>&lt;!ELEMENT name ( #PCDATA )&gt;<br>&lt;!ELEMENT quantity ( #PCDATA )&gt;<br>] &gt;<br>a. Give a small example of data corresponding to the above DTD.<br>b. Show how to map this DTD to a relational schema. You can assume that<br>part names are unique, that is, whereever a part appears, its subpart struc-<br>ture will be the same.<br>Bibliographical Notes<br>The XML Cover Pages site (www.oasis-open.org/cover/) contains a wealth of XML<br>information, including tutorial introductions to XML, standards, publications, and<br>software. The World Wide Web Consortium (W3C) acts as the standards body for<br>Web-related standards, including basic XML and all the XML-related languages such<br>as XPath, XSLT and XQuery. A large number of technical reports deﬁning the XML<br>related standards are available at www.w3c.org.<br>Fernandez et al. [2000] gives an algebra for XML. Quilt is described in Chamberlin<br>et al. [2000]. Sahuguet [2001] describes a system, based on the Quilt language, for<br>querying XML. Deutsch et al. [1999b] describes the XML-QL language. Integration of<br>keyword querying into XML is outlined by Florescu et al. [2000]. Query optimiza-<br>tion for XML is described in McHugh and Widom [1999]. Fernandez and Morishima<br>[2001] describe efﬁcient evaluation of XML queries in middleware systems. Other<br>work on querying and manipulating XML data includes Chawathe [1999], Deutsch<br>et al. [1999a], and Shanmugasundaram et al. [2000].<br>Florescu and Kossmann [1999], Kanne and Moerkotte [2000], and Shanmugasun-<br>daram et al. [1999] describe storage of XML data. Schning [2001] describes a database<br>designed for XML. XML support in commercial databases is described in Banerjee<br>et al. [2000], Cheng and Xu [2000] and Rys [2001]. See Chapters 25 through 27 for<br>more information on XML support in commercial databases. The use of XML for data<br>integration is described by Liu et al. [2000], Draper et al. [2001], Baru et al. [1999], and<br>Carey et al. [2000].<br>Tools<br>A number of tools to deal with XML are available in the public domain. The site<br>www.oasis-open.org/cover/ contains links to a variety of software tools for XML and<br>XSL (including XSLT). Kweelt (available at http://db.cis.upenn.edu/Kweelt/) is a pub-<br>licly available XML querying system based on the Quilt language.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>IV. Data Storage and <br>Querying<br>Introduction<br>393<br>© The McGraw−Hill <br>Companies, 2001<br>P<br>A<br>R<br>T<br>4<br>Data Storage and Querying<br>Although a database system provides a high-level view of data, ultimately data have<br>to be stored as bits on one or more storage devices. A vast majority of databases today<br>store data on magnetic disk and fetch data into main space memory for processing,<br>or copy data onto tapes and other backup devices for archival storage. The physical<br>characteristics of storage devices play a major role in the way data are stored, in<br>particular because access to a random piece of data on disk is much slower than<br>memory access: Disk access takes tens of milliseconds, whereas memory access takes<br>a tenth of a microsecond.<br>Chapter 11 begins with an overview of physical storage media, including mecha-<br>nisms to minimize the chance of data loss due to failures. The chapter then describes<br>how records are mapped to ﬁles, which in turn are mapped to bits on the disk. Stor-<br>age and retrieval of objects is also covered in Chapter 11.<br>Many queries reference only a small proportion of the records in a ﬁle. An index<br>is a structure that helps locate desired records of a relation quickly, without examin-<br>ing all records. The index in this textbook is an example, although, unlike database<br>indices, it is meant for human use. Chapter 12 describes several types of indices used<br>in database systems.<br>User queries have to be executed on the database contents, which reside on storage<br>devices. It is usually convenient to break up queries into smaller operations, roughly<br>corresponding to the relational algebra operations. Chapter 13 describes how queries<br>are processed, presenting algorithms for implementing individual operations, and<br>then outlining how the operations are executed in synchrony, to process a query.<br>There are many alternative ways of processing a query, which can have widely<br>varying costs. Query optimization refers to the process of ﬁnding the lowest-cost<br>method of evaluating a given query. Chapter 14 describes the process of query opti-<br>mization.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>IV. Data Storage and <br>Querying<br>11. Storage and File <br>Structure<br>394<br>© The McGraw−Hill <br>Companies, 2001<br>C<br>H<br>A<br>P<br>T<br>E<br>R<br>1<br>1<br>Storage and File Structure<br>In preceding chapters, we have emphasized the higher-level models of a database.<br>For example, at the conceptual or logical level, we viewed the database, in the relational<br>model, as a collection of tables. Indeed, the logical model of the database is the correct<br>level for database users to focus on. This is because the goal of a database system is<br>to simplify and facilitate access to data; users of the system should not be burdened<br>unnecessarily with the physical details of the implementation of the system.<br>In this chapter, however, as well as in Chapters 12, 13, and 14, we probe below<br>the higher levels as we describe various methods for implementing the data models<br>and languages presented in preceding chapters. We start with characteristics of the<br>underlying storage media, such as disk and tape systems. We then deﬁne various<br>data structures that will allow fast access to data. We consider several alternative<br>structures, each best suited to a different kind of access to data. The ﬁnal choice of<br>data structure needs to be made on the basis of the expected use of the system and of<br>the physical characteristics of the speciﬁc machine.<br>11.1<br>Overview of Physical Storage Media<br>Several types of data storage exist in most computer systems. These storage media<br>are classiﬁed by the speed with which data can be accessed, by the cost per unit of<br>data to buy the medium, and by the medium’s reliability. Among the media typically<br>available are these:<br>• Cache. The cache is the fastest and most costly form of storage. Cache memory<br>is small; its use is managed by the computer system hardware. We shall not<br>be concerned about managing cache storage in the database system.<br>• Main memory. The storage medium used for data that are available to be op-<br>erated on is main memory. The general-purpose machine instructions operate<br>on main memory. Although main memory may contain many megabytes of<br>393<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>IV. Data Storage and <br>Querying<br>11. Storage and File <br>Structure<br>395<br>© The McGraw−Hill <br>Companies, 2001<br>394<br>Chapter 11<br>Storage and File Structure<br>data, or even gigabytes of data in large server systems, it is generally too small<br>(or too expensive) for storing the entire database. The contents of main mem-<br>ory are usually lost if a power failure or system crash occurs.<br>• Flash memory. Also known as electrically erasable programmable read-only mem-<br>ory (EEPROM), ﬂash memory differs from main memory in that data survive<br>power failure. Reading data from ﬂash memory takes less than 100 nanosec-<br>onds (a nanosecond is 1/1000 of a microsecond), which is roughly as fast as<br>reading data from main memory. However, writing data to ﬂash memory is<br>more complicated—data can be written once, which takes about 4 to 10 mi-<br>croseconds, but cannot be overwritten directly. To overwrite memory that has<br>been written already, we have to erase an entire bank of memory at once; it<br>is then ready to be written again. A drawback of ﬂash memory is that it can<br>support only a limited number of erase cycles, ranging from 10,000 to 1 mil-<br>lion. Flash memory has found popularity as a replacement for magnetic disks<br>for storing small volumes of data (5 to 10 megabytes) in low-cost computer<br>systems, such as computer systems that are embedded in other devices, in<br>hand-held computers, and in other digital electronic devices such as digital<br>cameras.<br>• Magnetic-disk storage. The primary medium for the long-term on-line stor-<br>age of data is the magnetic disk. Usually, the entire database is stored on mag-<br>netic disk. The system must move the data from disk to main memory so that<br>they can be accessed. After the system has performed the designated opera-<br>tions, the data that have been modiﬁed must be written to disk.<br>The size of magnetic disks currently ranges from a few gigabytes to 80 giga-<br>bytes. Both the lower and upper end of this range have been growing at about<br>50 percent per year, and we can expect much larger capacity disks every year.<br>Disk storage survives power failures and system crashes. Disk-storage devices<br>themselves may sometimes fail and thus destroy data, but such failures usu-<br>ally occur much less frequently than do system crashes.<br>• Optical storage. The most popular forms of optical storage are the compact<br>disk (CD), which can hold about 640 megabytes of data, and the digital video<br>disk (DVD) which can hold 4.7 or 8.5 gigabytes of data per side of the disk (or<br>up to 17 gigabytes on a two-sided disk). Data are stored optically on a disk,<br>and are read by a laser. The optical disks used in read-only compact disks<br>(CD-ROM) or read-only digital video disk (DVD-ROM) cannot be written, but<br>are supplied with data prerecorded.<br>There are “record-once” versions of compact disk (called CD-R) and digital<br>video disk (called DVD-R), which can be written only once; such disks are also<br>called write-once, read-many (WORM) disks. There are also “multiple-write”<br>versions of compact disk (called CD-RW) and digital video disk (DVD-RW and<br>DVD-RAM), which can be written multiple times. Recordable compact disks<br>are magnetic–optical storage devices that use optical means to read magnet-<br>ically encoded data. Such disks are useful for archival storage of data as well<br>as distribution of data.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>IV. Data Storage and <br>Querying<br>11. Storage and File <br>Structure<br>396<br>© The McGraw−Hill <br>Companies, 2001<br>11.1<br>Overview of Physical Storage Media<br>395<br>Jukebox systems contain a few drives and numerous disks that can be<br>loaded into one of the drives automatically (by a robot arm) on demand.<br>• Tape storage. Tape storage is used primarily for backup and archival data.<br>Although magnetic tape is much cheaper than disks, access to data is much<br>slower, because the tape must be accessed sequentially from the beginning.<br>For this reason, tape storage is referred to as sequential-access storage. In con-<br>trast, disk storage is referred to as direct-access storage because it is possible<br>to read data from any location on disk.<br>Tapes have a high capacity (40 gigabyte to 300 gigabytes tapes are currently<br>available), and can be removed from the tape drive, so they are well suited to<br>cheap archival storage. Tape jukeboxes are used to hold exceptionally large<br>collections of data, such as remote-sensing data from satellites, which could<br>include as much as hundreds of terabytes (1 terabyte = 1012 bytes), or even a<br>petabyte (1 petabyte = 1015 bytes) of data.<br>The various storage media can be organized in a hierarchy (Figure 11.1) according<br>to their speed and their cost. The higher levels are expensive, but are fast. As we move<br>down the hierarchy, the cost per bit decreases, whereas the access time increases. This<br>trade-off is reasonable; if a given storage system were both faster and less expensive<br>than another—other properties being the same—then there would be no reason to<br>use the slower, more expensive memory. In fact, many early storage devices, includ-<br>ing paper tape and core memories, are relegated to museums now that magnetic tape<br>and semiconductor memory have become faster and cheaper. Magnetic tapes them-<br>selves were used to store active data back when disks were expensive and had low<br>cache<br>main memory<br>flash memory<br>magnetic disk<br>optical disk<br>magnetic tapes<br>Figure 11.1<br>Storage-device hierarchy.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>IV. Data Storage and <br>Querying<br>11. Storage and File <br>Structure<br>397<br>© The McGraw−Hill <br>Companies, 2001<br>396<br>Chapter 11<br>Storage and File Structure<br>storage capacity. Today, almost all active data are stored on disks, except in rare cases<br>where they are stored on tape or in optical jukeboxes.<br>The fastest storage media—for example, cache and main memory—are referred<br>to as primary storage. The media in the next level in the hierarchy—for example,<br>magnetic disks—are referred to as secondary storage, or online storage. The media<br>in the lowest level in the hierarchy—for example, magnetic tape and optical-disk<br>jukeboxes—are referred to as tertiary storage, or ofﬂine storage.<br>In addition to the speed and cost of the various storage systems, there is also the<br>issue of storage volatility. Volatile storage loses its contents when the power to the<br>device is removed. In the hierarchy shown in Figure 11.1, the storage systems from<br>main memory up are volatile, whereas the storage systems below main memory are<br>nonvolatile. In the absence of expensive battery and generator backup systems, data<br>must be written to nonvolatile storage for safekeeping. We shall return to this subject<br>in Chapter 17.<br>11.2<br>Magnetic Disks<br>Magnetic disks provide the bulk of secondary storage for modern computer systems.<br>Disk capacities have been growing at over 50 percent per year, but the storage re-<br>quirements of large applications have also been growing very fast, in some cases even<br>faster than the growth rate of disk capacities. A large database may require hundreds<br>of disks.<br>11.2.1<br>Physical Characteristics of Disks<br>Physically, disks are relatively simple (Figure 11.2). Each disk platter has a ﬂat cir-<br>cular shape. Its two surfaces are covered with a magnetic material, and information<br>is recorded on the surfaces. Platters are made from rigid metal or glass and are cov-<br>ered (usually on both sides) with magnetic recording material. We call such magnetic<br>disks hard disks, to distinguish them from ﬂoppy disks, which are made from ﬂexi-<br>ble material.<br>When the disk is in use, a drive motor spins it at a constant high speed (usually 60,<br>90, or 120 revolutions per second, but disks running at 250 revolutions per second are<br>available). There is a read–write head positioned just above the surface of the platter.<br>The disk surface is logically divided into tracks, which are subdivided into sectors.<br>A sector is the smallest unit of information that can be read from or written to the<br>disk. In currently available disks, sector sizes are typically 512 bytes; there are over<br>16,000 tracks on each platter, and 2 to 4 platters per disk. The inner tracks (closer to<br>the spindle) are of smaller length, and in current-generation disks, the outer tracks<br>contain more sectors than the inner tracks; typical numbers are around 200 sectors<br>per track in the inner tracks, and around 400 sectors per track in the outer tracks. The<br>numbers above vary among different models; higher-capacity models usually have<br>more sectors per track and more tracks on each platter.<br>The read–write head stores information on a sector magnetically as reversals of<br>the direction of magnetization of the magnetic material. There may be hundreds of<br>concentric tracks on a disk surface, containing thousands of sectors.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>IV. Data Storage and <br>Querying<br>11. Storage and File <br>Structure<br>398<br>© The McGraw−Hill <br>Companies, 2001<br>11.2<br>Magnetic Disks<br>397<br>track t<br>sector s<br>spindle<br>cylinder c<br>platter<br>arm<br>read-write<br>head<br>arm assembly<br>rotation<br>Figure 11.2<br>Moving-head disk mechanism.<br>Each side of a platter of a disk has a read–write head, which moves across the<br>platter to access different tracks. A disk typically contains many platters, and the read<br>–write heads of all the tracks are mounted on a single assembly called a disk arm,<br>and move together. The disk platters mounted on a spindle and the heads mounted<br>on a disk arm are together known as head–disk assemblies. Since the heads on all<br>the platters move together, when the head on one platter is on the ith track, the heads<br>on all other platters are also on the ith track of their respective platters. Hence, the<br>ith tracks of all the platters together are called the ith cylinder.<br>Today, disks with a platter diameter of 3 1<br>2 inches dominate the market. They have<br>a lower cost and faster seek times (due to smaller seek distances) than do the larger-<br>diameter disks (up to 14 inches) that were common earlier, yet they provide high<br>storage capacity. Smaller-diameter disks are used in portable devices such as laptop<br>computers.<br>The read–write heads are kept as close as possible to the disk surface to increase<br>the recording density. The head typically ﬂoats or ﬂies only microns from the disk<br>surface; the spinning of the disk creates a small breeze, and the head assembly is<br>shaped so that the breeze keeps the head ﬂoating just above the disk surface. Because<br>the head ﬂoats so close to the surface, platters must be machined carefully to be ﬂat.<br>Head crashes can be a problem. If the head contacts the disk surface, the head can<br>scrape the recording medium off the disk, destroying the data that had been there.<br>Usually, the head touching the surface causes the removed medium to become air-<br>borne and to come between the other heads and their platters, causing more crashes.<br>Under normal circumstances, a head crash results in failure of the entire disk, which<br>must then be replaced. Current-generation disk drives use a thin ﬁlm of magnetic<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>IV. Data Storage and <br>Querying<br>11. Storage and File <br>Structure<br>399<br>© The McGraw−Hill <br>Companies, 2001<br>398<br>Chapter 11<br>Storage and File Structure<br>metal as recording medium. They are much less susceptible to failure by head crashes<br>than the older oxide-coated disks.<br>A ﬁxed-head disk has a separate head for each track. This arrangement allows the<br>computer to switch from track to track quickly, without having to move the head as-<br>sembly, but because of the large number of heads, the device is extremely expensive.<br>Some disk systems have multiple disk arms, allowing more than one track on the<br>same platter to be accessed at a time. Fixed-head disks and multiple-arm disks were<br>used in high-performance mainframe systems, but are no longer in production.<br>A disk controller interfaces between the computer system and the actual hard-<br>ware of the disk drive. It accepts high-level command</span><br><br><span style="background-color: #FFD6A5;" title="Chunk 49 | Start: 980098 | End: 1000098 | Tokens: 3368">s to read or write a sector, and<br>initiates actions, such as moving the disk arm to the right track and actually reading<br>or writing the data. Disk controllers also attach checksums to each sector that is writ-<br>ten; the checksum is computed from the data written to the sector. When the sector is<br>read back, the controller computes the checksum again from the retrieved data and<br>compares it with the stored checksum; if the data are corrupted, with a high proba-<br>bility the newly computed checksum will not match the stored checksum. If such an<br>error occurs, the controller will retry the read several times; if the error continues to<br>occur, the controller will signal a read failure.<br>Another interesting task that disk controllers perform is remapping of bad sectors.<br>If the controller detects that a sector is damaged when the disk is initially formatted,<br>or when an attempt is made to write the sector, it can logically map the sector to a<br>different physical location (allocated from a pool of extra sectors set aside for this<br>purpose). The remapping is noted on disk or in nonvolatile memory, and the write is<br>carried out on the new location.<br>Figure 11.3 shows how disks are connected to a computer system. Like other stor-<br>age units, disks are connected to a computer system or to a controller through a high-<br>speed interconnection. In modern disk systems, lower-level functions of the disk con-<br>troller, such as control of the disk arm, computing and veriﬁcation of checksums, and<br>remapping of bad sectors, are implemented within the disk drive unit.<br>The AT attachment (ATA) interface (which is a faster version of the integrated<br>drive electronics (IDE) interface used earlier in IBM PCs) and a small-computer-<br>system interconnect (SCSI; pronounced “scuzzy”) are commonly used to connect<br>disk<br>controller<br>system bus<br>disks<br>Figure 11.3<br>Disk subsystem.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>IV. Data Storage and <br>Querying<br>11. Storage and File <br>Structure<br>400<br>© The McGraw−Hill <br>Companies, 2001<br>11.2<br>Magnetic Disks<br>399<br>disks to personal computers and workstations. Mainframe and server systems usu-<br>ally have a faster and more expensive interface, such as high-capacity versions of the<br>SCSI interface, and the Fibre Channel interface.<br>While disks are usually connected directly by cables to the disk controller, they can<br>be situated remotely and connected by a high-speed network to the disk controller. In<br>the storage area network (SAN) architecture, large numbers of disks are connected<br>by a high-speed network to a number of server computers. The disks are usually<br>organized locally using redundant arrays of independent disks (RAID) storage or-<br>ganizations, but the RAID organization may be hidden from the server computers:<br>the disk subsystems pretend each RAID system is a very large and very reliable disk.<br>The controller and the disk continue to use SCSI or Fibre Channel interfaces to talk<br>with each other, although they may be separated by a network. Remote access to<br>disks across a storage area network means that disks can be shared by multiple com-<br>puters, which could run different parts of an application in parallel. Remote access<br>also means that disks containing important data can be kept in a central server room<br>where they can be monitored and maintained by system administrators, instead of<br>being scattered in different parts of an organization.<br>11.2.2<br>Performance Measures of Disks<br>The main measures of the qualities of a disk are capacity, access time, data-transfer<br>rate, and reliability.<br>Access time is the time from when a read or write request is issued to when data<br>transfer begins. To access (that is, to read or write) data on a given sector of a disk,<br>the arm ﬁrst must move so that it is positioned over the correct track, and then must<br>wait for the sector to appear under it as the disk rotates. The time for repositioning<br>the arm is called the seek time, and it increases with the distance that the arm must<br>move. Typical seek times range from 2 to 30 milliseconds, depending on how far the<br>track is from the initial arm position. Smaller disks tend to have lower seek times<br>since the head has to travel a smaller distance.<br>The average seek time is the average of the seek times, measured over a sequence<br>of (uniformly distributed) random requests. If all tracks have the same number of<br>sectors, and we disregard the time required for the head to start moving and to stop<br>moving, we can show that the average seek time is one-third the worst case seek<br>time. Taking these factors into account, the average seek time is around one-half of<br>the maximum seek time. Average seek times currently range between 4 milliseconds<br>and 10 milliseconds, depending on the disk model.<br>Once the seek has started, the time spent waiting for the sector to be accessed<br>to appear under the head is called the rotational latency time. Rotational speeds<br>of disks today range from 5400 rotations per minute (90 rotations per second) up to<br>15,000 rotations per minute (250 rotations per second), or, equivalently, 4 milliseconds<br>to 11.1 milliseconds per rotation. On an average, one-half of a rotation of the disk is<br>required for the beginning of the desired sector to appear under the head. Thus, the<br>average latency time of the disk is one-half the time for a full rotation of the disk.<br>The access time is then the sum of the seek time and the latency, and ranges from<br>8 to 20 milliseconds. Once the ﬁrst sector of the data to be accessed has come under<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>IV. Data Storage and <br>Querying<br>11. Storage and File <br>Structure<br>401<br>© The McGraw−Hill <br>Companies, 2001<br>400<br>Chapter 11<br>Storage and File Structure<br>the head, data transfer begins. The data-transfer rate is the rate at which data can be<br>retrieved from or stored to the disk. Current disk systems claim to support maximum<br>transfer rates of about 25 to 40 megabytes per second, although actual transfer rates<br>may be signiﬁcantly less, at about 4 to 8 megabytes per second.<br>The ﬁnal commonly used measure of a disk is the mean time to failure (MTTF),<br>which is a measure of the reliability of the disk. The mean time to failure of a disk (or<br>of any other system) is the amount of time that, on average, we can expect the system<br>to run continuously without any failure. According to vendors’ claims, the mean<br>time to failure of disks today ranges from 30,000 to 1,200,000 hours—about 3.4 to 136<br>years. In practice the claimed mean time to failure is computed on the probability of<br>failure when the disk is new—the ﬁgure means that given 1000 relatively new disks,<br>if the MTTF is 1,200,000 hours, on an average one of them will fail in 1200 hours. A<br>mean time to failure of 1,200,000 hours does not imply that the disk can be expected<br>to function for 136 years! Most disks have an expected life span of about 5 years, and<br>have signiﬁcantly higher rates of failure once they become more than a few years old.<br>There may be multiple disks sharing a disk interface. The widely used ATA-4 in-<br>terface standard (also called Ultra-DMA) supports 33 megabytes per second transfer<br>rates, while ATA-5 supports 66 megabytes per second. SCSI-3 (Ultra2 wide SCSI)<br>supports 40 megabytes per second, while the more expensive Fibre Channel inter-<br>face supports up to 256 megabytes per second. The transfer rate of the interface is<br>shared between all disks attached to the interface.<br>11.2.3<br>Optimization of Disk-Block Access<br>Requests for disk I/O are generated both by the ﬁle system and by the virtual memory<br>manager found in most operating systems. Each request speciﬁes the address on the<br>disk to be referenced; that address is in the form of a block number. A block is a con-<br>tiguous sequence of sectors from a single track of one platter. Block sizes range from<br>512 bytes to several kilobytes. Data are transferred between disk and main memory in<br>units of blocks. The lower levels of the ﬁle-system manager convert block addresses<br>into the hardware-level cylinder, surface, and sector number.<br>Since access to data on disk is several orders of magnitude slower than access to<br>data in main memory, equipment designers have focused on techniques for improv-<br>ing the speed of access to blocks on disk. One such technique, buffering of blocks<br>in memory to satisfy future requests, is discussed in Section 11.5. Here, we discuss<br>several other techniques.<br>• Scheduling. If several blocks from a cylinder need to be transferred from disk<br>to main memory, we may be able to save access time by requesting the blocks<br>in the order in which they will pass under the heads. If the desired blocks<br>are on different cylinders, it is advantageous to request the blocks in an or-<br>der that minimizes disk-arm movement. Disk-arm–scheduling algorithms<br>attempt to order accesses to tracks in a fashion that increases the number of<br>accesses that can be processed. A commonly used algorithm is the elevator<br>algorithm, which works in the same way many elevators do. Suppose that,<br>initially, the arm is moving from the innermost track toward the outside of<br>the disk. Under the elevator algorithms control, for each track for which there<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>IV. Data Storage and <br>Querying<br>11. Storage and File <br>Structure<br>402<br>© The McGraw−Hill <br>Companies, 2001<br>11.2<br>Magnetic Disks<br>401<br>is an access request, the arm stops at that track, services requests for the track,<br>and then continues moving outward until there are no waiting requests for<br>tracks farther out. At this point, the arm changes direction, and moves toward<br>the inside, again stopping at each track for which there is a request, until it<br>reaches a track where there is no request for tracks farther toward the center.<br>Now, it reverses direction and starts a new cycle. Disk controllers usually per-<br>form the task of reordering read requests to improve performance, since they<br>are intimately aware of the organization of blocks on disk, of the rotational<br>position of the disk platters, and of the position of the disk arm.<br>• File organization. To reduce block-access time, we can organize blocks on disk<br>in a way that corresponds closely to the way we expect data to be accessed.<br>For example, if we expect a ﬁle to be accessed sequentially, then we should<br>ideally keep all the blocks of the ﬁle sequentially on adjacent cylinders. Older<br>operating systems, such as the IBM mainframe operating systems, provided<br>programmers ﬁne control on placement of ﬁles, allowing a programmer to<br>reserve a set of cylinders for storing a ﬁle. However, this control places a bur-<br>den on the programmer or system administrator to decide, for example, how<br>many cylinders to allocate for a ﬁle, and may require costly reorganization if<br>data are inserted to or deleted from the ﬁle.<br>Subsequent operating systems, such as Unix and personal-computer oper-<br>ating systems, hide the disk organization from users, and manage the alloca-<br>tion internally. However, over time, a sequential ﬁle may become fragmented;<br>that is, its blocks become scattered all over the disk. To reduce fragmentation,<br>the system can make a backup copy of the data on disk and restore the entire<br>disk. The restore operation writes back the blocks of each ﬁle contiguously (or<br>nearly so). Some systems (such as different versions of the Windows operating<br>system) have utilities that scan the disk and then move blocks to decrease the<br>fragmentation. The performance increases realized from these techniques can<br>be large, but the system is generally unusable while these utilities operate.<br>• Nonvolatile write buffers. Since the contents of main memory are lost in<br>a power failure, information about database updates has to be recorded on<br>disk to survive possible system crashes. For this reason, the performance of<br>update-intensive database applications, such as transaction-processing sys-<br>tems, is heavily dependent on the speed of disk writes.<br>We can use nonvolatile random-access memory (NV-RAM) to speed up<br>disk writes drastically. The contents of nonvolatile RAM are not lost in power<br>failure. A common way to implement nonvolatile RAM is to use battery–<br>backed-up RAM. The idea is that, when the database system (or the operat-<br>ing system) requests that a block be written to disk, the disk controller writes<br>the block to a nonvolatile RAM buffer, and immediately notiﬁes the operating<br>system that the write completed successfully. The controller writes the data to<br>their destination on disk whenever the disk does not have any other requests,<br>or when the nonvolatile RAM buffer becomes full. When the database system<br>requests a block write, it notices a delay only if the nonvolatile RAM buffer<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>IV. Data Storage and <br>Querying<br>11. Storage and File <br>Structure<br>403<br>© The McGraw−Hill <br>Companies, 2001<br>402<br>Chapter 11<br>Storage and File Structure<br>is full. On recovery from a system crash, any pending buffered writes in the<br>nonvolatile RAM are written back to the disk.<br>An example illustrates how much nonvolatile RAM improves performance.<br>Assume that write requests are received in a random fashion, with the disk<br>being busy on average 90 percent of the time.1 If we have a nonvolatile RAM<br>buffer of 50 blocks, then, on average, only once per minute will a write ﬁnd<br>the buffer to be full (and therefore have to wait for a disk write to ﬁnish). Dou-<br>bling the buffer to 100 blocks results in approximately only one write per hour<br>ﬁnding the buffer to be full. Thus, in most cases, disk writes can be executed<br>without the database system waiting for a seek or rotational latency.<br>• Log disk. Another approach to reducing write latencies is to use a log disk—<br>that is, a disk devoted to writing a sequential log—in much the same way as<br>a nonvolatile RAM buffer. All access to the log disk is sequential, essentially<br>eliminating seek time, and several consecutive blocks can be written at once,<br>making writes to the log disk several times faster than random writes. As<br>before, the data have to be written to their actual location on disk as well, but<br>the log disk can do the write later, without the database system having to wait<br>for the write to complete. Furthermore, the log disk can reorder the writes to<br>minimize disk arm movement. If the system crashes before some writes to the<br>actual disk location have completed, when the system comes back up it reads<br>the log disk to ﬁnd those writes that had not been completed, and carries them<br>out then.<br>File systems that support log disks as above are called journaling ﬁle sys-<br>tems. Journaling ﬁle systems can be implemented even without a separate log<br>disk, keeping data and the log on the same disk. Doing so reduces the mone-<br>tary cost, at the expense of lower performance.<br>The log-based ﬁle system is an extreme version of the log-disk approach.<br>Data are not written back to their original destination on disk; instead, the<br>ﬁle system keeps track of where in the log disk the blocks were written most<br>recently, and retrieves them from that location. The log disk itself is compacted<br>periodically, so that old writes that have subsequently been overwritten can<br>be removed. This approach improves write performance, but generates a high<br>degree of fragmentation for ﬁles that are updated often. As we noted earlier,<br>such fragmentation increases seek time for sequential reading of ﬁles.<br>11.3<br>RAID<br>The data storage requirements of some applications (in particular Web, database, and<br>multimedia data applications) have been growing so fast that a large number of disks<br>are needed to store data for such applications, even though disk drive capacities have<br>been growing very fast.<br>1.<br>For the statistically inclined reader, we assume Poisson distribution of arrivals. The exact arrival rate<br>and rate of service are not needed since the disk utilization provides enough information for our calcula-<br>tions.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>IV. Data Storage and <br>Querying<br>11. Storage and File <br>Structure<br>404<br>© The McGraw−Hill <br>Companies, 2001<br>11.3<br>RAID<br>403<br>Having a large number of disks in a system presents opportunities for improving<br>the rate at which data can be read or written, if the disks are operated in parallel. Par-<br>allelism can also be used to perform several independent reads or writes in parallel.<br>Furthermore, this setup offers the potential for improving the reliability of data stor-<br>age, because redundant information can be stored on multiple disks. Thus, failure of<br>one disk does not lead to loss of data.<br>A variety of disk-organization techniques, collectively called redundant arrays of<br>independent disks (RAID), have been proposed to achieve improved performance<br>and reliability.<br>In the past, system designers viewed storage systems composed of several small<br>cheap disks as a cost-effective alternative to using large, expensive disks; the cost per<br>megabyte of the smaller disks was less than that of larger disks. In fact, the I in RAID,<br>which now stands for independent, originally stood for inexpensive. Today, however,<br>all disks are physically small, and larger-capacity disks actually have a lower cost per<br>megabyte. RAID systems are used for their higher reliability and higher performance<br>rate, rather than for economic reasons.<br>11.3.1<br>Improvement of Reliability via Redundancy<br>Let us ﬁrst consider reliability. The chance that some disk out of a set of N disks will<br>fail is much higher than the chance that a speciﬁc single disk will fail. Suppose that<br>the mean time to failure of a disk is 100,000 hours, or slightly over 11 years. Then,<br>the mean time to failure of some disk in an array of 100 disks will be 100,000 / 100 =<br>1000 hours, or around 42 days, which is not long at all! If we store only one copy of<br>the data, then each disk failure will result in loss of a signiﬁcant amount of data (as<br>discussed in Section 11.2.1). Such a high rate of data loss is unacceptable.<br>The solution to the problem of reliability is to introduce redundancy; that is, we<br>store extra information that is not needed normally, but that can be used in the event<br>of failure of a disk to rebuild the lost information. Thus, even if a disk fails, data are<br>not lost, so the effective mean time to failure is increased, provided that we count<br>only failures that lead to loss of data or to nonavailability of data.<br>The simplest (but most expensive) approach to introducing redundancy is to du-<br>plicate every disk. This technique is called mirroring (or, sometimes, shadowing). A<br>logical disk then consists of two physical disks, and every write is carried out on both<br>disks. If one of the disks fails, the data can be read from the other. Data will be lost<br>only if the second disk fails before the ﬁrst failed disk is repaired.<br>The mean time to failure (where failure is the loss of data) of a mirrored disk de-<br>pends on the mean time to failure of the individual disks, as well as on the mean<br>time to repair, which is the time it takes (on an average) to replace a failed disk and<br>to restore the data on it. Suppose that the failures of the two disks are independent;<br>that is, there is no connection between the failure of one disk and the failure of the<br>other. Then, if the mean time to failure of a single disk is 100,000 hours, and the mean<br>time to repair is 10 hours, then the mean time to data loss of a mirrored disk system is<br>1000002/(2 ∗10) = 500∗106 hours, or 57,000 years! (We do not go into the derivations<br>here; references in the bibliographical notes provide the details.)<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>IV. Data Storage and <br>Querying<br>11. Storage and File <br>Structure<br>405<br>© The McGraw−Hill <br>Companies, 2001<br>404<br>Chapter 11<br>Storage and File Structure<br>You should be aware that the assumption of independence of disk failures is not<br>valid. Power failures, and natural disasters such as earthquakes, ﬁres, and ﬂoods,<br>may result in damage to both disks at the same time. As disks age, the probability of<br></span><br><br><span style="background-color: #FDFFB6;" title="Chunk 50 | Start: 1000100 | End: 1020100 | Tokens: 3506">failure increases, increasing the chance that a second disk will fail while the ﬁrst is<br>being repaired. In spite of all these considerations, however, mirrored-disk systems<br>offer much higher reliability than do single-disk systems. Mirrored-disk systems with<br>mean time to data loss of about 500,000 to 1,000,000 hours, or 55 to 110 years, are<br>available today.<br>Power failures are a particular source of concern, since they occur far more fre-<br>quently than do natural disasters. Power failures are not a concern if there is no data<br>transfer to disk in progress when they occur. However, even with mirroring of disks,<br>if writes are in progress to the same block in both disks, and power fails before both<br>blocks are fully written, the two blocks can be in an inconsistent state. The solution<br>to this problem is to write one copy ﬁrst, then the next, so that one of the two copies<br>is always consistent. Some extra actions are required when we restart after a power<br>failure, to recover from incomplete writes. This matter is examined in Exercise 11.4.<br>11.3.2<br>Improvement in Performance via Parallelism<br>Now let us consider the beneﬁt of parallel access to multiple disks. With disk mirror-<br>ing, the rate at which read requests can be handled is doubled, since read requests<br>can be sent to either disk (as long as both disks in a pair are functional, as is almost<br>always the case). The transfer rate of each read is the same as in a single-disk system,<br>but the number of reads per unit time has doubled.<br>With multiple disks, we can improve the transfer rate as well (or instead) by strip-<br>ing data across multiple disks. In its simplest form, data striping consists of splitting<br>the bits of each byte across multiple disks; such striping is called bit-level striping.<br>For example, if we have an array of eight disks, we write bit i of each byte to disk<br>i. The array of eight disks can be treated as a single disk with sectors that are eight<br>times the normal size, and, more important, that has eight times the transfer rate. In<br>such an organization, every disk participates in every access (read or write), so the<br>number of accesses that can be processed per second is about the same as on a sin-<br>gle disk, but each access can read eight times as many data in the same time as on a<br>single disk. Bit-level striping can be generalized to a number of disks that either is a<br>multiple of 8 or a factor of 8. For example, if we use an array of four disks, bits i and<br>4 + i of each byte go to disk i.<br>Block-level striping stripes blocks across multiple disks. It treats the array of disks<br>as a single large disk, and it gives blocks logical numbers; we assume the block num-<br>bers start from 0. With an array of n disks, block-level striping assigns logical block i<br>of the disk array to disk (i mod n) + 1; it uses the ⌊i/n⌋th physical block of the disk<br>to store logical block i. For example, with 8 disks, logical block 0 is stored in physical<br>block 0 of disk 1, while logical block 11 is stored in physical block 1 of disk 4. When<br>reading a large ﬁle, block-level striping fetches n blocks at a time in parallel from the<br>n disks, giving a high data transfer rate for large reads. When a single block is read,<br>the data transfer rate is the same as on one disk, but the remaining n −1 disks are<br>free to perform other actions.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>IV. Data Storage and <br>Querying<br>11. Storage and File <br>Structure<br>406<br>© The McGraw−Hill <br>Companies, 2001<br>11.3<br>RAID<br>405<br>Block level striping is the most commonly used form of data striping. Other levels<br>of striping, such as bytes of a sector or sectors of a block also are possible.<br>In summary, there are two main goals of parallelism in a disk system:<br>1. Load-balance multiple small accesses (block accesses), so that the throughput<br>of such accesses increases.<br>2. Parallelize large accesses so that the response time of large accesses is reduced.<br>11.3.3<br>RAID Levels<br>Mirroring provides high reliability, but it is expensive. Striping provides high data-<br>transfer rates, but does not improve reliability. Various alternative schemes aim to<br>provide redundancy at lower cost by combining disk striping with “parity” bits<br>(which we describe next). These schemes have different cost–performance trade-offs.<br>The schemes are classiﬁed into RAID levels, as in Figure 11.4. (In the ﬁgure, P indi-<br>cates error-correcting bits, and C indicates a second copy of the data.) For all levels,<br>the ﬁgure depicts four disk’s worth of data, and the extra disks depicted are used to<br>store redundant information for failure recovery.<br>• RAID level 0 refers to disk arrays with striping at the level of blocks, but<br>without any redundancy (such as mirroring or parity bits). Figure 11.4a shows<br>an array of size 4.<br>• RAID level 1 refers to disk mirroring with block striping. Figure 11.4b shows<br>a mirrored organization that holds four disks worth of data.<br>• RAID level 2, known as memory-style error-correcting-code (ECC) organiza-<br>tion, employs parity bits. Memory systems have long used parity bits for error<br>detection and correction. Each byte in a memory system may have a parity bit<br>associated with it that records whether the numbers of bits in the byte that are<br>set to 1 is even (parity = 0) or odd (parity = 1). If one of the bits in the byte<br>gets damaged (either a 1 becomes a 0, or a 0 becomes a 1), the parity of the<br>byte changes and thus will not match the stored parity. Similarly, if the stored<br>parity bit gets damaged, it will not match the computed parity. Thus, all 1-bit<br>errors will be detected by the memory system. Error-correcting schemes store<br>2 or more extra bits, and can reconstruct the data if a single bit gets damaged.<br>The idea of error-correcting codes can be used directly in disk arrays by<br>striping bytes across disks. For example, the ﬁrst bit of each byte could be<br>stored in disk 1, the second bit in disk 2, and so on until the eighth bit is<br>stored in disk 8, and the error-correction bits are stored in further disks.<br>Figure 11.4c shows the level 2 scheme. The disks labeled P store the error-<br>correction bits. If one of the disks fails, the remaining bits of the byte and the<br>associated error-correction bits can be read from other disks, and can be used<br>to reconstruct the damaged data. Figure 11.4c shows an array of size 4; note<br>RAID level 2 requires only three disks’ overhead for four disks of data, unlike<br>RAID level 1, which required four disks’ overhead.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>IV. Data Storage and <br>Querying<br>11. Storage and File <br>Structure<br>407<br>© The McGraw−Hill <br>Companies, 2001<br>406<br>Chapter 11<br>Storage and File Structure<br>(a) RAID 0: nonredundant striping<br>(b) RAID 1: mirrored disks<br>(c) RAID 2: memory-style error-correcting codes<br>(d) RAID 3: bit-interleaved parity<br>(e) RAID 4: block-interleaved parity<br>(f) RAID 5: block-interleaved distributed parity<br>(g) RAID 6: P + Q redundancy<br>P<br>P<br>P<br>P<br>P<br>P<br>P<br>P<br>P<br>P<br>C<br>C<br>C<br>C<br>P<br>P<br>P<br>P<br>P<br>P<br>Figure 11.4<br>RAID levels.<br>• RAID level 3, bit-interleaved parity organization, improves on level 2 by<br>exploiting the fact that disk controllers, unlike memory systems, can detect<br>whether a sector has been read correctly, so a single parity bit can be used for<br>error correction, as well as for detection. The idea is as follows. If one of the<br>sectors gets damaged, the system knows exactly which sector it is, and, for<br>each bit in the sector, the system can ﬁgure out whether it is a 1 or a 0 by com-<br>puting the parity of the corresponding bits from sectors in the other disks. If<br>the parity of the remaining bits is equal to the stored parity, the missing bit is<br>0; otherwise, it is 1.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>IV. Data Storage and <br>Querying<br>11. Storage and File <br>Structure<br>408<br>© The McGraw−Hill <br>Companies, 2001<br>11.3<br>RAID<br>407<br>RAID level 3 is as good as level 2, but is less expensive in the number of<br>extra disks (it has only a one-disk overhead), so level 2 is not used in practice.<br>Figure 11.4d shows the level 3 scheme.<br>RAID level 3 has two beneﬁts over level 1. It needs only one parity disk for<br>several regular disks, whereas Level 1 needs one mirror disk for every disk,<br>and thus reduces the storage overhead. Since reads and writes of a byte are<br>spread out over multiple disks, with N-way striping of data, the transfer rate<br>for reading or writing a single block is N times faster than a RAID level 1 or-<br>ganization using N-way striping. On the other hand, RAID level 3 supports a<br>lower number of I/O operations per second, since every disk has to participate<br>in every I/O request.<br>• RAID level 4, block-interleaved parity organization, uses block level striping,<br>like RAID 0, and in addition keeps a parity block on a separate disk for cor-<br>responding blocks from N other disks. This scheme is shown pictorially in<br>Figure 11.4e. If one of the disks fails, the parity block can be used with the<br>corresponding blocks from the other disks to restore the blocks of the failed<br>disk.<br>A block read accesses only one disk, allowing other requests to be pro-<br>cessed by the other disks. Thus, the data-transfer rate for each access is slower,<br>but multiple read accesses can proceed in parallel, leading to a higher overall<br>I/O rate. The transfer rates for large reads is high, since all the disks can be<br>read in parallel; large writes also have high transfer rates, since the data and<br>parity can be written in parallel.<br>Small independent writes, on the other hand, cannot be performed in par-<br>allel. A write of a block has to access the disk on which the block is stored,<br>as well as the parity disk, since the parity block has to be updated. Moreover,<br>both the old value of the parity block and the old value of the block being<br>written have to be read for the new parity to be computed. Thus, a single<br>write requires four disk accesses: two to read the two old blocks, and two to<br>write the two blocks.<br>• RAID level 5, block-interleaved distributed parity, improves on level 4 by<br>partitioning data and parity among all N + 1 disks, instead of storing data in<br>N disks and parity in one disk. In level 5, all disks can participate in satisfying<br>read requests, unlike RAID level 4, where the parity disk cannot participate,<br>so level 5 increases the total number of requests that can be met in a given<br>amount of time. For each set of N logical blocks, one of the disks stores the<br>parity, and the other N disks store the blocks.<br>Figure 11.4f shows the setup. The P’s are distributed across all the disks.<br>For example, with an array of 5 disks, the parity block, labelled Pk, for logical<br>blocks 4k, 4k +1, 4k +2, 4k +3 is stored in disk (k mod 5) +1; the correspond-<br>ing blocks of the other four disks store the 4 data blocks 4k to 4k + 3. The<br>following table indicates how the ﬁrst 20 blocks, numbered 0 to 19, and their<br>parity blocks are laid out. The pattern shown gets repeated on further blocks.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>IV. Data Storage and <br>Querying<br>11. Storage and File <br>Structure<br>409<br>© The McGraw−Hill <br>Companies, 2001<br>408<br>Chapter 11<br>Storage and File Structure<br>P0<br>4<br>8<br>12<br>16<br>0<br>P1<br>9<br>13<br>17<br>1<br>5<br>P2<br>14<br>18<br>2<br>6<br>10<br>P3<br>19<br>3<br>7<br>11<br>15<br>P4<br>Note that a parity block cannot store parity for blocks in the same disk,<br>since then a disk failure would result in loss of data as well as of parity, and<br>hence would not be recoverable. Level 5 subsumes level 4, since it offers better<br>read–write performance at the same cost, so level 4 is not used in practice.<br>• RAID level 6, the P + Q redundancy scheme, is much like RAID level 5, but<br>stores extra redundant information to guard against multiple disk failures.<br>Instead of using parity, level 6 uses error-correcting codes such as the Reed–<br>Solomon codes (see the bibliographical notes). In the scheme in Figure 11.4g,<br>2 bits of redundant data are stored for every 4 bits of data—unlike 1 parity bit<br>in level 5—and the system can tolerate two disk failures.<br>Finally, we note that several variations have been proposed to the basic RAID schemes<br>described here.<br>Some vendors use their own terminology to describe their RAID implementations.2<br>However, the terminology we have presented is the most widely used.<br>11.3.4<br>Choice of RAID Level<br>The factors to be taken into account when choosing a RAID level are<br>• Monetary cost of extra disk storage requirements<br>• Performance requirements in terms of number of I/O operations<br>• Performance when a disk has failed<br>• Performance during rebuild (that is, while the data in a failed disk is being<br>rebuilt on a new disk)<br>The time to rebuild the data of a failed disk can be signiﬁcant, and varies with<br>the RAID level that is used. Rebuilding is easiest for RAID level 1, since data can<br>be copied from another disk; for the other levels, we need to access all the other<br>disks in the array to rebuild data of a failed disk. The rebuild performance of a RAID<br>system may be an important factor if continuous availability of data is required, as it<br>is in high-performance database systems. Furthermore, since rebuild time can form a<br>signiﬁcant part of the repair time, rebuild performance also inﬂuences the mean time<br>to data loss.<br>2.<br>For example, some products use RAID level 1 to refer to mirroring without striping, and level 1+0 or<br>level 10 to refer to mirroring with striping. Such a distinction is not really necessary since not striping can<br>simply be viewed as a special case of striping, namely striping across 1 disk.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>IV. Data Storage and <br>Querying<br>11. Storage and File <br>Structure<br>410<br>© The McGraw−Hill <br>Companies, 2001<br>11.3<br>RAID<br>409<br>RAID level 0 is used in high-performance applications where data safety is not<br>critical. Since RAID levels 2 and 4 are subsumed by RAID levels 3 and 5, the choice<br>of RAID levels is restricted to the remaining levels. Bit striping (level 3) is rarely used<br>since block striping (level 5) gives as good data transfer rates for large transfers, while<br>using fewer disks for small transfers. For small transfers, the disk access time domi-<br>nates anyway, so the beneﬁt of parallel reads diminishes. In fact, level 3 may perform<br>worse than level 5 for a small transfer, since the transfer completes only when cor-<br>responding sectors on all disks have been fetched; the average latency for the disk<br>array thus becomes very close to the worst-case latency for a single disk, negating<br>the beneﬁts of higher transfer rates. Level 6 is not supported currently by many RAID<br>implementations, but it offers better reliability than level 5 and can be used in appli-<br>cations where data safety is very important.<br>The choice between RAID level 1 and level 5 is harder to make. RAID level 1 is<br>popular for applications such as storage of log ﬁles in a database system, since it<br>offers the best write performance. RAID level 5 has a lower storage overhead than<br>level 1, but has a higher time overhead for writes. For applications where data are<br>read frequently, and written rarely, level 5 is the preferred choice.<br>Disk storage capacities have been growing at a rate of over 50 percent per year for<br>many years, and the cost per byte has been falling at the same rate. As a result, for<br>many existing database applications with moderate storage requirements, the mon-<br>etary cost of the extra disk storage needed for mirroring has become relatively small<br>(the extra monetary cost, however, remains a signiﬁcant issue for storage-intensive<br>applications such as video data storage). Access speeds have improved at a much<br>slower rate (around a factor of 3 over 10 years), while the number of I/O operations<br>required per second has increased tremendously, particularly for Web application<br>servers.<br>RAID level 5, which increases the number of I/O operations needed to write a<br>single logical block, pays a signiﬁcant time penalty in terms of write performance.<br>RAID level 1 is therefore the RAID level of choice for many applications with moderate<br>storage requirements, and high I/O requirements.<br>RAID system designers have to make several other decisions as well. For example,<br>how many disks should there be in an array? How many bits should be protected by<br>each parity bit? If there are more disks in an array, data-transfer rates are higher, but<br>the system would be more expensive. If there are more bits protected by a parity bit,<br>the space overhead due to parity bits is lower, but there is an increased chance that a<br>second disk will fail before the ﬁrst failed disk is repaired, and that will result in data<br>loss.<br>11.3.5<br>Hardware Issues<br>Another issue in the choice of RAID implementations is at the level of hardware.<br>RAID can be implemented with no change at the hardware level, using only software<br>modiﬁcation. Such RAID implementations are called software RAID. However, there<br>are signiﬁcant beneﬁts to be had by building special-purpose hardware to support<br>RAID, which we outline below; systems with special hardware support are called<br>hardware RAID systems.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>IV. Data Storage and <br>Querying<br>11. Storage and File <br>Structure<br>411<br>© The McGraw−Hill <br>Companies, 2001<br>410<br>Chapter 11<br>Storage and File Structure<br>Hardware RAID implementations can use nonvolatile RAM to record writes that<br>need to be executed; in case of power failure before a write is completed, when the<br>system comes back up, it retrieves information about incomplete writes from non-<br>volatile RAM and then completes the writes. Without such hardware support, extra<br>work needs to be done to detect blocks that may have been partially written before<br>power failure (see Exercise 11.4).<br>Some hardware RAID implementations permit hot swapping; that is, faulty disks<br>can be removed and replaced by new ones without turning power off. Hot swapping<br>reduces the mean time to repair, since replacement of a disk does not have to wait<br>until a time when the system can be shut down. In fact many critical systems today<br>run on a 24 × 7 schedule; that is, they run 24 hours a day, 7 days a week, providing<br>no time for shutting down and replacing a failed disk. Further, many RAID imple-<br>mentations assign a spare disk for each array (or for a set of disk arrays). If a disk<br>fails, the spare disk is immediately used as a replacement. As a result, the mean time<br>to repair is reduced greatly, minimizing the chance of any data loss. The failed disk<br>can be replaced at leisure.<br>The power supply, or the disk controller, or even the system interconnection in<br>a RAID system could become a single point of failure, that could stop functioning of<br>the RAID system. To avoid this possibility, good RAID implementations have multiple<br>redundant power supplies (with battery backups so they continue to function even<br>if power fails). Such RAID systems have multiple disk controllers, and multiple in-<br>terconnections to connect them to the computer system (or to a network of computer<br>systems). Thus, failure of any single component will not stop the functioning of the<br>RAID system.<br>11.3.6<br>Other RAID Applications<br>The concepts of RAID have been generalized to other storage devices, including ar-<br>rays of tapes, and even to the broadcast of data over wireless systems. When applied<br>to arrays of tapes, the RAID structures are able to recover data even if one of the tapes<br>in an array of tapes is damaged. When applied to broadcast of data, a block of data<br>is split into short units and is broadcast along with a parity unit; if one of the units is<br>not received for any reason, it can be reconstructed from the other units.<br>11.4<br>Tertiary Storage<br>In a large database system, some of the data may have to reside on tertiary storage.<br>The two most common tertiary storage media are optical disks and magnetic tapes.<br>11.4.1<br>Optical Disks<br>Compact disks are a popular medium for distributing software, multimedia data<br>such as audio and images, and other electronically published information. They have<br>a fairly large capacity (640 megabytes), and they are cheap to mass-produce.</span><br><br><span style="background-color: #CAFFBF;" title="Chunk 51 | Start: 1020102 | End: 1040102 | Tokens: 3321"><br>Digital video disks (DVDs) are replacing compact disks in applications that require<br>very large amounts of data. Disks in the DVD-5 format can store 4.7 gigabytes of data<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>IV. Data Storage and <br>Querying<br>11. Storage and File <br>Structure<br>412<br>© The McGraw−Hill <br>Companies, 2001<br>11.4<br>Tertiary Storage<br>411<br>(in one recording layer), while disks in the DVD-9 format can store 8.5 gigabytes of<br>data (in two recording layers). Recording on both sides of a disk yields even larger<br>capacities; DVD-10 and DVD-18 formats, which are the two-sided versions of DVD-5<br>and DVD-9, can store 9.4 gigabytes and 17 gigabytes respectively.<br>CD and DVD drives have much longer seek times (100 milliseconds is common)<br>than do magnetic-disk drives, since the head assembly is heavier. Rotational speeds<br>are typically lower than those of magnetic disks, although the faster CD and DVD<br>drives have rotation speeds of about 3000 rotations per minute, which is comparable<br>to speeds of lower-end magnetic-disk drives. Rotational speeds of CD drives origi-<br>nally corresponded to the audio CD standards, and the speeds of DVD drives origi-<br>nally corresponded to the DVD video standards, but current-generation drives rotate<br>at many times the standard rate.<br>Data transfer rates are somewhat less than for magnetic disks. Current CD drives<br>read at around 3 to 6 megabytes per second, and current DVD drives read at 8 to 15<br>megabytes per second. Like magnetic disk drives, optical disks store more data in<br>outside tracks and less data in inner tracks. The transfer rate of optical drives is char-<br>acterized as n×, which means the drive supports transfers at n times the standard<br>rate; rates of around 50× for CD and 12× for DVD are now common.<br>The record-once versions of optical disks (CD-R, and increasingly, DVD-R) are pop-<br>ular for distribution of data and particularly for archival storage of data because they<br>have a high capacity, have a longer lifetime than magnetic disks, and can be removed<br>and stored at a remote location. Since they cannot be overwritten, they can be used<br>to store information that should not be modiﬁed, such as audit trails. The multiple-<br>write versions (CD-RW, DVD-RW, and DVD-RAM) are also used for archival purposes.<br>Jukeboxes are devices that store a large number of optical disks (up to several hun-<br>dred) and load them automatically on demand to one of a small number (usually, 1 to<br>10) of drives. The aggregate storage capacity of such a system can be many terabytes.<br>When a disk is accessed, it is loaded by a mechanical arm from a rack onto a drive<br>(any disk that was already in the drive must ﬁrst be placed back on the rack). The<br>disk load/unload time is usually of the order of a few seconds—very much slower<br>than disk access times.<br>11.4.2<br>Magnetic Tapes<br>Although magnetic tapes are relatively permanent, and can hold large volumes of<br>data, they are slow in comparison to magnetic and optical disks. Even more impor-<br>tant, magnetic tapes are limited to sequential access. Thus, they cannot provide ran-<br>dom access for secondary-storage requirements, although historically, prior to the<br>use of magnetic disks, tapes were used as a secondary-storage medium.<br>Tapes are used mainly for backup, for storage of infrequently used information,<br>and as an ofﬂine medium for transferring information from one system to another.<br>Tapes are also used for storing large volumes of data, such as video or image data,<br>that either do not need to be accessible quickly or are so voluminous that magnetic-<br>disk storage would be too expensive.<br>A tape is kept in a spool, and is wound or rewound past a read–write head.<br>Moving to the correct spot on a tape can take seconds or even minutes, rather than<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>IV. Data Storage and <br>Querying<br>11. Storage and File <br>Structure<br>413<br>© The McGraw−Hill <br>Companies, 2001<br>412<br>Chapter 11<br>Storage and File Structure<br>milliseconds; once positioned, however, tape drives can write data at densities and<br>speeds approaching those of disk drives. Capacities vary, depending on the length<br>and width of the tape and on the density at which the head can read and write.<br>The market is currently fragmented among a wide variety of tape formats. Currently<br>available tape capacities range from a few gigabytes [with the Digital Audio Tape<br>(DAT) format], 10 to 40 gigabytes [with the Digital Linear Tape (DLT) format], 100<br>gigabytes and higher (with the Ultrium format), to 330 gigabytes (with Ampex heli-<br>cal scan tape formats). Data transfer rates are of the order of a few to tens of megabytes<br>per second.<br>Tape devices are quite reliable, and good tape drive systems perform a read of the<br>just-written data to ensure that it has been recorded correctly. Tapes, however, have<br>limits on the number of times that they can be read or written reliably.<br>Some tape formats (such as the Accelis format) support faster seek times (of the<br>order of tens of seconds), which is important for applications that need quick access<br>to very large amounts of data, larger than what would ﬁt economically on a disk<br>drive. Most other tape formats provide larger capacities, at the cost of slower access;<br>such formats are ideal for data backup, where fast seeks are not important.<br>Tape jukeboxes, like optical disk jukeboxes, hold large numbers of tapes, with<br>a few drives onto which the tapes can be mounted; they are used for storing large<br>volumes of data, ranging up to many terabytes (1012 bytes), with access times on<br>the order of seconds to a few minutes. Applications that need such enormous data<br>storage include imaging systems that gather data by remote-sensing satellites, and<br>large video libraries for television broadcasters.<br>11.5<br>Storage Access<br>A database is mapped into a number of different ﬁles, which are maintained by the<br>underlying operating system. These ﬁles reside permanently on disks, with backups<br>on tapes. Each ﬁle is partitioned into ﬁxed-length storage units called blocks, which<br>are the units of both storage allocation and data transfer. We shall discuss in Section<br>11.6 various ways to organize the data logically in ﬁles.<br>A block may contain several data items. The exact set of data items that a block<br>contains is determined by the form of physical data organization being used (see<br>Section 11.6). We shall assume that no data item spans two or more blocks. This<br>assumption is realistic for most data-processing applications, such as our banking<br>example.<br>A major goal of the database system is to minimize the number of block transfers<br>between the disk and memory. One way to reduce the number of disk accesses is to<br>keep as many blocks as possible in main memory. The goal is to maximize the chance<br>that, when a block is accessed, it is already in main memory, and, thus, no disk access<br>is required.<br>Since it is not possible to keep all blocks in main memory, we need to manage the<br>allocation of the space available in main memory for the storage of blocks. The buffer<br>is that part of main memory available for storage of copies of disk blocks. There is<br>always a copy kept on disk of every block, but the copy on disk may be a version<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>IV. Data Storage and <br>Querying<br>11. Storage and File <br>Structure<br>414<br>© The McGraw−Hill <br>Companies, 2001<br>11.5<br>Storage Access<br>413<br>of the block older than the version in the buffer. The subsystem responsible for the<br>allocation of buffer space is called the buffer manager.<br>11.5.1<br>Buffer Manager<br>Programs in a database system make requests (that is, calls) on the buffer manager<br>when they need a block from disk. If the block is already in the buffer, the buffer man-<br>ager passes the address of the block in main memory to the requester. If the block is<br>not in the buffer, the buffer manager ﬁrst allocates space in the buffer for the block,<br>throwing out some other block, if necessary, to make space for the new block. The<br>thrown-out block is written back to disk only if it has been modiﬁed since the most<br>recent time that it was written to the disk. Then, the buffer manager reads in the re-<br>quested block from the disk to the buffer, and passes the address of the block in main<br>memory to the requester. The internal actions of the buffer manager are transparent<br>to the programs that issue disk-block requests.<br>If you are familiar with operating-system concepts, you will note that the buffer<br>manager appears to be nothing more than a virtual-memory manager, like those<br>found in most operating systems. One difference is that the size of the database may<br>be much more than the hardware address space of a machine, so memory addresses<br>are not sufﬁcient to address all disk blocks. Further, to serve the database system<br>well, the buffer manager must use techniques more sophisticated than typical virtual-<br>memory management schemes:<br>• Buffer replacement strategy. When there is no room left in the buffer, a block<br>must be removed from the buffer before a new one can be read in. Most oper-<br>ating systems use a least recently used (LRU) scheme, in which the block that<br>was referenced least recently is written back to disk and is removed from the<br>buffer. This simple approach can be improved on for database applications.<br>• Pinned blocks. For the database system to be able to recover from crashes<br>(Chapter 17), it is necessary to restrict those times when a block may be written<br>back to disk. For instance, most recovery systems require that a block should<br>not be written to disk while an update on the block is in progress. A block that<br>is not allowed to be written back to disk is said to be pinned. Although many<br>operating systems do not support pinned blocks, such a feature is essential for<br>a database system that is resilient to crashes.<br>• Forced output of blocks. There are situations in which it is necessary to write<br>back the block to disk, even though the buffer space that it occupies is not<br>needed. This write is called the forced output of a block. We shall see the<br>reason for forced output in Chapter 17; brieﬂy, main-memory contents and<br>thus buffer contents are lost in a crash, whereas data on disk usually survive<br>a crash.<br>11.5.2<br>Buffer-Replacement Policies<br>The goal of a replacement strategy for blocks in the buffer is to minimize accesses<br>to the disk. For general-purpose programs, it is not possible to predict accurately<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>IV. Data Storage and <br>Querying<br>11. Storage and File <br>Structure<br>415<br>© The McGraw−Hill <br>Companies, 2001<br>414<br>Chapter 11<br>Storage and File Structure<br>for each tuple b of borrower do<br>for each tuple c of customer do<br>if b[customer-name] = c[customer-name]<br>then begin<br>let x be a tuple deﬁned as follows:<br>x[customer-name] := b[customer-name]<br>x[loan-number] := b[loan-number]<br>x[customer-street] := c[customer-street]<br>x[customer-city] := c[customer-city]<br>include tuple x as part of result of borrower<br> customer<br>end<br>end<br>end<br>Figure 11.5<br>Procedure for computing join.<br>which blocks will be referenced. Therefore, operating systems use the past pattern of<br>block references as a predictor of future references. The assumption generally made<br>is that blocks that have been referenced recently are likely to be referenced again.<br>Therefore, if a block must be replaced, the least recently referenced block is replaced.<br>This approach is called the least recently used (LRU) block-replacement scheme.<br>LRU is an acceptable replacement scheme in operating systems. However, a data-<br>base system is able to predict the pattern of future references more accurately than an<br>operating system. A user request to the database system involves several steps. The<br>database system is often able to determine in advance which blocks will be needed by<br>looking at each of the steps required to perform the user-requested operation. Thus,<br>unlike operating systems, which must rely on the past to predict the future, database<br>systems may have information regarding at least the short-term future.<br>To illustrate how information about future block access allows us to improve the<br>LRU strategy, consider the processing of the relational-algebra expression<br>borrower<br> customer<br>Assume that the strategy chosen to process this request is given by the pseudocode<br>program shown in Figure 11.5. (We shall study other strategies in Chapter 13.)<br>Assume that the two relations of this example are stored in separate ﬁles. In this<br>example, we can see that, once a tuple of borrower has been processed, that tuple is not<br>needed again. Therefore, once processing of an entire block of borrower tuples is com-<br>pleted, that block is no longer needed in main memory, even though it has been used<br>recently. The buffer manager should be instructed to free the space occupied by a<br>borrower block as soon as the ﬁnal tuple has been processed. This buffer-management<br>strategy is called the toss-immediate strategy.<br>Now consider blocks containing customer tuples. We need to examine every block<br>of customer tuples once for each tuple of the borrower relation. When processing of<br>a customer block is completed, we know that that block will not be accessed again<br>until all other customer blocks have been processed. Thus, the most recently used<br>customer block will be the ﬁnal block to be re-referenced, and the least recently used<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>IV. Data Storage and <br>Querying<br>11. Storage and File <br>Structure<br>416<br>© The McGraw−Hill <br>Companies, 2001<br>11.6<br>File Organization<br>415<br>customer block is the block that will be referenced next. This assumption set is the<br>exact opposite of the one that forms the basis for the LRU strategy. Indeed, the optimal<br>strategy for block replacement is the most recently used (MRU) strategy. If a customer<br>block must be removed from the buffer, the MRU strategy chooses the most recently<br>used block.<br>For the MRU strategy to work correctly for our example, the system must pin the<br>customer block currently being processed. After the ﬁnal customer tuple has been pro-<br>cessed, the block is unpinned, and it becomes the most recently used block.<br>In addition to using knowledge that the system may have about the request being<br>processed, the buffer manager can use statistical information about the probability<br>that a request will reference a particular relation. For example, the data dictionary<br>that (as we will see in detail in Section 11.8) keeps track of the logical schema of the<br>relations as well as their physical storage information is one of the most frequently<br>accessed parts of the database. Thus, the buffer manager should try not to remove<br>data-dictionary blocks from main memory, unless other factors dictate that it do so.<br>In Chapter 12, we discuss indices for ﬁles. Since an index for a ﬁle may be accessed<br>more frequently than the ﬁle itself, the buffer manager should, in general, not remove<br>index blocks from main memory if alternatives are available.<br>The ideal database block-replacement strategy needs knowledge of the database<br>operations—both those being performed and those that will be performed in the<br>future. No single strategy is known that handles all the possible scenarios well. In-<br>deed, a surprisingly large number of database systems use LRU, despite that strat-<br>egy’s faults. The exercises explore alternative strategies.<br>The strategy that the buffer manager uses for block replacement is inﬂuenced by<br>factors other than the time at which the block will be referenced again. If the system<br>is processing requests by several users concurrently, the concurrency-control sub-<br>system (Chapter 16) may need to delay certain requests, to ensure preservation of<br>database consistency. If the buffer manager is given information from the concurrency-<br>control subsystem indicating which requests are being delayed, it can use this infor-<br>mation to alter its block-replacement strategy. Speciﬁcally, blocks needed by active<br>(nondelayed) requests can be retained in the buffer at the expense of blocks needed<br>by the delayed requests.<br>The crash-recovery subsystem (Chapter 17) imposes stringent constraints on block<br>replacement. If a block has been modiﬁed, the buffer manager is not allowed to write<br>back the new version of the block in the buffer to disk, since that would destroy<br>the old version. Instead, the block manager must seek permission from the crash-<br>recovery subsystem before writing out a block. The crash-recovery subsystem may<br>demand that certain other blocks be force-output before it grants permission to the<br>buffer manager to output the block requested. In Chapter 17, we deﬁne precisely the<br>interaction between the buffer manager and the crash-recovery subsystem.<br>11.6<br>File Organization<br>A ﬁle is organized logically as a sequence of records. These records are mapped onto<br>disk blocks. Files are provided as a basic construct in operating systems, so we shall<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>IV. Data Storage and <br>Querying<br>11. Storage and File <br>Structure<br>417<br>© The McGraw−Hill <br>Companies, 2001<br>416<br>Chapter 11<br>Storage and File Structure<br>record 0<br>record 1<br>record 2<br>record 3<br>record 4<br>record 5<br>record 6<br>record 7<br>record 8<br>400<br>350<br>700<br>500<br>700<br>900<br>Perryridge<br>Perryridge<br>Perryridge<br>Round Hill<br>Mianus<br>Downtown<br>Downtown<br>Redwood<br>Brighton<br>750<br> <br>600<br> <br>A-102<br>A-305<br>A-215<br>A-101<br>A-222<br>A-201<br>A-217<br>A-110<br>A-218<br>700<br>Figure 11.6<br>File containing account records.<br>assume the existence of an underlying ﬁle system. We need to consider ways of repre-<br>senting logical data models in terms of ﬁles.<br>Although blocks are of a ﬁxed size determined by the physical properties of the<br>disk and by the operating system, record sizes vary. In a relational database, tuples<br>of distinct relations are generally of different sizes.<br>One approach to mapping the database to ﬁles is to use several ﬁles, and to store<br>records of only one ﬁxed length in any given ﬁle. An alternative is to structure our<br>ﬁles so that we can accommodate multiple lengths for records; however, ﬁles of ﬁxed-<br>length records are easier to implement than are ﬁles of variable-length records. Many<br>of the techniques used for the former can be applied to the variable-length case. Thus,<br>we begin by considering a ﬁle of ﬁxed-length records.<br>11.6.1<br>Fixed-Length Records<br>As an example, let us consider a ﬁle of account records for our bank database. Each<br>record of this ﬁle is deﬁned as:<br>type deposit = record<br>account-number : char(10);<br>branch-name : char (22);<br>balance : real;<br>end<br>If we assume that each character occupies 1 byte and that a real occupies 8 bytes,<br>our account record is 40 bytes long. A simple approach is to use the ﬁrst 40 bytes<br>for the ﬁrst record, the next 40 bytes for the second record, and so on (Figure 11.6).<br>However, there are two problems with this simple approach:<br>1. It is difﬁcult to delete a record from this structure. The space occupied by the<br>record to be deleted must be ﬁlled with some other record of the ﬁle, or we<br>must have a way of marking deleted records so that they can be ignored.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>IV. Data Storage and <br>Querying<br>11. Storage and File <br>Structure<br>418<br>© The McGraw−Hill <br>Companies, 2001<br>11.6<br>File Organization<br>417<br>record 0<br>record 1<br>record 3<br>record 4<br>record 5<br>record 6<br>record 7<br>record 8<br>A-102<br>A-305<br>A-101<br>A-222<br>A-201<br>A-217<br>A-110<br>A-218<br>400<br>350<br>500<br>700<br>900<br>Perryridge<br>Perryridge<br>Perryridge<br>Round Hill<br>Downtown<br>Downtown<br>Redwood<br>Brighton<br>750<br> <br>600<br> <br>700<br>Figure 11.7<br>File of Figure 11.6, with record 2 deleted and all records moved.<br>2. Unless the block size happens to be a multiple of 40 (which is unlikely), some<br>records will cross block boundaries. That is, part of the record will be stored<br>in one block and part in another. It would thus require two block accesses to<br>read or write such a record.<br>When a record is deleted, we could move the record that came after it into the<br>space formerly occupied by the deleted record, and so on, until every record fol-<br>lowing the deleted </span><br><br><span style="background-color: #9BF6FF;" title="Chunk 52 | Start: 1040104 | End: 1060104 | Tokens: 3324">record has been moved ahead (Figure 11.7). Such an approach<br>requires moving a large number of records. It might be easier simply to move the<br>ﬁnal record of the ﬁle into the space occupied by the deleted record (Figure 11.8).<br>It is undesirable to move records to occupy the space freed by a deleted record,<br>since doing so requires additional block accesses. Since insertions tend to be more fre-<br>quent than deletions, it is acceptable to leave open the space occupied by the deleted<br>record, and to wait for a subsequent insertion before reusing the space. A simple<br>marker on a deleted record is not sufﬁcient, since it is hard to ﬁnd this available space<br>when an insertion is being done. Thus, we need to introduce an additional structure.<br>At the beginning of the ﬁle, we allocate a certain number of bytes as a ﬁle header.<br>The header will contain a variety of information about the ﬁle. For now, all we need<br>to store there is the address of the ﬁrst record whose contents are deleted. We use this<br>record 0<br>record 1<br>record 3<br>record 4<br>record 5<br>record 6<br>record 7<br>record 8<br>400<br>700<br>350<br>500<br>700<br>750<br>900<br>Perryridge<br>Perryridge<br>Perryridge<br>Round Hill<br>Downtown<br>Downtown<br>Redwood<br>Brighton<br> <br>600<br> <br>A-102<br>A-305<br>A-101<br>A-222<br>A-201<br>A-217<br>A-110<br>A-218<br>Figure 11.8<br>File of Figure 11.6, with record 2 deleted and ﬁnal record moved.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>IV. Data Storage and <br>Querying<br>11. Storage and File <br>Structure<br>419<br>© The McGraw−Hill <br>Companies, 2001<br>418<br>Chapter 11<br>Storage and File Structure<br>header<br>record 0<br>record 2<br>record 3<br>record 4<br>record 5<br>record 6<br>record 7<br>record 8<br>record 1<br>400<br>700<br>500<br>900<br>Perryridge<br>Mianus<br>Downtown<br>Perryridge<br> <br>A-102<br>A-215<br>A-101<br>A-201<br>600<br>Perryridge<br>Downtown<br>700<br>A-110<br>A-218<br>Figure 11.9<br>File of Figure 11.6, with free list after deletion of records 1, 4, and 6.<br>ﬁrst record to store the address of the second available record, and so on. Intuitively,<br>we can think of these stored addresses as pointers, since they point to the location of<br>a record. The deleted records thus form a linked list, which is often referred to as a<br>free list. Figure 11.9 shows the ﬁle of Figure 11.6, with the free list, after records 1, 4,<br>and 6 have been deleted.<br>On insertion of a new record, we use the record pointed to by the header. We<br>change the header pointer to point to the next available record. If no space is avail-<br>able, we add the new record to the end of the ﬁle.<br>Insertion and deletion for ﬁles of ﬁxed-length records are simple to implement,<br>because the space made available by a deleted record is exactly the space needed to<br>insert a record. If we allow records of variable length in a ﬁle, this match no longer<br>holds. An inserted record may not ﬁt in the space left free by a deleted record, or it<br>may ﬁll only part of that space.<br>11.6.2<br>Variable-Length Records<br>Variable-length records arise in database systems in several ways:<br>• Storage of multiple record types in a ﬁle<br>• Record types that allow variable lengths for one or more ﬁelds<br>• Record types that allow repeating ﬁelds<br>Different techniques for implementing variable-length records exist. For purposes of<br>illustration, we shall use one example to demonstrate the various implementation<br>techniques. We shall consider a different representation of the account information<br>stored in the ﬁle of Figure 11.6, in which we use one variable-length record for each<br>branch name and for all the account information for that branch. The format of the<br>record is<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>IV. Data Storage and <br>Querying<br>11. Storage and File <br>Structure<br>420<br>© The McGraw−Hill <br>Companies, 2001<br>11.6<br>File Organization<br>419<br>type account-list = record<br>branch-name : char (22);<br>account-info : array [1 .. ∞] of<br>record;<br>account-number : char(10);<br>balance : real;<br>end<br>end<br>We deﬁne account-info as an array with an arbitrary number of elements. That is,<br>the type deﬁnition does not limit the number of elements in the array, although any<br>actual record will have a speciﬁc number of elements in its array. There is no limit on<br>how large a record can be (up to, of course, the size of the disk storage!).<br>11.6.2.1<br>Byte-String Representation<br>A simple method for implementing variable-length records is to attach a special end-<br>of-record (⊥) symbol to the end of each record. We can then store each record as a<br>string of consecutive bytes. Figure 11.10 shows such an organization to represent the<br>ﬁle of ﬁxed-length records of Figure 11.6 as variable-length records. An alternative<br>version of the byte-string representation stores the record length at the beginning of<br>each record, instead of using end-of-record symbols.<br>The byte-string representation as described in Figure 11.10 has some disadvan-<br>tages:<br>• It is not easy to reuse space occupied formerly by a deleted record. Although<br>techniques exist to manage insertion and deletion, they lead to a large number<br>of small fragments of disk storage that are wasted.<br>• There is no space, in general, for records to grow longer. If a variable-length<br>record becomes longer, it must be moved—movement is costly if pointers to<br>the record are stored elsewhere in the database (e.g., in indices, or in other<br>records), since the pointers must be located and updated.<br>Thus, the basic byte-string representation described here not usually used for imple-<br>menting variable-length records. However, a modiﬁed form of the byte-string repre-<br>400<br>350<br>700<br>500<br>700<br>750<br>⊥<br>⊥<br>⊥<br>⊥<br>⊥<br>⊥<br>0<br>2<br>3<br>4<br>5<br>1<br>900<br>700<br>600<br>Round Hill<br>Perryridge<br>Downtown<br>Mianus<br>Brighton<br>Redwood<br>A-102<br>A-201<br>A-218<br>A-110<br>A-305<br>A-215<br>A-101<br>A-222<br>A-217<br>Figure 11.10<br>Byte-string representation of variable-length records.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>IV. Data Storage and <br>Querying<br>11. Storage and File <br>Structure<br>421<br>© The McGraw−Hill <br>Companies, 2001<br>420<br>Chapter 11<br>Storage and File Structure<br>Size<br># Entries<br>Free Space  <br>Location<br>Block Header<br>End of Free Space<br>Records<br>Figure 11.11<br>Slotted-page structure.<br>sentation, called the slotted-page structure, is commonly used for organizing records<br>within a single block.<br>The slotted-page structure appears in Figure 11.11. There is a header at the begin-<br>ning of each block, containing the following information:<br>1. The number of record entries in the header<br>2. The end of free space in the block<br>3. An array whose entries contain the location and size of each record<br>The actual records are allocated contiguously in the block, starting from the end of<br>the block. The free space in the block is contiguous, between the ﬁnal entry in the<br>header array, and the ﬁrst record. If a record is inserted, space is allocated for it at the<br>end of free space, and an entry containing its size and location is added to the header.<br>If a record is deleted, the space that it occupies is freed, and its entry is set to<br>deleted (its size is set to −1, for example). Further, the records in the block before the<br>deleted record are moved, so that the free space created by the deletion gets occupied,<br>and all free space is again between the ﬁnal entry in the header array and the ﬁrst<br>record. The end-of-free-space pointer in the header is appropriately updated as well.<br>Records can be grown or shrunk by similar techniques, as long as there is space in<br>the block. The cost of moving the records is not too high, since the size of a block is<br>limited: A typical value is 4 kilobytes.<br>The slotted-page structure requires that there be no pointers that point directly<br>to records. Instead, pointers must point to the entry in the header that contains the<br>actual location of the record. This level of indirection allows records to be moved to<br>prevent fragmentation of space inside a block, while supporting indirect pointers to<br>the record.<br>11.6.2.2<br>Fixed-Length Representation<br>Another way to implement variable-length records efﬁciently in a ﬁle system is to<br>use one or more ﬁxed-length records to represent one variable-length record.<br>There are two ways of doing this:<br>1. Reserved space. If there is a maximum record length that is never exceeded,<br>we can use ﬁxed-length records of that length. Unused space (for records<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>IV. Data Storage and <br>Querying<br>11. Storage and File <br>Structure<br>422<br>© The McGraw−Hill <br>Companies, 2001<br>11.6<br>File Organization<br>421<br>400<br>350<br>700<br>500<br>700<br>750<br>⊥<br>⊥<br>⊥<br>⊥<br>⊥<br>0<br>2<br>3<br>4<br>5<br>1<br>900<br>700<br>600<br>Round Hill<br>Perryridge<br>Downtown<br>Mianus<br>Brighton<br>Redwood<br>A-102<br>A-201<br>A-218<br>A-110<br>A-305<br>A-215<br>A-101<br>A-222<br>A-217<br>⊥<br>⊥<br>⊥<br>⊥<br>⊥<br>⊥<br>⊥<br>⊥<br>⊥<br>⊥<br>⊥<br>⊥<br>⊥<br>Figure 11.12<br>File of Figure 11.10, using the reserved-space method.<br>shorter than the maximum space) is ﬁlled with a special null, or end-of-record,<br>symbol.<br>2. List representation. We can represent variable-length records by lists of ﬁxed-<br>length records, chained together by pointers.<br>If we choose to apply the reserved-space method to our account example, we need<br>to select a maximum record length. Figure 11.12 shows how the ﬁle of Figure 11.10<br>would be represented if we allowed a maximum of three accounts per branch. A<br>record in this ﬁle is of the account-list type, but with the array containing exactly<br>three elements. Those branches with fewer than three accounts (for example, Round<br>Hill) have records with null ﬁelds. We use the symbol ⊥to represent this situation in<br>Figure 11.12. In practice, a particular value that can never represent real data is used<br>(for example, an account number that is blank, or a name beginning with “*”).<br>The reserved-space method is useful when most records have a length close to<br>the maximum. Otherwise, a signiﬁcant amount of space may be wasted. In our bank<br>example, some branches may have many more accounts than others. This situation<br>leads us to consider the linked list method. To represent the ﬁle by the linked list<br>method, we add a pointer ﬁeld as we did in Figure 11.9. The resulting structure ap-<br>pears in Figure 11.13.<br>0<br>Perryridge<br>A-102<br>400<br>1<br>Round Hill<br>A-305<br>350<br>2<br>Mianus<br>A-215<br>700<br>3<br>Downtown<br>A-101<br>500<br>4<br>Redwood<br>A-222<br>700<br>5<br>A-201<br>900<br>6<br>Brighton<br>A-217<br>750<br>7<br> <br>A-110<br>600<br>8<br> <br>A-218<br>700<br>Figure 11.13<br>File of Figure 11.10 using linked lists.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>IV. Data Storage and <br>Querying<br>11. Storage and File <br>Structure<br>423<br>© The McGraw−Hill <br>Companies, 2001<br>422<br>Chapter 11<br>Storage and File Structure<br>Perryridge<br>A-102<br>400<br>Round Hill<br>A-305<br>350<br>Mianus<br>A-215<br>700<br>Downtown<br>A-101<br>500<br>Redwood<br>A-222<br>700<br>Brighton<br>A-217<br>750<br>A-201<br>900<br>A-218<br>700<br>A-110<br>600<br>anchor<br>block<br>overflow<br>block<br>Figure 11.14<br>Anchor-block and overﬂow-block structures.<br>The ﬁle structures of Figures 11.9 and 11.13 both use pointers; the difference is<br>that, in Figure 11.9, we use pointers to chain together only deleted records, whereas<br>in Figure 11.13, we chain together all records pertaining to the same branch.<br>A disadvantage to the structure of Figure 11.13 is that we waste space in all records<br>except the ﬁrst in a chain. The ﬁrst record needs to have the branch-name value, but<br>subsequent records do not. Nevertheless, we need to include a ﬁeld for branch-name<br>in all records, lest the records not be of ﬁxed length. This wasted space is signiﬁcant,<br>since we expect, in practice, that each branch has a large number of accounts. To deal<br>with this problem, we allow two kinds of blocks in our ﬁle:<br>1. Anchor block, which contains the ﬁrst record of a chain<br>2. Overﬂow block, which contains records other than those that are the ﬁrst<br>record of a chain<br>Thus, all records within a block have the same length, even though not all records in<br>the ﬁle have the same length. Figure 11.14 shows this ﬁle structure.<br>11.7<br>Organization of Records in Files<br>So far, we have studied how records are represented in a ﬁle structure. An instance<br>of a relation is a set of records. Given a set of records, the next question is how to<br>organize them in a ﬁle. Several of the possible ways of organizing records in ﬁles are:<br>• Heap ﬁle organization. Any record can be placed anywhere in the ﬁle where<br>there is space for the record. There is no ordering of records. Typically, there is<br>a single ﬁle for each relation<br>• Sequential ﬁle organization. Records are stored in sequential order, accord-<br>ing to the value of a “search key” of each record. Section 11.7.1 describes this<br>organization.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>IV. Data Storage and <br>Querying<br>11. Storage and File <br>Structure<br>424<br>© The McGraw−Hill <br>Companies, 2001<br>11.7<br>Organization of Records in Files<br>423<br>• Hashing ﬁle organization. A hash function is computed on some attribute of<br>each record. The result of the hash function speciﬁes in which block of the<br>ﬁle the record should be placed. Chapter 12 describes this organization; it is<br>closely related to the indexing structures described in that chapter.<br>Generally, a separate ﬁle is used to store the records of each relation. However,<br>in a clustering ﬁle organization, records of several different relations are stored in<br>the same ﬁle; further, related records of the different relations are stored on the same<br>block, so that one I/O operation fetches related records from all the relations. For<br>example, records of the two relations can be considered to be related if they would<br>match in a join of the two relations. Section 11.7.2 describes this organization.<br>11.7.1<br>Sequential File Organization<br>A sequential ﬁle is designed for efﬁcient processing of records in sorted order based<br>on some search-key. A search key is any attribute or set of attributes; it need not be<br>the primary key, or even a superkey. To permit fast retrieval of records in search-key<br>order, we chain together records by pointers. The pointer in each record points to<br>the next record in search-key order. Furthermore, to minimize the number of block<br>accesses in sequential ﬁle processing, we store records physically in search-key order,<br>or as close to search-key order as possible.<br>Figure 11.15 shows a sequential ﬁle of account records taken from our banking<br>example. In that example, the records are stored in search-key order, using branch-<br>name as the search key.<br>The sequential ﬁle organization allows records to be read in sorted order; that can<br>be useful for display purposes, as well as for certain query-processing algorithms<br>that we shall study in Chapter 13.<br>It is difﬁcult, however, to maintain physical sequential order as records are in-<br>serted and deleted, since it is costly to move many records as a result of a single<br>750<br>500<br>600<br>700<br>400<br>900<br>700<br>700<br>350<br>Downtown<br>Brighton<br>Mianus<br>Downtown<br>Perryridge<br>Perryridge<br>Perryridge<br>Redwood<br>Round Hill<br>A-217<br>A-101<br>A-110<br>A-215<br>A-102<br>A-201<br>A-218<br>A-222<br>A-305<br>Figure 11.15<br>Sequential ﬁle for account records.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>IV. Data Storage and <br>Querying<br>11. Storage and File <br>Structure<br>425<br>© The McGraw−Hill <br>Companies, 2001<br>424<br>Chapter 11<br>Storage and File Structure<br>750<br>500<br>600<br>700<br>400<br>900<br>700<br>700<br>350<br>Downtown<br>Brighton<br>Mianus<br>Downtown<br>Perryridge<br>Perryridge<br>Perryridge<br>Redwood<br>Round Hill<br>A-217<br>A-101<br>A-110<br>A-215<br>A-102<br>A-201<br>A-218<br>A-222<br>A-305<br>800<br>North Town<br>A-888<br>Figure 11.16<br>Sequential ﬁle after an insertion.<br>insertion or deletion. We can manage deletion by using pointer chains, as we saw<br>previously. For insertion, we apply the following rules:<br>1. Locate the record in the ﬁle that comes before the record to be inserted in<br>search-key order.<br>2. If there is a free record (that is, space left after a deletion) within the same block<br>as this record, insert the new record there. Otherwise, insert the new record in<br>an overﬂow block. In either case, adjust the pointers so as to chain together the<br>records in search-key order.<br>Figure 11.16 shows the ﬁle of Figure 11.15 after the insertion of the record (North<br>Town, A-888, 800). The structure in Figure 11.16 allows fast insertion of new records,<br>but forces sequential ﬁle-processing applications to process records in an order that<br>does not match the physical order of the records.<br>If relatively few records need to be stored in overﬂow blocks, this approach works<br>well. Eventually, however, the correspondence between search-key order and physi-<br>cal order may be totally lost, in which case sequential processing will become much<br>less efﬁcient. At this point, the ﬁle should be reorganized so that it is once again phys-<br>ically in sequential order. Such reorganizations are costly, and must be done during<br>times when the system load is low. The frequency with which reorganizations are<br>needed depends on the frequency of insertion of new records. In the extreme case in<br>which insertions rarely occur, it is possible always to keep the ﬁle in physically sorted<br>order. In such a case, the pointer ﬁeld in Figure 11.15 is not needed.<br>11.7.2<br>Clustering File Organization<br>Many relational-database systems store each relation in a separate ﬁle, so that they<br>can take full advantage of the ﬁle system that the operating system provides. Usu-<br>ally, tuples of a relation can be represented as ﬁxed-length records. Thus, relations<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>IV. Data Storage and <br>Querying<br>11. Storage and File <br>Structure<br>426<br>© The McGraw−Hill <br>Companies, 2001<br>11.7<br>Organization of Records in Files<br>425<br>can be mapped to a simple ﬁle structure. This simple implementation of a relational<br>database system is well suited to low-cost database implementations as in, for exam-<br>ple, embedded systems or portable devices. In such systems, the size of the database<br>is small, so little is gained from a sophisticated ﬁle structure. Furthermore, in such<br>environments, it is essential that the overall size of the object code for the database<br>system be small. A simple ﬁle structure reduces the amount of code needed to imple-<br>ment the system.<br>This simple approach to relational-database implementation becomes less satis-<br>factory as the size of the database increases. We have seen that there are performance<br>advantages to be gained from careful assignment of records to blocks, and from care-<br>ful organization of the blocks themselves. Clearly, a more complicated ﬁle structure<br>may be beneﬁcial, even if we retain the strategy of storing each relation in a separate<br>ﬁle.<br>However, many large-scale database systems do not rely directly on the underly-<br>ing operating system for ﬁle management. Instead, one large operating-system ﬁle is<br>allocated to the database system. The database system stores all relations in this one<br>ﬁle, and manages the ﬁle itself. To see the advantage of storing many relations in one<br>ﬁle, consider the following SQL query for the bank database:<br>select account-number, customer-name, customer-street, customer-city<br>from depositor, customer<br>where depositor.customer-name = customer.customer-name<br>This query computes a join of the depositor and customer relations. Thus, for each<br>tuple of depositor, the system must locate the customer tuples with the same value for<br>customer-name. Ideally, these records will be located with the help of indices, which we<br>shall discuss in Chapter 12. Regardless of how these records are located, however,<br>they need to be transferred from disk into main memory. In the worst case, each<br>record will reside on a different block, forcing us to do one block read for each record<br>required by the query.<br>As a concrete example, consider the depositor and customer relations of Figures<br>11.17 and 11.18, respectively. In Figure 11.19, we show a ﬁle structure designed for ef-<br>ﬁcient execution of queries involving depositor<br> customer. The depositor tuples for<br>each customer-name are stored near the customer tuple for the corresponding customer-<br>name. This structure mixes together tuples of two relations, but allows for efﬁcient<br>processing of the join. When a tuple of the customer relation is read, the entire block<br>containing that tuple is copied from disk into main memory. Since the corresponding<br>customer-name<br>account-number<br>Hayes<br>A-102<br>Hayes<br>A-220<br>Hayes<br>A-503<br>Turner<br>A-305<br>Figure 11.17<br>The depositor relation.<br>Silberschatz−Korth−Sudarshan: <br>Database </span><br><br><span style="background-color: #A0C4FF;" title="Chunk 53 | Start: 1060106 | End: 1080106 | Tokens: 3186">System <br>Concepts, Fourth Edition<br>IV. Data Storage and <br>Querying<br>11. Storage and File <br>Structure<br>427<br>© The McGraw−Hill <br>Companies, 2001<br>426<br>Chapter 11<br>Storage and File Structure<br>customer-name<br>customer-street<br>customer-city<br>Hayes<br>Main<br>Brooklyn<br>Turner<br>Putnam<br>Stamford<br>Figure 11.18<br>The customer relation.<br>depositor tuples are stored on the disk near the customer tuple, the block containing the<br>customer tuple contains tuples of the depositor relation needed to process the query. If<br>a customer has so many accounts that the depositor records do not ﬁt in one block, the<br>remaining records appear on nearby blocks.<br>A clustering ﬁle organization is a ﬁle organization, such as that illustrated in Fig-<br>ure 11.19 that stores related records of two or more relations in each block. Such a ﬁle<br>organization allows us to read records that would satisfy the join condition by using<br>one block read. Thus, we are able to process this particular query more efﬁciently.<br>Our use of clustering has enhanced processing of a particular join (depositor<br> cus-<br>tomer), but it results in slowing processing of other types of query. For example,<br>select *<br>from customer<br>requires more block accesses than it did in the scheme under which we stored each<br>relation in a separate ﬁle. Instead of several customer records appearing in one block,<br>each record is located in a distinct block. Indeed, simply ﬁnding all the customer<br>records is not possible without some additional structure. To locate all tuples of the<br>customer relation in the structure of Figure 11.19, we need to chain together all the<br>records of that relation using pointers, as in Figure 11.20.<br>When clustering is to be used depends on the types of query that the database de-<br>signer believes to be most frequent. Careful use of clustering can produce signiﬁcant<br>performance gains in query processing.<br>11.8<br>Data-Dictionary Storage<br>So far, we have considered only the representation of the relations themselves. A<br>relational-database system needs to maintain data about the relations, such as the<br>Brooklyn<br>Stamford<br>A-102<br>Main<br>A-503<br>A-220<br>A-305<br>Putnam     <br>Hayes<br>Hayes<br>Hayes<br>Hayes<br>Turner<br>Turner<br>Figure 11.19<br>Clustering ﬁle structure.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>IV. Data Storage and <br>Querying<br>11. Storage and File <br>Structure<br>428<br>© The McGraw−Hill <br>Companies, 2001<br>11.8<br>Data-Dictionary Storage<br>427<br>Brooklyn<br>Stamford<br>A-102<br>Main<br>A-503<br>A-220<br>A-305<br>Putnam     <br>Hayes<br>Hayes<br>Hayes<br>Hayes<br>Turner<br>Turner<br>Figure 11.20<br>Clustering ﬁle structure with pointer chains.<br>schema of the relations. This information is called the data dictionary, or system<br>catalog. Among the types of information that the system must store are these:<br>• Names of the relations<br>• Names of the attributes of each relation<br>• Domains and lengths of attributes<br>• Names of views deﬁned on the database, and deﬁnitions of those views<br>• Integrity constraints (for example, key constraints)<br>In addition, many systems keep the following data on users of the system:<br>• Names of authorized users<br>• Accounting information about users<br>• Passwords or other information used to authenticate users<br>Further, the database may store statistical and descriptive data about the relations,<br>such as:<br>• Number of tuples in each relation<br>• Method of storage for each relation (for example, clustered or nonclustered)<br>The data dictionary may also note the storage organization (sequential, hash or heap)<br>of relations, and the location where each relation is stored:<br>• If relations are stored in operating system ﬁles, the dictionary would note the<br>names of the ﬁle (or ﬁles) containing each relation.<br>• If the database stores all relations in a single ﬁle, the dictionary may note the<br>blocks containing records of each relation in a data structure such as a linked<br>list.<br>In Chapter 12, in which we study indices, we shall see a need to store information<br>about each index on each of the relations:<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>IV. Data Storage and <br>Querying<br>11. Storage and File <br>Structure<br>429<br>© The McGraw−Hill <br>Companies, 2001<br>428<br>Chapter 11<br>Storage and File Structure<br>• Name of the index<br>• Name of the relation being indexed<br>• Attributes on which the index is deﬁned<br>• Type of index formed<br>All this information constitutes, in effect, a miniature database. Some database<br>systems store this information by using special-purpose data structures and code.<br>It is generally preferable to store the data about the database in the database itself.<br>By using the database to store system data, we simplify the overall structure of the<br>system and harness the full power of the database for fast access to system data.<br>The exact choice of how to represent system data by relations must be made by<br>the system designers. One possible representation, with primary keys underlined, is<br>Relation-metadata (relation-name, number-of-attributes, storage-organization, location)<br>Attribute-metadata (attribute-name, relation-name, domain-type, position, length)<br>User-metadata (user-name, encrypted-password, group)<br>Index-metadata (index-name, relation-name, index-type, index-attributes)<br>View-metadata (view-name, deﬁnition)<br>In this representation, the attribute index-attributes of the relation Index-metadata is<br>assumed to contain a list of one or more attributes, which can be represented by a<br>character string such as “branch-name, branch-city”. The Index-metadata relation is thus<br>not in ﬁrst normal form; it can be normalized, but the above representation is likely<br>to be more efﬁcient to access. The data dictionary is often stored in a non-normalized<br>form to achieve fast access.<br>The storage organization and location of the Relation-metadata itself must be recor-<br>ded elsewhere (for example, in the database code itself), since we need this informa-<br>tion to ﬁnd the contents of Relation-metadata.<br>11.9<br>Storage for Object-Oriented Databases∗∗<br>The ﬁle-organization techniques described in Section 11.7—the heap, sequential,<br>hashing and clustering organizations—can also be used for storing objects in an<br>object-oriented database. However, some extra features are needed to support object-<br>oriented database features, such as set-valued ﬁelds and persistent pointers.<br>11.9.1<br>Mapping of Objects to Files<br>The mapping of objects to ﬁles is in many ways like the mapping of tuples to ﬁles in a<br>relational system. At the lowest level of data representation, both tuples and the data<br>parts of objects are simply sequences of bytes. We can therefore store object data in<br>the ﬁle structures described in this chapter, with some modiﬁcations which we note<br>next.<br>Objects in object-oriented databases may lack the uniformity of tuples in relational<br>databases. For example, ﬁelds of records may be sets; in relational databases, in con-<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>IV. Data Storage and <br>Querying<br>11. Storage and File <br>Structure<br>430<br>© The McGraw−Hill <br>Companies, 2001<br>11.9<br>Storage for Object-Oriented Databases∗∗<br>429<br>trast, data are typically required to be (at least) in ﬁrst normal form. Furthermore,<br>objects may be extremely large. Such objects have to be managed differently from<br>records in a relational system.<br>We can implement set-valued ﬁelds that have a small number of elements using<br>data structures such as linked lists. Set-valued ﬁelds that have a larger number of<br>elements can be implemented as relations in the database. Set-valued ﬁelds of ob-<br>jects can also be eliminated at the storage level by normalization: A relation is created<br>containing one tuple for each value of a set-valued ﬁeld of an object. Each tuple also<br>contains the object identiﬁer of the object. However, this relation is not made visible<br>to the upper levels of the database system. The storage system gives the upper levels<br>of the database system the view of a set-valued ﬁeld, even though the set-valued ﬁeld<br>has actually been normalized by creating a new relation.<br>Some applications include extremely large objects that are not easily decomposed<br>into smaller components. Such large objects may each be stored in a separate ﬁle. We<br>discuss this idea further in Section 11.9.6.<br>11.9.2<br>Implementation of Object Identiﬁers<br>Since objects are identiﬁed by object identiﬁers (OIDs), an object-storage system needs<br>a mechanism to locate an object, given an OID. If the OIDs are logical OIDs—that is,<br>they do not specify the location of the object—then the storage system must maintain<br>an index that maps OIDs to the actual location of the object. If the OIDs are physical<br>OIDs—that is, they encode the location of the object—then the object can be found<br>directly. Physical OIDs typically have the following three parts:<br>1. A volume or ﬁle identiﬁer<br>2. A block identiﬁer within the volume or ﬁle<br>3. An offset within the block<br>A volume is a logical unit of storage that usually corresponds to a disk.<br>In addition, physical OIDs may contain a unique identiﬁer, which is an integer<br>that distinguishes the OID from the identiﬁers of other objects that happened to be<br>stored at the same location earlier, and were deleted or moved elsewhere. The unique<br>identiﬁer is also stored with the object, and the identiﬁers in an OID and the corre-<br>sponding object should match. If the unique identiﬁer in a physical OID does not<br>match the unique identiﬁer in the object to which that OID points, the system detects<br>that the pointer is a dangling pointer, and signals an error. (A dangling pointer is a<br>pointer that does not point to a valid object.) Figure 11.21 illustrates this scheme.<br>Such pointer errors occur when physical OIDs corresponding to old objects that<br>have been deleted are used accidentally. If the space occupied by the object had been<br>reallocated, there may be a new object in the location, and it may get incorrectly<br>addressed by the identiﬁer of the old object. If a dangling pointer is not detected,<br>it could cause corruption of a new object stored at the same location. The unique<br>identiﬁer helps to detect such errors, since the unique identiﬁers of the old physical<br>OID and the new object will not match.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>IV. Data Storage and <br>Querying<br>11. Storage and File <br>Structure<br>431<br>© The McGraw−Hill <br>Companies, 2001<br>430<br>Chapter 11<br>Storage and File Structure<br>(b)  Example of use<br>Location<br>Unique-Id<br>Data<br>519.56850.1200<br>51<br>......<br>519.56850.1200<br>51<br>519.56850.1200<br>50<br>Good OID<br>Bad OID<br> Volume.Block.Offset  <br>Unique-Id       Data<br>Physical Object Identifier<br>Object<br>(a)  General structure<br>Unique-Id<br>Figure 11.21<br>Unique identiﬁers in an OID.<br>Suppose that an object has to be moved to a new block, perhaps because the size of<br>the object has increased, and the old block has no extra space. Then, the physical OID<br>will point to the old block, which no longer contains the object. Rather than change<br>the OID of the object (which involves changing every object that points to this one),<br>we leave behind a forwarding address at the old location. When the database tries<br>to locate the object, it ﬁnds the forwarding address instead of the object; it then uses<br>the forwarding address to locate the object.<br>11.9.3<br>Management of Persistent Pointers<br>We implement persistent pointers in a persistent programming language by using<br>OIDs. In some implementations, persistent pointers are physical OIDs; in others, they<br>are logical OIDs. An important difference between persistent pointers and in-memory<br>pointers is the size of the pointer. In-memory pointers need to be only big enough<br>to address all virtual memory. On most current computers, in-memory pointers are<br>usually 4 bytes long, which is sufﬁcient to address 4 gigabytes of memory. The most<br>recent computer architectures have pointers that are 8 bytes long,<br>Persistent pointers need to address all the data in a database. Since database sys-<br>tems are often bigger than 4 gigabytes, persistent pointers are usually at least 8 bytes<br>long. Many object-oriented databases also provide unique identiﬁers in persistent<br>pointers, to catch dangling references. This feature further increases the size of persis-<br>tent pointers. Thus, persistent pointers may be substantially longer than in-memory<br>pointers.<br>The action of looking up an object, given its identiﬁer, is called dereferencing.<br>Given an in-memory pointer (as in C++), looking up the object is merely a memory<br>reference. Given a persistent pointer, dereferencing an object has an extra step—ﬁnd-<br>ing the actual location of the object in memory by looking up the persistent pointer<br>in a table. If the object is not already in memory, it has to be loaded from disk. We<br>can implement the table lookup fairly efﬁciently by using a hash table data structure,<br>but the lookup is still slow compared to a pointer dereference, even if the object is<br>already in memory.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>IV. Data Storage and <br>Querying<br>11. Storage and File <br>Structure<br>432<br>© The McGraw−Hill <br>Companies, 2001<br>11.9<br>Storage for Object-Oriented Databases∗∗<br>431<br>Pointer swizzling is a way to cut down the cost of locating persistent objects that<br>are already present in memory. The idea is that, when a persistent pointer is ﬁrst<br>dereferenced, the system locates the object and brings it into memory if it is not<br>already there. Now the system carries out an extra step—it stores an in-memory<br>pointer to the object in place of the persistent pointer. The next time that the same<br>persistent pointer is dereferenced, the in-memory location can be read out directly,<br>so the costs of locating the object are avoided. (When persistent objects have to be<br>moved from memory back to disk to make space for other persistent objects, the<br>system must carry out an extra step to ensure that the object is still in memory. Cor-<br>respondingly, when an object is written out, any persistent pointers that it contained<br>and that were swizzled have to be deswizzled, that is, converted back to their per-<br>sistent representation. Pointer swizzling on pointer dereference, as described here, is<br>called software swizzling.<br>Buffer management is more complicated if pointer swizzling is used, since the<br>physical location of an object must not change once that object is brought into the<br>buffer. One way to ensure that it will not change is to pin pages containing swiz-<br>zled objects in the buffer pool, so that they are never replaced until the program<br>that performed the swizzling has ﬁnished execution. See the bibliographical notes<br>for more complex buffer-management schemes, based on virtual-memory mapping<br>techniques, that make it unnecessary to pin the buffer pages.<br>11.9.4<br>Hardware Swizzling<br>Having two types of pointers, persistent and transient (in-memory), is inconvenient.<br>Programmers have to remember the type of the pointers, and may have to write<br>code twice—once for the persistent pointers and once for the in-memory pointers. It<br>would be simpler if both persistent and in-memory pointers were of the same type.<br>A simple way to merge persistent and in-memory pointer types is just to extend<br>the length of in-memory pointers to the same size as persistent pointers, and to use 1<br>bit of the identiﬁer to distinguish between persistent and in-memory pointers. How-<br>ever, the storage cost of longer persistent pointers will have to be borne by in-memory<br>pointers as well; understandably, this scheme is unpopular.<br>We shall describe a technique called hardware swizzling, which uses the virtual-<br>memory-management hardware present in most current computer systems to ad-<br>dress this problem. When data in a virtual memory page are accessed, and the oper-<br>ating system detects that the page does not have real storage allocated for it, or has<br>been access protected, then a segmentation violation is said to occur.3 Many oper-<br>ating systems provide a mechanism to specify a function to be called when a seg-<br>mentation violation occurs, and a mechanism to allocate storage for a page in virtual<br>address space, and to set that page’s access permissions. In most Unix systems, the<br>mmap system call provides this latter functionality. Hardware swizzling makes clever<br>use of the above mechanisms.<br>3.<br>The term page fault is sometimes used instead of segmentation violation, although access protection<br>violations are generally not considered to be page faults.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>IV. Data Storage and <br>Querying<br>11. Storage and File <br>Structure<br>433<br>© The McGraw−Hill <br>Companies, 2001<br>432<br>Chapter 11<br>Storage and File Structure<br>Hardware swizzling has two major advantages over software swizzling:<br>1. It is able to store persistent pointers in objects in the same amount of space as<br>in-memory pointers require (along with extra storage external to the object).<br>2. It transparently converts between persistent pointers and in-memory pointers<br>in a clever and efﬁcient way. Software written to deal with in-memory pointers<br>can thereby deal with persistent pointers as well, without any changes.<br>11.9.4.1<br>Pointer Representation<br>Hardware swizzling uses the following representation of persistent pointers con-<br>tained in objects that are on disk. A persistent pointer is conceptually split into two<br>parts: a page identiﬁer in the database, and an offset within the page.4 The page<br>identiﬁer in a persistent pointer is actually a small indirect pointer, which we call<br>the short page identiﬁer. Each page (or other unit of storage) has a translation table<br>that provides a mapping from the short page identiﬁers to full database page identi-<br>ﬁers. The system has to look up the short page identiﬁer in a persistent pointer in the<br>translation table to ﬁnd the full page identiﬁer.<br>The translation table, in the worst case, will be only as big as the maximum number<br>of pointers that can be contained in objects in a page; with a page size of 4096, and<br>a pointer size of 4 bytes, the maximum number of pointers is 1024. In practice, the<br>translation table is likely to contain much less than the maximum number of elements<br>(1024 in our example) and will not consume excessive space. The short page identiﬁer<br>needs to have only enough bits to identify a row in the table; with a maximum table<br>size of 1024, only 10 bits are required. Hence, a small number of bits is enough to<br>store the short page identiﬁer. Thus, the translation table permits an entire persistent<br>pointer to ﬁt into the same space as an in-memory pointer. Even though only a few<br>bits are needed for the short page identiﬁer, all the bits of an in-memory pointer,<br>other than the page-offset bits, are used as the short page identiﬁer. This architecture<br>facilitates swizzling, as we shall see.<br>The persistent-pointer representation scheme appears in Figure 11.22, where there<br>are three objects in the page, each containing a persistent pointer. The translation ta-<br>ble gives the mapping between short page identiﬁers and the full database page iden-<br>tiﬁers for each of the short page identiﬁers in these persistent pointers. The database<br>page identiﬁers are shown in the format volume.page.offset.<br>Each page maintains extra information so that all persistent pointers in the page<br>can be found. The system updates the information when an object is created or deleted<br>in the page. The need to locate all the persistent pointers in a page will become clear<br>later.<br>4.<br>The term page is generally used to refer to a real-memory or virtual-memory page, and the term<br>block is used to refer to disk blocks in the database. In hardware swizzling, these have to be of the same<br>size, and database blocks are fetched into virtual memory pages. We shall use the terms page and block<br>interchangeably in this section.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>IV. Data Storage and <br>Querying<br>11. Storage and File <br>Structure<br>434<br>© The McGraw−Hill <br>Companies, 2001<br>11.9<br>Storage for Object-Oriented Databases∗∗<br>433<br>2395<br>679.34278<br>4867<br>519.56850<br>object 1<br>object 2<br>object 3<br>PageId  Off.<br>255<br>2395<br>PageId  Off.<br>PageId  Off.<br>2395<br>translation table<br>PageID  FullPageID<br>020<br>170<br>4867</span><br><br><span style="background-color: #BDB2FF;" title="Chunk 54 | Start: 1080108 | End: 1100108 | Tokens: 3256"><br>Figure 11.22<br>Page image before swizzling.<br>11.9.4.2<br>Swizzling Pointers on a Page<br>Initially no page of the database has been allocated a page in virtual memory. Virtual-<br>memory pages may be allocated to database pages even before they are actually<br>loaded, as we will see shortly. Database pages get loaded into virtual-memory when<br>the database system needs to access data on the page. Before a database page is<br>loaded, the system allocates a virtual-memory page to the database page if one has<br>not already been allocated. The system then loads the database page into the virtual-<br>memory page it has allocated to it.<br>When the system loads a database page P into virtual memory, it does pointer<br>swizzling on the page: It locates all persistent pointers contained in objects in page<br>P, using the extra information stored in the page. It takes the following actions for<br>each persistent pointer in the page. (Let the value of the persistent pointer be ⟨pi, oi⟩,<br>where pi is the short page identiﬁer and oi is the offset within the page. Let Pi be the<br>full page identiﬁer of pi, found in the translation table in page P.)<br>1. If page Pi does not already have a virtual-memory page allocated to it, the<br>system now allocates a free page in virtual memory to it. The page Pi will<br>reside at this virtual-memory location if and when it is brought in. At this<br>point, the page in virtual address space does not have any storage allocated<br>for it, either in memory or on disk; it is merely a range of addresses reserved<br>for the database page. The system allocates actual space when it actually loads<br>the database page Pi into virtual memory.<br>2. Let the virtual-memory page allocated (either earlier or in the preceding step)<br>for Pi be vi. The system updates the persistent pointer being considered, whose<br>value is ⟨pi, oi⟩, by replacing pi with vi.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>IV. Data Storage and <br>Querying<br>11. Storage and File <br>Structure<br>435<br>© The McGraw−Hill <br>Companies, 2001<br>434<br>Chapter 11<br>Storage and File Structure<br>679.34278<br>4867<br>519.56850<br>5001<br>object 1<br>object 2<br>object 3<br>PageId  Off.<br>255<br>PageId  Off.<br>PageId  Off.<br>5001<br>4867<br>020<br>170<br>5001<br>translation table<br>PageID  FullPageID<br>Figure 11.23<br>Page image after swizzling.<br>Figure 11.23 shows the state of the page from Figure 11.22 after the system has<br>brought that page into memory and swizzled the pointers in it. Here, we assume<br>that the page whose database page identiﬁer is 679.34278 has been mapped to page<br>5001 in memory, whereas the page whose identiﬁer is 519.56850 has been mapped to<br>page 4867 (which is the same as the short page identiﬁer). All the pointers in objects<br>have been updated to reﬂect the new mapping, and can now be used as in-memory<br>pointers.<br>At the end of the translation phase for a page, the objects in the page satisfy an<br>important property: All persistent pointers contained in objects in the page have been<br>converted to in-memory pointers. Thus, objects in in-memory pages contain only in-<br>memory pointers. Routines that use these objects do not even need to know about the<br>existence of persistent pointers! For example, existing libraries written for in-memory<br>objects can be used unchanged for persistent objects. That is indeed an important<br>advantage!<br>11.9.4.3<br>Pointer Dereference<br>Consider the ﬁrst time that an in-memory pointer to a virtual-memory page vi is<br>dereferenced, when storage has not yet been allocated for the page. As we described,<br>a segmentation violation will occur, and will result in a function call on the database<br>system. The database system takes the following actions:<br>1. It ﬁrst determines what database page was allocated to virtual-memory page<br>vi; let the full page identiﬁer of the database page be Pi. (If no database page<br>has been allocated to vi, the pointer is incorrect, and the system ﬂags an error.)<br>2. It allocates storage space for page vi, and loads the database page Pi into<br>virtual-memory page vi.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>IV. Data Storage and <br>Querying<br>11. Storage and File <br>Structure<br>436<br>© The McGraw−Hill <br>Companies, 2001<br>11.9<br>Storage for Object-Oriented Databases∗∗<br>435<br>3. It carries out pointer swizzling out on page Pi, as described earlier in “Swiz-<br>zling Pointer on a Page”.<br>4. After swizzling all persistent pointers in P, the system allows the pointer<br>dereference that resulted in the segmentation violation to continue. The pointer<br>dereference will ﬁnd the object for which it was looking loaded in memory.<br>If any swizzled pointer that points to an object in page vi is dereferenced later,<br>the dereference proceeds just like any other virtual-memory access, with no extra<br>overheads. In contrast, if swizzling is not used, there is considerable overhead in<br>locating the buffer page containing the object and then accessing it. This overhead<br>has to be incurred on every access to objects in the page, whereas when swizzling is<br>performed, the overhead is incurred only on the ﬁrst access to an object in the page.<br>Later accesses operate at regular virtual-memory access speeds. Hardware swizzling<br>thus gives excellent performance beneﬁts to applications that repeatedly dereference<br>pointers.<br>11.9.4.4<br>Optimizations<br>Software swizzling performs a deswizzling operation when a page in memory has<br>to be written back to the database, to convert in-memory pointers back to persistent<br>pointers. Hardware swizzling can avoid this step—when the system does pointer<br>swizzling for the page, it simply updates the translation table for the page, so that<br>the page-identiﬁer part of the swizzled in-memory pointers can be used to look up<br>the table. For example, as shown in Figure 11.23, database page 679.34278 (with short<br>identiﬁer 2395 in the page shown) is mapped to virtual-memory page 5001. At this<br>point, not only is the pointer in object 1 updated from 2395255 to 5001255, but also<br>the short identiﬁer in the table is updated to 5001. Thus, the short identiﬁer 5001 in<br>object 1 and in the table match each other again. Therefore, the page can be written<br>back to disk without any deswizzling.<br>Several optimizations can be carried out on the basic scheme described here. When<br>the system swizzles page P, for each page P ′ referred to by any persistent pointer in<br>P, it attempts to allocate P ′ to the virtual address location indicated by the short page<br>identiﬁer of P ′ on page P. If the system can allocate the page in this attempt, pointers<br>to it do not need to be updated. In our swizzling example, page 519.56850 with short<br>page identiﬁer 4867 was mapped to virtual-memory page 4867, which is the same as<br>its short page identiﬁer. We can see that the pointer in object 2 to this page did not<br>need to be changed during swizzling. If every page can be allocated to its appropriate<br>location in virtual address space, none of the pointers need to be translated, and the<br>cost of swizzling is reduced signiﬁcantly.<br>Hardware swizzling works even if the database is bigger than virtual memory,<br>but only as long as all the pages that a particular process accesses ﬁt into the virtual<br>memory of the process. If they do not, a page that has been brought into virtual<br>memory will have to be replaced, and that replacement is hard to do, since there may<br>be in-memory pointers to objects in that page.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>IV. Data Storage and <br>Querying<br>11. Storage and File <br>Structure<br>437<br>© The McGraw−Hill <br>Companies, 2001<br>436<br>Chapter 11<br>Storage and File Structure<br>Hardware swizzling can also be used at the level of sets of pages (often called<br>segments), instead of for a single page. For set-level swizzling, the system uses a<br>single translation table for all pages in the segment. It loads pages in the segment and<br>swizzles them as and when they are required; they need not be loaded all together.<br>11.9.5<br>Disk Versus Memory Structure of Objects<br>The format in which objects are stored in memory may be different from the for-<br>mat in which they are stored on disk in the database. One reason may be the use of<br>software swizzling, where the structures of persistent and in-memory pointers are<br>different. Another reason may be that we want to have the database accessible from<br>different machines, possibly based on different architectures, and from different lan-<br>guages, and from programs compiled under different compilers, all of which result<br>in differences in the in-memory representation.<br>Consider, for example, a data-structure deﬁnition in a programming language<br>such as C++. The physical structure (such as sizes and representation of integers)<br>in the object depends on the machine on which the program is run.5 Further, the<br>physical structure may also depend on which compiler is used—in a language as<br>complex as C++, different choices for translation from the high-level description to<br>the physical structure are possible, and each compiler can make its own choice.<br>The solution to this problem is to make the physical representation of objects in the<br>database independent of the machine and of the compiler. The system can convert<br>the object from the disk representation to the form that is required on the speciﬁc<br>machine, language, and compiler, when that object is brought into memory. It can do<br>this conversion transparently at the same time that it swizzles pointers in the object,<br>so the programmer does not need to worry about the conversion.<br>The ﬁrst step in implementing such a scheme is to deﬁne a common language<br>for describing the structure of objects—that is, a data-deﬁnition language. One such<br>language is the Object Deﬁnition Language (ODL) developed by the Object Database<br>Management Group (ODMG). ODL has mappings deﬁned to the Java, C++, and Small-<br>talk languages, so potentially we may manipulate objects in an ODMG-compliant<br>database using any of these languages.<br>The deﬁnition of the structure of each class in the database is stored (logically) in<br>the databases. The code to translate an object in the database to the representation<br>that is manipulated with the programming language (and vice versa) depends on<br>the machine as well as on the compiler for the language. We can generate this code<br>automatically, using the stored deﬁnition of the class of the object.<br>An unexpected source of differences between the disk and in-memory representa-<br>tions of data is the hidden-pointers in objects. Hidden pointers are transient pointers<br>5.<br>For<br>instance,<br>the<br>Motorola<br>680x0<br>architectures,<br>the<br>IBM<br>360<br>architecture,<br>and<br>the<br>Intel<br>80386/80486/Pentium/Pentium-II/Pentium-III architectures all have 4-byte integers. However, they dif-<br>fer in how the bits of an integer are laid out within a word. In earlier-generation personal computers,<br>integers were 2 bytes long; in newer workstation architectures, such as the Compaq Alpha, Intel Itanium,<br>and Sun UltraSparc architectures, integers can be 8 bytes long.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>IV. Data Storage and <br>Querying<br>11. Storage and File <br>Structure<br>438<br>© The McGraw−Hill <br>Companies, 2001<br>11.9<br>Storage for Object-Oriented Databases∗∗<br>437<br>that compilers generate and store in objects. These pointers point (indirectly) to tables<br>used to implement certain methods of the object. The tables are typically compiled<br>into executable object code, and their exact location depends on the executable object<br>code; hence, they may be different for different processes. Therefore, when a process<br>accesses an object, the hidden pointers must be ﬁxed to point to the correct location.<br>The hidden pointers can be initialized at the same time that data-representation con-<br>versions are carried out.<br>11.9.6<br>Large Objects<br>Objects may also be extremely large; for instance, multimedia objects may occupy<br>several megabytes of space. Exceptionally large data items, such as video sequences,<br>may run into gigabytes, although they are usually split into multiple objects, each on<br>the order of a few megabytes or less. Large objects containing binary data are called<br>binary large objects (blobs), while large objects containing character data, are called<br>character large objects (clobs), as we saw in Section 9.2.1.<br>Most relational databases restrict the size of a record to be no larger than the size<br>of a page, to simplify buffer management and free-space management. Large objects<br>and long ﬁelds are often stored in a special ﬁle (or collection of ﬁles) reserved for<br>long-ﬁeld storage.<br>Allocation of buffer pages presents a problem with managing large objects. Large<br>objects may need to be stored in a contiguous sequence of bytes when they are<br>brought into memory; in that case, if an object is bigger than a page, contiguous pages<br>of the buffer pool must be allocated to store it, which makes buffer management more<br>difﬁcult.<br>We often modify large objects by updating part of the object, or by inserting or<br>deleting parts of the object, rather than by writing the entire object. If inserts and<br>deletes need to be supported, we can handle large objects by using B-tree structures<br>(which we study in Chapter 12). B-tree structures permit us to read the entire object,<br>as well as to insert and delete parts of the object.<br>For practical reasons, we may manipulate large objects by using application pro-<br>grams, instead of doing so within the database:<br>• Text data. Text is usually treated as a byte string manipulated by editors and<br>formatters.<br>• Image/Graphical data. Graphical data may be represented as a bitmap or as<br>a set of lines, boxes, and other geometric objects. Although some graphical<br>data often are managed within the database system itself, special application<br>software is used for many cases, such as integrated circuit design.<br>• Audio and video data. Audio and video data are typically a digitized, com-<br>pressed representation created and displayed by separate application soft-<br>ware. Data are usually modiﬁed with special-purpose editing software, out-<br>side the database system.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>IV. Data Storage and <br>Querying<br>11. Storage and File <br>Structure<br>439<br>© The McGraw−Hill <br>Companies, 2001<br>438<br>Chapter 11<br>Storage and File Structure<br>The most widely used method for updating such data is the checkout/checkin<br>method. A user or an application would check out a copy of a long-ﬁeld object, op-<br>erate on this copy with special-purpose application programs, and then check in the<br>modiﬁed copy. Checkout and a checkin correspond roughly to read and write. In some<br>systems, a checkin may create a new version of the object without deleting the old<br>version.<br>11.10<br>Summary<br>• Several types of data storage exist in most computer systems. They are clas-<br>siﬁed by the speed with which they can access data, by their cost per unit<br>of data to buy the memory, and by their reliability. Among the media avail-<br>able are cache, main memory, ﬂash memory, magnetic disks, optical disks, and<br>magnetic tapes.<br>• Two factors determine the reliability of storage media: whether a power fail-<br>ure or system crash causes data to be lost, and what the likelihood is of phys-<br>ical failure of the storage devise.<br>• We can reduce the likelihood of physical failure by retaining multiple copies<br>of data. For disks, we can use mirroring. Or we can use more sophisticated<br>methods based on redundant arrays of independent disks (RAIDs). By striping<br>data across disks, these methods offer high throughput rates on large accesses;<br>by introducing redundancy across disks, they improve reliability greatly. Sev-<br>eral different RAID organizations are possible, each with different cost, perfor-<br>mance and reliability characteristics. RAID level 1 (mirroring) and RAID level<br>5 are the most commonly used.<br>• We can organize a ﬁle logically as a sequence of records mapped onto disk<br>blocks. One approach to mapping the database to ﬁles is to use several ﬁles,<br>and to store records of only one ﬁxed length in any given ﬁle. An alternative is<br>to structure ﬁles so that they can accommodate multiple lengths for records.<br>There are different techniques for implementing variable-length records, in-<br>cluding the slotted-page method, the pointer method, and the reserved-space<br>method.<br>• Since data are transferred between disk storage and main memory in units<br>of a block, it is worthwhile to assign ﬁle records to blocks in such a way that<br>a single block contains related records. If we can access several of the records<br>we want with only one block access, we save disk accesses. Since disk accesses<br>are usually the bottleneck in the performance of a database system, careful<br>assignment of records to blocks can pay signiﬁcant performance dividends.<br>• One way to reduce the number of disk accesses is to keep as many blocks as<br>possible in main memory. Since it is not possible to keep all blocks in main<br>memory, we need to manage the allocation of the space available in main<br>memory for the storage of blocks. The buffer is that part of main memory avail-<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>IV. Data Storage and <br>Querying<br>11. Storage and File <br>Structure<br>440<br>© The McGraw−Hill <br>Companies, 2001<br>11.10<br>Summary<br>439<br>able for storage of copies of disk blocks. The subsystem responsible for the<br>allocation of buffer space is called the buffer manager.<br>• Storage systems for object-oriented databases are somewhat different from<br>storage systems for relational databases: They must deal with large objects, for<br>example, and must support persistent pointers. There are schemes to detect<br>dangling persistent pointers.<br>• Software- and hardware-based swizzling schemes permit efﬁcient derefer-<br>encing of persistent pointers. The hardware-based schemes use the virtual-<br>memory-management support implemented in hardware, and made accessi-<br>ble to user programs by many current-generation operating systems.<br>Review Terms<br>• Physical storage media<br>  Cache<br>  Main memory<br>  Flash memory<br>  Magnetic disk<br>  Optical storage<br>• Magnetic disk<br>  Platter<br>  Hard disks<br>  Floppy disks<br>  Tracks<br>  Sectors<br>  Read–write head<br>  Disk arm<br>  Cylinder<br>  Disk controller<br>  Checksums<br>  Remapping of bad sectors<br>• Performance measures of disks<br>  Access time<br>  Seek time<br>  Rotational latency<br>  Data-transfer rate<br>  Mean time to failure (MTTF)<br>• Disk block<br>• Optimization of disk-block access<br>  Disk-arm scheduling<br>  Elevator algorithm<br>  File organization<br>  Defragmenting<br>  Nonvolatile write buffers<br>  Nonvolatile random-access<br>memory (NV-RAM)<br>  Log disk<br>  Log-based ﬁle system<br>• Redundant arrays of independent<br>disks (RAID)<br>  Mirroring<br>  Data striping<br>  Bit-level striping<br>  Block-level striping<br>• RAID levels<br>  Level 0 (block striping, no<br>redundancy)<br>  Level 1 (block striping,<br>mirroring)<br>  Level 3 (bit striping, parity)<br>  Level 5 (block striping,<br>distributed parity)<br>  Level 6 (block striping, P + Q<br>redundancy)<br>• Rebuild performance<br>• Software RAID<br>• Hardware RAID<br>• Hot swapping<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>IV. Data Storage and <br>Querying<br>11. Storage and File <br>Structure<br>441<br>© The McGraw−Hill <br>Companies, 2001<br>440<br>Chapter 11<br>Storage and File Structure<br>• Tertiary storage<br>  Optical disks<br>  Magnetic tapes<br>  Jukeboxes<br>• Buffer<br>  Buffer manager<br>  Pinned blocks<br>  Forced output of blocks<br>• Buffer-replacement policies<br>  Least recently used (LRU)<br>  Toss-immediate<br>  Most recently used (MRU)<br>• File<br>• File organization<br>  File header<br>  Free list<br>• Variable-length records<br>  Byte-string representation<br>  Slotted-page structure<br>  Reserved space<br>  List representation<br>• Heap ﬁle organization<br>• Sequential ﬁle organization<br>• Hashing ﬁle organization<br>• Clustering ﬁle organization<br>• Search key<br>• Data dictionary<br>• System catalog<br>• Storage structures for OODBs<br>• Object identiﬁer (OID)<br>  Logical OID<br>  Physical OID<br>  Unique identiﬁer<br>  Dangling pointer<br>  Forwarding address<br>• Pointer swizzling<br>  Dereferencing<br>  Deswizzling<br>  Software swizzling<br>  Hardware swizzling<br>  Segmentation violation<br>  Page fault<br>• Hidden pointers<br>• Large objects<br>Exercises<br>11.1 List the physical storage media available on the</span><br><br><span style="background-color: #FFC6FF;" title="Chunk 55 | Start: 1100110 | End: 1120110 | Tokens: 3291"> computers you use routinely.<br>Give the speed with which data can be accessed on each medium.<br>11.2 How does the remapping of bad sectors by disk controllers affect data-retrieval<br>rates?<br>11.3 Consider the following data and parity-block arrangement on four disks:<br>Disk 1<br>Disk 2<br>Disk 3<br>Disk 4<br>...<br>...<br>...<br>...<br>B1<br>P1<br>B8<br>B2<br>B5<br>P2<br>B3<br>B6<br>B9<br>B4<br>B7<br>B10<br>The Bi’s represent data blocks; the Pi’s represent parity blocks. Parity block Pi<br>is the parity block for data blocks B4i−3 to B4i. What, if any, problem might<br>this arrangement present?<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>IV. Data Storage and <br>Querying<br>11. Storage and File <br>Structure<br>442<br>© The McGraw−Hill <br>Companies, 2001<br>Exercises<br>441<br>11.4 A power failure that occurs while a disk block is being written could result in<br>the block being only partially written. Assume that partially written blocks can<br>be detected. An atomic block write is one where either the disk block is fully<br>written or nothing is written (i.e., there are no partial writes). Suggest schemes<br>for getting the effect of atomic block writes with the following RAID schemes.<br>Your schemes should involve work on recovery from failure.<br>a. RAID level 1 (mirroring)<br>b. RAID level 5 (block interleaved, distributed parity)<br>11.5 RAID systems typically allow you to replace failed disks without stopping ac-<br>cess to the system. Thus, the data in the failed disk must be rebuilt and written<br>to the replacement disk while the system is in operation. With which of the<br>RAID levels is the amount of interference between the rebuild and ongoing<br>disk accesses least? Explain your answer.<br>11.6 Give an example of a relational-algebra expression and a query-processing<br>strategy in each of the following situations:<br>a. MRU is preferable to LRU.<br>b. LRU is preferable to MRU.<br>11.7 Consider the deletion of record 5 from the ﬁle of Figure 11.8. Compare the<br>relative merits of the following techniques for implementing the deletion:<br>a. Move record 6 to the space occupied by record 5, and move record 7 to the<br>space occupied by record 6.<br>b. Move record 7 to the space occupied by record 5.<br>c. Mark record 5 as deleted, and move no records.<br>11.8 Show the structure of the ﬁle of Figure 11.9 after each of the following steps:<br>a. Insert (Brighton, A-323, 1600).<br>b. Delete record 2.<br>c. Insert (Brighton, A-626, 2000).<br>11.9 Give an example of a database application in which the reserved-space method<br>of representing variable-length records is preferable to the pointer method. Ex-<br>plain your answer.<br>11.10 Give an example of a database application in which the pointer method of rep-<br>resenting variable-length records is preferable to the reserved-space method.<br>Explain your answer.<br>11.11 Show the structure of the ﬁle of Figure 11.12 after each of the following steps:<br>a. Insert (Mianus, A-101, 2800).<br>b. Insert (Brighton, A-323, 1600).<br>c. Delete (Perryridge, A-102, 400).<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>IV. Data Storage and <br>Querying<br>11. Storage and File <br>Structure<br>443<br>© The McGraw−Hill <br>Companies, 2001<br>442<br>Chapter 11<br>Storage and File Structure<br>11.12 What happens if you attempt to insert the record<br>(Perryridge, A-929, 3000)<br>into the ﬁle of Figure 11.12?<br>11.13 Show the structure of the ﬁle of Figure 11.13 after each of the following steps:<br>a. Insert (Mianus, A-101, 2800).<br>b. Insert (Brighton, A-323, 1600).<br>c. Delete (Perryridge, A-102, 400).<br>11.14 Explain why the allocation of records to blocks affects database-system perfor-<br>mance signiﬁcantly.<br>11.15 If possible, determine the buffer-management strategy used by the operating<br>system running on your local computer system, and what mechanisms it pro-<br>vides to control replacement of pages. Discuss how the control on replacement<br>that it provides would be useful for the implementation of database systems.<br>11.16 In the sequential ﬁle organization, why is an overﬂow block used even if there<br>is, at the moment, only one overﬂow record?<br>11.17 List two advantages and two disadvantages of each of the following strategies<br>for storing a relational database:<br>a. Store each relation in one ﬁle.<br>b. Store multiple relations (perhaps even the entire database) in one ﬁle.<br>11.18 Consider a relational database with two relations:<br>course (course-name, room, instructor)<br>enrollment (course-name, student-name, grade)<br>Deﬁne instances of these relations for three courses, each of which enrolls ﬁve<br>students. Give a ﬁle structure of these relations that uses clustering.<br>11.19 Consider the following bitmap technique for tracking free space in a ﬁle. For<br>each block in the ﬁle, two bits are maintained in the bitmap. If the block is<br>between 0 and 30 percent full the bits are 00, between 30 and 60 percent the<br>bits are 01, between 60 and 90 percent the bits are 10, and above 90 percent the<br>bits are 11. Such bitmaps can be kept in memory even for quite large ﬁles.<br>a. Describe how to keep the bitmap up-to-date on record insertions and dele-<br>tions.<br>b. Outline the beneﬁt of the bitmap technique over free lists when searching<br>for free space and when updating free space information.<br>11.20 Give a normalized version of the Index-metadata relation, and explain why us-<br>ing the normalized version would result in worse performance.<br>11.21 Explain why a physical OID must contain more information than a pointer to a<br>physical storage location.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>IV. Data Storage and <br>Querying<br>11. Storage and File <br>Structure<br>444<br>© The McGraw−Hill <br>Companies, 2001<br>Bibliographical Notes<br>443<br>11.22 If physical OIDs are used, an object can be relocated by keeping a forwarding<br>pointer to its new location. In case an object gets forwarded multiple times,<br>what would be the effect on retrieval speed? Suggest a technique to avoid mul-<br>tiple accesses in such a case.<br>11.23 Deﬁne the term dangling pointer. Describe how the unique-id scheme helps in<br>detecting dangling pointers in an object-oriented database.<br>11.24 Consider the example on page 435, which shows that there is no need for<br>deswizzling if hardware swizzling is used. Explain why, in that example, it<br>is safe to change the short identiﬁer of page 679.34278 from 2395 to 5001. Can<br>some other page already have short identiﬁer 5001? If it could, how can you<br>handle that situation?<br>Bibliographical Notes<br>Patterson and Hennessy [1995] discusses the hardware aspects of translation look-<br>aside buffers, caches and memory-management units. Rosch and Wethington [1999]<br>presents an excellent overview of computer hardware, including extensive cover-<br>age of all types of storage technology such as ﬂoppy disks, magnetic disks, optical<br>disks, tapes, and storage interfaces. Ruemmler and Wilkes [1994] presents a survey<br>of magnetic-disk technology. Flash memory is discussed by Dippert and Levy [1993].<br>The speciﬁcations of current-generation disk drives can be obtained from the Web<br>sites of their manufacturers, such as IBM, Seagate, and Maxtor.<br>Alternative disk organizations that provide a high degree of fault tolerance in-<br>clude those described by Gray et al. [1990] and Bitton and Gray [1988]. Disk striping<br>is described by Salem and Garcia-Molina [1986]. Discussions of redundant arrays of<br>inexpensive disks (RAID) are presented by Patterson et al. [1988] and Chen and Pat-<br>terson [1990]. Chen et al. [1994] presents an excellent survey of RAID principles and<br>implementation. Reed–Solomon codes are covered in Pless [1989]. The log-based ﬁle<br>system, which makes disk access sequential, is described in Rosenblum and Ouster-<br>hout [1991].<br>In systems that support mobile computing, data may be broadcast repeatedly. The<br>broadcast medium can be viewed as a level of the storage hierarchy—as a broadcast<br>disk with high latency. These issues are discussed in Acharya et al. [1995]. Caching<br>and buffer management for mobile computing is discussed in Barbar´a and Imielinski<br>[1994]. Further discussion of storage issues in mobile computing appears in Douglis<br>et al. [1994].<br>Basic data structures are discussed in Cormen et al. [1990]. There are several papers<br>describing the storage structure of speciﬁc database systems. Astrahan et al. [1976]<br>discusses System R. Chamberlin et al. [1981] reviews System R in retrospect. The<br>Oracle 8 Concepts Manual (Oracle [1997]) describes the storage organization of the<br>Oracle 8 database system. The structure of the Wisconsin Storage System (WiSS) is<br>described in Chou et al. [1985]. A software tool for the physical design of relational<br>databases is described by Finkelstein et al. [1988].<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>IV. Data Storage and <br>Querying<br>11. Storage and File <br>Structure<br>445<br>© The McGraw−Hill <br>Companies, 2001<br>444<br>Chapter 11<br>Storage and File Structure<br>Buffer management is discussed in most operating system texts, including in Sil-<br>berschatz and Galvin [1998]. Stonebraker [1981] discusses the relationship between<br>database-system buffer managers and operating-system buffer managers. Chou and<br>Dewitt [1985] presents algorithms for buffer management in database systems, and<br>describes a performance evaluation. Bridge et al. [1997] describes techniques used in<br>the buffer manager of the Oracle database system.<br>Descriptions and performance comparisons of different swizzling techniques are<br>given in Wilson [1990], Moss [1990], and White and DeWitt [1992]. White and De-<br>Witt [1994] describes the virtual-memory-mapped buffer-management scheme used<br>in the ObjectStore OODB system and in the QuickStore storage manager. Using this<br>scheme, we can map disk pages to a ﬁxed virtual-memory address, even if they are<br>not pinned in the buffer. The Exodus object storage manager is described in Carey<br>et al. [1986]. Biliris and Orenstein [1994] provides a survey of storage systems for<br>object-oriented databases. Jagadish et al. [1994] describes a storage manager for main-<br>memory databases.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>IV. Data Storage and <br>Querying<br>12. Indexing and Hashing<br>446<br>© The McGraw−Hill <br>Companies, 2001<br>C<br>H<br>A<br>P<br>T<br>E<br>R<br>1<br>2<br>Indexing and Hashing<br>Many queries reference only a small proportion of the records in a ﬁle. For exam-<br>ple, a query like “Find all accounts at the Perryridge branch” or “Find the balance<br>of account number A-101” references only a fraction of the account records. It is in-<br>efﬁcient for the system to read every record and to check the branch-name ﬁeld for<br>the name “Perryridge,” or the account-number ﬁeld for the value A-101. Ideally, the<br>system should be able to locate these records directly. To allow these forms of access,<br>we design additional structures that we associate with ﬁles.<br>12.1<br>Basic Concepts<br>An index for a ﬁle in a database system works in much the same way as the index<br>in this textbook. If we want to learn about a particular topic (speciﬁed by a word or<br>a phrase) in this textbook, we can search for the topic in the index at the back of the<br>book, ﬁnd the pages where it occurs, and then read the pages to ﬁnd the information<br>we are looking for. The words in the index are in sorted order, making it easy to ﬁnd<br>the word we are looking for. Moreover, the index is much smaller than the book,<br>further reducing the effort needed to ﬁnd the words we are looking for.<br>Card catalogs in libraries worked in a similar manner (although they are rarely<br>used any longer). To ﬁnd a book by a particular author, we would search in the author<br>catalog, and a card in the catalog tells us where to ﬁnd the book. To assist us in<br>searching the catalog, the library would keep the cards in alphabetic order by authors,<br>with one card for each author of each book.<br>Database system indices play the same role as book indices or card catalogs in<br>libraries. For example, to retrieve an account record given the account number, the<br>database system would look up an index to ﬁnd on which disk block the correspond-<br>ing record resides, and then fetch the disk block, to get the account record.<br>Keeping a sorted list of account numbers would not work well on very large<br>databases with millions of accounts, since the index would itself be very big; further,<br>445<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>IV. Data Storage and <br>Querying<br>12. Indexing and Hashing<br>447<br>© The McGraw−Hill <br>Companies, 2001<br>446<br>Chapter 12<br>Indexing and Hashing<br>even though keeping the index sorted reduces the search time, ﬁnding an account<br>can still be rather time-consuming. Instead, more sophisticated indexing techniques<br>may be used. We shall discuss several of these techniques in this chapter.<br>There are two basic kinds of indices:<br>• Ordered indices. Based on a sorted ordering of the values.<br>• Hash indices. Based on a uniform distribution of values across a range of<br>buckets. The bucket to which a value is assigned is determined by a function,<br>called a hash function.<br>We shall consider several techniques for both ordered indexing and hashing. No<br>one technique is the best. Rather, each technique is best suited to particular database<br>applications. Each technique must be evaluated on the basis of these factors:<br>• Access types: The types of access that are supported efﬁciently. Access types<br>can include ﬁnding records with a speciﬁed attribute value and ﬁnding records<br>whose attribute values fall in a speciﬁed range.<br>• Access time: The time it takes to ﬁnd a particular data item, or set of items,<br>using the technique in question.<br>• Insertion time: The time it takes to insert a new data item. This value includes<br>the time it takes to ﬁnd the correct place to insert the new data item, as well as<br>the time it takes to update the index structure.<br>• Deletion time: The time it takes to delete a data item. This value includes the<br>time it takes to ﬁnd the item to be deleted, as well as the time it takes to update<br>the index structure.<br>• Space overhead: The additional space occupied by an index structure. Pro-<br>vided that the amount of additional space is moderate, it is usually worth-<br>while to sacriﬁce the space to achieve improved performance.<br>We often want to have more than one index for a ﬁle. For example, libraries main-<br>tained several card catalogs: for author, for subject, and for title.<br>An attribute or set of attributes used to look up records in a ﬁle is called a search<br>key. Note that this deﬁnition of key differs from that used in primary key, candidate<br>key, and superkey. This duplicate meaning for key is (unfortunately) well established<br>in practice. Using our notion of a search key, we see that if there are several indices<br>on a ﬁle, there are several search keys.<br>12.2<br>Ordered Indices<br>To gain fast random access to records in a ﬁle, we can use an index structure. Each<br>index structure is associated with a particular search key. Just like the index of a book<br>or a library catalog, an ordered index stores the values of the search keys in sorted<br>order, and associates with each search key the records that contain it.<br>The records in the indexed ﬁle may themselves be stored in some sorted order, just<br>as books in a library are stored according to some attribute such as the Dewey deci-<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>IV. Data Storage and <br>Querying<br>12. Indexing and Hashing<br>448<br>© The McGraw−Hill <br>Companies, 2001<br>12.2<br>Ordered Indices<br>447<br>A-217     Brighton<br>750<br>A-101     Downtown<br>500<br>A-110     Downtown<br>600<br>A-215     Mianus<br>700<br>A-102     Perryridge<br>400<br>A-201     Perryridge<br>900<br>A-218     Perryridge<br>700<br>A-222     Redwood<br>700<br>A-305     Round Hill<br>350<br>Brighton<br>Mianus<br>Redwood<br>Figure 12.1<br>Sequential ﬁle for account records.<br>mal number. A ﬁle may have several indices, on different search keys. If the ﬁle con-<br>taining the records is sequentially ordered, a primary index is an index whose search<br>key also deﬁnes the sequential order of the ﬁle. (The term primary index is sometimes<br>used to mean an index on a primary key. However, such usage is nonstandard and<br>should be avoided.) Primary indices are also called clustering indices. The search<br>key of a primary index is usually the primary key, although that is not necessarily so.<br>Indices whose search key speciﬁes an order different from the sequential order of the<br>ﬁle are called secondary indices, or nonclustering indices.<br>12.2.1<br>Primary Index<br>In this section, we assume that all ﬁles are ordered sequentially on some search key.<br>Such ﬁles, with a primary index on the search key, are called index-sequential ﬁles.<br>They represent one of the oldest index schemes used in database systems. They are<br>designed for applications that require both sequential processing of the entire ﬁle and<br>random access to individual records.<br>Figure 12.1 shows a sequential ﬁle of account records taken from our banking ex-<br>ample. In the example of Figure 12.1, the records are stored in search-key order, with<br>branch-name used as the search key.<br>12.2.1.1<br>Dense and Sparse Indices<br>An index record, or index entry, consists of a search-key value, and pointers to one<br>or more records with that value as their search-key value. The pointer to a record<br>consists of the identiﬁer of a disk block and an offset within the disk block to identify<br>the record within the block.<br>There are two types of ordered indices that we can use:<br>• Dense index: An index record appears for every search-key value in the ﬁle.<br>In a dense primary index, the index record contains the search-key value and<br>a pointer to the ﬁrst data record with that search-key value. The rest of the<br>records with the same search key-value would be stored sequentially after the<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>IV. Data Storage and <br>Querying<br>12. Indexing and Hashing<br>449<br>© The McGraw−Hill <br>Companies, 2001<br>448<br>Chapter 12<br>Indexing and Hashing<br>ﬁrst record, since, because the index is a primary one, records are sorted on<br>the same search key.<br>Dense index implementations may store a list of pointers to all records with<br>the same search-key value; doing so is not essential for primary indices.<br>• Sparse index: An index record appears for only some of the search-key values.<br>As is true in dense indices, each index record contains a search-key value and<br>a pointer to the ﬁrst data record with that search-key value. To locate a record,<br>we ﬁnd the index entry with the largest search-key value that is less than or<br>equal to the search-key value for which we are looking. We start at the record<br>pointed to by that index entry, and follow the pointers in the ﬁle until we ﬁnd<br>the desired record.<br>Figures 12.2 and 12.3 show dense and sparse indices, respectively, for the account<br>ﬁle. Suppose that we are looking up records for the Perryridge branch. Using the<br>dense index of Figure 12.2, we follow the pointer directly to the ﬁrst Perryridge<br>record. We process this record, and follow the pointer in that record to locate the<br>next record in search-key (branch-name) order. We continue processing records until<br>we encounter a record for a branch other than Perryridge. If we are using the sparse<br>index (Figure 12.3), we do not ﬁnd an index entry for “Perryridge.” Since the last en-<br>try (in alphabetic order) before “Perryridge” is “Mianus,” we follow that pointer. We<br>then read the account ﬁle in sequential order until we ﬁnd the ﬁrst Perryridge record,<br>and begin processing at that point.<br>As we have seen, it is generally faster to locate a record if we have a dense index<br>rather than a sparse index. However, sparse indices have advantages over dense in-<br>dices in that they require less space and they impose less maintenance overhead for<br>insertions and deletions.<br>There is a trade-off that the system designer must make between access time and<br>space overhead. Although the decision regarding this trade-off depends on the spe-<br>ciﬁc application, a good compromise is to have a sparse index with one index entry<br>per block. The reason this design is a good trade-off is that the dominant cost in pro-<br>A-217     Brighton<br>750<br>A-101     Downtown<br>500<br>A-110     Downtown<br>600<br>A-215     Mianus<br>700<br>A-102     Perryridge<br>400<br>A-201  </span><br><br><span style="background-color: #FFADAD;" title="Chunk 56 | Start: 1120112 | End: 1140112 | Tokens: 3514">   Perryridge<br>900<br>A-218     Perryridge<br>700<br>A-222     Redwood<br>700<br>A-305     Round Hill<br>350<br>Brighton<br>Downtown<br>Mianus<br>Perryridge<br>Redwood<br>Round Hill<br>Figure 12.2<br>Dense index.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>IV. Data Storage and <br>Querying<br>12. Indexing and Hashing<br>450<br>© The McGraw−Hill <br>Companies, 2001<br>12.2<br>Ordered Indices<br>449<br>A-217     Brighton<br>750<br>A-101     Downtown<br>500<br>A-110     Downtown<br>600<br>A-215     Mianus<br>700<br>A-102     Perryridge<br>400<br>A-201     Perryridge<br>900<br>A-218     Perryridge<br>700<br>A-222     Redwood<br>700<br>A-305     Round Hill<br>350<br>Brighton<br>Mianus<br>Redwood<br>Figure 12.3<br>Sparse index.<br>cessing a database request is the time that it takes to bring a block from disk into<br>main memory. Once we have brought in the block, the time to scan the entire block<br>is negligible. Using this sparse index, we locate the block containing the record that<br>we are seeking. Thus, unless the record is on an overﬂow block (see Section 11.7.1),<br>we minimize block accesses while keeping the size of the index (and thus, our space<br>overhead) as small as possible.<br>For the preceding technique to be fully general, we must consider the case where<br>records for one search-key value occupy several blocks. It is easy to modify our<br>scheme to handle this situation.<br>12.2.1.2<br>Multilevel Indices<br>Even if we use a sparse index, the index itself may become too large for efﬁcient<br>processing. It is not unreasonable, in practice, to have a ﬁle with 100,000 records, with<br>10 records stored in each block. If we have one index record per block, the index has<br>10,000 records. Index records are smaller than data records, so let us assume that 100<br>index records ﬁt on a block. Thus, our index occupies 100 blocks. Such large indices<br>are stored as sequential ﬁles on disk.<br>If an index is sufﬁciently small to be kept in main memory, the search time to ﬁnd<br>an entry is low. However, if the index is so large that it must be kept on disk, a search<br>for an entry requires several disk block reads. Binary search can be used on the index<br>ﬁle to locate an entry, but the search still has a large cost. If the index occupies b<br>blocks, binary search requires as many as ⌈log2(b)⌉blocks to be read. (⌈x⌉denotes<br>the least integer that is greater than or equal to x; that is, we round upward.) For our<br>100-block index, binary search requires seven block reads. On a disk system where a<br>block read takes 30 milliseconds, the search will take 210 milliseconds, which is long.<br>Note that, if overﬂow blocks have been used, binary search will not be possible. In<br>that case, a sequential search is typically used, and that requires b block reads, which<br>will take even longer. Thus, the process of searching a large index may be costly.<br>To deal with this problem, we treat the index just as we would treat any other<br>sequential ﬁle, and construct a sparse index on the primary index, as in Figure 12.4.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>IV. Data Storage and <br>Querying<br>12. Indexing and Hashing<br>451<br>© The McGraw−Hill <br>Companies, 2001<br>450<br>Chapter 12<br>Indexing and Hashing<br>To locate a record, we ﬁrst use binary search on the outer index to ﬁnd the record for<br>the largest search-key value less than or equal to the one that we desire. The pointer<br>points to a block of the inner index. We scan this block until we ﬁnd the record that<br>has the largest search-key value less than or equal to the one that we desire. The<br>pointer in this record points to the block of the ﬁle that contains the record for which<br>we are looking.<br>Using the two levels of indexing, we have read only one index block, rather than<br>the seven we read with binary search, if we assume that the outer index is already in<br>main memory. If our ﬁle is extremely large, even the outer index may grow too large<br>to ﬁt in main memory. In such a case, we can create yet another level of index. Indeed,<br>we can repeat this process as many times as necessary. Indices with two or more<br>levels are called multilevel indices. Searching for records with a multilevel index<br>requires signiﬁcantly fewer I/O operations than does searching for records by binary<br>search. Each level of index could correspond to a unit of physical storage. Thus, we<br>may have indices at the track, cylinder, and disk levels.<br>A typical dictionary is an example of a multilevel index in the nondatabase world.<br>The header of each page lists the ﬁrst word alphabetically on that page. Such a book<br>outer index<br>inner index<br>index<br>block 0<br>index<br>block 1<br>data<br>block 0<br>data<br>block 1<br>Figure 12.4<br>Two-level sparse index.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>IV. Data Storage and <br>Querying<br>12. Indexing and Hashing<br>452<br>© The McGraw−Hill <br>Companies, 2001<br>12.2<br>Ordered Indices<br>451<br>index is a multilevel index: The words at the top of each page of the book index form<br>a sparse index on the contents of the dictionary pages.<br>Multilevel indices are closely related to tree structures, such as the binary trees<br>used for in-memory indexing. We shall examine the relationship later, in Section 12.3.<br>12.2.1.3<br>Index Update<br>Regardless of what form of index is used, every index must be updated whenever a<br>record is either inserted into or deleted from the ﬁle. We ﬁrst describe algorithms for<br>updating single-level indices.<br>• Insertion. First, the system performs a lookup using the search-key value that<br>appears in the record to be inserted. Again, the actions the system takes next<br>depend on whether the index is dense or sparse:<br>  Dense indices:<br>1. If the search-key value does not appear in the index, the system inserts<br>an index record with the search-key value in the index at the appro-<br>priate position.<br>2. Otherwise the following actions are taken:<br>a. If the index record stores pointers to all records with the same<br>search-key value, the system adds a pointer to the new record to<br>the index record.<br>b. Otherwise, the index record stores a pointer to only the ﬁrst record<br>with the search-key value. The system then places the record being<br>inserted after the other records with the same search-key values.<br>  Sparse indices: We assume that the index stores an entry for each block.<br>If the system creates a new block, it inserts the ﬁrst search-key value (in<br>search-key order) appearing in the new block into the index. On the other<br>hand, if the new record has the least search-key value in its block, the<br>system updates the index entry pointing to the block; if not, the system<br>makes no change to the index.<br>• Deletion. To delete a record, the system ﬁrst looks up the record to be deleted.<br>The actions the system takes next depend on whether the index is dense or<br>sparse:<br>  Dense indices:<br>1. If the deleted record was the only record with its particular search-key<br>value, then the system deletes the corresponding index record from<br>the index.<br>2. Otherwise the following actions are taken:<br>a. If the index record stores pointers to all records with the same<br>search-key value, the system deletes the pointer to the deleted re-<br>cord from the index record.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>IV. Data Storage and <br>Querying<br>12. Indexing and Hashing<br>453<br>© The McGraw−Hill <br>Companies, 2001<br>452<br>Chapter 12<br>Indexing and Hashing<br>b. Otherwise, the index record stores a pointer to only the ﬁrst record<br>with the search-key value. In this case, if the deleted record was<br>the ﬁrst record with the search-key value, the system updates the<br>index record to point to the next record.<br>  Sparse indices:<br>1. If the index does not contain an index record with the search-key value<br>of the deleted record, nothing needs to be done to the index.<br>2. Otherwise the system takes the following actions:<br>a. If the deleted record was the only record with its search key, the<br>system replaces the corresponding index record with an index rec-<br>ord for the next search-key value (in search-key order). If the next<br>search-key value already has an index entry, the entry is deleted<br>instead of being replaced.<br>b. Otherwise, if the index record for the search-key value points to the<br>record being deleted, the system updates the index record to point<br>to the next record with the same search-key value.<br>Insertion and deletion algorithms for multilevel indices are a simple extension of<br>the scheme just described. On deletion or insertion, the system updates the lowest-<br>level index as described. As far as the second level is concerned, the lowest-level in-<br>dex is merely a ﬁle containing records—thus, if there is any change in the lowest-level<br>index, the system updates the second-level index as described. The same technique<br>applies to further levels of the index, if there are any.<br>12.2.2<br>Secondary Indices<br>Secondary indices must be dense, with an index entry for every search-key value, and<br>a pointer to every record in the ﬁle. A primary index may be sparse, storing only some<br>of the search-key values, since it is always possible to ﬁnd records with intermediate<br>search-key values by a sequential access to a part of the ﬁle, as described earlier. If a<br>secondary index stores only some of the search-key values, records with intermediate<br>search-key values may be anywhere in the ﬁle and, in general, we cannot ﬁnd them<br>without searching the entire ﬁle.<br>A secondary index on a candidate key looks just like a dense primary index, except<br>that the records pointed to by successive values in the index are not stored sequen-<br>tially. In general, however, secondary indices may have a different structure from<br>primary indices. If the search key of a primary index is not a candidate key, it sufﬁces<br>if the index points to the ﬁrst record with a particular value for the search key, since<br>the other records can be fetched by a sequential scan of the ﬁle.<br>In contrast, if the search key of a secondary index is not a candidate key, it is<br>not enough to point to just the ﬁrst record with each search-key value. The remain-<br>ing records with the same search-key value could be anywhere in the ﬁle, since the<br>records are ordered by the search key of the primary index, rather than by the search<br>key of the secondary index. Therefore, a secondary index must contain pointers to all<br>the records.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>IV. Data Storage and <br>Querying<br>12. Indexing and Hashing<br>454<br>© The McGraw−Hill <br>Companies, 2001<br>12.3<br>B+-Tree Index Files<br>453<br>350<br>400<br>500<br>600<br>700<br>750<br>900<br>A-217     Brighton<br>750<br>A-101     Downtown<br>500<br>A-110     Downtown<br>600<br>A-215     Mianus<br>700<br>A-102     Perryridge<br>400<br>A-201     Perryridge<br>900<br>A-218     Perryridge<br>700<br>A-222     Redwood<br>700<br>A-305     Round Hill<br>350<br>Figure 12.5<br>Secondary index on account ﬁle, on noncandidate key balance.<br>We can use an extra level of indirection to implement secondary indices on search<br>keys that are not candidate keys. The pointers in such a secondary index do not point<br>directly to the ﬁle. Instead, each points to a bucket that contains pointers to the ﬁle.<br>Figure 12.5 shows the structure of a secondary index that uses an extra level of indi-<br>rection on the account ﬁle, on the search key balance.<br>A sequential scan in primary index order is efﬁcient because records in the ﬁle are<br>stored physically in the same order as the index order. However, we cannot (except in<br>rare special cases) store a ﬁle physically ordered both by the search key of the primary<br>index, and the search key of a secondary index. Because secondary-key order and<br>physical-key order differ, if we attempt to scan the ﬁle sequentially in secondary-key<br>order, the reading of each record is likely to require the reading of a new block from<br>disk, which is very slow.<br>The procedure described earlier for deletion and insertion can also be applied to<br>secondary indices; the actions taken are those described for dense indices storing a<br>pointer to every record in the ﬁle. If a ﬁle has multiple indices, whenever the ﬁle is<br>modiﬁed, every index must be updated.<br>Secondary indices improve the performance of queries that use keys other than<br>the search key of the primary index. However, they impose a signiﬁcant overhead<br>on modiﬁcation of the database. The designer of a database decides which secondary<br>indices are desirable on the basis of an estimate of the relative frequency of queries<br>and modiﬁcations.<br>12.3<br>B+-Tree Index Files<br>The main disadvantage of the index-sequential ﬁle organization is that performance<br>degrades as the ﬁle grows, both for index lookups and for sequential scans through<br>the data. Although this degradation can be remedied by reorganization of the ﬁle,<br>frequent reorganizations are undesirable.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>IV. Data Storage and <br>Querying<br>12. Indexing and Hashing<br>455<br>© The McGraw−Hill <br>Companies, 2001<br>454<br>Chapter 12<br>Indexing and Hashing<br>P1<br>P2<br>Pn – 1<br>Pn <br>Kn – 1<br>. . .<br>K1<br>Figure 12.6<br>Typical node of a B+-tree.<br>The B+-tree index structure is the most widely used of several index structures<br>that maintain their efﬁciency despite insertion and deletion of data. A B+-tree index<br>takes the form of a balanced tree in which every path from the root of the tree to a<br>leaf of the tree is of the same length. Each nonleaf node in the tree has between ⌈n/2⌉<br>and n children, where n is ﬁxed for a particular tree.<br>We shall see that the B+-tree structure imposes performance overhead on inser-<br>tion and deletion, and adds space overhead. The overhead is acceptable even for fre-<br>quently modiﬁed ﬁles, since the cost of ﬁle reorganization is avoided. Furthermore,<br>since nodes may be as much as half empty (if they have the minimum number of<br>children), there is some wasted space. This space overhead, too, is acceptable given<br>the performance beneﬁts of the B+-tree structure.<br>12.3.1<br>Structure of a B+-Tree<br>A B+-tree index is a multilevel index, but it has a structure that differs from that of the<br>multilevel index-sequential ﬁle. Figure 12.6 shows a typical node of a B+-tree. It con-<br>tains up to n −1 search-key values K1, K2, . . . , Kn −1, and n pointers P1, P2, . . . , Pn.<br>The search-key values within a node are kept in sorted order; thus, if i &lt; j, then<br>Ki &lt; Kj.<br>We consider ﬁrst the structure of the leaf nodes. For i = 1, 2, . . . , n −1, pointer<br>Pi points to either a ﬁle record with search-key value Ki or to a bucket of pointers,<br>each of which points to a ﬁle record with search-key value Ki. The bucket structure<br>is used only if the search key does not form a primary key, and if the ﬁle is not sorted<br>in the search-key value order. Pointer Pn has a special purpose that we shall discuss<br>shortly.<br>Figure 12.7 shows one leaf node of a B+-tree for the account ﬁle, in which we have<br>chosen n to be 3, and the search key is branch-name. Note that, since the account ﬁle<br>is ordered by branch-name, the pointers in the leaf node point directly to the ﬁle.<br>Now that we have seen the structure of a leaf node, let us consider how search-key<br>values are assigned to particular nodes. Each leaf can hold up to n −1 values. We<br>allow leaf nodes to contain as few as ⌈(n −1)/2⌉values. The ranges of values in each<br>leaf do not overlap. Thus, if Li and Lj are leaf nodes and i &lt; j, then every search-<br>key value in Li is less than every search-key value in Lj. If the B+-tree index is to be<br>a dense index, every search-key value must appear in some leaf node.<br>Now we can explain the use of the pointer Pn. Since there is a linear order on the<br>leaves based on the search-key values that they contain, we use Pn to chain together<br>the leaf nodes in search-key order. This ordering allows for efﬁcient sequential pro-<br>cessing of the ﬁle.<br>The nonleaf nodes of the B+-tree form a multilevel (sparse) index on the leaf nodes.<br>The structure of nonleaf nodes is the same as that for leaf nodes, except that all point-<br>ers are pointers to tree nodes. A nonleaf node may hold up to n pointers, and must<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>IV. Data Storage and <br>Querying<br>12. Indexing and Hashing<br>456<br>© The McGraw−Hill <br>Companies, 2001<br>12.3<br>B+-Tree Index Files<br>455<br> Brighton<br>Downtown<br>A-212   Brighton<br>750<br>A-101   Downtown<br>500<br>A-110   Downtown<br>600<br>account file<br>leaf node<br>. . .<br>Figure 12.7<br>A leaf node for account B+-tree index (n = 3).<br>hold at least ⌈n/2⌉pointers. The number of pointers in a node is called the fanout of<br>the node.<br>Let us consider a node containing m pointers. For i = 2, 3, . . . , m −1, pointer Pi<br>points to the subtree that contains search-key values less than Ki and greater than or<br>equal to Ki −1. Pointer Pm points to the part of the subtree that contains those key<br>values greater than or equal to Km −1, and pointer P1 points to the part of the subtree<br>that contains those search-key values less than K1.<br>Unlike other nonleaf nodes, the root node can hold fewer than ⌈n/2⌉pointers;<br>however, it must hold at least two pointers, unless the tree consists of only one node.<br>It is always possible to construct a B+-tree, for any n, that satisﬁes the preceding<br>requirements. Figure 12.8 shows a complete B+-tree for the account ﬁle (n = 3). For<br>simplicity, we have omitted both the pointers to the ﬁle itself and the null pointers.<br>As an example of a B+-tree for which the root must have less than ⌈n/2⌉values,<br>Figure 12.9 shows a B+-tree for the account ﬁle with n = 5.<br>These examples of B+-trees are all balanced. That is, the length of every path from<br>the root to a leaf node is the same. This property is a requirement for a B+-tree. In-<br>deed, the “B” in B+-tree stands for “balanced.” It is the balance property of B+-trees<br>that ensures good performance for lookup, insertion, and deletion.<br>Perryridge<br>Mianus<br>Redwood<br>Redwood     Round Hill<br>Perryridge<br>Mianus<br>Brighton    Downtown<br>Figure 12.8<br>B+-tree for account ﬁle (n = 3).<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>IV. Data Storage and <br>Querying<br>12. Indexing and Hashing<br>457<br>© The McGraw−Hill <br>Companies, 2001<br>456<br>Chapter 12<br>Indexing and Hashing<br> Perryridge<br>Perryridge    Redwood    Round Hill<br>Brighton      Downtown        Mianus<br>Figure 12.9<br>B+-tree for account ﬁle with n = 5.<br>12.3.2<br>Queries on B+-Trees<br>Let us consider how we process queries on a B+-tree. Suppose that we wish to ﬁnd<br>all records with a search-key value of V. Figure 12.10 presents pseudocode for doing<br>so. Intuitively, the procedure works as follows. First, we examine the root node, look-<br>ing for the smallest search-key value greater than V. Suppose that we ﬁnd that this<br>search-key value is Ki. We then follow pointer Pi to another node. If we ﬁnd no such<br>value, then k ≥Km−1, where m is the number of pointers in the node. In this case<br>we follow Pm to another node. In the node we reached above, again we look for the<br>smallest search-key value greater than V, and once again follow the corresponding<br>pointer as above. Eventually, we reach a leaf node. At the leaf node, if we ﬁnd search-<br>key value Ki equals V , then pointer Pi directs us to the desired record or bucket. If<br>the value V is not found in the leaf node, no record with key value V exists.<br>Thus, in processing a query, we traverse a path in the tree from the root to some<br>leaf node. If there are K search-key values in the ﬁle, the path is no longer than<br>⌈log⌈n/2⌉(K)⌉.<br>In practice, only a few nodes need to be accessed, Typically, a node is made to<br>be the same size as a disk block, which is typically 4 kilobytes. With a search-key<br>size of 12 bytes, and a disk-pointer size of 8 bytes, n is around 200. Even with a<br>more conservative estimate of 32 bytes for the search-key size, n is around 100. With<br>n = 100, if we have 1 million search-key values in the ﬁle, a lookup requires only<br>procedure ﬁnd(value V )<br>set C = root node<br>while C is not a leaf node begin<br>Let Ki = smallest search-key value, if any, greater than V<br>if there is no such value then begin<br>Let m = the number of pointers in the node<br>set C = node pointed to by Pm<br>end<br>else set C = the node pointed to by Pi<br>end<br>if there is a key value Ki in C such that Ki = V<br>then pointer Pi directs us to the desired record or bucket<br>else no record with key value k exists<br>end procedure</span><br><br><span style="background-color: #FFD6A5;" title="Chunk 57 | Start: 1140114 | End: 1160114 | Tokens: 3530"><br>Figure 12.10<br>Querying a B+-tree.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>IV. Data Storage and <br>Querying<br>12. Indexing and Hashing<br>458<br>© The McGraw−Hill <br>Companies, 2001<br>12.3<br>B+-Tree Index Files<br>457<br>⌈log50(1,000,000)⌉= 4 nodes to be accessed. Thus, at most four blocks need to be<br>read from disk for the lookup. The root node of the tree is usually heavily accessed<br>and is likely to be in the buffer, so typically only three or fewer blocks need to be read<br>from disk.<br>An important difference between B+-tree structures and in-memory tree struc-<br>tures, such as binary trees, is the size of a node, and as a result, the height of the<br>tree. In a binary tree, each node is small, and has at most two pointers. In a B+-tree,<br>each node is large—typically a disk block—and a node can have a large number of<br>pointers. Thus, B+-trees tend to be fat and short, unlike thin and tall binary trees. In<br>a balanced binary tree, the path for a lookup can be of length ⌈log2(K)⌉, where K is<br>the number of search-key values. With K = 1,000,000 as in the previous example, a<br>balanced binary tree requires around 20 node accesses. If each node were on a differ-<br>ent disk block, 20 block reads would be required to process a lookup, in contrast to<br>the four block reads for the B+-tree.<br>12.3.3<br>Updates on B+-Trees<br>Insertion and deletion are more complicated than lookup, since it may be necessary to<br>split a node that becomes too large as the result of an insertion, or to coalesce nodes<br>(that is, combine nodes) if a node becomes too small (fewer than ⌈n/2⌉pointers).<br>Furthermore, when a node is split or a pair of nodes is combined, we must ensure<br>that balance is preserved. To introduce the idea behind insertion and deletion in a<br>B+-tree, we shall assume temporarily that nodes never become too large or too small.<br>Under this assumption, insertion and deletion are performed as deﬁned next.<br>• Insertion. Using the same technique as for lookup, we ﬁnd the leaf node in<br>which the search-key value would appear. If the search-key value already ap-<br>pears in the leaf node, we add the new record to the ﬁle and, if necessary, add<br>to the bucket a pointer to the record. If the search-key value does not appear,<br>we insert the value in the leaf node, and position it such that the search keys<br>are still in order. We then insert the new record in the ﬁle and, if necessary,<br>create a new bucket with the appropriate pointer.<br>• Deletion. Using the same technique as for lookup, we ﬁnd the record to be<br>deleted, and remove it from the ﬁle. We remove the search-key value from the<br>leaf node if there is no bucket associated with that search-key value or if the<br>bucket becomes empty as a result of the deletion.<br>We now consider an example in which a node must be split. Assume that we wish<br>to insert a record with a branch-name value of “Clearview” into the B+-tree of Fig-<br>ure 12.8. Using the algorithm for lookup, we ﬁnd that “Clearview” should appear<br>in the node containing “Brighton” and “Downtown.” There is no room to insert the<br>search-key value “Clearview.” Therefore, the node is split into two nodes. Figure 12.11<br>shows the two leaf nodes that result from inserting “Clearview” and splitting the<br>node containing “Brighton” and “Downtown.” In general, we take the n search-key<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>IV. Data Storage and <br>Querying<br>12. Indexing and Hashing<br>459<br>© The McGraw−Hill <br>Companies, 2001<br>458<br>Chapter 12<br>Indexing and Hashing<br>Brighton     Clearview<br>Downtown<br>Figure 12.11<br>Split of leaf node on insertion of “Clearview.”<br>values (the n −1 values in the leaf node plus the value being inserted), and put the<br>ﬁrst ⌈n/2⌉in the existing node and the remaining values in a new node.<br>Having split a leaf node, we must insert the new leaf node into the B+-tree struc-<br>ture. In our example, the new node has “Downtown” as its smallest search-key value.<br>We need to insert this search-key value into the parent of the leaf node that was split.<br>The B+-tree of Figure 12.12 shows the result of the insertion. The search-key value<br>“Downtown” was inserted into the parent. It was possible to perform this insertion<br>because there was room for an added search-key value. If there were no room, the<br>parent would have had to be split. In the worst case, all nodes along the path to the<br>root must be split. If the root itself is split, the entire tree becomes deeper.<br>The general technique for insertion into a B+-tree is to determine the leaf node l<br>into which insertion must occur. If a split results, insert the new node into the parent<br>of node l. If this insertion causes a split, proceed recursively up the tree until either<br>an insertion does not cause a split or a new root is created.<br>Figure 12.13 outlines the insertion algorithm in pseudocode. In the pseudocode,<br>L.Ki and L.Pi denote the ith value and the ith pointer in node L, respectively. The<br>pseudocode also makes use of the function parent(L) to ﬁnd the parent of a node L.<br>We can compute a list of nodes in the path from the root to the leaf while initially<br>ﬁnding the leaf node, and can use it later to ﬁnd the parent of any node in the path<br>efﬁciently. The pseudocode refers to inserting an entry (V, P) into a node. In the case<br>of leaf nodes, the pointer to an entry actually precedes the key value, so the leaf node<br>actually stores P before V . For internal nodes, P is stored just after V .<br>We now consider deletions that cause tree nodes to contain too few pointers. First,<br>let us delete “Downtown” from the B+-tree of Figure 12.12. We locate the entry for<br>“Downtown” by using our lookup algorithm. When we delete the entry for “Down-<br>town” from its leaf node, the leaf becomes empty. Since, in our example, n = 3 and<br>0 &lt; ⌈(n−1)/2⌉, this node must be eliminated from the B+-tree. To delete a leaf node,<br> Perryridge<br> Downtown      Mianus<br>Redwood<br>Redwood    Round Hill<br>Mianus<br>Downtown<br>Brighton       Clearview<br>Perryridge<br>Figure 12.12<br>Insertion of “Clearview” into the B+-tree of Figure 12.8.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>IV. Data Storage and <br>Querying<br>12. Indexing and Hashing<br>460<br>© The McGraw−Hill <br>Companies, 2001<br>12.3<br>B+-Tree Index Files<br>459<br>procedure insert(value V , pointer P)<br>ﬁnd the leaf node L that should contain value V<br>insert entry(L, V , P)<br>end procedure<br>procedure insert entry(node L, value V , pointer P)<br>if (L has space for (V, P))<br>then insert (V, P) in L<br>else begin /* Split L */<br>Create node L′<br>Let V ′ be the value in L.K1, . . . , L.Kn−1, V such that exactly<br>⌈n/2⌉of the values L.K1, . . . , L.Kn−1, V are less than V ′<br>Let m be the lowest value such that L.Km ≥V ′<br>/* Note: V ′ must be either L.Km or V */<br>if (L is a leaf) then begin<br>move L.Pm, L.Km, . . . , L.Pn−1, L.Kn−1 to L′<br>if (V &lt; V ′) then insert (P, V ) in L<br>else insert (P, V ) in L′<br>end<br>else begin<br>if (V = V ′) /* V is smallest value to go to L′ */<br>then add P, L.Km, . . . , L.Pn−1, L.Kn−1, L.Pn to L′<br>else add L.Pm, . . . , L.Pn−1, L.Kn−1, L.Pn to L′<br>delete L.Km, . . . , L.Pn−1, L.Kn−1, L.Pn from L<br>if (V &lt; V ′) then insert (V, P) in L<br>else if (V &gt; V ′) then insert (V, P) in L′<br>/* Case of V = V ′ handled already */<br>end<br>if (L is not the root of the tree)<br>then insert entry(parent(L), V ′, L′);<br>else begin<br>create a new node R with child nodes L and L′ and<br>the single value V ′<br>make R the root of the tree<br>end<br>if (L) is a leaf node then begin /* Fix next child pointers */<br>set L′.Pn = L.Pn;<br>set L.Pn = L′<br>end<br>end<br>end procedure<br>Figure 12.13<br>Insertion of entry in a B+-tree.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>IV. Data Storage and <br>Querying<br>12. Indexing and Hashing<br>461<br>© The McGraw−Hill <br>Companies, 2001<br>460<br>Chapter 12<br>Indexing and Hashing<br>Perryridge<br>Mianus<br>Redwood<br>Redwood      Round Hill<br>Perryridge<br>Mianus<br>Brighton      Clearview<br>Figure 12.14<br>Deletion of “Downtown” from the B+-tree of Figure 12.12.<br>we must delete the pointer to it from its parent. In our example, this deletion leaves<br>the parent node, which formerly contained three pointers, with only two pointers.<br>Since 2<br>≥⌈n/2⌉, the node is still sufﬁciently large, and the deletion operation is<br>complete. The resulting B+-tree appears in Figure 12.14.<br>When we make a deletion from a parent of a leaf node, the parent node itself may<br>become too small. That is exactly what happens if we delete “Perryridge” from the<br>B+-tree of Figure 12.14. Deletion of the Perryridge entry causes a leaf node to become<br>empty. When we delete the pointer to this node in the latter’s parent, the parent is left<br>with only one pointer. Since n = 3, ⌈n/2⌉= 2, and thus only one pointer is too few.<br>However, since the parent node contains useful information, we cannot simply delete<br>it. Instead, we look at the sibling node (the nonleaf node containing the one search<br>key, Mianus). This sibling node has room to accommodate the information contained<br>in our now-too-small node, so we coalesce these nodes, such that the sibling node<br>now contains the keys “Mianus” and “Redwood.” The other node (the node contain-<br>ing only the search key “Redwood”) now contains redundant information and can be<br>deleted from its parent (which happens to be the root in our example). Figure 12.15<br>shows the result. Notice that the root has only one child pointer after the deletion, so<br>it is deleted and its sole child becomes the root. So the depth of the B+-tree has been<br>decreased by 1.<br>It is not always possible to coalesce nodes. As an illustration, delete “Perryridge”<br>from the B+-tree of Figure 12.12. In this example, the “Downtown” entry is still part<br>of the tree. Once again, the leaf node containing “Perryridge” becomes empty. The<br>parent of the leaf node becomes too small (only one pointer). However, in this ex-<br>ample, the sibling node already contains the maximum number of pointers: three.<br>Thus, it cannot accommodate an additional pointer. The solution in this case is to re-<br>distribute the pointers such that each sibling has two pointers. The result appears in<br>Mianus       Redwood<br>Redwood     Round Hill<br>Mianus<br>Brighton      Clearview<br>Figure 12.15<br>Deletion of “Perryridge” from the B+-tree of Figure 12.14.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>IV. Data Storage and <br>Querying<br>12. Indexing and Hashing<br>462<br>© The McGraw−Hill <br>Companies, 2001<br>12.3<br>B+-Tree Index Files<br>461<br>Mianus<br>Downtown  <br>Redwood<br>Redwood      Round Hill<br>Mianus<br>Downtown<br>Brighton      Clearview<br>Figure 12.16<br>Deletion of “Perryridge” from the B+-tree of Figure 12.12.<br>Figure 12.16. Note that the redistribution of values necessitates a change of a search-<br>key value in the parent of the two siblings.<br>In general, to delete a value in a B+-tree, we perform a lookup on the value and<br>delete it. If the node is too small, we delete it from its parent. This deletion results<br>in recursive application of the deletion algorithm until the root is reached, a parent<br>remains adequately full after deletion, or redistribution is applied.<br>Figure 12.17 outlines the pseudocode for deletion from a B+-tree. The procedure<br>swap variables(L, L′) merely swaps the values of the (pointer) variables L and L′;<br>this swap has no effect on the tree itself. The pseudocode uses the condition “too few<br>pointers/values.” For nonleaf nodes, this criterion means less than ⌈n/2⌉pointers;<br>for leaf nodes, it means less than ⌈(n −1)/2⌉values. The pseudocode redistributes<br>entries by borrowing a single entry from an adjacent node. We can also redistribute<br>entries by repartitioning entries equally between the two nodes. The pseudocode<br>refers to deleting an entry (V, P) from a node. In the case of leaf nodes, the pointer to<br>an entry actually precedes the key value, so the pointer P precedes the key value V .<br>For internal nodes, P follows the key value V .<br>It is worth noting that, as a result of deletion, a key value that is present in an<br>internal node of the B+-tree may not be present at any leaf of the tree.<br>Although insertion and deletion operations on B+-trees are complicated, they re-<br>quire relatively few I/O operations, which is an important beneﬁt since I/O opera-<br>tions are expensive. It can be shown that the number of I/O operations needed for a<br>worst-case insertion or deletion is proportional to log⌈n/2⌉(K), where n is the max-<br>imum number of pointers in a node, and K is the number of search-key values. In<br>other words, the cost of insertion and deletion operations is proportional to the height<br>of the B+-tree, and is therefore low. It is the speed of operation on B+-trees that makes<br>them a frequently used index structure in database implementations.<br>12.3.4<br>B+-Tree File Organization<br>As mentioned in Section 12.3, the main drawback of index-sequential ﬁle organiza-<br>tion is the degradation of performance as the ﬁle grows: With growth, an increasing<br>percentage of index records and actual records become out of order, and are stored in<br>overﬂow blocks. We solve the degradation of index lookups by using B+-tree indices<br>on the ﬁle. We solve the degradation problem for storing the actual records by using<br>the leaf level of the B+-tree to organize the blocks containing the actual records. We<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>IV. Data Storage and <br>Querying<br>12. Indexing and Hashing<br>463<br>© The McGraw−Hill <br>Companies, 2001<br>462<br>Chapter 12<br>Indexing and Hashing<br>procedure delete(value V , pointer P)<br>ﬁnd the leaf node L that contains (V, P)<br>delete entry(L, V, P)<br>end procedure<br>procedure delete entry(node L, value V , pointer P)<br>delete (V, P) from L<br>if (L is the root and L has only one remaining child)<br>then make the child of L the new root of the tree and delete L<br>else if (L has too few values/pointers) then begin<br>Let L′ be the previous or next child of parent(L)<br>Let V ′ be the value between pointers L and L′ in parent(L)<br>if (entries in L and L′ can ﬁt in a single node)<br>then begin /* Coalesce nodes */<br>if (L is a predecessor of L′) then swap variables(L, L′)<br>if (L is not a leaf)<br>then append V ′ and all pointers and values in L to L′<br>else append all (Ki, Pi) pairs in L to L′; set L′.Pn = L.Pn<br>delete entry(parent(L), V ′, L); delete node L<br>end<br>else begin /* Redistribution: borrow an entry from L′ */<br>if (L′ is a predecessor of L) then begin<br>if (L is a non-leaf node) then begin<br>let m be such that L′.Pm is the last pointer in L′<br>remove (L′.Km−1, L′.Pm) from L′<br>insert (L′.Pm, V ′) as the ﬁrst pointer and value in L,<br>by shifting other pointers and values right<br>replace V ′ in parent(L) by L′.Km−1<br>end<br>else begin<br>let m be such that (L′.Pm, L′.Km) is the last pointer/value<br>pair in L′<br>remove (L′.Pm, L′.Km) from L′<br>insert (L′.Pm, L′.Km) as the ﬁrst pointer and value in L,<br>by shifting other pointers and values right<br>replace V ′ in parent(L) by L′.Km<br>end<br>end<br>else . . . symmetric to the then case . . .<br>end<br>end<br>end procedure<br>Figure 12.17<br>Deletion of entry from a B+-tree.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>IV. Data Storage and <br>Querying<br>12. Indexing and Hashing<br>464<br>© The McGraw−Hill <br>Companies, 2001<br>12.3<br>B+-Tree Index Files<br>463<br>use the B+-tree structure not only as an index, but also as an organizer for records in<br>a ﬁle. In a B+-tree ﬁle organization, the leaf nodes of the tree store records, instead of<br>storing pointers to records. Figure 12.18 shows an example of a B+-tree ﬁle organiza-<br>tion. Since records are usually larger than pointers, the maximum number of records<br>that can be stored in a leaf node is less than the number of pointers in a nonleaf node.<br>However, the leaf nodes are still required to be at least half full.<br>Insertion and deletion of records from a B+-tree ﬁle organization are handled in<br>the same way as insertion and deletion of entries in a B+-tree index. When a record<br>with a given key value v is inserted, the system locates the block that should contain<br>the record by searching the B+-tree for the largest key in the tree that is ≤v. If the<br>block located has enough free space for the record, the system stores the record in the<br>block. Otherwise, as in B+-tree insertion, the system splits the block in two, and redis-<br>tributes the records in it (in the B+-tree–key order) to create space for the new record.<br>The split propagates up the B+-tree in the normal fashion. When we delete a record,<br>the system ﬁrst removes it from the block containing it. If a block B becomes less<br>than half full as a result, the records in B are redistributed with the records in an ad-<br>jacent block B′. Assuming ﬁxed-sized records, each block will hold at least one-half<br>as many records as the maximum that it can hold. The system updates the nonleaf<br>nodes of the B+-tree in the usual fashion.<br>When we use a B+-tree for ﬁle organization, space utilization is particularly im-<br>portant, since the space occupied by the records is likely to be much more than the<br>space occupied by keys and pointers. We can improve the utilization of space in a B+-<br>tree by involving more sibling nodes in redistribution during splits and merges. The<br>technique is applicable to both leaf nodes and internal nodes, and works as follows.<br>During insertion, if a node is full the system attempts to redistribute some of its<br>entries to one of the adjacent nodes, to make space for a new entry. If this attempt fails<br>because the adjacent nodes are themselves full, the system splits the node, and splits<br>the entries evenly among one of the adjacent nodes and the two nodes that it obtained<br>by splitting the original node. Since the three nodes together contain one more record<br>than can ﬁt in two nodes, each node will be about two-thirds full. More precisely, each<br>node will have at least ⌊2n/3⌋entries, where n is the maximum number of entries that<br>the node can hold. (⌊x⌋denotes the greatest integer that is less than or equal to x; that<br>is, we drop the fractional part, if any.)<br>C       F<br>K      M<br>I<br>(A,4)    (B,8)<br>(C,1)    (D,9)   (E,4)<br>(F,7)    (G,3)    (H,3)<br>(I,4)      (J,8)<br>(K,1)    (L,6)<br>(M,4)   (N,8)    (P,6)<br>Figure 12.18<br>B+-tree ﬁle organization.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>IV. Data Storage and <br>Querying<br>12. Indexing and Hashing<br>465<br>© The McGraw−Hill <br>Companies, 2001<br>464<br>Chapter 12<br>Indexing and Hashing<br>During deletion of a record, if the occupancy of a node falls below ⌊2n/3⌋, the<br>system attempts to borrow an entry from one of the sibling nodes. If both sibling<br>nodes have ⌊2n/3⌋records, instead of borrowing an entry, the system redistributes<br>the entries in the node and in the two siblings evenly between two of the nodes, and<br>deletes the third node. We can use this approach because the total number of entries<br>is 3⌊2n/3⌋−1, which is less than 2n. With three adjacent nodes used for redistribution,<br>each node can be guaranteed to have ⌊3n/4⌋entries. In general, if m nodes (m −1<br>siblings) are involved in redistribution, each node can be guaranteed to contain at<br>least ⌊(m −1)n/m⌋entries. However, the cost of update becomes higher as more<br>sibling nodes are involved in the redistribution.<br>12.4<br>B-Tree Index Files<br>B-tree indices are similar to B+-tree indices. The primary distinction between the two<br>approaches is that a B-tree eliminates the redundant storage of search-key values.<br>In the B+-tree of Figure 12.12, the search keys “Downtown,” “Mianus,” “Redwood,”<br>and “Perryridge” appear twice. Every search-key value appears in some leaf node;<br>several are repeated in nonleaf nodes.<br>A B-tree allows search-key values to appear only once. Figure 12.19 shows a B-tree<br>that represents the same search keys as the B+-tree of Figure 12.12. Since search keys<br>are not repeated in the B-tree, we may be able to store the index in fewer tree nodes<br>than in the corresponding B+-tree index. However, since search keys that appear in<br>nonleaf nodes appear nowhere else in the B-tree, we are forced to include an addi-<br>tional pointer ﬁeld for each search key in a nonleaf node. These additional pointers<br>point to either ﬁle records or buckets for the associated search key.<br>A generalized B-tree leaf node appears in Figure 12.20a; a nonleaf node appears<br>in Figure 12.20b. L</span><br><br><span style="background-color: #FDFFB6;" title="Chunk 58 | Start: 1160116 | End: 1180116 | Tokens: 3409">eaf nodes are the same as in B+-trees. In nonleaf nodes, the point-<br>ers Pi are the tree pointers that we used also for B+-trees, while the pointers Bi are<br>bucket or ﬁle-record pointers. In the generalized B-tree in the ﬁgure, there are n −1<br>keys in the leaf node, but there are m −1 keys in the nonleaf node. This discrepancy<br>occurs because nonleaf nodes must include pointers Bi, thus reducing the number of<br>Downtown        Redwood<br>Round Hill<br>Mianus      Perryridge<br>Brighton       Clearview<br>Downtown<br>bucket<br>Redwood<br>bucket<br>Brighton<br>bucket<br>Clearview<br>bucket<br>Mianus<br>bucket<br>Perryridge<br>bucket<br>Round Hill<br>bucket<br>Figure 12.19<br>B-tree equivalent of B+-tree in Figure 12.12.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>IV. Data Storage and <br>Querying<br>12. Indexing and Hashing<br>466<br>© The McGraw−Hill <br>Companies, 2001<br>12.5<br>Static Hashing<br>465<br>P1         K1          P2           .  .  .            Pn−1           Kn−1          Pn<br>(a)<br>P1         B1          K1          P2           B2            K2           .  .  .          Pm−1           Bm−1           Km−1           Pm<br>(b)<br>Figure 12.20<br>Typical nodes of a B-tree. (a) Leaf node. (b) Nonleaf node.<br>search keys that can be held in these nodes. Clearly, m &lt; n, but the exact relationship<br>between m and n depends on the relative size of search keys and pointers.<br>The number of nodes accessed in a lookup in a B-tree depends on where the search<br>key is located. A lookup on a B+-tree requires traversal of a path from the root of the<br>tree to some leaf node. In contrast, it is sometimes possible to ﬁnd the desired value<br>in a B-tree before reaching a leaf node. However, roughly n times as many keys are<br>stored in the leaf level of a B-tree as in the nonleaf levels, and, since n is typically<br>large, the beneﬁt of ﬁnding certain values early is relatively small. Moreover, the fact<br>that fewer search keys appear in a nonleaf B-tree node, compared to B+-trees, implies<br>that a B-tree has a smaller fanout and therefore may have depth greater than that of<br>the corresponding B+-tree. Thus, lookup in a B-tree is faster for some search keys<br>but slower for others, although, in general, lookup time is still proportional to the<br>logarithm of the number of search keys.<br>Deletion in a B-tree is more complicated. In a B+-tree, the deleted entry always<br>appears in a leaf. In a B-tree, the deleted entry may appear in a nonleaf node. The<br>proper value must be selected as a replacement from the subtree of the node contain-<br>ing the deleted entry. Speciﬁcally, if search key Ki is deleted, the smallest search key<br>appearing in the subtree of pointer Pi + 1 must be moved to the ﬁeld formerly occu-<br>pied by Ki. Further actions need to be taken if the leaf node now has too few entries.<br>In contrast, insertion in a B-tree is only slightly more complicated than is insertion in<br>a B+-tree.<br>The space advantages of B-trees are marginal for large indices, and usually do not<br>outweigh the disadvantages that we have noted. Thus, many database system imple-<br>menters prefer the structural simplicity of a B+-tree. The exercises explore details of<br>the insertion and deletion algorithms for B-trees.<br>12.5<br>Static Hashing<br>One disadvantage of sequential ﬁle organization is that we must access an index<br>structure to locate data, or must use binary search, and that results in more I/O op-<br>erations. File organizations based on the technique of hashing allow us to avoid ac-<br>cessing an index structure. Hashing also provides a way of constructing indices. We<br>study ﬁle organizations and indices based on hashing in the following sections.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>IV. Data Storage and <br>Querying<br>12. Indexing and Hashing<br>467<br>© The McGraw−Hill <br>Companies, 2001<br>466<br>Chapter 12<br>Indexing and Hashing<br>12.5.1<br>Hash File Organization<br>In a hash ﬁle organization, we obtain the address of the disk block containing a<br>desired record directly by computing a function on the search-key value of the record.<br>In our description of hashing, we shall use the term bucket to denote a unit of storage<br>that can store one or more records. A bucket is typically a disk block, but could be<br>chosen to be smaller or larger than a disk block.<br>Formally, let K denote the set of all search-key values, and let B denote the set of<br>all bucket addresses. A hash function h is a function from K to B. Let h denote a hash<br>function.<br>To insert a record with search key Ki, we compute h(Ki), which gives the address<br>of the bucket for that record. Assume for now that there is space in the bucket to store<br>the record. Then, the record is stored in that bucket.<br>To perform a lookup on a search-key value Ki, we simply compute h(Ki), then<br>search the bucket with that address. Suppose that two search keys, K5 and K7, have<br>the same hash value; that is, h(K5) = h(K7). If we perform a lookup on K5, the<br>bucket h(K5) contains records with search-key values K5 and records with search-<br>key values K7. Thus, we have to check the search-key value of every record in the<br>bucket to verify that the record is one that we want.<br>Deletion is equally straightforward. If the search-key value of the record to be<br>deleted is Ki, we compute h(Ki), then search the corresponding bucket for that<br>record, and delete the record from the bucket.<br>12.5.1.1<br>Hash Functions<br>The worst possible hash function maps all search-key values to the same bucket. Such<br>a function is undesirable because all the records have to be kept in the same bucket.<br>A lookup has to examine every such record to ﬁnd the one desired. An ideal hash<br>function distributes the stored keys uniformly across all the buckets, so that every<br>bucket has the same number of records.<br>Since we do not know at design time precisely which search-key values will be<br>stored in the ﬁle, we want to choose a hash function that assigns search-key values to<br>buckets in such a way that the distribution has these qualities:<br>• The distribution is uniform. That is, the hash function assigns each bucket the<br>same number of search-key values from the set of all possible search-key val-<br>ues.<br>• The distribution is random. That is, in the average case, each bucket will have<br>nearly the same number of values assigned to it, regardless of the actual dis-<br>tribution of search-key values. More precisely, the hash value will not be cor-<br>related to any externally visible ordering on the search-key values, such as<br>alphabetic ordering or ordering by the length of the search keys; the hash<br>function will appear to be random.<br>As an illustration of these principles, let us choose a hash function for the account<br>ﬁle using the search key branch-name. The hash function that we choose must have<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>IV. Data Storage and <br>Querying<br>12. Indexing and Hashing<br>468<br>© The McGraw−Hill <br>Companies, 2001<br>12.5<br>Static Hashing<br>467<br>the desirable properties not only on the example account ﬁle that we have been using,<br>but also on an account ﬁle of realistic size for a large bank with many branches.<br>Assume that we decide to have 26 buckets, and we deﬁne a hash function that<br>maps names beginning with the ith letter of the alphabet to the ith bucket. This hash<br>function has the virtue of simplicity, but it fails to provide a uniform distribution,<br>since we expect more branch names to begin with such letters as B and R than Q and<br>X, for example.<br>Now suppose that we want a hash function on the search key balance. Suppose that<br>the minimum balance is 1 and the maximum balance is 100,000, and we use a hash<br>function that divides the values into 10 ranges, 1–10,000, 10,001–20,000 and so on. The<br>distribution of search-key values is uniform (since each bucket has the same number<br>of different balance values), but is not random. But records with balances between 1<br>and 10,000 are far more common than are records with balances between 90,001 and<br>100,000. As a result, the distribution of records is not uniform—some buckets receive<br>more records than others do. If the function has a random distribution, even if there<br>are such correlations in the search keys, the randomness of the distribution will make<br>it very likely that all buckets will have roughly the same number of records, as long<br>as each search key occurs in only a small fraction of the records. (If a single search<br>key occurs in a large fraction of the records, the bucket containing it is likely to have<br>more records than other buckets, regardless of the hash function used.)<br>Typical hash functions perform computation on the internal binary machine rep-<br>resentation of characters in the search key. A simple hash function of this type ﬁrst<br>computes the sum of the binary representations of the characters of a key, then re-<br>turns the sum modulo the number of buckets. Figure 12.21 shows the application of<br>such a scheme, with 10 buckets, to the account ﬁle, under the assumption that the ith<br>letter in the alphabet is represented by the integer i.<br>Hash functions require careful design. A bad hash function may result in lookup<br>taking time proportional to the number of search keys in the ﬁle. A well-designed<br>function gives an average-case lookup time that is a (small) constant, independent of<br>the number of search keys in the ﬁle.<br>12.5.1.2<br>Handling of Bucket Overﬂows<br>So far, we have assumed that, when a record is inserted, the bucket to which it is<br>mapped has space to store the record. If the bucket does not have enough space, a<br>bucket overﬂow is said to occur. Bucket overﬂow can occur for several reasons:<br>• Insufﬁcient buckets. The number of buckets, which we denote nB, must be<br>chosen such that nB &gt; nr/fr, where nr denotes the total number of records<br>that will be stored, and fr denotes the number of records that will ﬁt in a<br>bucket. This designation, of course, assumes that the total number of records<br>is known when the hash function is chosen.<br>• Skew. Some buckets are assigned more records than are others, so a bucket<br>may overﬂow even when other buckets still have space. This situation is called<br>bucket skew. Skew can occur for two reasons:<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>IV. Data Storage and <br>Querying<br>12. Indexing and Hashing<br>469<br>© The McGraw−Hill <br>Companies, 2001<br>468<br>Chapter 12<br>Indexing and Hashing<br>bucket 0<br>bucket 1<br>bucket 2<br>bucket 3<br>A-217<br>Brighton<br>750<br>A-305<br>Round Hill<br>350<br>bucket 4<br>A-222<br>Redwood<br>700<br>bucket 5<br>A-102<br>Perryridge<br>400<br>A-201<br>Perryridge<br>900<br>A-218<br>Perryridge<br>700<br>bucket 6<br>bucket 7<br>A-215<br>Mianus<br>700<br>bucket 8<br>A-101<br>Downtown<br>500<br>A-110<br>Downtown<br>600<br>bucket 9<br>Figure 12.21<br>Hash organization of account ﬁle, with branch-name as the key.<br>1. Multiple records may have the same search key.<br>2. The chosen hash function may result in nonuniform distribution of search<br>keys.<br>So that the probability of bucket overﬂow is reduced, the number of buckets is<br>chosen to be (nr/fr) ∗(1 + d), where d is a fudge factor, typically around 0.2. Some<br>space is wasted: About 20 percent of the space in the buckets will be empty. But the<br>beneﬁt is that the probability of overﬂow is reduced.<br>Despite allocation of a few more buckets than required, bucket overﬂow can still<br>occur. We handle bucket overﬂow by using overﬂow buckets. If a record must be<br>inserted into a bucket b, and b is already full, the system provides an overﬂow bucket<br>for b, and inserts the record into the overﬂow bucket. If the overﬂow bucket is also<br>full, the system provides another overﬂow bucket, and so on. All the overﬂow buck-<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>IV. Data Storage and <br>Querying<br>12. Indexing and Hashing<br>470<br>© The McGraw−Hill <br>Companies, 2001<br>12.5<br>Static Hashing<br>469<br>overflow  buckets for bucket 1<br>bucket 0<br>bucket 1<br>bucket 2<br>bucket 3<br>Figure 12.22<br>Overﬂow chaining in a hash structure.<br>ets of a given bucket are chained together in a linked list, as in Figure 12.22. Overﬂow<br>handling using such a linked list is called overﬂow chaining.<br>We must change the lookup algorithm slightly to handle overﬂow chaining. As<br>before, the system uses the hash function on the search key to identify a bucket b. The<br>system must examine all the records in bucket b to see whether they match the search<br>key, as before. In addition, if bucket b has overﬂow buckets, the system must examine<br>the records in all the overﬂow buckets also.<br>The form of hash structure that we have just described is sometimes referred to<br>as closed hashing. Under an alternative approach, called open hashing, the set of<br>buckets is ﬁxed, and there are no overﬂow chains. Instead, if a bucket is full, the sys-<br>tem inserts records in some other bucket in the initial set of buckets B. One policy is<br>to use the next bucket (in cyclic order) that has space; this policy is called linear prob-<br>ing. Other policies, such as computing further hash functions, are also used. Open<br>hashing has been used to construct symbol tables for compilers and assemblers, but<br>closed hashing is preferable for database systems. The reason is that deletion un-<br>der open hashing is troublesome. Usually, compilers and assemblers perform only<br>lookup and insertion operations on their symbol tables. However, in a database sys-<br>tem, it is important to be able to handle deletion as well as insertion. Thus, open<br>hashing is of only minor importance in database implementation.<br>An important drawback to the form of hashing that we have described is that<br>we must choose the hash function when we implement the system, and it cannot be<br>changed easily thereafter if the ﬁle being indexed grows or shrinks. Since the function<br>h maps search-key values to a ﬁxed set B of bucket addresses, we waste space if B is<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>IV. Data Storage and <br>Querying<br>12. Indexing and Hashing<br>471<br>© The McGraw−Hill <br>Companies, 2001<br>470<br>Chapter 12<br>Indexing and Hashing<br>made large to handle future growth of the ﬁle. If B is too small, the buckets contain<br>records of many different search-key values, and bucket overﬂows can occur. As the<br>ﬁle grows, performance suffers. We study later, in Section 12.6, how the number of<br>buckets and the hash function can be changed dynamically.<br>12.5.2<br>Hash Indices<br>Hashing can be used not only for ﬁle organization, but also for index-structure cre-<br>ation. A hash index organizes the search keys, with their associated pointers, into a<br>hash ﬁle structure. We construct a hash index as follows. We apply a hash function<br>on a search key to identify a bucket, and store the key and its associated pointers<br>in the bucket (or in overﬂow buckets). Figure 12.23 shows a secondary hash index<br>on the account ﬁle, for the search key account-number. The hash function in the ﬁgure<br>computes the sum of the digits of the account number modulo 7. The hash index has<br>seven buckets, each of size 2 (realistic indices would, of course, have much larger<br>bucket 0<br>bucket 1<br>A-215<br>A-305<br>bucket 2<br>A-101<br>A-110<br>bucket 3<br>A-217<br>A-102<br>A-201<br>bucket 4<br>A-218<br>bucket 5<br>bucket 6<br>A-222<br>A-217    Brighton<br>750<br>A-101    Downtown<br>500<br>A-110    Downtown<br>600<br>A-215     Mianus<br>700<br>A-102     Perryridge<br>400<br>A-201     Perryridge<br>900<br>A-218     Perryridge<br>700<br>A-222     Redwood<br>700<br>A-305     Round Hill<br>350<br>Figure 12.23<br>Hash index on search key account-number of account ﬁle.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>IV. Data Storage and <br>Querying<br>12. Indexing and Hashing<br>472<br>© The McGraw−Hill <br>Companies, 2001<br>12.6<br>Dynamic Hashing<br>471<br>bucket sizes). One of the buckets has three keys mapped to it, so it has an overﬂow<br>bucket. In this example, account-number is a primary key for account, so each search-<br>key has only one associated pointer. In general, multiple pointers can be associated<br>with each key.<br>We use the term hash index to denote hash ﬁle structures as well as secondary<br>hash indices. Strictly speaking, hash indices are only secondary index structures. A<br>hash index is never needed as a primary index structure, since, if a ﬁle itself is orga-<br>nized by hashing, there is no need for a separate hash index structure on it. However,<br>since hash ﬁle organization provides the same direct access to records that indexing<br>provides, we pretend that a ﬁle organized by hashing also has a primary hash index<br>on it.<br>12.6<br>Dynamic Hashing<br>As we have seen, the need to ﬁx the set B of bucket addresses presents a serious<br>problem with the static hashing technique of the previous section. Most databases<br>grow larger over time. If we are to use static hashing for such a database, we have<br>three classes of options:<br>1. Choose a hash function based on the current ﬁle size. This option will result<br>in performance degradation as the database grows.<br>2. Choose a hash function based on the anticipated size of the ﬁle at some point<br>in the future. Although performance degradation is avoided, a signiﬁcant<br>amount of space may be wasted initially.<br>3. Periodically reorganize the hash structure in response to ﬁle growth. Such a<br>reorganization involves choosing a new hash function, recomputing the hash<br>function on every record in the ﬁle, and generating new bucket assignments.<br>This reorganization is a massive, time-consuming operation. Furthermore, it<br>is necessary to forbid access to the ﬁle during reorganization.<br>Several dynamic hashing techniques allow the hash function to be modiﬁed dy-<br>namically to accommodate the growth or shrinkage of the database. In this section<br>we describe one form of dynamic hashing, called extendable hashing. The biblio-<br>graphical notes provide references to other forms of dynamic hashing.<br>12.6.1<br>Data Structure<br>Extendable hashing copes with changes in database size by splitting and coalescing<br>buckets as the database grows and shrinks. As a result, space efﬁciency is retained.<br>Moreover, since the reorganization is performed on only one bucket at a time, the<br>resulting performance overhead is acceptably low.<br>With extendable hashing, we choose a hash function h with the desirable prop-<br>erties of uniformity and randomness. However, this hash function generates val-<br>ues over a relatively large range—namely, b-bit binary integers. A typical value for<br>b is 32.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>IV. Data Storage and <br>Querying<br>12. Indexing and Hashing<br>473<br>© The McGraw−Hill <br>Companies, 2001<br>472<br>Chapter 12<br>Indexing and Hashing<br>i1<br>bucket 1<br>i2<br>bucket 2<br>i3<br>bucket 3<br>i<br>hash prefix<br>00 . .<br>01 . .<br>10 . .<br>11 . .<br>.<br>.<br>.<br>.<br>.<br>.<br>bucket address table<br>Figure 12.24<br>General extendable hash structure.<br>We do not create a bucket for each hash value. Indeed, 232 is over 4 billion, and<br>that many buckets is unreasonable for all but the largest databases. Instead, we create<br>buckets on demand, as records are inserted into the ﬁle. We do not use the entire b<br>bits of the hash value initially. At any point, we use i bits, where 0 ≤i ≤b. These i<br>bits are used as an offset into an additional table of bucket addresses. The value of i<br>grows and shrinks with the size of the database.<br>Figure 12.24 shows a general extendable hash structure. The i appearing above<br>the bucket address table in the ﬁgure indicates that i bits of the hash value h(K) are<br>required to determine the correct bucket for K. This number will, of course, change<br>as the ﬁle grows. Although i bits are required to ﬁnd the correct entry in the bucket<br>address table, several consecutive table entries may point to the same bucket. All<br>such entries will have a common hash preﬁx, but the length of this preﬁx may be less<br>than i. Therefore, we associate with each bucket an integer giving the length of the<br>common hash preﬁx. In Figure 12.24 the integer associated with bucket j is shown as<br>ij. The number of bucket-address-table entries that point to bucket j is<br>2(i −ij)<br>12.6.2<br>Queries and Updates<br>We now see how to perform lookup, insertion, and deletion on an extendable hash<br>structure.<br>To locate the bucket containing search-key value Kl, the system takes the ﬁrst i<br>high-order bits of h(Kl), looks at the corresponding table entry for this bit string, and<br>follows the bucket pointer in the table entry.<br>To insert a record wi</span><br><br><span style="background-color: #CAFFBF;" title="Chunk 59 | Start: 1180118 | End: 1200118 | Tokens: 3386">th search-key value Kl, the system follows the same procedure<br>for lookup as before, ending up in some bucket—say, j. If there is room in the bucket,<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>IV. Data Storage and <br>Querying<br>12. Indexing and Hashing<br>474<br>© The McGraw−Hill <br>Companies, 2001<br>12.6<br>Dynamic Hashing<br>473<br>the system inserts the record in the bucket. If, on the other hand, the bucket is full, it<br>must split the bucket and redistribute the current records, plus the new one. To split<br>the bucket, the system must ﬁrst determine from the hash value whether it needs to<br>increase the number of bits that it uses.<br>• If i = ij, only one entry in the bucket address table points to bucket j. There-<br>fore, the system needs to increase the size of the bucket address table so that<br>it can include pointers to the two buckets that result from splitting bucket j. It<br>does so by considering an additional bit of the hash value. It increments the<br>value of i by 1, thus doubling the size of the bucket address table. It replaces<br>each entry by two entries, both of which contain the same pointer as the orig-<br>inal entry. Now two entries in the bucket address table point to bucket j. The<br>system allocates a new bucket (bucket z), and sets the second entry to point<br>to the new bucket. It sets ij and iz to i. Next, it rehashes each record in bucket<br>j and, depending on the ﬁrst i bits (remember the system has added 1 to i),<br>either keeps it in bucket j or allocates it to the newly created bucket.<br>The system now reattempts the insertion of the new record. Usually, the<br>attempt will succeed. However, if all the records in bucket j, as well as the<br>new record, have the same hash-value preﬁx, it will be necessary to split a<br>bucket again, since all the records in bucket j and the new record are assigned<br>to the same bucket. If the hash function has been chosen carefully, it is unlikely<br>that a single insertion will require that a bucket be split more than once, unless<br>there are a large number of records with the same search key. If all the records<br>in bucket j have the same search-key value, no amount of splitting will help. In<br>such cases, overﬂow buckets are used to store the records, as in static hashing.<br>• If i<br>&gt;<br>ij, then more than one entry in the bucket address table points to<br>bucket j. Thus, the system can split bucket j without increasing the size of<br>the bucket address table. Observe that all the entries that point to bucket j<br>correspond to hash preﬁxes that have the same value on the leftmost ij bits.<br>The system allocates a new bucket (bucket z), and set ij and iz to the value<br>that results from adding 1 to the original ij value. Next, the system needs to<br>adjust the entries in the bucket address table that previously pointed to bucket<br>j. (Note that with the new value for ij, not all the entries correspond to hash<br>preﬁxes that have the same value on the leftmost ij bits.) The system leaves<br>the ﬁrst half of the entries as they were (pointing to bucket j), and sets all the<br>remaining entries to point to the newly created bucket (bucket z). Next, as in<br>the previous case, the system rehashes each record in bucket j, and allocates it<br>either to bucket j or to the newly created bucket z.<br>The system then reattempts the insert. In the unlikely case that it again fails,<br>it applies one of the two cases, i = ij or i &gt; ij, as appropriate.<br>Note that, in both cases, the system needs to recompute the hash function on only the<br>records in bucket j.<br>To delete a record with search-key value Kl, the system follows the same proce-<br>dure for lookup as before, ending up in some bucket—say, j. It removes both the<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>IV. Data Storage and <br>Querying<br>12. Indexing and Hashing<br>475<br>© The McGraw−Hill <br>Companies, 2001<br>474<br>Chapter 12<br>Indexing and Hashing<br>A-217<br>Brighton<br>750<br>A-101<br>Downtown<br>500<br>A-1 10<br>Downtown<br>600<br>A-215<br>Mianus<br>700<br>A-102<br>Perryridge<br>400<br>A-201<br>Perryridge<br>900<br>A-218<br>Perryridge<br>700<br>A-222<br>Redwood<br>700<br>A-305<br>Round Hill<br>350<br>Figure 12.25<br>Sample account ﬁle.<br>search key from the bucket and the record from the ﬁle. The bucket too is removed<br>if it becomes empty. Note that, at this point, several buckets can be coalesced, and<br>the size of the bucket address table can be cut in half. The procedure for deciding on<br>which buckets can be coalesced and how to coalesce buckets is left to you to do as an<br>exercise. The conditions under which the bucket address table can be reduced in size<br>are also left to you as an exercise. Unlike coalescing of buckets, changing the size of<br>the bucket address table is a rather expensive operation if the table is large. Therefore<br>it may be worthwhile to reduce the bucket address table size only if the number of<br>buckets reduces greatly.<br>Our example account ﬁle in Figure 12.25 illustrates the operation of insertion. The<br>32-bit hash values on branch-name appear in Figure 12.26. Assume that, initially, the<br>ﬁle is empty, as in Figure 12.27. We insert the records one by one. To illustrate all<br>the features of extendable hashing in a small structure, we shall make the unrealistic<br>assumption that a bucket can hold only two records.<br>We insert the record (A-217, Brighton, 750). The bucket address table contains a<br>pointer to the one bucket, and the system inserts the record. Next, we insert the record<br>(A-101, Downtown, 500). The system also places this record in the one bucket of our<br>structure.<br>When we attempt to insert the next record (Downtown, A-110, 600), we ﬁnd that<br>the bucket is full. Since i = i0, we need to increase the number of bits that we use<br>from the hash value. We now use 1 bit, allowing us 21 = 2 buckets. This increase in<br>branch-name<br>h(branch-name)<br>Brighton<br>0010 1101 1111 1011 0010 1100 00110000<br>1010 0011 1010 0000 1100 0110 10011111<br>1100 0111 1110 1101 1011 1111 00111010<br>1111 0001 0010 0100 1001 0011 01101101<br>0011 0101 1010 0110 1100 1001 11101011<br>1101 1000 0011 1111 1001 1100 00000001<br>Downtown<br>Mianus<br>Perryridge<br>Redwood<br>Round Hill<br>Figure 12.26<br>Hash function for branch-name.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>IV. Data Storage and <br>Querying<br>12. Indexing and Hashing<br>476<br>© The McGraw−Hill <br>Companies, 2001<br>12.6<br>Dynamic Hashing<br>475<br>0<br>bucket 1<br>hash prefix<br>bucket address table<br>0<br>Figure 12.27<br>Initial extendable hash structure.<br>the number of bits necessitates doubling the size of the bucket address table to two<br>entries. The system splits the bucket, placing in the new bucket those records whose<br>search key has a hash value beginning with 1, and leaving in the original bucket the<br>other records. Figure 12.28 shows the state of our structure after the split.<br>Next, we insert (A-215, Mianus, 700). Since the ﬁrst bit of h(Mianus) is 1, we must<br>insert this record into the bucket pointed to by the “1” entry in the bucket address<br>table. Once again, we ﬁnd the bucket full and i = i1. We increase the number of<br>bits that we use from the hash to 2. This increase in the number of bits necessitates<br>doubling the size of the bucket address table to four entries, as in Figure 12.29. Since<br>the bucket of Figure 12.28 for hash preﬁx 0 was not split, the two entries of the bucket<br>address table of 00 and 01 both point to this bucket.<br>For each record in the bucket of Figure 12.28 for hash preﬁx 1 (the bucket being<br>split), the system examines the ﬁrst 2 bits of the hash value to determine which bucket<br>of the new structure should hold it.<br>Next, we insert (A-102, Perryridge, 400), which goes in the same bucket as Mianus.<br>The following insertion, of (A-201, Perryridge, 900), results in a bucket overﬂow, lead-<br>ing to an increase in the number of bits, and a doubling of the size of the bucket<br>address table. The insertion of the third Perryridge record, (A-218, Perryridge, 700),<br>leads to another overﬂow. However, this overﬂow cannot be handled by increasing<br>the number of bits, since there are three records with exactly the same hash value.<br>Hence the system uses an overﬂow bucket, as in Figure 12.30.<br>We continue in this manner until we have inserted all the account records of Fig-<br>ure 12.25. The resulting structure appears in Figure 12.31.<br>1<br>1<br>hash prefix<br>A-217  Brighton<br>750<br>A-101  Downtown<br>500<br>A-110  Downtown<br>600<br>bucket address table<br>1<br>Figure 12.28<br>Hash structure after three insertions.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>IV. Data Storage and <br>Querying<br>12. Indexing and Hashing<br>477<br>© The McGraw−Hill <br>Companies, 2001<br>476<br>Chapter 12<br>Indexing and Hashing<br>hash prefix<br>bucket address table<br>2<br>A-217<br>750<br>A-101<br>500<br>A-110<br>600<br>A-215<br>700<br>Mianus<br>Downtown<br>Downtown<br>Brighton<br>2<br>1<br>2 <br>Figure 12.29<br>Hash structure after four insertions.<br>12.6.3<br>Comparison with Other Schemes<br>We now examine the advantages and disadvantages of extendable hashing, com-<br>pared with the other schemes that we have discussed. The main advantage of ex-<br>tendable hashing is that performance does not degrade as the ﬁle grows. Further-<br>more, there is minimal space overhead. Although the bucket address table incurs<br>additional overhead, it contains one pointer for each hash value for the current pre-<br>hash prefix<br>bucket address table<br>A-217<br>750<br>A-101<br>500<br>A-110<br>600<br>A-215<br>700<br>A-102<br>400<br>A-201<br>900<br>700<br>Brighton<br>Downtown<br>Downtown<br>Mianus<br>Perryridge<br>A-218 Perryridge<br>Perryridge<br>3<br>3<br>1<br>2<br>3<br>3<br>Figure 12.30<br>Hash structure after seven insertions.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>IV. Data Storage and <br>Querying<br>12. Indexing and Hashing<br>478<br>© The McGraw−Hill <br>Companies, 2001<br>12.7<br>Comparison of Ordered Indexing and Hashing<br>477<br>hash prefix<br>bucket address table<br>A-217<br>750<br>A-222<br>700<br>A-101<br>500<br>A-110<br>600<br>A-215<br>700<br>A-305<br>350<br>A-102<br>400<br>A-201<br>900<br>A-218<br>700<br>Brighton<br>Redwood<br>Downtown<br>Downtown<br>Mianus<br>Round Hill<br>Perryridge<br>Perryridge<br>Perryridge<br>3<br>1<br>2<br>3<br>3<br>3<br>Figure 12.31<br>Extendable hash structure for the account ﬁle.<br>ﬁx length. This table is thus small. The main space saving of extendable hashing<br>over other forms of hashing is that no buckets need to be reserved for future growth;<br>rather, buckets can be allocated dynamically.<br>A disadvantage of extendable hashing is that lookup involves an additional level<br>of indirection, since the system must access the bucket address table before access-<br>ing the bucket itself. This extra reference has only a minor effect on performance.<br>Although the hash structures that we discussed in Section 12.5 do not have this ex-<br>tra level of indirection, they lose their minor performance advantage as they become<br>full.<br>Thus, extendable hashing appears to be a highly attractive technique, provided<br>that we are willing to accept the added complexity involved in its implementation.<br>The bibliographical notes reference more detailed descriptions of extendable hashing<br>implementation. The bibliographical notes also provide references to another form of<br>dynamic hashing called linear hashing, which avoids the extra level of indirection<br>associated with extendable hashing, at the possible cost of more overﬂow buckets.<br>12.7<br>ComparisonofOrderedIndexingandHashing<br>We have seen several ordered-indexing schemes and several hashing schemes. We<br>can organize ﬁles of records as ordered ﬁles, by using index-sequential organization<br>or B+-tree organizations. Alternatively, we can organize the ﬁles by using hashing.<br>Finally, we can organize them as heap ﬁles, where the records are not ordered in any<br>particular way.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>IV. Data Storage and <br>Querying<br>12. Indexing and Hashing<br>479<br>© The McGraw−Hill <br>Companies, 2001<br>478<br>Chapter 12<br>Indexing and Hashing<br>Each scheme has advantages in certain situations. A database-system implemen-<br>tor could provide many schemes, leaving the ﬁnal decision of which schemes to use<br>to the database designer. However, such an approach requires the implementor to<br>write more code, adding both to the cost of the system and to the space that the sys-<br>tem occupies. Most database systems support B+-trees and may additionally support<br>some form of hash ﬁle organization or hash indices.<br>To make a wise choice of ﬁle organization and indexing techniques for a relation,<br>the implementor or the database designer must consider the following issues:<br>• Is the cost of periodic reorganization of the index or hash organization accept-<br>able?<br>• What is the relative frequency of insertion and deletion?<br>• Is it desirable to optimize average access time at the expense of increasing the<br>worst-case access time?<br>• What types of queries are users likely to pose?<br>We have already examined the ﬁrst three of these issues, ﬁrst in our review of the<br>relative merits of speciﬁc indexing techniques, and again in our discussion of hashing<br>techniques. The fourth issue, the expected type of query, is critical to the choice of<br>ordered indexing or hashing.<br>If most queries are of the form<br>select A1, A2, . . . , An<br>from r<br>where Ai = c<br>then, to process this query, the system will perform a lookup on an ordered index<br>or a hash structure for attribute Ai, for value c. For queries of this form, a hashing<br>scheme is preferable. An ordered-index lookup requires time proportional to the log<br>of the number of values in r for Ai. In a hash structure, however, the average lookup<br>time is a constant independent of the size of the database. The only advantage to<br>an index over a hash structure for this form of query is that the worst-case lookup<br>time is proportional to the log of the number of values in r for Ai. By contrast, for<br>hashing, the worst-case lookup time is proportional to the number of values in r<br>for Ai. However, the worst-case lookup time is unlikely to occur with hashing, and<br>hashing is preferable in this case.<br>Ordered-index techniques are preferable to hashing in cases where the query spec-<br>iﬁes a range of values. Such a query takes the following form:<br>select A1, A2, ..., An<br>from r<br>where Ai ≤c2 and Ai ≥c1<br>In other words, the preceding query ﬁnds all the records with Ai values between c1<br>and c2.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>IV. Data Storage and <br>Querying<br>12. Indexing and Hashing<br>480<br>© The McGraw−Hill <br>Companies, 2001<br>12.8<br>Index Deﬁnition in SQL<br>479<br>Let us consider how we process this query using an ordered index. First, we per-<br>form a lookup on value c1. Once we have found the bucket for value c1, we follow<br>the pointer chain in the index to read the next bucket in order, and we continue in<br>this manner until we reach c2.<br>If, instead of an ordered index, we have a hash structure, we can perform a lookup<br>on c1 and can locate the corresponding bucket—but it is not easy, in general, to de-<br>termine the next bucket that must be examined. The difﬁculty arises because a good<br>hash function assigns values randomly to buckets. Thus, there is no simple notion of<br>“next bucket in sorted order.” The reason we cannot chain buckets together in sorted<br>order on Ai is that each bucket is assigned many search-key values. Since values are<br>scattered randomly by the hash function, the values in the speciﬁed range are likely<br>to be scattered across many or all of the buckets. Therefore, we have to read all the<br>buckets to ﬁnd the required search keys.<br>Usually the designer will choose ordered indexing unless it is known in advance<br>that range queries will be infrequent, in which case hashing would be chosen. Hash<br>organizations are particularly useful for temporary ﬁles created during query pro-<br>cessing, if lookups based on a key value are required, but no range queries will be<br>performed.<br>12.8<br>Index Deﬁnition in SQL<br>The SQL standard does not provide any way for the database user or administrator<br>to control what indices are created and maintained in the database system. Indices<br>are not required for correctness, since they are redundant data structures. However,<br>indices are important for efﬁcient processing of transactions, including both update<br>transactions and queries. Indices are also important for efﬁcient enforcement of in-<br>tegrity constraints. For example, typical implementations enforce a key declaration<br>(Chapter 6) by creating an index with the declared key as the search key of the index.<br>In principle, a database system can decide automatically what indices to create.<br>However, because of the space cost of indices, as well as the effect of indices on up-<br>date processing, it is not easy to automatically make the right choices about what<br>indices to maintain. Therefore, most SQL implementations provide the programmer<br>control over creation and removal of indices via data-deﬁnition-language commands.<br>We illustrate the syntax of these commands next. Although the syntax that we<br>show is widely used and supported by many database systems, it is not part of the<br>SQL:1999 standard. The SQL standards (up to SQL:1999, at least) do not support con-<br>trol of the physical database schema, and have restricted themselves to the logical<br>database schema.<br>We create an index by the create index command, which takes the form<br>create index &lt;index-name&gt; on &lt;relation-name&gt; (&lt;attribute-list&gt;)<br>The attribute-list is the list of attributes of the relations that form the search key for<br>the index.<br>To deﬁne an index name b-index on the branch relation with branch-name as the<br>search key, we write<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>IV. Data Storage and <br>Querying<br>12. Indexing and Hashing<br>481<br>© The McGraw−Hill <br>Companies, 2001<br>480<br>Chapter 12<br>Indexing and Hashing<br>create index b-index on branch (branch-name)<br>If we wish to declare that the search key is a candidate key, we add the attribute<br>unique to the index deﬁnition. Thus, the command<br>create unique index b-index on branch (branch-name)<br>declares branch-name to be a candidate key for branch. If, at the time we enter the<br>create unique index command, branch-name is not a candidate key, the system will<br>display an error message, and the attempt to create the index will fail. If the index-<br>creation attempt succeeds, any subsequent attempt to insert a tuple that violates the<br>key declaration will fail. Note that the unique feature is redundant if the database<br>system supports the unique declaration of the SQL standard.<br>Many database systems also provide a way to specify the type of index to be used<br>(such as B+-tree or hashing). Some database systems also permit one of the indices<br>on a relation to be declared to be clustered; the system then stores the relation sorted<br>by the search-key of the clustered index.<br>The index name we speciﬁed for an index is required to drop an index. The drop<br>index command takes the form:<br>drop index &lt;index-name&gt;<br>12.9<br>Multiple-Key Access<br>Until now, we have assumed implicitly that only one index (or hash table) is used to<br>process a query on a relation. However, for certain types of queries, it is advantageous<br>to use multiple indices if they exist.<br>12.9.1<br>Using Multiple Single-Key Indices<br>Assume that the account ﬁle has two indices: one for branch-name and one for balance.<br>Consider the following query: “Find all account numbers at the Perryridge branch<br>with balances equal to $1000.” We write<br>select loan-number<br>from account<br>where branch-name = “Perryridge” and balance = 1000<br>There are three strategies possible for processing this query:<br>1. Use the index on branch-name to ﬁnd all records pertaining to the Perryridge<br>branch. Examine each such record to see whether balance = 1000.<br>2. Use the index on balance to ﬁnd all records pertaining to accounts with bal-<br>ances of $1000. Examine each such record to see whether branch-name = “Per-<br>ryridge.”<br>3. Use the index on branch-name to ﬁnd pointers to all records pertaining to the<br>Perryridge branch. Also, use the index on balance to ﬁnd pointers to all records<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>IV. Data Storage and <br>Querying<br>12. Indexing and Hashing<br>482<br>© The McGraw−Hill <br>Companies, 2001<br>12.9<br>Multiple-Key Access<br>481<br>pertaining to accounts with a balance of $1000. Take the intersection of these<br>two sets of pointers. Those pointers that are in the intersection point to records<br>pertaining </span><br><br><span style="background-color: #9BF6FF;" title="Chunk 60 | Start: 1200120 | End: 1220120 | Tokens: 3559">to both Perryridge and accounts with a balance of $1000.<br>The third strategy is the only one of the three that takes advantage of the existence<br>of multiple indices. However, even this strategy may be a poor choice if all of the<br>following hold:<br>• There are many records pertaining to the Perryridge branch.<br>• There are many records pertaining to accounts with a balance of $1000.<br>• There are only a few records pertaining to both the Perryridge branch and<br>accounts with a balance of $1000.<br>If these conditions hold, we must scan a large number of pointers to produce a small<br>result. An index structure called a “bitmap index” greatly speeds up the intersection<br>operation used in the third strategy. Bitmap indices are outlined in Section 12.9.4.<br>12.9.2<br>Indices on Multiple Keys<br>An alternative strategy for this case is to create and use an index on a search key<br>(branch-name, balance)—that is, the search key consisting of the branch name concate-<br>nated with the account balance. The structure of the index is the same as that of any<br>other index, the only difference being that the search key is not a single attribute, but<br>rather is a list of attributes. The search key can be represented as a tuple of values,<br>of the form (a1, . . . , an), where the indexed attributes are A1, . . . , An. The ordering<br>of search-key values is the lexicographic ordering. For example, for the case of two<br>attribute search keys, (a1, a2) &lt; (b1, b2) if either a1 &lt; b1 or a1 = b1 and a2 &lt; b2.<br>Lexicographic ordering is basically the same as alphabetic ordering of words.<br>The use of an ordered-index structure on multiple attributes has a few short-<br>comings. As an illustration, consider the query<br>select loan-number<br>from account<br>where branch-name &lt; “Perryridge” and balance = 1000<br>We can answer this query by using an ordered index on the search key (branch-name,<br>balance): For each value of branch-name that is less than “Perryridge” in alphabetic<br>order, the system locates records with a balance value of 1000. However, each record<br>is likely to be in a different disk block, because of the ordering of records in the ﬁle,<br>leading to many I/O operations.<br>The difference between this query and the previous one is that the condition on<br>branch-name is a comparison condition, rather than an equality condition.<br>To speed the processing of general multiple search-key queries (which can involve<br>one or more comparison operations), we can use several special structures. We shall<br>consider the grid ﬁle in Section 12.9.3. There is another structure, called the R-tree, that<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>IV. Data Storage and <br>Querying<br>12. Indexing and Hashing<br>483<br>© The McGraw−Hill <br>Companies, 2001<br>482<br>Chapter 12<br>Indexing and Hashing<br>can be used for this purpose. The R-tree is an extension of the B+-tree to handle in-<br>dexing on multiple dimensions. Since the R-tree is used primarily with geographical<br>data types, we describe the structure in Chapter 23.<br>12.9.3<br>Grid Files<br>Figure 12.32 shows part of a grid ﬁle for the search keys branch-name and balance on<br>the account ﬁle. The two-dimensional array in the ﬁgure is called the grid array, and<br>the one-dimensional arrays are called linear scales. The grid ﬁle has a single grid array,<br>and one linear scale for each search-key attribute.<br>Search keys are mapped to cells in this way. Each cell in the grid array has a pointer<br>to a bucket that contains the search-key values and pointers to records. Only some<br>of the buckets and pointers from the cells are shown in the ﬁgure. To conserve space,<br>multiple elements of the array can point to the same bucket. The dotted boxes in the<br>ﬁgure indicate which cells point to the same bucket.<br>Suppose that we want to insert in the grid-ﬁle index a record whose search-key<br>value is (“Brighton”, 500000). To ﬁnd the cell to which the key is mapped, we inde-<br>pendently locate the row and column to which the cell belongs.<br>We ﬁrst use the linear scales on branch-name to locate the row of the cell to which<br>the search key maps. To do so, we search the array to ﬁnd the least element that is<br>greater than “Brighton”. In this case, it is the ﬁrst element, so the search key maps to<br>the row marked 0. If it were the ith element, the search key would map to row i −1.<br>If the search key is greater than or equal to all elements in the linear scale, it maps to<br>4<br>Townsend<br>3<br>Perryridge<br>2<br>Mianus<br>1<br>Central<br>Linear scale for<br>branch-name<br>4<br>3<br>2<br>1<br>0<br>Grid Array<br>0<br>1<br>2<br>3<br>4<br>5<br>6<br>1K<br>2K<br>5K<br>10K<br>50K<br>100K<br>Buckets<br>1<br>2<br>3<br>4<br>5<br>6<br>Bi<br>Bj<br>Linear scale for balance<br>Figure 12.32<br>Grid ﬁle on keys branch-name and balance of the account ﬁle.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>IV. Data Storage and <br>Querying<br>12. Indexing and Hashing<br>484<br>© The McGraw−Hill <br>Companies, 2001<br>12.9<br>Multiple-Key Access<br>483<br>the ﬁnal row. Next, we use the linear scale on balance to ﬁnd out similarly to which<br>column the search key maps. In this case, the balance 500000 maps to column 6.<br>Thus, the search-key value (“Brighton”, 500000) maps to the cell in row 0, column<br>6. Similarly, (“Downtown”, 60000) would map to the cell in row 1 column 5. Both cells<br>point to the same bucket (as indicated by the dotted box), so, in both cases, the system<br>stores the search-key values and the pointer to the record in the bucket labeled Bj in<br>the ﬁgure.<br>To perform a lookup to answer our example query, with the search condition of<br>branch-name &lt; “Perryridge” and balance = 1000<br>we ﬁnd all rows that can contain branch names less than “Perryridge”, using the<br>linear scale on branch-name. In this case, these rows are 0, 1, and 2. Rows 3 and beyond<br>contain branch names greater than or equal to “Perryridge”. Similarly, we ﬁnd that<br>only column 1 can contain a balance value of 1000. In this case, only column 1 satisﬁes<br>this condition. Thus, only the cells in column 1, rows 0, 1, and 2, can contain entries<br>that satisfy the search condition.<br>We therefore look up all entries in the buckets pointed to from these three cells. In<br>this case, there are only two buckets, since two of the cells point to the same bucket,<br>as indicated by the dotted boxes in the ﬁgure. The buckets may contain some search<br>keys that do not satisfy the required condition, so each search key in the buckets must<br>be tested again to see whether it satisﬁes the search condition. We have to examine<br>only a small number of buckets, however, to answer this query.<br>We must choose the linear scales in such a way that the records are uniformly dis-<br>tributed across the cells. When a bucket—call it A—becomes full and an entry has to<br>be inserted in it, the system allocates an extra bucket, B. If more than one cell points<br>to A, the system changes the cell pointers so that some point to A and others to B.<br>The entries in bucket A and the new entry are then redistributed between A and B ac-<br>cording to the cells to which they map. If only one cell points to bucket A, B becomes<br>an overﬂow bucket for A. To improve performance in such a situation, we must re-<br>organize the grid ﬁle, with an expanded grid array and expanded linear scales. The<br>process is much like the expansion of the bucket address table in extensible hashing,<br>and is left for you to do as an exercise.<br>It is conceptually simple to extend the grid-ﬁle approach to any number of search<br>keys. If we want our structure to be used for queries on n keys, we construct an n-<br>dimensional grid array with n linear scales.<br>The grid structure is suitable also for queries involving one search key. Consider<br>this query:<br>select *<br>from account<br>where branch-name = “Perryridge”<br>The linear scale on branch-name tells us that only cells in row 3 can satisfy this condi-<br>tion. Since there is no condition on balance, we examine all buckets pointed to by cells<br>in row 3 to ﬁnd entries pertaining to Perryridge. Thus, we can use a grid-ﬁle index on<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>IV. Data Storage and <br>Querying<br>12. Indexing and Hashing<br>485<br>© The McGraw−Hill <br>Companies, 2001<br>484<br>Chapter 12<br>Indexing and Hashing<br>two search keys to answer queries on either search key by itself, as well as to answer<br>queries on both search keys. Thus, a single grid-ﬁle index can serve the role of three<br>separate indices. If each index were maintained separately, the three together would<br>occupy more space, and the cost of updating them would be high.<br>Grid ﬁles provide a signiﬁcant decrease in processing time for multiple-key queries.<br>However, they impose a space overhead (the grid directory can become large), as<br>well as a performance overhead on record insertion and deletion. Further, it is hard<br>to choose partitioning ranges for the keys such that the distribution of records is uni-<br>form. If insertions to the ﬁle are frequent, reorganization will have to be carried out<br>periodically, and that can have a high cost.<br>12.9.4<br>Bitmap Indices<br>Bitmap indices are a specialized type of index designed for easy querying on multiple<br>keys, although each bitmap index is built on a single key.<br>For bitmap indices to be used, records in a relation must be numbered sequen-<br>tially, starting from, say, 0. Given a number n, it must be easy to retrieve the record<br>numbered n. This is particularly easy to achieve if records are ﬁxed in size, and allo-<br>cated on consecutive blocks of a ﬁle. The record number can then be translated easily<br>into a block number and a number that identiﬁes the record within the block.<br>Consider a relation r, with an attribute A that can take on only one of a small num-<br>ber (for example, 2 to 20) values. For instance, a relation customer-info may have an<br>attribute gender, which can take only values m (male) or f (female). Another example<br>would be an attribute income-level, where income has been broken up into 5 levels:<br>L1: $0 −9999, L2: $10, 000 −19, 999, L3: 20, 000 −39, 999, L4: 40, 000 −74, 999, and<br>L5: 75, 000 −∞. Here, the raw data can take on many values, but a data analyst has<br>split the values into a small number of ranges to simplify analysis of the data.<br>12.9.4.1<br>Bitmap Index Structure<br>A bitmap is simply an array of bits. In its simplest form, a bitmap index on the<br>attribute A of relation r consists of one bitmap for each value that A can take. Each<br>bitmap has as many bits as the number of records in the relation. The ith bit of the<br>bitmap for value vj is set to 1 if the record numbered i has the value vj for attribute<br>A. All other bits of the bitmap are set to 0.<br>In our example, there is one bitmap for the value m and one for f. The ith bit of<br>the bitmap for m is set to 1 if the gender value of the record numbered i is m. All<br>other bits of the bitmap for m are set to 0. Similarly, the bitmap for f has the value<br>1 for bits corresponding to records with the value f for the gender attribute; all other<br>bits have the value 0. Figure 12.33 shows an example of bitmap indices on a relation<br>customer-info.<br>We now consider when bitmaps are useful. The simplest way of retrieving all<br>records with value m (or value f) would be to simply read all records of the relation<br>and select those records with value m (or f, respectively). The bitmap index doesn’t<br>really help to speed up such a selection.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>IV. Data Storage and <br>Querying<br>12. Indexing and Hashing<br>486<br>© The McGraw−Hill <br>Companies, 2001<br>12.9<br>Multiple-Key Access<br>485<br>Bitmaps for <br>m       1 0 0 1 0<br>gender<br>f         0 1 1 0 1<br>L2      0 1 0 0 0<br>L3      0 0 0 0 1<br>L4      0 0 0 1 0<br>L5      0 0 0 0 0<br>record<br>number<br>Diana      f<br>Peter       m<br>Kathy      f<br>Mary       f<br>John        m<br>1<br>3<br>4<br>2<br>0<br>Brooklyn         L2<br>Brooklyn         L4<br>Perryridge       L3<br>Jonestown       L1<br>Perryridge       L1<br>income<br>-level<br>name    gender      address<br>L1      1 0 1 0 0<br>income-level<br>Bitmaps for <br>Figure 12.33<br>Bitmap indices on relation customer-info.<br>In fact, bitmap indices are useful for selections mainly when there are selections<br>on multiple keys. Suppose we create a bitmap index on attribute income-level, which<br>we described earlier, in addition to the bitmap index on gender.<br>Consider now a query that selects women with income in the range 10, 000 −<br>19, 999. This query can be expressed as σgender=f∧income-level=L2(r). To evaluate this<br>selection, we fetch the bitmaps for gender value f and the bitmap for income-level value<br>L2, and perform an intersection (logical-and) of the two bitmaps. In other words, we<br>compute a new bitmap where bit i has value 1 if the ith bit of the two bitmaps are<br>both 1, and has a value 0 otherwise. In the example in Figure 12.33, the intersection<br>of the bitmap for gender = f (01101) and the bitmap for income-level = L1 (10100)<br>gives the bitmap 00100.<br>Since the ﬁrst attribute can take 2 values, and the second can take 5 values, we<br>would expect only about 1 in 10 records, on an average, to satisfy a combined condi-<br>tion on the two attributes. If there are further conditions, the fraction of records sat-<br>isfying all the conditions is likely to be quite small. The system can then compute the<br>query result by ﬁnding all bits with value 1 in the intersection bitmap, and retrieving<br>the corresponding records. If the fraction is large, scanning the entire relation would<br>remain the cheaper alternative.<br>Another important use of bitmaps is to count the number of tuples satisfying a<br>given selection. Such queries are important for data analysis. For instance, if we wish<br>to ﬁnd out how many women have an income level L2, we compute the intersection<br>of the two bitmaps, and then count the number of bits that are 1 in the intersection<br>bitmap. We can thus get the desired result from the bitmap index, without even ac-<br>cessing the relation.<br>Bitmap indices are generally quite small compared to the actual relation size. Rec-<br>ords are typically at least tens of bytes to hundreds of bytes long, whereas a single<br>bit represents the record in a bitmap. Thus the space occupied by a single bitmap<br>is usually less than 1 percent of the space occupied by the relation. For instance, if<br>the record size for a given relation is 100 bytes, then the space occupied by a single<br>bitmap would be 1<br>8 of 1 percent of the space occupied by the relation. If an attribute A<br>of the relation can take on only one of 8 values, a bitmap index on attribute A would<br>consist of 8 bitmaps, which together occupy only 1 percent of the size of the relation.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>IV. Data Storage and <br>Querying<br>12. Indexing and Hashing<br>487<br>© The McGraw−Hill <br>Companies, 2001<br>486<br>Chapter 12<br>Indexing and Hashing<br>Deletion of records creates gaps in the sequence of records, since shifting records<br>(or record numbers) to ﬁll gaps would be extremely expensive. To recognize deleted<br>records, we can store an existence bitmap, in which bit i is 0 if record i does not exist<br>and 1 otherwise. We will see the need for existence bitmaps in Section 12.9.4.2. Inser-<br>tion of records should not affect the sequence numbering of other records. Therefore,<br>we can do insertion either by appending records to the end of the ﬁle or by replacing<br>deleted records.<br>12.9.4.2<br>Efﬁcient Implementation of Bitmap Operations<br>We can compute the intersection of two bitmaps easily by using a for loop: the ith<br>iteration of the loop computes the and of the ith bits of the two bitmaps. We can<br>speed up computation of the intersection greatly by using bit-wise and instructions<br>supported by most computer instruction sets. A word usually consists of 32 or 64<br>bits, depending on the architecture of the computer. A bit-wise and instruction takes<br>two words as input and outputs a word where each bit is the logical and of the bits in<br>corresponding positions of the input words. What is important to note is that a single<br>bit-wise and instruction can compute the intersection of 32 or 64 bits at once.<br>If a relation had 1 million records, each bitmap would contain 1 million bits, or<br>equivalently 128 Kbytes. Only 31,250 instructions are needed to compute the intersec-<br>tion of two bitmaps for our relation, assuming a 32-bit word length. Thus, computing<br>bitmap intersections is an extremely fast operation.<br>Just like bitmap intersection is useful for computing the and of two conditions,<br>bitmap union is useful for computing the or of two conditions. The procedure for<br>bitmap union is exactly the same as for intersection, except we use bit-wise or in-<br>structions instead of bit-wise and instructions.<br>The complement operation can be used to compute a predicate involving the nega-<br>tion of a condition, such as not (income-level = L1). The complement of a bitmap is<br>generated by complementing every bit of the bitmap (the complement of 1 is 0 and<br>the complement of 0 is 1). It may appear that not (income-level = L1) can be imple-<br>mented by just computing the complement of the bitmap for income level L1. If some<br>records have been deleted, however, just computing the complement of a bitmap is<br>not sufﬁcient. Bits corresponding to such records would be 0 in the original bitmap,<br>but would become 1 in the complement, although the records don’t exist. A similar<br>problem also arises when the value of an attribute is null. For instance, if the value<br>of income-level is null, the bit would be 0 in the original bitmap for value L1, and 1 in<br>the complement bitmap.<br>To make sure that the bits corresponding to deleted records are set to 0 in the result,<br>the complement bitmap must be intersected with the existence bitmap to turn off the<br>bits for deleted records. Similarly, to handle null values, the complement bitmap must<br>also be intersected with the complement of the bitmap for the value null.1<br>Counting the number of bits that are 1 in a bitmap can be done fast by a clever<br>technique. We can maintain an array with 256 entries, where the ith entry stores the<br>1.<br>Handling predicates such as is unknown would cause further complications, which would in general<br>require use of an extra bitmap to to track which operation results are unknown.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>IV. Data Storage and <br>Querying<br>12. Indexing and Hashing<br>488<br>© The McGraw−Hill <br>Companies, 2001<br>12.10<br>Summary<br>487<br>number of bits that are 1 in the binary representation of i. Set the total count initially<br>to 0. We take each byte of the bitmap, use it to index into this array, and add the<br>stored count to the total count. The number of addition operations would be 1<br>8 of the<br>number of tuples, and thus the counting process is very efﬁcient. A large array (using<br>216 = 65536 entries), indexed by pairs of bytes, would give even higher speedup, but<br>at a higher storage cost.<br>12.9.4.3<br>Bitmaps and B+-Trees<br>Bitmaps can be combined with regular B+-tree indices for relations where a few at-<br>tribute values are extremely common, and other values also occur, but much less<br>frequently. In a B+-tree index leaf, for each value we would normally maintain a list<br>of all records with that value for the indexed attribute. Each element of the list would<br>be a record identiﬁer, consisting of at least 32 bits, and usually more. For a value that<br>occurs in many records, we store a bitmap instead of a list of records.<br>Suppose a particular value vi occurs in<br>1<br>16 of the records of a relation. Let N be<br>the number of records in the relation, and assume that a record has a 64-bit number<br>identifying it. The bitmap needs only 1 bit per record, or N bits in total. In contrast,<br>the list representation requires 64 bits per record where the value occurs, or 64 ∗<br>N/16 = 4N bits. Thus, a bitmap is preferable for representing the list of records for<br>value vi. In our example (with a 64-bit record identiﬁer), if fewer than 1 in 64 records<br>have a particular value, the list representation is preferable for identifying records<br>with that value, since it uses fewer bits than the bitmap representation. If more than<br>1 in 64 records have that value, the bitmap representation is preferable.<br>Thus, bitmaps can be used as a compressed storage mechanism at the leaf nodes<br>of B+-trees, for those values that occur v</span><br><br><span style="background-color: #A0C4FF;" title="Chunk 61 | Start: 1220122 | End: 1240122 | Tokens: 3267">ery frequently.<br>12.10<br>Summary<br>• Many queries reference only a small proportion of the records in a ﬁle. To<br>reduce the overhead in searching for these records, we can construct indices<br>for the ﬁles that store the database.<br>• Index-sequential ﬁles are one of the oldest index schemes used in database<br>systems. To permit fast retrieval of records in search-key order, records are<br>stored sequentially, and out-of-order records are chained together. To allow<br>fast random access, we use an index structure.<br>• There are two types of indices that we can use: dense indices and sparse<br>indices. Dense indices contain entries for every search-key value, whereas<br>sparse indices contain entries only for some search-key values.<br>• If the sort order of a search key matches the sort order of a relation, an index<br>on the search key is called a primary index. The other indices are called sec-<br>ondary indices. Secondary indices improve the performance of queries that use<br>search keys other than the primary one. However, they impose an overhead<br>on modiﬁcation of the database.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>IV. Data Storage and <br>Querying<br>12. Indexing and Hashing<br>489<br>© The McGraw−Hill <br>Companies, 2001<br>488<br>Chapter 12<br>Indexing and Hashing<br>• The primary disadvantage of the index-sequential ﬁle organization is that per-<br>formance degrades as the ﬁle grows. To overcome this deﬁciency, we can use<br>a B+-tree index.<br>• A B+-tree index takes the form of a balanced tree, in which every path from the<br>root of the tree to a leaf of the tree is of the same length. The height of a B+-<br>tree is proportional to the logarithm to the base N of the number of records<br>in the relation, where each nonleaf node stores N pointers; the value of N is<br>often around 50 or 100. B+-trees are much shorter than other balanced binary-<br>tree structures such as AVL trees, and therefore require fewer disk accesses to<br>locate records.<br>• Lookup on B+-trees is straightforward and efﬁcient. Insertion and deletion,<br>however, are somewhat more complicated, but still efﬁcient. The number of<br>operations required for lookup, insertion, and deletion on B+-trees is propor-<br>tional to the logarithm to the base N of the number of records in the relation,<br>where each nonleaf node stores N pointers.<br>• We can use B+-trees for indexing a ﬁle containing records, as well as to orga-<br>nize records into a ﬁle.<br>• B-tree indices are similar to B+-tree indices. The primary advantage of a B-tree<br>is that the B-tree eliminates the redundant storage of search-key values. The<br>major disadvantages are overall complexity and reduced fanout for a given<br>node size. System designers almost universally prefer B+-tree indices over B-<br>tree indices in practice.<br>• Sequential ﬁle organizations require an index structure to locate data. File or-<br>ganizations based on hashing, by contrast, allow us to ﬁnd the address of a<br>data item directly by computing a function on the search-key value of the de-<br>sired record. Since we do not know at design time precisely which search-key<br>values will be stored in the ﬁle, a good hash function to choose is one that as-<br>signs search-key values to buckets such that the distribution is both uniform<br>and random.<br>• Static hashing uses hash functions in which the set of bucket addresses is ﬁxed.<br>Such hash functions cannot easily accommodate databases that grow signiﬁ-<br>cantly larger over time. There are several dynamic hashing techniques that allow<br>the hash function to be modiﬁed. One example is extendable hashing, which<br>copes with changes in database size by splitting and coalescing buckets as the<br>database grows and shrinks.<br>• We can also use hashing to create secondary indices; such indices are called<br>hash indices. For notational convenience, we assume hash ﬁle organizations<br>have an implicit hash index on the search key used for hashing.<br>• Ordered indices such as B+-trees and hash indices can be used for selections<br>based on equality conditions involving single attributes. When multiple<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>IV. Data Storage and <br>Querying<br>12. Indexing and Hashing<br>490<br>© The McGraw−Hill <br>Companies, 2001<br>Exercises<br>489<br>attributes are involved in a selection condition, we can intersect record iden-<br>tiﬁers retrieved from multiple indices.<br>• Grid ﬁles provide a general means of indexing on multiple attributes.<br>• Bitmap indices provide a very compact representation for indexing attributes<br>with very few distinct values. Intersection operations are extremely fast on<br>bitmaps, making them ideal for supporting queries on multiple attributes.<br>Review Terms<br>• Access types<br>• Access time<br>• Insertion time<br>• Deletion time<br>• Space overhead<br>• Ordered index<br>• Primary index<br>• Clustering index<br>• Secondary index<br>• Nonclustering index<br>• Index-sequential ﬁle<br>• Index record/entry<br>• Dense index<br>• Sparse index<br>• Multilevel index<br>• Sequential scan<br>• B+-Tree index<br>• Balanced tree<br>• B+-Tree ﬁle organization<br>• B-Tree index<br>• Static hashing<br>• Hash ﬁle organization<br>• Hash index<br>• Bucket<br>• Hash function<br>• Bucket overﬂow<br>• Skew<br>• Closed hashing<br>• Dynamic hashing<br>• Extendable hashing<br>• Multiple-key access<br>• Indices on multiple keys<br>• Grid ﬁles<br>• Bitmap index<br>• Bitmap operations<br>  Intersection<br>  Union<br>  Complement<br>  Existence bitmap<br>Exercises<br>12.1 When is it preferable to use a dense index rather than a sparse index? Explain<br>your answer.<br>12.2 Since indices speed query processing, why might they not be kept on several<br>search keys? List as many reasons as possible.<br>12.3 What is the difference between a primary index and a secondary index?<br>12.4 Is it possible in general to have two primary indices on the same relation for<br>different search keys? Explain your answer.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>IV. Data Storage and <br>Querying<br>12. Indexing and Hashing<br>491<br>© The McGraw−Hill <br>Companies, 2001<br>490<br>Chapter 12<br>Indexing and Hashing<br>12.5 Construct a B+-tree for the following set of key values:<br>(2, 3, 5, 7, 11, 17, 19, 23, 29, 31)<br>Assume that the tree is initially empty and values are added in ascending or-<br>der. Construct B+-trees for the cases where the number of pointers that will ﬁt<br>in one node is as follows:<br>a. Four<br>b. Six<br>c. Eight<br>12.6 For each B+-tree of Exercise 12.5, show the steps involved in the following<br>queries:<br>a. Find records with a search-key value of 11.<br>b. Find records with a search-key value between 7 and 17, inclusive.<br>12.7 For each B+-tree of Exercise 12.5, show the form of the tree after each of the<br>following series of operations:<br>a. Insert 9.<br>b. Insert 10.<br>c. Insert 8.<br>d. Delete 23.<br>e. Delete 19.<br>12.8 Consider the modiﬁed redistribution scheme for B+-trees described in page<br>463. What is the expected height of the tree as a function of n?<br>12.9 Repeat Exercise 12.5 for a B-tree.<br>12.10 Explain the distinction between closed and open hashing. Discuss the relative<br>merits of each technique in database applications.<br>12.11 What are the causes of bucket overﬂow in a hash ﬁle organization? What can<br>be done to reduce the occurrence of bucket overﬂows?<br>12.12 Suppose that we are using extendable hashing on a ﬁle that contains records<br>with the following search-key values:<br>2, 3, 5, 7, 11, 17, 19, 23, 29, 31<br>Show the extendable hash structure for this ﬁle if the hash function is h(x) = x<br>mod 8 and buckets can hold three records.<br>12.13 Show how the extendable hash structure of Exercise 12.12 changes as the result<br>of each of the following steps:<br>a. Delete 11.<br>b. Delete 31.<br>c. Insert 1.<br>d. Insert 15.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>IV. Data Storage and <br>Querying<br>12. Indexing and Hashing<br>492<br>© The McGraw−Hill <br>Companies, 2001<br>Bibliographical Notes<br>491<br>12.14 Give pseudocode for deletion of entries from an extendable hash structure,<br>including details of when and how to coalesce buckets. Do not bother about<br>reducing the size of the bucket address table.<br>12.15 Suggest an efﬁcient way to test if the bucket address table in extendable hash-<br>ing can be reduced in size, by storing an extra count with the bucket address<br>table. Give details of how the count should be maintained when buckets are<br>split, coalesced or deleted.<br>(Note: Reducing the size of the bucket address table is an expensive oper-<br>ation, and subsequent inserts may cause the table to grow again. Therefore, it<br>is best not to reduce the size as soon as it is possible to do so, but instead do<br>it only if the number of index entries becomes small compared to the bucket<br>address table size.)<br>12.16 Why is a hash structure not the best choice for a search key on which range<br>queries are likely?<br>12.17 Consider a grid ﬁle in which we wish to avoid overﬂow buckets for perfor-<br>mance reasons. In cases where an overﬂow bucket would be needed, we in-<br>stead reorganize the grid ﬁle. Present an algorithm for such a reorganization.<br>12.18 Consider the account relation shown in Figure 12.25.<br>a. Construct a bitmap index on the attributes branch-name and balance, divid-<br>ing balance values into 4 ranges: below 250, 250 to below 500, 500 to below<br>750, and 750 and above.<br>b. Consider a query that requests all accounts in Downtown with a balance of<br>500 or more. Outline the steps in answering the query, and show the ﬁnal<br>and intermediate bitmaps constructed to answer the query.<br>12.19 Show how to compute existence bitmaps from other bitmaps. Make sure that<br>your technique works even in the presence of null values, by using a bitmap<br>for the value null.<br>12.20 How does data encryption affect index schemes? In particular, how might it<br>affect schemes that attempt to store data in sorted order?<br>Bibliographical Notes<br>Discussions of the basic data structures in indexing and hashing can be found in<br>Cormen et al. [1990]. B-tree indices were ﬁrst introduced in Bayer [1972] and Bayer<br>and McCreight [1972]. B+-trees are discussed in Comer [1979], Bayer and Unterauer<br>[1977] and Knuth [1973]. The bibliographic notes in Chapter 16 provides references to<br>research on allowing concurrent accesses and updates on B+-trees. Gray and Reuter<br>[1993] provide a good description of issues in the implementation of B+-trees.<br>Several alternative tree and treelike search structures have been proposed. Tries<br>are trees whose structure is based on the “digits” of keys (for example, a dictionary<br>thumb index, which has one entry for each letter). Such trees may not be balanced<br>in the sense that B+-trees are. Tries are discussed by Ramesh et al. [1989], Orenstein<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>IV. Data Storage and <br>Querying<br>12. Indexing and Hashing<br>493<br>© The McGraw−Hill <br>Companies, 2001<br>492<br>Chapter 12<br>Indexing and Hashing<br>[1982], Litwin [1981] and Fredkin [1960]. Related work includes the digital B-trees of<br>Lomet [1981].<br>Knuth [1973] analyzes a large number of different hashing techniques. Several dy-<br>namic hashing schemes exist. Extendable hashing was introduced by Fagin et al.<br>[1979]. Linear hashing was introduced by Litwin [1978] and Litwin [1980]; Larson<br>[1982] presents a performance analysis of linear hashing. Ellis [1987] examined con-<br>currency with linear hashing. Larson [1988] presents a variant of linear hashing. An-<br>other scheme, called dynamic hashing, was proposed by Larson [1978]. An alterna-<br>tive given by Ramakrishna and Larson [1989] allows retrieval in a single disk access<br>at the price of a high overhead for a small fraction of database modiﬁcations. Par-<br>titioned hashing is an extension of hashing to multiple attributes, and is covered in<br>Rivest [1976], Burkhard [1976] and Burkhard [1979].<br>The grid ﬁle structure appears in Nievergelt et al. [1984] and Hinrichs [1985].<br>Bitmap indices, and variants called bit-sliced indices and projection indices are de-<br>scribed in O’Neil and Quass [1997]. They were ﬁrst introduced in the IBM Model<br>204 ﬁle manager on the AS 400 platform. They provide very large speedups on cer-<br>tain types of queries, and are today implemented on most database systems. Recent<br>research on bitmap indices includes Wu and Buchmann [1998], Chan and Ioannidis<br>[1998], Chan and Ioannidis [1999], and Johnson [1999a].<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>IV. Data Storage and <br>Querying<br>13. Query Processing<br>494<br>© The McGraw−Hill <br>Companies, 2001<br>C<br>H<br>A<br>P<br>T<br>E<br>R<br>1<br>3<br>Query Processing<br>Query processing refers to the range of activities involved in extracting data from<br>a database. The activities include translation of queries in high-level database lan-<br>guages into expressions that can be used at the physical level of the ﬁle system, a<br>variety of query-optimizing transformations, and actual evaluation of queries.<br>13.1<br>Overview<br>The steps involved in processing a query appear in Figure 13.1. The basic steps are<br>1. Parsing and translation<br>2. Optimization<br>3. Evaluation<br>Before query processing can begin, the system must translate the query into a us-<br>able form. A language such as SQL is suitable for human use, but is ill-suited to be<br>the system’s internal representation of a query. A more useful internal representation<br>is one based on the extended relational algebra.<br>Thus, the ﬁrst action the system must take in query processing is to translate a<br>given query into its internal form. This translation process is similar to the work<br>performed by the parser of a compiler. In generating the internal form of the query,<br>the parser checks the syntax of the user’s query, veriﬁes that the relation names ap-<br>pearing in the query are names of the relations in the database, and so on. The sys-<br>tem constructs a parse-tree representation of the query, which it then translates into<br>a relational-algebra expression. If the query was expressed in terms of a view, the<br>translation phase also replaces all uses of the view by the relational-algebra expres-<br>493<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>IV. Data Storage and <br>Querying<br>13. Query Processing<br>495<br>© The McGraw−Hill <br>Companies, 2001<br>494<br>Chapter 13<br>Query Processing<br>query<br>output<br>query<br>parser and<br>translator<br>evaluation engine<br>relational algebra<br>expression<br>execution plan<br>optimizer<br>data<br>statistics<br>about data<br>Figure 13.1<br>Steps in query processing.<br>sion that deﬁnes the view.1 Most compiler texts cover parsing (see the bibliographical<br>notes).<br>Given a query, there are generally a variety of methods for computing the answer.<br>For example, we have seen that, in SQL, a query could be expressed in several differ-<br>ent ways. Each SQL query can itself be translated into a relational-algebra expression<br>in one of several ways. Furthermore, the relational-algebra representation of a query<br>speciﬁes only partially how to evaluate a query; there are usually several ways to<br>evaluate relational-algebra expressions. As an illustration, consider the query<br>select balance<br>from account<br>where balance &lt; 2500<br>This query can be translated into either of the following relational-algebra expres-<br>sions:<br>• σbalance&lt;2500 (Πbalance (account))<br>• Πbalance (σbalance&lt;2500 (account))<br>Further, we can execute each relational-algebra operation by one of several dif-<br>ferent algorithms. For example, to implement the preceding selection, we can search<br>every tuple in account to ﬁnd tuples with balance less than 2500. If a B+-tree index is<br>available on the attribute balance, we can use the index instead to locate the tuples.<br>To specify fully how to evaluate a query, we need not only to provide the relational-<br>algebra expression, but also to annotate it with instructions specifying how to eval-<br>1.<br>For materialized views, the expression deﬁning the view has already been evaluated and stored. There-<br>fore, the stored relation can be used, instead of uses of the view being replaced by the expression deﬁning<br>the view. Recursive views are handled differently, via a ﬁxed-point procedure, as discussed in Section 5.2.6.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>IV. Data Storage and <br>Querying<br>13. Query Processing<br>496<br>© The McGraw−Hill <br>Companies, 2001<br>13.2<br>Measures of Query Cost<br>495<br>Π balance<br>σ balance &lt; 2500; use index 1<br>account<br>Figure 13.2<br>A query-evaluation plan.<br>uate each operation. Annotations may state the algorithm to be used for a speciﬁc<br>operation, or the particular index or indices to use. A relational-algebra operation<br>annotated with instructions on how to evaluate it is called an evaluation primitive.<br>A sequence of primitive operations that can be used to evaluate a query is a query-<br>execution plan or query-evaluation plan. Figure 13.2 illustrates an evaluation plan<br>for our example query, in which a particular index (denoted in the ﬁgure as “in-<br>dex 1”) is speciﬁed for the selection operation. The query-execution engine takes a<br>query-evaluation plan, executes that plan, and returns the answers to the query.<br>The different evaluation plans for a given query can have different costs. We do not<br>expect users to write their queries in a way that suggests the most efﬁcient evaluation<br>plan. Rather, it is the responsibility of the system to construct a query-evaluation plan<br>that minimizes the cost of query evaluation. Chapter 14 describes query optimization<br>in detail.<br>Once the query plan is chosen, the query is evaluated with that plan, and the result<br>of the query is output.<br>The sequence of steps already described for processing a query is representa-<br>tive; not all databases exactly follow those steps. For instance, instead of using the<br>relational-algebra representation, several databases use an annotated parse-tree rep-<br>resentation based on the structure of the given SQL query. However, the concepts that<br>we describe here form the basis of query processing in databases.<br>In order to optimize a query, a query optimizer must know the cost of each oper-<br>ation. Although the exact cost is hard to compute, since it depends on many param-<br>eters such as actual memory available to the operation, it is possible to get a rough<br>estimate of execution cost for each operation.<br>Section 13.2 outlines how we measure the cost of a query. Sections 13.3 through<br>13.6 cover the evaluation of individual relational-algebra operations. Several opera-<br>tions may be grouped together into a pipeline, in which each of the operations starts<br>working on its input tuples even as they are being generated by another operation.<br>In Section 13.7, we examine how to coordinate the execution of multiple operations<br>in a query evaluation plan, in particular, how to use pipelined operations to avoid<br>writing intermediate results to disk.<br>13.2<br>Measures of Query Cost<br>The cost of query evaluation can be measured in terms of a number of different re-<br>sources, including disk accesses, CPU time to execute a query, and, in a distributed<br>or parallel database system, the cost of communication (which we discuss later, in<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>IV. Data Storage and <br>Querying<br>13. Query Processing<br>497<br>© The McGraw−Hill <br>Companies, 2001<br>496<br>Chapter 13<br>Query Processing<br>Chapters 19 and 20). The response time for a query-evaluation plan (that is, the clock<br>time required to execute the plan), assuming no other activity is going on on the com-<br>puter, would account for all these costs, and could be used as a good measure of the<br>cost of the plan.<br>In large database systems, however, disk accesses (which we measure as the num-<br>ber of transfers of blocks from disk) are usually the most important cost, since disk ac-<br>cesses are slow compared to in-memory operations. Moreover, CPU speeds have been<br>improving much faster than have disk speeds. Thus, it is likely that the time spent in<br>disk activity will continue to dominate the total time to execute a query. Finally, esti-<br>mating the CPU time is relatively hard, compared to estimating the disk-access cost.<br>Therefore, most people consider the disk-access cost a reasonable measure of the cost<br>of a query-evaluation plan.<br>We use the number of block transfers from disk as a measure of the actual cost. To<br>simplify our computation of disk-access cost, we assume that all t</span><br><br><span style="background-color: #BDB2FF;" title="Chunk 62 | Start: 1240124 | End: 1260124 | Tokens: 3504">ransfers of blocks<br>have the same cost. This assumption ignores the variance arising from rotational<br>latency (waiting for the desired data to spin under the read–write head) and seek<br>time (the time that it takes to move the head over the desired track or cylinder). To<br>get more precise numbers, we need to distinguish between sequential I/O, where<br>the blocks read are contiguous on disk, and random I/O, where the blocks are non-<br>contiguous, and an extra seek cost must be paid for each disk I/O operation. We also<br>need to distinguish between reads and writes of blocks, since it takes more time to<br>write a block to disk than to read a block from disk. A more accurate measure would<br>therefore estimate<br>1. The number of seek operations performed<br>2. The number of blocks read<br>3. The number of blocks written<br>and then add up these numbers after multiplying them by the average seek time,<br>average transfer time for reading a block, and average transfer time for writing a<br>block, respectively. Real-life query optimizers also take CPU costs into account when<br>computing the cost of an operation. For simplicity we ignore these details, and leave<br>it to you to work out more precise cost estimates for various operations.<br>The cost estimates we give ignore the cost of writing the ﬁnal result of an operation<br>back to disk. These are taken into account separately where required. The costs of all<br>the algorithms that we consider depend on the size of the buffer in main memory.<br>In the best case, all data can be read into the buffers, and the disk does not need<br>to be accessed again. In the worst case, we assume that the buffer can hold only<br>a few blocks of data—approximately one block per relation. When presenting cost<br>estimates, we generally assume the worst case.<br>13.3<br>Selection Operation<br>In query processing, the ﬁle scan is the lowest-level operator to access data. File scans<br>are search algorithms that locate and retrieve records that fulﬁll a selection condition.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>IV. Data Storage and <br>Querying<br>13. Query Processing<br>498<br>© The McGraw−Hill <br>Companies, 2001<br>13.3<br>Selection Operation<br>497<br>In relational systems, a ﬁle scan allows an entire relation to be read in those cases<br>where the relation is stored in a single, dedicated ﬁle.<br>13.3.1<br>Basic Algorithms<br>Consider a selection operation on a relation whose tuples are stored together in one<br>ﬁle. Two scan algorithms to implement the selection operation are:<br>• A1 (linear search). In a linear search, the system scans each ﬁle block and tests<br>all records to see whether they satisfy the selection condition. For a selection<br>on a key attribute, the system can terminate the scan if the required record is<br>found, without looking at the other records of the relation.<br>The cost of linear search, in terms of number of I/O operations, is br, where<br>br denotes the number of blocks in the ﬁle. Selections on key attributes have<br>an average cost of br/2, but still have a worst-case cost of br.<br>Although it may be slower than other algorithms for implementing selec-<br>tion, the linear search algorithm can be applied to any ﬁle, regardless of the<br>ordering of the ﬁle, or the availability of indices, or the nature of the selection<br>operation. The other algorithms that we shall study are not applicable in all<br>cases, but when applicable they are generally faster than linear search.<br>• A2 (binary search). If the ﬁle is ordered on an attribute, and the selection con-<br>dition is an equality comparison on the attribute, we can use a binary search to<br>locate records that satisfy the selection. The system performs the binary search<br>on the blocks of the ﬁle.<br>The number of blocks that need to be examined to ﬁnd a block containing<br>the required records is ⌈log2(br)⌉, where br denotes the number of blocks in<br>the ﬁle. If the selection is on a nonkey attribute, more than one block may<br>contain required records, and the cost of reading the extra blocks has to be<br>added to the cost estimate. We can estimate this number by estimating the<br>size of the selection result (which we cover in Section 14.2), and dividing it by<br>the average number of records that are stored per block of the relation.<br>13.3.2<br>Selections Using Indices<br>Index structures are referred to as access paths, since they provide a path through<br>which data can be located and accessed. In Chapter 12, we pointed out that it is<br>efﬁcient to read the records of a ﬁle in an order corresponding closely to physical<br>order. Recall that a primary index is an index that allows the records of a ﬁle to be read<br>in an order that corresponds to the physical order in the ﬁle. An index that is not a<br>primary index is called a secondary index.<br>Search algorithms that use an index are referred to as index scans. Ordered indices,<br>such as B+-trees, also permit access to tuples in a sorted order, which is useful for<br>implementing range queries. Although indices can provide fast, direct, and ordered<br>access, they impose the overhead of access to those blocks containing the index. We<br>use the selection predicate to guide us in the choice of the index to use in processing<br>the query. Search algorithms that use an index are:<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>IV. Data Storage and <br>Querying<br>13. Query Processing<br>499<br>© The McGraw−Hill <br>Companies, 2001<br>498<br>Chapter 13<br>Query Processing<br>• A3 (primary index, equality on key). For an equality comparison on a key<br>attribute with a primary index, we can use the index to retrieve a single record<br>that satisﬁes the corresponding equality condition.<br>If a B+-tree is used, the cost of the operation, in terms of I/O operations, is<br>equal to the height of the tree plus one I/O to fetch the record.<br>• A4 (primary index, equality on nonkey). We can retrieve multiple records<br>by using a primary index when the selection condition speciﬁes an equality<br>comparison on a nonkey attribute, A. The only difference from the previous<br>case is that multiple records may need to be fetched. However, the records<br>would be stored consecutively in the ﬁle since the ﬁle is sorted on the search<br>key.<br>The cost of the operation is proportional to the height of the tree, plus the<br>number of blocks containing records with the speciﬁed search key.<br>• A5 (secondary index, equality). Selections specifying an equality condition<br>can use a secondary index. This strategy can retrieve a single record if the<br>equality condition is on a key; multiple records may get retrieved if the index-<br>ing ﬁeld is not a key.<br>In the ﬁrst case, only one record is retrieved, and the cost is equal to the<br>height of the tree plus one I/O operation to fetch the record. In the second<br>case each record may be resident on a different block, which may result in one<br>I/O operation per retrieved record. The cost could become even worse than<br>that of linear search if a large number of records are retrieved.<br>If B+-tree ﬁle organizations are used to store relations, records may be moved<br>between blocks when leaf nodes are split or merged, and when records are redis-<br>tributed. If secondary indices store pointers to records’ physical location, the pointers<br>will have to be updated when records are moved. In some systems, such as Com-<br>paq’s Non-Stop SQL System, the secondary indices instead store the key value in<br>the B+-tree ﬁle organization. Accessing a record through a secondary index is then<br>even more expensive since a search has to be performed on the B+-tree used in the<br>ﬁle organization. The cost formulae described for secondary indices will have to be<br>modiﬁed appropriately if such indices are used.<br>13.3.3<br>Selections Involving Comparisons<br>Consider a selection of the form σA≤v(r). We can implement the selection either by<br>using a linear or binary search or by using indices in one of the following ways:<br>• A6 (primary index, comparison). A primary ordered index (for example, a<br>primary B+-tree index) can be used when the selection condition is a compar-<br>ison. For comparison conditions of the form A &gt; v or A ≥v, a primary index<br>on A can be used to direct the retrieval of tuples, as follows. For A ≥v, we<br>look up the value v in the index to ﬁnd the ﬁrst tuple in the ﬁle that has a value<br>of A = v. A ﬁle scan starting from that tuple up to the end of the ﬁle returns<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>IV. Data Storage and <br>Querying<br>13. Query Processing<br>500<br>© The McGraw−Hill <br>Companies, 2001<br>13.3<br>Selection Operation<br>499<br>all tuples that satisfy the condition. For A &gt; v, the ﬁle scan starts with the ﬁrst<br>tuple such that A &gt; v.<br>For comparisons of the form A &lt; v or A ≤v, an index lookup is not re-<br>quired. For A &lt; v, we use a simple ﬁle scan starting from the beginning of<br>the ﬁle, and continuing up to (but not including) the ﬁrst tuple with attribute<br>A = v. The case of A ≤v is similar, except that the scan continues up to (but<br>not including) the ﬁrst tuple with attribute A &gt; v. In either case, the index is<br>not useful.<br>• A7 (secondary index, comparison). We can use a secondary ordered index to<br>guide retrieval for comparison conditions involving &lt;, ≤, ≥, or &gt;. The lowest-<br>level index blocks are scanned, either from the smallest value up to v (for &lt;<br>and ≤), or from v up to the maximum value (for &gt; and ≥).<br>The secondary index provides pointers to the records, but to get the actual<br>records we have to fetch the records by using the pointers. This step may re-<br>quire an I/O operation for each record fetched, since consecutive records may<br>be on different disk blocks. If the number of retrieved records is large, using<br>the secondary index may be even more expensive than using linear search.<br>Therefore the secondary index should be used only if very few records are<br>selected.<br>13.3.4<br>Implementation of Complex Selections<br>So far, we have considered only simple selection conditions of the form A op B, where<br>op is an equality or comparison operation. We now consider more complex selection<br>predicates.<br>• Conjunction: A conjunctive selection is a selection of the form<br>σθ1∧θ2∧···∧θn(r)<br>• Disjunction: A disjunctive selection is a selection of the form<br>σθ1∨θ2∨···∨θn(r)<br>A disjunctive condition is satisﬁed by the union of all records satisfying the<br>individual, simple conditions θi.<br>• Negation: The result of a selection σ¬θ(r) is the set of tuples of r for which the<br>condition θ evaluates to false. In the absence of nulls, this set is simply the set<br>of tuples that are not in σθ(r).<br>We can implement a selection operation involving either a conjunction or a dis-<br>junction of simple conditions by using one of the following algorithms:<br>• A8 (conjunctive selection using one index). We ﬁrst determine whether an<br>access path is available for an attribute in one of the simple conditions. If one<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>IV. Data Storage and <br>Querying<br>13. Query Processing<br>501<br>© The McGraw−Hill <br>Companies, 2001<br>500<br>Chapter 13<br>Query Processing<br>is, one of the selection algorithms A2 through A7 can retrieve records satis-<br>fying that condition. We complete the operation by testing, in the memory<br>buffer, whether or not each retrieved record satisﬁes the remaining simple<br>conditions.<br>To reduce the cost, we choose a θi and one of algorithms A1 through A7 for<br>which the combination results in the least cost for σθi(r). The cost of algorithm<br>A8 is given by the cost of the chosen algorithm.<br>• A9 (conjunctive selection using composite index). An appropriate compos-<br>ite index (that is, an index on multiple attributes) may be available for some<br>conjunctive selections. If the selection speciﬁes an equality condition on two<br>or more attributes, and a composite index exists on these combined attribute<br>ﬁelds, then the index can be searched directly. The type of index determines<br>which of algorithms A3, A4, or A5 will be used.<br>• A10 (conjunctive selection by intersection of identiﬁers). Another alterna-<br>tive for implementing conjunctive selection operations involves the use of<br>record pointers or record identiﬁers. This algorithm requires indices with<br>record pointers, on the ﬁelds involved in the individual conditions. The algo-<br>rithm scans each index for pointers to tuples that satisfy an individual condi-<br>tion. The intersection of all the retrieved pointers is the set of pointers to tuples<br>that satisfy the conjunctive condition. The algorithm then uses the pointers to<br>retrieve the actual records. If indices are not available on all the individual<br>conditions, then the algorithm tests the retrieved records against the remain-<br>ing conditions.<br>The cost of algorithm A10 is the sum of the costs of the individual index<br>scans, plus the cost of retrieving the records in the intersection of the retrieved<br>lists of pointers. This cost can be reduced by sorting the list of pointers and<br>retrieving records in the sorted order. Thereby, (1) all pointers to records in a<br>block come together, hence all selected records in the block can be retrieved<br>using a single I/O operation, and (2) blocks are read in sorted order, minimiz-<br>ing disk arm movement. Section 13.4 describes sorting algorithms.<br>• A11 (disjunctive selection by union of identiﬁers). If access paths are avail-<br>able on all the conditions of a disjunctive selection, each index is scanned for<br>pointers to tuples that satisfy the individual condition. The union of all the<br>retrieved pointers yields the set of pointers to all tuples that satisfy the dis-<br>junctive condition. We then use the pointers to retrieve the actual records.<br>However, if even one of the conditions does not have an access path, we<br>will have to perform a linear scan of the relation to ﬁnd tuples that satisfy the<br>condition. Therefore, if there is even one such condition in the disjunct, the<br>most efﬁcient access method is a linear scan, with the disjunctive condition<br>tested on each tuple during the scan.<br>The implementation of selections with negation conditions is left to you as an ex-<br>ercise (Exercise 13.10).<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>IV. Data Storage and <br>Querying<br>13. Query Processing<br>502<br>© The McGraw−Hill <br>Companies, 2001<br>13.4<br>Sorting<br>501<br>13.4<br>Sorting<br>Sorting of data plays an important role in database systems for two reasons. First,<br>SQL queries can specify that the output be sorted. Second, and equally important for<br>query processing, several of the relational operations, such as joins, can be imple-<br>mented efﬁciently if the input relations are ﬁrst sorted. Thus, we discuss sorting here<br>before discussing the join operation in Section 13.5.<br>We can sort a relation by building an index on the sort key, and then using that<br>index to read the relation in sorted order. However, such a process orders the relation<br>only logically, through an index, rather than physically. Hence, the reading of tuples<br>in the sorted order may lead to a disk access for each record, which can be very<br>expensive, since the number of records can be much larger than the number of blocks.<br>For this reason, it may be desirable to order the records physically.<br>The problem of sorting has been studied extensively, both for relations that ﬁt<br>entirely in main memory, and for relations that are bigger than memory. In the ﬁrst<br>case, standard sorting techniques such as quick-sort can be used. Here, we discuss<br>how to handle the second case.<br>Sorting of relations that do not ﬁt in memory is called external sorting. The most<br>commonly used technique for external sorting is the external sort–merge algorithm.<br>We describe the external sort–merge algorithm next. Let M denote the number of<br>page frames in the main-memory buffer (the number of disk blocks whose contents<br>can be buffered in main memory).<br>1. In the ﬁrst stage, a number of sorted runs are created; each run is sorted, but<br>contains only some of the records of the relation.<br>i = 0;<br>repeat<br>read M blocks of the relation, or the rest of the relation,<br>whichever is smaller;<br>sort the in-memory part of the relation;<br>write the sorted data to run ﬁle Ri;<br>i = i + 1;<br>until the end of the relation<br>2. In the second stage, the runs are merged. Suppose, for now, that the total num-<br>ber of runs, N, is less than M, so that we can allocate one page frame to each<br>run and have space left to hold one page of output. The merge stage operates<br>as follows:<br>read one block of each of the N ﬁles Ri into a buffer page in memory;<br>repeat<br>choose the ﬁrst tuple (in sort order) among all buffer pages;<br>write the tuple to the output, and delete it from the buffer page;<br>if the buffer page of any run Ri is empty and not end-of-ﬁle(Ri)<br>then read the next block of Ri into the buffer page;<br>until all buffer pages are empty<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>IV. Data Storage and <br>Querying<br>13. Query Processing<br>503<br>© The McGraw−Hill <br>Companies, 2001<br>502<br>Chapter 13<br>Query Processing<br>The output of the merge stage is the sorted relation. The output ﬁle is buffered<br>to reduce the number of disk write operations. The preceding merge operation is a<br>generalization of the two-way merge used by the standard in-memory sort–merge<br>algorithm; it merges N runs, so it is called an N-way merge.<br>In general, if the relation is much larger than memory, there may be M or more<br>runs generated in the ﬁrst stage, and it is not possible to allocate a page frame for each<br>run during the merge stage. In this case, the merge operation proceeds in multiple<br>passes. Since there is enough memory for M −1 input buffer pages, each merge can<br>take M −1 runs as input.<br>The initial pass functions in this way: It merges the ﬁrst M −1 runs (as described<br>in item 2 above) to get a single run for the next pass. Then, it merges the next M −1<br>runs similarly, and so on, until it has processed all the initial runs. At this point, the<br>number of runs has been reduced by a factor of M −1. If this reduced number of runs<br>is still greater than or equal to M, another pass is made, with the runs created by the<br>ﬁrst pass as input. Each pass reduces the number of runs by a factor of M −1. The<br>passes repeat as many times as required, until the number of runs is less than M; a<br>ﬁnal pass then generates the sorted output.<br>Figure 13.3 illustrates the steps of the external sort–merge for an example relation.<br>For illustration purposes, we assume that only one tuple ﬁts in a block (fr = 1), and<br>we assume that memory holds at most three page frames. During the merge stage,<br>two page frames are used for input and one for output.<br>g<br>a   <br>d   31<br>c    33<br>b   14<br>e   16<br>r   16<br>d   21<br>m    3<br>p     2<br>d     7<br>a   14<br>a    14<br>a    19<br>b    14<br>c    33<br>d     7<br>d    21<br>d    31<br>e    16<br>g    24<br>m    3<br>p     2<br>r    16<br>a    19<br>b    14<br>c    33<br>d    31<br>e    16<br>g    24<br>a    14<br>d     7<br>d    21<br>m    3<br>p     2<br>r    16<br>a   19<br>d   31<br>g   24<br>b   14<br>c   33<br>e   16<br>d   21<br>m    3<br>r    16<br>a    14<br>d     7<br>p     2<br>initial<br>relation<br>create<br>runs<br>merge<br>pass–1<br>merge<br>pass–2<br>runs<br>runs<br>sorted<br>output<br>24<br>24<br>Figure 13.3<br>External sorting using sort–merge.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>IV. Data Storage and <br>Querying<br>13. Query Processing<br>504<br>© The McGraw−Hill <br>Companies, 2001<br>13.5<br>Join Operation<br>503<br>We compute how many block transfers are required for the external sort merge<br>in this way: Let br denote the number of blocks containing records of relation r. The<br>ﬁrst stage reads every block of the relation and writes them out again, giving a total<br>of 2br disk accesses. The initial number of runs is ⌈br/M⌉. Since the number of runs<br>decreases by a factor of M −1 in each merge pass, the total number of merge passes<br>required is ⌈logM−1(br/M)⌉. Each of these passes reads every block of the relation<br>once and writes it out once, with two exceptions. First, the ﬁnal pass can produce<br>the sorted output without writing its result to disk. Second, there may be runs that<br>are not read in or written out during a pass—for example, if there are M runs to<br>be merged in a pass, M −1 are read in and merged, and one run is not accessed<br>during the pass. Ignoring the (relatively small) savings due to the latter effect, the</span><br><br><span style="background-color: #FFC6FF;" title="Chunk 63 | Start: 1260126 | End: 1280126 | Tokens: 3632"><br>total number of disk accesses for external sorting of the relation is<br>br(2⌈logM−1(br/M)⌉+ 1)<br>Applying this equation to the example in Figure 13.3, we get a total of 12∗(4+1) =<br>60 block transfers, as you can verify from the ﬁgure. Note that this value does not<br>include the cost of writing out the ﬁnal result.<br>13.5<br>Join Operation<br>In this section, we study several algorithms for computing the join of relations, and<br>we analyze their respective costs.<br>We use the term equi-join to refer to a join of the form r<br>r.A=s.B s, where A and<br>B are attributes or sets of attributes of relations r and s respectively.<br>We use as a running example the expression<br>depositor<br> customer<br>We assume the following information about the two relations:<br>• Number of records of customer: ncustomer = 10, 000.<br>• Number of blocks of customer: bcustomer = 400.<br>• Number of records of depositor: ndepositor = 5000.<br>• Number of blocks of depositor: bdepositor = 100.<br>13.5.1<br>Nested-Loop Join<br>Figure 13.4 shows a simple algorithm to compute the theta join, r<br>θ s, of two rela-<br>tions r and s. This algorithm is called the nested-loop join algorithm, since it basi-<br>cally consists of a pair of nested for loops. Relation r is called the outer relation and<br>relation s the inner relation of the join, since the loop for r encloses the loop for s.<br>The algorithm uses the notation tr · ts, where tr and ts are tuples; tr · ts denotes the<br>tuple constructed by concatenating the attribute values of tuples tr and ts.<br>Like the linear ﬁle-scan algorithm for selection, the nested-loop join algorithm re-<br>quires no indices, and it can be used regardless of what the join condition is. Extend-<br>ing the algorithm to compute the natural join is straightforward, since the natural<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>IV. Data Storage and <br>Querying<br>13. Query Processing<br>505<br>© The McGraw−Hill <br>Companies, 2001<br>504<br>Chapter 13<br>Query Processing<br>for each tuple tr in r do begin<br>for each tuple ts in s do begin<br>test pair (tr, ts) to see if they satisfy the join condition θ<br>if they do, add tr · ts to the result.<br>end<br>end<br>Figure 13.4<br>Nested-loop join.<br>join can be expressed as a theta join followed by elimination of repeated attributes by<br>a projection. The only change required is an extra step of deleting repeated attributes<br>from the tuple tr · ts, before adding it to the result.<br>The nested-loop join algorithm is expensive, since it examines every pair of tuples<br>in the two relations. Consider the cost of the nested-loop join algorithm. The number<br>of pairs of tuples to be considered is nr ∗ns, where nr denotes the number of tuples in<br>r, and ns denotes the number of tuples in s. For each record in r, we have to perform<br>a complete scan on s. In the worst case, the buffer can hold only one block of each<br>relation, and a total of nr ∗bs + br block accesses would be required, where br and<br>bs denote the number of blocks containing tuples of r and s respectively. In the best<br>case, there is enough space for both relations to ﬁt simultaneously in memory, so each<br>block would have to be read only once; hence, only br + bs block accesses would be<br>required.<br>If one of the relations ﬁts entirely in main memory, it is beneﬁcial to use that re-<br>lation as the inner relation, since the inner relation would then be read only once.<br>Therefore, if s is small enough to ﬁt in main memory, our strategy requires only a<br>total br + bs accesses—the same cost as that for the case where both relations ﬁt in<br>memory.<br>Now consider the natural join of depositor and customer. Assume for now that we<br>have no indices whatsoever on either relation, and that we are not willing to create<br>any index. We can use the nested loops to compute the join; assume that depositor<br>is the outer relation and customer is the inner relation in the join. We will have to<br>examine 5000 ∗10000 = 50 ∗106 pairs of tuples. In the worst case, the number of<br>block accesses is 5000 ∗400 + 100 = 2,000,100. In the best-case scenario, however, we<br>can read both relations only once, and perform the computation. This computation<br>requires at most 100+400 = 500 block accesses—a signiﬁcant improvement over the<br>worst-case scenario. If we had used customer as the relation for the outer loop and<br>depositor for the inner loop, the worst-case cost of our ﬁnal strategy would have been<br>lower: 10000 ∗100 + 400 = 1,000,400.<br>13.5.2<br>Block Nested-Loop Join<br>If the buffer is too small to hold either relation entirely in memory, we can still ob-<br>tain a major saving in block accesses if we process the relations on a per-block basis,<br>rather than on a per-tuple basis. Figure 13.5 shows block nested-loop join, which is<br>a variant of the nested-loop join where every block of the inner relation is paired with<br>every block of the outer relation. Within each pair of blocks, every tuple in one block<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>IV. Data Storage and <br>Querying<br>13. Query Processing<br>506<br>© The McGraw−Hill <br>Companies, 2001<br>13.5<br>Join Operation<br>505<br>for each block Br of r do begin<br>for each block Bs of s do begin<br>for each tuple tr in Br do begin<br>for each tuple ts in Bs do begin<br>test pair (tr, ts) to see if they satisfy the join condition<br>if they do, add tr · ts to the result.<br>end<br>end<br>end<br>end<br>Figure 13.5<br>Block nested-loop join.<br>is paired with every tuple in the other block, to generate all pairs of tuples. As before,<br>all pairs of tuples that satisfy the join condition are added to the result.<br>The primary difference in cost between the block nested-loop join and the basic<br>nested-loop join is that, in the worst case, each block in the inner relation s is read<br>only once for each block in the outer relation, instead of once for each tuple in the<br>outer relation. Thus, in the worst case, there will be a total of br ∗bs + br block ac-<br>cesses, where br and bs denote the number of blocks containing records of r and s<br>respectively. Clearly, it is more efﬁcient to use the smaller relation as the outer re-<br>lation, in case neither of the relations ﬁts in memory. In the best case, there will be<br>br + bs block accesses.<br>Now return to our example of computing depositor<br> customer, using the block<br>nested-loop join algorithm. In the worst case we have to read each block of customer<br>once for each block of depositor. Thus, in the worst case, a total of 100 ∗400 + 100 =<br>40, 100 block accesses are required. This cost is a signiﬁcant improvement over the<br>5000 ∗400 + 100 = 2, 000, 100 block accesses needed in the worst case for the basic<br>nested-loop join. The number of block accesses in the best case remains the same—<br>namely, 100 + 400 = 500.<br>The performance of the nested-loop and block nested-loop procedures can be fur-<br>ther improved:<br>• If the join attributes in a natural join or an equi-join form a key on the inner<br>relation, then for each outer relation tuple the inner loop can terminate as soon<br>as the ﬁrst match is found.<br>• In the block nested-loop algorithm, instead of using disk blocks as the block-<br>ing unit for the outer relation, we can use the biggest size that can ﬁt in mem-<br>ory, while leaving enough space for the buffers of the inner relation and the<br>output. In other words, if memory has M blocks, we read in M −2 blocks<br>of the outer relation at a time, and when we read each block of the inner re-<br>lation we join it with all the M −2 blocks of the outer relation. This change<br>reduces the number of scans of the inner relation from br to ⌈br/(M −2)⌉,<br>where br is the number of blocks of the outer relation. The total cost is then<br>⌈br/(M −2)⌉∗bs + br.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>IV. Data Storage and <br>Querying<br>13. Query Processing<br>507<br>© The McGraw−Hill <br>Companies, 2001<br>506<br>Chapter 13<br>Query Processing<br>• We can scan the inner loop alternately forward and backward. This scanning<br>method orders the requests for disk blocks so that the data remaining in the<br>buffer from the previous scan can be reused, thus reducing the number of disk<br>accesses needed.<br>• If an index is available on the inner loop’s join attribute, we can replace ﬁle<br>scans with more efﬁcient index lookups. Section 13.5.3 describes this optimiza-<br>tion.<br>13.5.3<br>Indexed Nested-Loop Join<br>In a nested-loop join (Figure 13.4), if an index is available on the inner loop’s join<br>attribute, index lookups can replace ﬁle scans. For each tuple tr in the outer relation<br>r, the index is used to look up tuples in s that will satisfy the join condition with<br>tuple tr.<br>This join method is called an indexed nested-loop join; it can be used with existing<br>indices, as well as with temporary indices created for the sole purpose of evaluating<br>the join.<br>Looking up tuples in s that will satisfy the join conditions with a given tuple tr is<br>essentially a selection on s. For example, consider depositor<br> customer. Suppose that<br>we have a depositor tuple with customer-name “John”. Then, the relevant tuples in s<br>are those that satisfy the selection “customer-name = John”.<br>The cost of an indexed nested-loop join can be computed as follows. For each tuple<br>in the outer relation r, a lookup is performed on the index for s, and the relevant<br>tuples are retrieved. In the worst case, there is space in the buffer for only one page<br>of r and one page of the index. Then, br disk accesses are needed to read relation<br>r, where br denotes the number of blocks containing records of r. For each tuple in<br>r, we perform an index lookup on s. Then, the cost of the join can be computed as<br>br + nr ∗c, where nr is the number of records in relation r, and c is the cost of a single<br>selection on s using the join condition. We have seen in Section 13.3 how to estimate<br>the cost of a single selection algorithm (possibly using indices); that estimate gives us<br>the value of c.<br>The cost formula indicates that, if indices are available on both relations r and s, it<br>is generally most efﬁcient to use the one with fewer tuples as the outer relation.<br>For example, consider an indexed nested-loop join of depositor<br> customer, with<br>depositor as the outer relation. Suppose also that customer has a primary B+-tree index<br>on the join attribute customer-name, which contains 20 entries on an average in each<br>index node. Since customer has 10,000 tuples, the height of the tree is 4, and one more<br>access is needed to ﬁnd the actual data. Since ndepositor is 5000, the total cost is 100 +<br>5000 ∗5 = 25, 100 disk accesses. This cost is lower than the 40, 100 accesses needed<br>for a block nested-loop join.<br>13.5.4<br>Merge Join<br>The merge join algorithm (also called the sort–merge join algorithm) can be used<br>to compute natural joins and equi-joins. Let r(R) and s(S) be the relations whose<br>natural join is to be computed, and let R∩S denote their common attributes. Suppose<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>IV. Data Storage and <br>Querying<br>13. Query Processing<br>508<br>© The McGraw−Hill <br>Companies, 2001<br>13.5<br>Join Operation<br>507<br>pr := address of ﬁrst tuple of r;<br>ps := address of ﬁrst tuple of s;<br>while (ps ̸= null and pr ̸= null) do<br>begin<br>ts := tuple to which ps points;<br>Ss := {ts};<br>set ps to point to next tuple of s;<br>done := false;<br>while (not done and ps ̸= null) do<br>begin<br>ts′ := tuple to which ps points;<br>if (ts′[JoinAttrs] = ts[JoinAttrs])<br>then begin<br>Ss := Ss ∪{ts′};<br>set ps to point to next tuple of s;<br>end<br>else done := true;<br>end<br>tr := tuple to which pr points;<br>while (pr ̸= null and tr[JoinAttrs] &lt; ts[JoinAttrs]) do<br>begin<br>set pr to point to next tuple of r;<br>tr := tuple to which pr points;<br>end<br>while (pr ̸= null and tr[JoinAttrs] = ts[JoinAttrs]) do<br>begin<br>for each ts in Ss do<br>begin<br>add ts<br> tr to result ;<br>end<br>set pr to point to next tuple of r;<br>tr := tuple to which pr points;<br>end<br>end.<br>Figure 13.6<br>Merge join.<br>that both relations are sorted on the attributes R∩S. Then, their join can be computed<br>by a process much like the merge stage in the merge–sort algorithm.<br>Figure 13.6 shows the merge join algorithm. In the algorithm, JoinAttrs refers to the<br>attributes in R ∩S, and tr<br> ts, where tr and ts are tuples that have the same values<br>for JoinAttrs, denotes the concatenation of the attributes of the tuples, followed by<br>projecting out repeated attributes. The merge join algorithm associates one pointer<br>with each relation. These pointers point initially to the ﬁrst tuple of the respective<br>relations. As the algorithm proceeds, the pointers move through the relation. A group<br>of tuples of one relation with the same value on the join attributes is read into Ss.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>IV. Data Storage and <br>Querying<br>13. Query Processing<br>509<br>© The McGraw−Hill <br>Companies, 2001<br>508<br>Chapter 13<br>Query Processing<br>The algorithm in Figure 13.6 requires that every set of tuples Ss ﬁt in main memory;<br>we shall look at extensions of the algorithm to avoid this requirement later in this<br>section. Then, the corresponding tuples (if any) of the other relation are read in, and<br>are processed as they are read.<br>Figure 13.7 shows two relations that are sorted on their join attribute a1. It is in-<br>structive to go through the steps of the merge join algorithm on the relations shown<br>in the ﬁgure.<br>Since the relations are in sorted order, tuples with the same value on the join at-<br>tributes are in consecutive order. Thereby, each tuple in the sorted order needs to be<br>read only once, and, as a result, each block is also read only once. Since it makes only<br>a single pass through both ﬁles, the merge join method is efﬁcient; the number of<br>block accesses is equal to the sum of the number of blocks in both ﬁles, br + bs.<br>If either of the input relations r and s is not sorted on the join attributes, they can be<br>sorted ﬁrst, and then the merge join algorithm can be used. The merge join algorithm<br>can also be easily extended from natural joins to the more general case of equi-joins.<br>Suppose the merge join scheme is applied to our example of depositor<br> customer.<br>The join attribute here is customer-name. Suppose that the relations are already sorted<br>on the join attribute customer-name. In this case, the merge join takes a total of 400 +<br>100 = 500 block accesses. Suppose the relations are not sorted, and the memory size<br>is the worst case of three blocks. Sorting customer takes 400 ∗(2⌈log2(400/3)⌉+ 1), or<br>6800, block transfers, with 400 more transfers to write out the result. Similarly, sorting<br>depositor takes 100∗(2⌈log2(100/3)⌉+1), or 1300, transfers, with 100 more transfers to<br>write it out. Thus, the total cost is 9100 block transfers if the relations are not sorted,<br>and the memory size is just 3 blocks.<br>With a memory size of 25 blocks, sorting the relation customer takes a total of just<br>400 ∗(2⌈log24(400/25) + 1) = 1200 block transfers, while sorting depositor takes 300<br>block transfers. Adding the cost of writing out the sorted results and reading them<br>back gives a total cost of 2500 block transfers if the relations are not sorted and the<br>memory size is 25 blocks.<br>As mentioned earlier, the merge join algorithm of Figure 13.6 requires that the set<br>Ss of all tuples with the same value for the join attributes must ﬁt in main memory.<br>a     3<br>b     1<br>d     8<br>d   13<br>f      7<br>m    5<br>q     6<br>a    A<br>b    G<br>c     L<br>d    N<br>m   B<br>a1   a2 <br>a1   a3<br>pr<br>ps<br>r<br>s<br>Figure 13.7<br>Sorted relations for merge join.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>IV. Data Storage and <br>Querying<br>13. Query Processing<br>510<br>© The McGraw−Hill <br>Companies, 2001<br>13.5<br>Join Operation<br>509<br>This requirement can usually be met, even if the relation s is large. If it cannot be met,<br>a block nested-loop join must be performed between Ss and the tuples in r with the<br>same values for the join attributes. The overall cost of the merge join increases as a<br>result.<br>It is also possible to perform a variation of the merge join operation on unsorted tu-<br>ples, if secondary indices exist on both join attributes. The algorithm scans the records<br>through the indices, resulting in their being retrieved in sorted order. This variation<br>presents a signiﬁcant drawback, however, since records may be scattered throughout<br>the ﬁle blocks. Hence, each tuple access could involve accessing a disk block, and<br>that is costly.<br>To avoid this cost, we can use a hybrid merge–join technique, which combines<br>indices with merge join. Suppose that one of the relations is sorted; the other is un-<br>sorted, but has a secondary B+-tree index on the join attributes. The hybrid merge–<br>join algorithm merges the sorted relation with the leaf entries of the secondary B+-<br>tree index. The result ﬁle contains tuples from the sorted relation and addresses for<br>tuples of the unsorted relation. The result ﬁle is then sorted on the addresses of tu-<br>ples of the unsorted relation, allowing efﬁcient retrieval of the corresponding tuples,<br>in physical storage order, to complete the join. Extensions of the technique to handle<br>two unsorted relations are left as an exercise for you.<br>13.5.5<br>Hash Join<br>Like the merge join algorithm, the hash join algorithm can be used to implement<br>natural joins and equi-joins. In the hash join algorithm, a hash function h is used to<br>partition tuples of both relations. The basic idea is to partition the tuples of each of<br>the relations into sets that have the same hash value on the join attributes.<br>We assume that<br>• h is a hash function mapping JoinAttrs values to {0, 1, . . . , nh}, where JoinAttrs<br>denotes the common attributes of r and s used in the natural join.<br>• Hr0, Hr1, . . . , Hrnh denote partitions of r tuples, each initially empty. Each tu-<br>ple tr ∈r is put in partition Hri, where i = h(tr[JoinAttrs]).<br>• Hs0, Hs1, ..., Hsnh denote partitions of s tuples, each initially empty. Each tuple<br>ts ∈s is put in partition Hsi, where i = h(ts[JoinAttrs]).<br>The hash function h should have the “goodness” properties of randomness and uni-<br>formity that we discussed in Chapter 12. Figure 13.8 depicts the partitioning of the<br>relations.<br>The idea behind the hash join algorithm is this: Suppose that an r tuple and an<br>s tuple satisfy the join condition; then, they will have the same value for the join<br>attributes. If that value is hashed to some value i, the r tuple has to be in Hri and the<br>s tuple in Hsi. Therefore, r tuples in Hri need only to be compared with s tuples in<br>Hsi; they do not need to be compared with s tuples in any other partition.<br>For example, if d is a tuple in depositor, c a tuple in customer, and h a hash function<br>on the customer-name attributes of the tuples, then d and c must be tested only if<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>IV. Data Storage and <br>Querying<br>13. Query Processing<br>511<br>© The McGraw−Hill <br>Companies, 2001<br>510<br>Chapter 13<br>Query Processing<br>0<br>1<br>2<br>3<br>4<br>0<br>1<br>2<br>3<br>4<br>r<br>s<br>.<br>.<br>.<br>.<br>.<br>.<br>.<br>.<br>partitions<br>of r<br>partitions<br>of s<br>Figure 13.8<br>Hash partitioning of relations.<br>h(c) = h(d). If h(c) ̸= h(d), then c and d must have different values for customer-name.<br>However, if h(c) = h(d), we must test c and d to see whether the values in their join<br>attributes are the same, since it is possible that c and d have different customer-names<br>that have the same hash value.<br>Figure 13.9 shows the details of the hash join algorithm to compute the natural<br>join of relations r and s. As in the merge join algorithm, tr<br> ts denotes the concate-<br>nation of the attributes of tuples tr and ts, followed by projecting out repeated at-<br>tributes. After the partitioning of the relations, the rest of the hash join code performs<br>a separate indexed nested-loop join on each of the partition pairs i, for i = 0, . . . , nh.<br>To do so, it ﬁrst builds a hash index on each Hsi, and then probes (that is, looks<br>up Hsi) with tuples from Hri. The relation s is the build input, and r is the probe<br>input.<br>The hash index on Hsi is built in memory, so there is no need to access the disk to<br>retrieve the tuples. The hash function used to build this hash index is different from<br>the hash function h used earlier, but is still applied to only the join attributes. In the<br>course of the indexed nested-loop join, t</span><br><br><span style="background-color: #FFADAD;" title="Chunk 64 | Start: 1280128 | End: 1300128 | Tokens: 3510">he system uses this hash index to retrieve<br>records that will match records in the probe input.<br>The build and probe phases require only a single pass through both the build<br>and probe inputs. It is straightforward to extend the hash join algorithm to compute<br>general equi-joins.<br>The value nh must be chosen to be large enough such that, for each i, the tuples<br>in the partition Hsi of the build relation, along with the hash index on the partition,<br>will ﬁt in memory. It is not necessary for the partitions of the probe relation to ﬁt in<br>memory. Clearly, it is best to use the smaller input relation as the build relation. If the<br>size of the build relation is bs blocks, then, for each of the nh partitions to be of size<br>less than or equal to M, nh must be at least ⌈bs/M⌉. More precisely stated, we have<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>IV. Data Storage and <br>Querying<br>13. Query Processing<br>512<br>© The McGraw−Hill <br>Companies, 2001<br>13.5<br>Join Operation<br>511<br>/* Partition s */<br>for each tuple ts in s do begin<br>i := h(ts[JoinAttrs]);<br>Hsi := Hsi ∪{ts};<br>end<br>/* Partition r */<br>for each tuple tr in r do begin<br>i := h(tr[JoinAttrs]);<br>Hri := Hri ∪{tr};<br>end<br>/* Perform join on each partition */<br>for i := 0 to nh do begin<br>read Hsi and build an in-memory hash index on it<br>for each tuple tr in Hri do begin<br>probe the hash index on Hsi to locate all tuples ts<br>such that ts[JoinAttrs] = tr[JoinAttrs]<br>for each matching tuple ts in Hsi do begin<br>add tr<br> ts to the result<br>end<br>end<br>end<br>Figure 13.9<br>Hash join.<br>to account for the extra space occupied by the hash index on the partition as well, so<br>nh should be correspondingly larger. For simplicity, we sometimes ignore the space<br>requirement of the hash index in our analysis.<br>13.5.5.1<br>Recursive Partitioning<br>If the value of nh is greater than or equal to the number of page frames of memory,<br>the relations cannot be partitioned in one pass, since there will not be enough buffer<br>pages. Instead, partitioning has to be done in repeated passes. In one pass, the input<br>can be split into at most as many partitions as there are page frames available for<br>use as output buffers. Each bucket generated by one pass is separately read in and<br>partitioned again in the next pass, to create smaller partitions. The hash function used<br>in a pass is, of course, different from the one used in the previous pass. The system<br>repeats this splitting of the input until each partition of the build input ﬁts in memory.<br>Such partitioning is called recursive partitioning.<br>A relation does not need recursive partitioning if M &gt; nh+1, or equivalently M &gt;<br>(bs/M) + 1, which simpliﬁes (approximately) to M &gt; √bs. For example, consider<br>a memory size of 12 megabytes, divided into 4-kilobyte blocks; it would contain a<br>total of 3000 blocks. We can use a memory of this size to partition relations of size<br>9 million blocks, which is 36 gigabytes. Similarly, a relation of size 1 gigabyte requires<br>√<br>250000 blocks, or about 2 megabytes, to avoid recursive partitioning.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>IV. Data Storage and <br>Querying<br>13. Query Processing<br>513<br>© The McGraw−Hill <br>Companies, 2001<br>512<br>Chapter 13<br>Query Processing<br>13.5.5.2<br>Handling of Overﬂows<br>Hash-table overﬂow occurs in partition i of the build relation s if the hash index on<br>Hsi is larger than main memory. Hash-table overﬂow can occur if there are many<br>tuples in the build relation with the same values for the join attributes, or if the hash<br>function does not have the properties of randomness and uniformity. In either case,<br>some of the partitions will have more tuples than the average, whereas others will<br>have fewer; partitioning is then said to be skewed.<br>We can handle a small amount of skew by increasing the number of partitions so<br>that the expected size of each partition (including the hash index on the partition)<br>is somewhat less than the size of memory. The number of partitions is therefore in-<br>creased by a small value called the fudge factor, which is usually about 20 percent of<br>the number of hash partitions computed as described in Section 13.5.5.<br>Even if we are conservative on the sizes of the partitions, by using a fudge factor,<br>overﬂows can still occur. Hash-table overﬂows can be handled by either overﬂow reso-<br>lution or overﬂow avoidance. Overﬂow resolution is performed during the build phase,<br>if a hash-index overﬂow is detected. Overﬂow resolution proceeds in this way: If Hsi,<br>for any i, is found to be too large, it is further partitioned into smaller partitions by<br>using a different hash function. Similarly, Hri is also partitioned using the new hash<br>function, and only tuples in the matching partitions need to be joined.<br>In contrast, overﬂow avoidance performs the partitioning carefully, so that over-<br>ﬂows never occur during the build phase. In overﬂow avoidance, the build relation s<br>is initially partitioned into many small partitions, and then some partitions are com-<br>bined in such a way that each combined partition ﬁts in memory. The probe relation<br>r is partitioned in the same way as the combined partitions on s, but the sizes of Hri<br>do not matter.<br>If a large number of tuples in s have the same value for the join attributes, the res-<br>olution and avoidance techniques may fail on some partitions. In that case, instead of<br>creating an in-memory hash index and using a nested-loop join to join the partitions,<br>we can use other join techniques, such as block nested-loop join, on those partitions.<br>13.5.5.3<br>Cost of Hash Join<br>We now consider the cost of a hash join. Our analysis assumes that there is no hash-<br>table overﬂow. First, consider the case where recursive partitioning is not required.<br>The partitioning of the two relations r and s calls for a complete reading of both rela-<br>tions, and a subsequent writing back of them. This operation requires 2(br +bs) block<br>accesses, where br and bs denote the number of blocks containing records of relations<br>r and s respectively. The build and probe phases read each of the partitions once, call-<br>ing for a further br + bs accesses. The number of blocks occupied by partitions could<br>be slightly more than br + bs, as a result of partially ﬁlled blocks. Accessing such par-<br>tially ﬁlled blocks can add an overhead of at most 2nh for each of the relations, since<br>each of the nh partitions could have a partially ﬁlled block that has to be written and<br>read back. Thus, the cost estimate for a hash join is<br>3(br + bs) + 4nh<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>IV. Data Storage and <br>Querying<br>13. Query Processing<br>514<br>© The McGraw−Hill <br>Companies, 2001<br>13.5<br>Join Operation<br>513<br>The overhead 4nh is quite small compared to br + bs, and can be ignored.<br>Now consider the case where recursive partitioning is required. Each pass reduces<br>the size of each of the partitions by an expected factor of M −1; and passes are<br>repeated until each partition is of size at most M blocks. The expected number of<br>passes required for partitioning s is therefore ⌈logM−1(bs) −1⌉. Since, in each pass,<br>every block of s is read in and written out, the total block transfers for partitioning of<br>s is 2bs⌈logM−1(bs) −1⌉. The number of passes for partitioning of r is the same as the<br>number of passes for partitioning of s, therefore the cost estimate for the join is<br>2(br + bs)⌈logM−1(bs) −1⌉+ br + bs<br>Consider, for example, the join customer<br> depositor. With a memory size of 20<br>blocks, depositor can be partitioned into ﬁve partitions, each of size 20 blocks, which<br>size will ﬁt into memory. Only one pass is required for the partitioning. The relation<br>customer is similarly partitioned into ﬁve partitions, each of size 80. Ignoring the cost<br>of writing partially ﬁlled blocks, the cost is 3(100 + 400) = 1500 block transfers.<br>The hash join can be improved if the main memory size is large. When the en-<br>tire build input can be kept in main memory, nh can be set to 0; then, the hash join<br>algorithm executes quickly, without partitioning the relations into temporary ﬁles,<br>regardless of the probe input’s size. The cost estimate goes down to br + bs.<br>13.5.5.4<br>Hybrid Hash–Join<br>The hybrid hash–join algorithm performs another optimization; it is useful when<br>memory sizes are relatively large, but not all of the build relation ﬁts in memory. The<br>partitioning phase of the hash join algorithm needs one block of memory as a buffer<br>for each partition that is created, and one block of memory as an input buffer. Hence,<br>a total of nh + 1 blocks of memory are needed for the partitioning the two relations.<br>If memory is larger than nh + 1, we can use the rest of memory (M −nh −1 blocks)<br>to buffer the ﬁrst partition of the build input (that is, Hs0), so that it will not need<br>to be written out and read back in. Further, the hash function is designed in such a<br>way that the hash index on Hs0 ﬁts in M −nh −1 blocks, in order that, at the end of<br>partitioning of s, Hs0 is completely in memory and a hash index can be built on Hs0.<br>When the system partitions r it again does not write tuples in Hr0 to disk; instead,<br>as it generates them, the system uses them to probe the memory-resident hash index<br>on Hs0, and to generate output tuples of the join. After they are used for probing,<br>the tuples can be discarded, so the partition Hr0 does not occupy any memory space.<br>Thus, a write and a read access have been saved for each block of both Hr0 and Hs0.<br>The system writes out tuples in the other partitions as usual, and joins them later.<br>The savings of hybrid hash–join can be signiﬁcant if the build input is only slightly<br>bigger than memory.<br>If the size of the build relation is bs, nh is approximately equal to bs/M. Thus,<br>hybrid hash–join is most useful if M &gt;&gt; bs/M, or M &gt;&gt; √bs, where the notation<br>&gt;&gt; denotes much larger than. For example, suppose the block size is 4 kilobytes, and<br>the build relation size is 1 gigabyte. Then, the hybrid hash–join algorithm is use-<br>ful if the size of memory is signiﬁcantly more than 2 megabytes; memory sizes of<br>100 megabytes or more are common on computers today.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>IV. Data Storage and <br>Querying<br>13. Query Processing<br>515<br>© The McGraw−Hill <br>Companies, 2001<br>514<br>Chapter 13<br>Query Processing<br>Consider the join customer<br> depositor again. With a memory size of 25 blocks,<br>depositor can be partitioned into ﬁve partitions, each of size 20 blocks, and the ﬁrst<br>of the partitions of the build relation can be kept in memory. It occupies 20 blocks<br>of memory; one block is for input and one block each is for buffering the other four<br>partitions. The relation customer can be similarly partitioned into ﬁve partitions each<br>of size 80, the ﬁrst of which the system uses right away for probing, instead of writing<br>it out and reading it back in. Ignoring the cost of writing partially ﬁlled blocks, the<br>cost is 3(80 + 320) + 20 + 80 = 1300 block transfers, instead of 1500 block transfers<br>without the hybrid hashing optimization.<br>13.5.6<br>Complex Joins<br>Nested-loop and block nested-loop joins can be used regardless of the join condi-<br>tions. The other join techniques are more efﬁcient than the nested-loop join and its<br>variants, but can handle only simple join conditions, such as natural joins or equi-<br>joins. We can implement joins with complex join conditions, such as conjunctions<br>and disjunctions, by using the efﬁcient join techniques, if we apply the techniques<br>developed in Section 13.3.4 for handling complex selections.<br>Consider the following join with a conjunctive condition:<br>r<br>θ1∧θ2∧···∧θn s<br>One or more of the join techniques described earlier may be applicable for joins on<br>the individual conditions r<br>θ1 s, r<br>θ2 s, r<br>θ3 s, and so on. We can compute the<br>overall join by ﬁrst computing the result of one of these simpler joins r<br>θi s; each<br>pair of tuples in the intermediate result consists of one tuple from r and one from s.<br>The result of the complete join consists of those tuples in the intermediate result that<br>satisfy the remaining conditions<br>θ1 ∧· · · ∧θi−1 ∧θi+1 ∧· · · ∧θn<br>These conditions can be tested as tuples in r<br>θi s are being generated.<br>A join whose condition is disjunctive can be computed in this way: Consider<br>r<br>θ1∨θ2∨···∨θn s<br>The join can be computed as the union of the records in individual joins r<br>θi s:<br>(r<br>θ1 s) ∪(r<br>θ2 s) ∪· · · ∪(r<br>θn s)<br>Section 13.6 describes algorithms for computing the union of relations.<br>13.6<br>Other Operations<br>Other relational operations and extended relational operations—such as duplicate<br>elimination, projection, set operations, outer join, and aggregation—can be imple-<br>mented as outlined in Sections 13.6.1 through 13.6.5.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>IV. Data Storage and <br>Querying<br>13. Query Processing<br>516<br>© The McGraw−Hill <br>Companies, 2001<br>13.6<br>Other Operations<br>515<br>13.6.1<br>Duplicate Elimination<br>We can implement duplicate elimination easily by sorting. Identical tuples will ap-<br>pear adjacent to each other during sorting, and all but one copy can be removed. With<br>external sort–merge, duplicates found while a run is being created can be removed<br>before the run is written to disk, thereby reducing the number of block transfers. The<br>remaining duplicates can be eliminated during merging, and the ﬁnal sorted run will<br>have no duplicates. The worst-case cost estimate for duplicate elimination is the same<br>as the worst-case cost estimate for sorting of the relation.<br>We can also implement duplicate elimination by hashing, as in the hash join algo-<br>rithm. First, the relation is partitioned on the basis of a hash function on the whole<br>tuple. Then, each partition is read in, and an in-memory hash index is constructed.<br>While constructing the hash index, a tuple is inserted only if it is not already present.<br>Otherwise, the tuple is discarded. After all tuples in the partition have been pro-<br>cessed, the tuples in the hash index are written to the result. The cost estimate is the<br>same as that for the cost of processing (partitioning and reading each partition) of the<br>build relation in a hash join.<br>Because of the relatively high cost of duplicate elimination, SQL requires an explicit<br>request by the user to remove duplicates; otherwise, the duplicates are retained.<br>13.6.2<br>Projection<br>We can implement projection easily by performing projection on each tuple, which<br>gives a relation that could have duplicate records, and then removing duplicate rec-<br>ords. Duplicates can be eliminated by the methods described in Section 13.6.1. If the<br>attributes in the projection list include a key of the relation, no duplicates will ex-<br>ist; hence, duplicate elimination is not required. Generalized projection (which was<br>discussed in Section 3.3.1) can be implemented in the same way as projection.<br>13.6.3<br>Set Operations<br>We can implement the union, intersection, and set-difference operations by ﬁrst sorting<br>both relations, and then scanning once through each of the sorted relations to produce<br>the result. In r ∪s, when a concurrent scan of both relations reveals the same tuple in<br>both ﬁles, only one of the tuples is retained. The result of r ∩s will contain only those<br>tuples that appear in both relations. We implement set difference, r −s, similarly, by<br>retaining tuples in r only if they are absent in s.<br>For all these operations, only one scan of the two input relations is required, so<br>the cost is br + bs. If the relations are not sorted initially, the cost of sorting has to be<br>included. Any sort order can be used in evaluation of set operations, provided that<br>both inputs have that same sort order.<br>Hashing provides another way to implement these set operations. The ﬁrst step<br>in each case is to partition the two relations by the same hash function, and thereby<br>create the partitions Hr0, Hr1, . . . , Hrnh and Hs0, Hs1, . . . , Hsnh. Depending on the<br>operation, the system then takes these steps on each partition i = 0, 1 . . . , nh:<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>IV. Data Storage and <br>Querying<br>13. Query Processing<br>517<br>© The McGraw−Hill <br>Companies, 2001<br>516<br>Chapter 13<br>Query Processing<br>• r ∪s<br>1. Build an in-memory hash index on Hri.<br>2. Add the tuples in Hsi to the hash index only if they are not already present.<br>3. Add the tuples in the hash index to the result.<br>• r ∩s<br>1. Build an in-memory hash index on Hri.<br>2. For each tuple in Hsi, probe the hash index, and output the tuple to the<br>result only if it is already present in the hash index.<br>• r −s<br>1. Build an in-memory hash index on Hri.<br>2. For each tuple in Hsi, probe the hash index, and, if the tuple is present in<br>the hash index, delete it from the hash index.<br>3. Add the tuples remaining in the hash index to the result.<br>13.6.4<br>Outer Join<br>Recall the outer-join operations described in Section 3.3.3. For example, the natural left<br>outer join customer<br> depositor contains the join of customer and depositor, and, in<br>addition, for each customer tuple t that has no matching tuple in depositor (that is,<br>where customer-name is not in depositor), the following tuple t1 is added to the result.<br>For all attributes in the schema of customer, tuple t1 has the same values as tuple t.<br>The remaining attributes (from the schema of depositor) of tuple t1 contain the value<br>null.<br>We can implement the outer-join operations by using one of two strategies:<br>1. Compute the corresponding join, and then add further tuples to the join re-<br>sult to get the outer-join result. Consider the left outer-join operation and two<br>relations: r(R) and s(S). To evaluate r<br>θ s, we ﬁrst compute r<br>θ s, and<br>save that result as temporary relation q1. Next, we compute r −ΠR(q1), which<br>gives tuples in r that did not participate in the join. We can use any of the algo-<br>rithms for computing the joins, projection, and set difference described earlier<br>to compute the outer joins. We pad each of these tuples with null values for<br>attributes from s, and add it to q1 to get the result of the outer join.<br>The right outer-join operation r<br> θ s is equivalent to s<br>θ r, and can<br>therefore be implemented in a symmetric fashion to the left outer join. We<br>can implement the full outer-join operation r<br> θ s by computing the join<br>r<br> s, and then adding the extra tuples of both the left and right outer-join<br>operations, as before.<br>2. Modify the join algorithms. It is easy to extend the nested-loop join algorithms<br>to compute the left outer join: Tuples in the outer relation that do not match<br>any tuple in the inner relation are written to the output after being padded<br>with null values. However, it is hard to extend the nested-loop join to compute<br>the full outer join.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>IV. Data Storage and <br>Querying<br>13. Query Processing<br>518<br>© The McGraw−Hill <br>Companies, 2001<br>13.6<br>Other Operations<br>517<br>Natural outer joins and outer joins with an equi-join condition can be com-<br>puted by extensions of the merge join and hash join algorithms. Merge join<br>can be extended to compute the full outer join as follows: When the merge<br>of the two relations is being done, tuples in either relation that did not match<br>any tuple in the other relation can be padded with nulls and written to the out-<br>put. Similarly, we can extend merge join to compute the left and right outer<br>joins by writing out nonmatching tuples (padded with nulls) from only one<br>of the relations. Since the relations are sorted, it is easy to detect whether or<br>not a tuple matches any tuples from the other relation. For example, when a<br>merge join of customer and depositor is done, the tuples are read in sorted or-<br>der of customer-name, and it is easy to check, for each tuple, whether there is a<br>matching tuple in the other.<br>The cost estimates for implementing outer joins using the merge join algo-<br>rithm are the same as are those for the corresponding join. The only difference<br>lies in size of the result, and therefore in the block transfers for writing it out,<br>which we did not count in our earlier cost estimates.<br>The extension of</span><br><br><span style="background-color: #FFD6A5;" title="Chunk 65 | Start: 1300130 | End: 1320130 | Tokens: 3320"> the hash join algorithm to compute outer joins is left for<br>you to do as an exercise (Exercise 13.11).<br>13.6.5<br>Aggregation<br>Recall the aggregation operator G, discussed in Section 3.3.2. For example, the oper-<br>ation<br>branch-nameGsum(balance)(account)<br>groups account tuples by branch, and computes the total balance of all the accounts<br>at each branch.<br>The aggregation operation can be implemented in the same way as duplicate elim-<br>ination. We use either sorting or hashing, just as we did for duplicate elimination,<br>but based on the grouping attributes (branch-name in the preceding example). How-<br>ever, instead of eliminating tuples with the same value for the grouping attribute, we<br>gather them into groups, and apply the aggregation operations on each group to get<br>the result.<br>The cost estimate for implementing the aggregation operation is the same as the<br>cost of duplicate elimination, for aggregate functions such as min, max, sum, count,<br>and avg.<br>Instead of gathering all the tuples in a group and then applying the aggregation<br>operations, we can implement the aggregation operations sum, min, max, count, and<br>avg on the ﬂy as the groups are being constructed. For the case of sum, min, and<br>max, when two tuples in the same group are found, the system replaces them by<br>a single tuple containing the sum, min, or max, respectively, of the columns being<br>aggregated. For the count operation, it maintains a running count for each group for<br>which a tuple has been found. Finally, we implement the avg operation by computing<br>the sum and the count values on the ﬂy, and ﬁnally dividing the sum by the count to<br>get the average.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>IV. Data Storage and <br>Querying<br>13. Query Processing<br>519<br>© The McGraw−Hill <br>Companies, 2001<br>518<br>Chapter 13<br>Query Processing<br>If all tuples of the result will ﬁt in memory, both the sort-based and the hash-based<br>implementations do not need to write any tuples to disk. As the tuples are read in,<br>they can be inserted in a sorted tree structure or in a hash index. When we use on the<br>ﬂy aggregation techniques, only one tuple needs to be stored for each of the groups.<br>Hence, the sorted tree structure or hash index will ﬁt in memory, and the aggregation<br>can be processed with just br block transfers, instead of with the 3br transfers that<br>would be required otherwise.<br>13.7<br>Evaluation of Expressions<br>So far, we have studied how individual relational operations are carried out. Now<br>we consider how to evaluate an expression containing multiple operations. The ob-<br>vious way to evaluate an expression is simply to evaluate one operation at a time,<br>in an appropriate order. The result of each evaluation is materialized in a temporary<br>relation for subsequent use. A disadvantage to this approach is the need to construct<br>the temporary relations, which (unless they are small) must be written to disk. An<br>alternative approach is to evaluate several operations simultaneously in a pipeline,<br>with the results of one operation passed on to the next, without the need to store a<br>temporary relation.<br>In Sections 13.7.1 and 13.7.2, we consider both the materialization approach and<br>the pipelining approach. We shall see that the costs of these approaches can differ<br>substantially, but also that there are cases where only the materialization approach is<br>feasible.<br>13.7.1<br>Materialization<br>It is easiest to understand intuitively how to evaluate an expression by looking at a<br>pictorial representation of the expression in an operator tree. Consider the expression<br>Πcustomer-name (σbalance&lt;2500 (account)<br> customer)<br>in Figure 13.10.<br>Π customer-name<br>σ balance &lt; 2500<br>account<br>customer<br>Figure 13.10<br>Pictorial representation of an expression.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>IV. Data Storage and <br>Querying<br>13. Query Processing<br>520<br>© The McGraw−Hill <br>Companies, 2001<br>13.7<br>Evaluation of Expressions<br>519<br>If we apply the materialization approach, we start from the lowest-level operations<br>in the expression (at the bottom of the tree). In our example, there is only one such op-<br>eration; the selection operation on account. The inputs to the lowest-level operations<br>are relations in the database. We execute these operations by the algorithms that we<br>studied earlier, and we store the results in temporary relations. We can use these tem-<br>porary relations to execute the operations at the next level up in the tree, where the<br>inputs now are either temporary relations or relations stored in the database. In our<br>example, the inputs to the join are the customer relation and the temporary relation<br>created by the selection on account. The join can now be evaluated, creating another<br>temporary relation.<br>By repeating the process, we will eventually evaluate the operation at the root of<br>the tree, giving the ﬁnal result of the expression. In our example, we get the ﬁnal<br>result by executing the projection operation at the root of the tree, using as input the<br>temporary relation created by the join.<br>Evaluation as just described is called materialized evaluation, since the results of<br>each intermediate operation are created (materialized) and then are used for evalua-<br>tion of the next-level operations.<br>The cost of a materialized evaluation is not simply the sum of the costs of the oper-<br>ations involved. When we computed the cost estimates of algorithms, we ignored the<br>cost of writing the result of the operation to disk. To compute the cost of evaluating<br>an expression as done here, we have to add the costs of all the operations, as well<br>as the cost of writing the intermediate results to disk. We assume that the records<br>of the result accumulate in a buffer, and, when the buffer is full, they are written to<br>disk. The cost of writing out the result can be estimated as nr/fr, where nr is the<br>estimated number of tuples in the result relation r, and fr is the blocking factor of the<br>result relation, that is, the number of records of r that will ﬁt in a block.<br>Double buffering (using two buffers, with one continuing execution of the al-<br>gorithm while the other is being written out) allows the algorithm to execute more<br>quickly by performing CPU activity in parallel with I/O activity.<br>13.7.2<br>Pipelining<br>We can improve query-evaluation efﬁciency by reducing the number of temporary<br>ﬁles that are produced. We achieve this reduction by combining several relational op-<br>erations into a pipeline of operations, in which the results of one operation are passed<br>along to the next operation in the pipeline. Evaluation as just described is called<br>pipelined evaluation. Combining operations into a pipeline eliminates the cost of<br>reading and writing temporary relations.<br>For example, consider the expression (Πa1,a2(r<br> s)). If materialization were ap-<br>plied, evaluation would involve creating a temporary relation to hold the result of the<br>join, and then reading back in the result to perform the projection. These operations<br>can be combined: When the join operation generates a tuple of its result, it passes that<br>tuple immediately to the project operation for processing. By combining the join and<br>the projection, we avoid creating the intermediate result, and instead create the ﬁnal<br>result directly.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>IV. Data Storage and <br>Querying<br>13. Query Processing<br>521<br>© The McGraw−Hill <br>Companies, 2001<br>520<br>Chapter 13<br>Query Processing<br>13.7.2.1<br>Implementation of Pipelining<br>We can implement a pipeline by constructing a single, complex operation that com-<br>bines the operations that constitute the pipeline. Although this approach may be fea-<br>sible for various frequently occurring situations, it is desirable in general to reuse the<br>code for individual operations in the construction of a pipeline. Therefore, each op-<br>eration in the pipeline is modeled as a separate process or thread within the system,<br>which takes a stream of tuples from its pipelined inputs, and generates a stream of<br>tuples for its output. For each pair of adjacent operations in the pipeline, the system<br>creates a buffer to hold tuples being passed from one operation to the next.<br>In the example of Figure 13.10, all three operations can be placed in a pipeline,<br>which passes the results of the selection to the join as they are generated. In turn,<br>it passes the results of the join to the projection as they are generated. The memory<br>requirements are low, since results of an operation are not stored for long. However,<br>as a result of pipelining, the inputs to the operations are not available all at once for<br>processing.<br>Pipelines can be executed in either of two ways:<br>1. Demand driven<br>2. Producer driven<br>In a demand-driven pipeline, the system makes repeated requests for tuples from<br>the operation at the top of the pipeline. Each time that an operation receives a request<br>for tuples, it computes the next tuple (or tuples) to be returned, and then returns<br>that tuple. If the inputs of the operation are not pipelined, the next tuple(s) to be<br>returned can be computed from the input relations, while the system keeps track of<br>what has been returned so far. If it has some pipelined inputs, the operation also<br>makes requests for tuples from its pipelined inputs. Using the tuples received from<br>its pipelined inputs, the operation computes tuples for its output, and passes them<br>up to its parent.<br>In a producer-driven pipeline, operations do not wait for requests to produce<br>tuples, but instead generate the tuples eagerly. Each operation at the bottom of a<br>pipeline continually generates output tuples, and puts them in its output buffer, until<br>the buffer is full. An operation at any other level of a pipeline generates output tuples<br>when it gets input tuples from lower down in the pipeline, until its output buffer is<br>full. Once the operation uses a tuple from a pipelined input, it removes the tuple<br>from its input buffer. In either case, once the output buffer is full, the operation waits<br>until its parent operation removes tuples from the buffer, so that the buffer has space<br>for more tuples. At this point, the operation generates more tuples, until the buffer<br>is full again. The operation repeats this process until all the output tuples have been<br>generated.<br>It is necessary for the system to switch between operations only when an output<br>buffer is full, or an input buffer is empty and more input tuples are needed to gener-<br>ate any more output tuples. In a parallel-processing system, operations in a pipeline<br>may be run concurrently on distinct processors (see Chapter 20).<br>Using producer-driven pipelining can be thought of as pushing data up an oper-<br>ation tree from below, whereas using demand-driven pipelining can be thought of as<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>IV. Data Storage and <br>Querying<br>13. Query Processing<br>522<br>© The McGraw−Hill <br>Companies, 2001<br>13.7<br>Evaluation of Expressions<br>521<br>pulling data up an operation tree from the top. Whereas tuples are generated eagerly<br>in producer-driven pipelining, they are generated lazily, on demand, in demand-<br>driven pipelining.<br>Each operation in a demand-driven pipeline can be implemented as an iterator,<br>which provides the following functions: open(), next(), and close(). After a call to<br>open(), each call to next() returns the next output tuple of the operation. The imple-<br>mentation of the operation in turn calls open() and next() on its inputs, to get its input<br>tuples when required. The function close() tells an iterator that no more tuples are<br>required. The iterator maintains the state of its execution in between calls, so that<br>successive next() requests receive successive result tuples.<br>For example, for an iterator implementing the select operation using linear search,<br>the open() operation starts a ﬁle scan, and the iterator’s state records the point to<br>which the ﬁle has been scanned. When the next() function is called, the ﬁle scan con-<br>tinues from after the previous point; when the next tuple satisfying the selection is<br>found by scanning the ﬁle, the tuple is returned after storing the point where it was<br>found in the iterator state. A merge–join iterator’s open() operation would open its<br>inputs, and if they are not already sorted, it would also sort the inputs. On calls to<br>next(), it would return the next pair of matching tuples. The state information would<br>consist of up to where each input had been scanned.<br>Details of the implementation of iterators are left for you to complete in Exer-<br>cise 13.12. Demand-driven pipelining is used more commonly than producer-driven<br>pipelining, because it is easier to implement.<br>13.7.2.2<br>Evaluation Algorithms for Pipelining<br>Consider a join operation whose left-hand–side input is pipelined. Since it is pipe-<br>lined, the input is not available all at once for processing by the join operation. This<br>unavailability limits the choice of join algorithm to be used. Merge join, for example,<br>cannot be used if the inputs are not sorted, since it is not possible to sort a relation<br>until all the tuples are available—thus, in effect, turning pipelining into materializa-<br>tion. However, indexed nested-loop join can be used: As tuples are received for the<br>left-hand side of the join, they can be used to index the right-hand–side relation, and<br>to generate tuples in the join result. This example illustrates that choices regarding<br>the algorithm used for an operation and choices regarding pipelining are not inde-<br>pendent.<br>The restrictions on the evaluation algorithms that are eligible for use are a limiting<br>factor for pipelining. As a result, despite the apparent advantages of pipelining, there<br>are cases where materialization achieves lower overall cost. Suppose that the join of<br>r and s is required, and input r is pipelined. If indexed nested-loop join is used to<br>support pipelining, one access to disk may be needed for every tuple in the pipelined<br>input relation. The cost of this technique is nr ∗HTi, where HTi is the height of the<br>index on s. With materialization, the cost of writing out r would be br. With a join<br>technique such as hash join, it may be possible to perform the join with a cost of<br>about 3(br + bs). If nr is substantially more than 4br + 3bs, materialization would be<br>cheaper.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>IV. Data Storage and <br>Querying<br>13. Query Processing<br>523<br>© The McGraw−Hill <br>Companies, 2001<br>522<br>Chapter 13<br>Query Processing<br>doner := false;<br>dones := false;<br>r := ∅;<br>s := ∅;<br>result := ∅;<br>while not doner or not dones do<br>begin<br>if queue is empty, then wait until queue is not empty;<br>t := top entry in queue;<br>if t = Endr then doner := true<br>else if t = Ends then dones := true<br>else if t is from input r<br>then<br>begin<br>r := r ∪{t};<br>result := result ∪({t}<br> s);<br>end<br>else /* t is from input s */<br>begin<br>s := s ∪{t};<br>result := result ∪(r<br> {t});<br>end<br>end<br>Figure 13.11<br>Pipelined join algorithm.<br>The effective use of pipelining requires the use of evaluation algorithms that can<br>generate output tuples even as tuples are received for the inputs to the operation. We<br>can distinguish between two cases:<br>1. Only one of the inputs to a join is pipelined.<br>2. Both inputs to the join are pipelined.<br>If only one of the inputs to a join is pipelined, indexed nested-loop join is a natural<br>choice. If the pipelined input tuples are sorted on the join attributes, and the join<br>condition is an equi-join, merge join can also be used. Hybrid hash–join can be used<br>too, with the pipelined input as the probe relation. However, tuples that are not in the<br>ﬁrst partition will be output only after the entire pipelined input relation is received.<br>Hybrid hash–join is useful if the nonpipelined input ﬁts entirely in memory, or if at<br>least most of that input ﬁts in memory.<br>If both inputs are pipelined, the choice of join algorithms is more restricted. If both<br>inputs are sorted on the join attribute, and the join condition is an equi-join, merge<br>join can be used. Another alternative is the pipelined join technique, shown in Figure<br>13.11. The algorithm assumes that the input tuples for both input relations, r and s,<br>are pipelined. Tuples made available for both relations are queued for processing in a<br>single queue. Special queue entries, called Endr and Ends, which serve as end-of-ﬁle<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>IV. Data Storage and <br>Querying<br>13. Query Processing<br>524<br>© The McGraw−Hill <br>Companies, 2001<br>13.8<br>Summary<br>523<br>markers, are inserted in the queue after all tuples from r and s (respectively) have<br>been generated. For efﬁcient evaluation, appropriate indices should be built on the<br>relations r and s. As tuples are added to r and s, the indices must be kept up to date.<br>13.8<br>Summary<br>• The ﬁrst action that the system must perform on a query is to translate the<br>query into its internal form, which (for relational database systems) is usually<br>based on the relational algebra. In the process of generating the internal form<br>of the query, the parser checks the syntax of the user’s query, veriﬁes that the<br>relation names appearing in the query are names of relations in the database,<br>and so on. If the query was expressed in terms of a view, the parser replaces all<br>references to the view name with the relational-algebra expression to compute<br>the view.<br>• Given a query, there are generally a variety of methods for computing the<br>answer. It is the responsibility of the query optimizer to transform the query<br>as entered by the user into an equivalent query that can be computed more<br>efﬁciently. Chapter 14 covers query optimization.<br>• We can process simple selection operations by performing a linear scan, by<br>doing a binary search, or by making use of indices. We can handle complex<br>selections by computing unions and intersections of the results of simple se-<br>lections.<br>• We can sort relations larger than memory by the external merge–sort algo-<br>rithm.<br>• Queries involving a natural join may be processed in several ways, depending<br>on the availability of indices and the form of physical storage for the relations.<br>  If the join result is almost as large as the Cartesian product of the two<br>relations, a block nested-loop join strategy may be advantageous.<br>  If indices are available, the indexed nested-loop join can be used.<br>  If the relations are sorted, a merge join may be desirable. It may be advan-<br>tageous to sort a relation prior to join computation (so as to allow use of<br>the merge join strategy).<br>  The hash join algorithm partitions the relations into several pieces, such<br>that each piece of one of the relations ﬁts in memory. The partitioning is<br>carried out with a hash function on the join attributes, so that correspond-<br>ing pairs of partitions can be joined independently.<br>• Duplicate elimination, projection, set operations (union, intersection and dif-<br>ference), and aggregation can be done by sorting or by hashing.<br>• Outer join operations can be implemented by simple extensions of join algo-<br>rithms.<br>• Hashing and sorting are dual, in the sense that any operation such as du-<br>plicate elimination, projection, aggregation, join, and outer join that can be<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>IV. Data Storage and <br>Querying<br>13. Query Processing<br>525<br>© The McGraw−Hill <br>Companies, 2001<br>524<br>Chapter 13<br>Query Processing<br>implemented by hashing can also be implemented by sorting, and vice versa;<br>that is, any operation that can be implemented by sorting can also be imple-<br>mented by hashing.<br>• An expression can be evaluated by means of materialization, where the sys-<br>tem computes the result of each subexpression and stores it on disk, and then<br>uses it to compute the result of the parent expression.<br>• Pipelining helps to avoid writing the results of many subexpressions to disk,<br>by using the results in the parent expression even as they are being generated.<br>Review Terms<br>• Query processing<br>• Evaluation primitive<br>• Query-execution plan<br>• Query-evaluation plan<br>• Query-execution engine<br>• Measures of query cost<br>• Seque</span><br><br><span style="background-color: #FDFFB6;" title="Chunk 66 | Start: 1320132 | End: 1340132 | Tokens: 3239">ntial I/O<br>• Random I/O<br>• File scan<br>• Linear search<br>• Binary search<br>• Selections using indices<br>• Access paths<br>• Index scans<br>• Conjunctive selection<br>• Disjunctive selection<br>• Composite index<br>• Intersection of identiﬁers<br>• External sorting<br>• External sort–merge<br>• Runs<br>• N-way merge<br>• Equi-join<br>• Nested-loop join<br>• Block nested-loop join<br>• Indexed nested-loop join<br>• Merge join<br>• Sort–merge join<br>• Hybrid merge–join<br>• Hash join<br>  Build<br>  Probe<br>  Build input<br>  Probe input<br>  Recursive partitioning<br>  Hash-table overﬂow<br>  Skew<br>  Fudge factor<br>  Overﬂow resolution<br>  Overﬂow avoidance<br>• Hybrid hash–join<br>• Operator tree<br>• Materialized evaluation<br>• Double buffering<br>• Pipelined evaluation<br>  Demand-driven pipeline<br>(lazy, pulling)<br>  Producer-driven pipeline<br>(eager, pushing)<br>  Iterator<br>• Pipelined join<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>IV. Data Storage and <br>Querying<br>13. Query Processing<br>526<br>© The McGraw−Hill <br>Companies, 2001<br>Exercises<br>525<br>Exercises<br>13.1 Why is it not desirable to force users to make an explicit choice of a query-<br>processing strategy? Are there cases in which it is desirable for users to be<br>aware of the costs of competing query-processing strategies? Explain your an-<br>swer.<br>13.2 Consider the following SQL query for our bank database:<br>select T.branch-name<br>from branch T, branch S<br>where T.assets &gt; S.assets and S.branch-city = “Brooklyn”<br>Write an efﬁcient relational-algebra expression that is equivalent to this query.<br>Justify your choice.<br>13.3 What are the advantages and disadvantages of hash indices relative to B+-tree<br>indices? How might the type of index available inﬂuence the choice of a query-<br>processing strategy?<br>13.4 Assume (for simplicity in this exercise) that only one tuple ﬁts in a block and<br>memory holds at most 3 page frames. Show the runs created on each pass of<br>the sort-merge algorithm, when applied to sort the following tuples on the ﬁrst<br>attribute: (kangaroo, 17), (wallaby, 21), (emu, 1), (wombat, 13), (platypus, 3),<br>(lion, 8), (warthog, 4), (zebra, 11), (meerkat, 6), (hyena, 9), (hornbill, 2), (baboon,<br>12).<br>13.5 Let relations r1(A, B, C) and r2(C, D, E) have the following properties: r1 has<br>20,000 tuples, r2 has 45,000 tuples, 25 tuples of r1 ﬁt on one block, and 30 tuples<br>of r2 ﬁt on one block. Estimate the number of block accesses required, using<br>each of the following join strategies for r1<br> r2:<br>a. Nested-loop join<br>b. Block nested-loop join<br>c. Merge join<br>d. Hash join<br>13.6 Design a variant of the hybrid merge–join algorithm for the case where both<br>relations are not physically sorted, but both have a sorted secondary index on<br>the join attributes.<br>13.7 The indexed nested-loop join algorithm described in Section 13.5.3 can be inef-<br>ﬁcient if the index is a secondary index, and there are multiple tuples with the<br>same value for the join attributes. Why is it inefﬁcient? Describe a way, using<br>sorting, to reduce the cost of retrieving tuples of the inner relation. Under what<br>conditions would this algorithm be more efﬁcient than hybrid merge–join?<br>13.8 Estimate the number of block accesses required by your solution to Exer-<br>cise 13.6 for r1<br> r2, where r1 and r2 are as deﬁned in Exercise 13.5.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>IV. Data Storage and <br>Querying<br>13. Query Processing<br>527<br>© The McGraw−Hill <br>Companies, 2001<br>526<br>Chapter 13<br>Query Processing<br>13.9 Let r and s be relations with no indices, and assume that the relations are not<br>sorted. Assuming inﬁnite memory, what is the lowest cost way (in terms of I/O<br>operations) to compute r<br> s? What is the amount of memory required for this<br>algorithm?<br>13.10 Suppose that a B+-tree index on branch-city is available on relation branch, and<br>that no other index is available. List different ways to handle the following<br>selections that involve negation?<br>a. σ¬(branch-city&lt;“Brooklyn”)(branch)<br>b. σ¬(branch-city=“Brooklyn”)(branch)<br>c. σ¬(branch-city&lt;“Brooklyn” ∨assets&lt;5000)(branch)<br>13.11 The hash join algorithm as described in Section 13.5.5 computes the natural join<br>of two relations. Describe how to extend the hash join algorithm to compute<br>the natural left outer join, the natural right outer join and the natural full outer<br>join. (Hint: Keep extra information with each tuple in the hash index, to detect<br>whether any tuple in the probe relation matches the tuple in the hash index.)<br>Try out your algorithm on the customer and depositor relations.<br>13.12 Write pseudocode for an iterator that implements indexed nested-loop join,<br>where the outer relation is pipelined. Use the standard iterator functions in<br>your pseudocode. Show what state information the iterator must maintain be-<br>tween calls.<br>13.13 Design sorting based and hashing algorithms for computing the division op-<br>eration.<br>Bibliographical Notes<br>A query processor must parse statements in the query language, and must translate<br>them into an internal form. Parsing of query languages differs little from parsing of<br>traditional programming languages. Most compiler texts, such as Aho et al. [1986],<br>cover the main parsing techniques, and present optimization from a programming-<br>language point of view.<br>Knuth [1973] presents an excellent description of external sorting algorithms,<br>including an optimization that can create initial runs that are (on the average) twice<br>the size of memory. Based on performance studies conducted in the mid-1970s, data-<br>base systems of that period used only nested-loop join and merge join. These stud-<br>ies, which were related to the development of System R, determined that either the<br>nested-loop join or merge join nearly always provided the optimal join method (Blas-<br>gen and Eswaran [1976]); hence, these two were the only join algorithms imple-<br>mented in System R. The System R study, however, did not include an analysis of<br>hash join algorithms. Today, hash joins are considered to be highly efﬁcient.<br>Hash join algorithms were initially developed for parallel database systems. Hash<br>join techniques are described in Kitsuregawa et al. [1983], and extensions including<br>hybrid hash join are described in Shapiro [1986]. Zeller and Gray [1990] and Davison<br>and Graefe [1994] describe hash join techniques that can adapt to the available mem-<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>IV. Data Storage and <br>Querying<br>13. Query Processing<br>528<br>© The McGraw−Hill <br>Companies, 2001<br>Bibliographical Notes<br>527<br>ory, which is important in systems where multiple queries may be running at the<br>same time. Graefe et al. [1998] describes the use of hash joins and hash teams, which<br>allow pipelining of hash-joins by using the same partitioning for all hash-joins in a<br>pipeline sequence, in the Microsoft SQL Server.<br>Graefe [1993] presents an excellent survey of query-evaluation techniques. An ear-<br>lier survey of query-processing techniques appears in Jarke and Koch [1984].<br>Query processing in main memory database is covered by DeWitt et al. [1984] and<br>Whang and Krishnamurthy [1990]. Kim [1982] and Kim [1984] describe join strategies<br>and the optimal use of available main memory.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>IV. Data Storage and <br>Querying<br>14. Query Optimization<br>529<br>© The McGraw−Hill <br>Companies, 2001<br>C<br>H<br>A<br>P<br>T<br>E<br>R<br>1<br>4<br>Query Optimization<br>Query optimization is the process of selecting the most efﬁcient query-evaluation<br>plan from among the many strategies usually possible for processing a given query,<br>especially if the query is complex. We do not expect users to write their queries so<br>that they can be processed efﬁciently. Rather, we expect the system to construct a<br>query-evaluation plan that minimizes the cost of query evaluation. This is where<br>query optimization comes into play.<br>One aspect of optimization occurs at the relational-algebra level, where the system<br>attempts to ﬁnd an expression that is equivalent to the given expression, but more<br>efﬁcient to execute. Another aspect is selecting a detailed strategy for processing the<br>query, such as choosing the algorithm to use for executing an operation, choosing the<br>speciﬁc indices to use, and so on.<br>The difference in cost (in terms of evaluation time) between a good strategy and a<br>bad strategy is often substantial, and may be several orders of magnitude. Hence, it<br>is worthwhile for the system to spend a substantial amount of time on the selection<br>of a good strategy for processing a query, even if the query is executed only once.<br>14.1<br>Overview<br>Consider the relational-algebra expression for the query “Find the names of all cus-<br>tomers who have an account at any branch located in Brooklyn.”<br>Πcustomer-name (σbranch−city = “Brooklyn” (branch<br> (account<br> depositor)))<br>This expression constructs a large intermediate relation, branch<br> account<br> depositor.<br>However, we are interested in only a few tuples of this relation (those pertaining to<br>branches located in Brooklyn), and in only one of the six attributes of this relation.<br>Since we are concerned with only those tuples in the branch relation that pertain to<br>branches located in Brooklyn, we do not need to consider those tuples that do not<br>529<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>IV. Data Storage and <br>Querying<br>14. Query Optimization<br>530<br>© The McGraw−Hill <br>Companies, 2001<br>530<br>Chapter 14<br>Query Optimization<br>Π customer-name<br>branch<br>depositor<br>σ branch-city=Brooklyn<br>account<br>depositor<br>account<br>σ branch-city=Brooklyn<br>(a) Initial expression tree<br>(b) Transformed expression tree<br>branch<br>Π customer-name<br>Figure 14.1<br>Equivalent expressions.<br>have branch-city = “Brooklyn”. By reducing the number of tuples of the branch rela-<br>tion that we need to access, we reduce the size of the intermediate result. Our query<br>is now represented by the relational-algebra expression<br>Πcustomer-name ( (σbranch-city = “Brooklyn” (branch))<br> (account<br> depositor))<br>which is equivalent to our original algebra expression, but which generates smaller<br>intermediate relations. Figure 14.1 depicts the initial and transformed expressions.<br>Given a relational-algebra expression, it is the job of the query optimizer to come<br>up with a query-evaluation plan that computes the same result as the given expres-<br>sion, and is the least costly way of generating the result (or, at least, is not much<br>costlier than the least costly way).<br>To choose among different query-evaluation plans, the optimizer has to estimate<br>the cost of each evaluation plan. Computing the precise cost of evaluation of a plan is<br>usually not possible without actually evaluating the plan. Instead, optimizers make<br>use of statistical information about the relations, such as relation sizes and index<br>depths, to make a good estimate of the cost of a plan. Disk access, which is slow<br>compared to memory access, usually dominates the cost of processing a query.<br>In Section 14.2 we describe how to estimate statistics of the results of each opera-<br>tion in a query plan. Using these statistics with the cost formulae in Chapter 13 allows<br>us to estimate the costs of individual operation. The individual costs are combined to<br>determine the estimated cost of evaluating a given relational-algebra expression, as<br>outlined earlier in Section 13.7.<br>To ﬁnd the least-costly query-evaluation plan, the optimizer needs to generate al-<br>ternative plans that produce the same result as the given expression, and to choose<br>the least costly one. Generation of query-evaluation plans involves two steps: (1) gen-<br>erating expressions that are logically equivalent to the given expression and (2) an-<br>notating the resultant expressions in alternative ways to generate alternative query<br>evaluation plans. The two steps are interleaved in the query optimizer—some ex-<br>pressions are generated and annotated, then further expressions are generated and<br>annotated, and so on.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>IV. Data Storage and <br>Querying<br>14. Query Optimization<br>531<br>© The McGraw−Hill <br>Companies, 2001<br>14.2<br>Estimating Statistics of Expression Results<br>531<br>To implement the ﬁrst step, the query optimizer must generate expressions equiv-<br>alent to a given expression. It does so by means of equivalence rules that specify how<br>to transform an expression into a logically equivalent one. We describe these rules in<br>Section 14.3.1. In Section 14.4, we describe how to choose a query-evaluation plan.<br>We can choose one based on the estimated cost of the plans. Since the cost is an es-<br>timate, the selected plan is not necessarily the least costly plan; however, as long as<br>the estimates are good, the plan is likely to be the least costly one, or not much more<br>costly than it. Such optimization, called cost-based optimization, is described in Sec-<br>tion 14.4.2.<br>Materialized views help to speed up processing of certain queries. In Section 14.5,<br>we study how to “maintain” materialized views—that is, to keep them up-to-date—<br>and how to perform query optimization with materialized views.<br>14.2<br>Estimating Statistics of Expression Results<br>The cost of an operation depends on the size and other statistics of its inputs. Given<br>an expression such as a<br> (b<br> c) to estimate the cost of joining a with (b<br> c), we<br>need to have estimates of statistics such as the size of b<br> c.<br>In this section we ﬁrst list some statistics about database relations that are stored in<br>database system catalogs, and then show how to use the statistics to estimate statistics<br>on the results of various relational operations.<br>One thing that will become clear later in this section is that the estimates are not<br>very accurate, since they are based on assumptions that may not hold exactly. A<br>query evaluation plan that has the lowest estimated execution cost may therefore<br>not actually have the lowest actual execution cost. However, real-world experience<br>has shown that even if estimates are not precise, the plans with the lowest estimated<br>costs usually have actual execution costs that are either the lowest actual execution<br>costs, or are close to the lowest actual execution costs.<br>14.2.1<br>Catalog Information<br>The DBMS catalog stores the following statistical information about database rela-<br>tions:<br>• nr, the number of tuples in the relation r.<br>• br, the number of blocks containing tuples of relation r.<br>• lr, the size of a tuple of relation r in bytes.<br>• fr, the blocking factor of relation r—that is, the number of tuples of relation r<br>that ﬁt into one block.<br>• V (A, r), the number of distinct values that appear in the relation r for attribute<br>A. This value is the same as the size of ΠA(r). If A is a key for relation r, V (A, r)<br>is nr.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>IV. Data Storage and <br>Querying<br>14. Query Optimization<br>532<br>© The McGraw−Hill <br>Companies, 2001<br>532<br>Chapter 14<br>Query Optimization<br>The last statistic, V (A, r), can also be maintained for sets of attributes, if desired,<br>instead of just for individual attributes. Thus, given a set of attributes, A, V (A, r) is<br>the size of ΠA(r).<br>If we assume that the tuples of relation r are stored together physically in a ﬁle,<br>the following equation holds:<br>br =<br>nr<br>fr<br><br>Statistics about indices, such as the heights of B+-tree indices and number of leaf<br>pages in the indices, are also maintained in the catalog.<br>If we wish to maintain accurate statistics, then, every time a relation is modiﬁed,<br>we must also update the statistics. This update incurs a substantial amount of over-<br>head. Therefore, most systems do not update the statistics on every modiﬁcation. In-<br>stead, they update the statistics during periods of light system load. As a result, the<br>statistics used for choosing a query-processing strategy may not be completely accu-<br>rate. However, if not too many updates occur in the intervals between the updates of<br>the statistics, the statistics will be sufﬁciently accurate to provide a good estimation<br>of the relative costs of the different plans.<br>The statistical information noted here is simpliﬁed. Real-world optimizers often<br>maintain further statistical information to improve the accuracy of their cost esti-<br>mates of evaluation plans. For instance, some databases store the distribution of val-<br>ues for each attribute as a histogram: in a histogram the values for the attribute are<br>divided into a number of ranges, and with each range the histogram associates the<br>number of tuples whose attribute value lies in that range. As an example of a his-<br>togram, the range of values for an attribute age of a relation person could be divided<br>into 0–9, 10–19, . . . , 90–99 (assuming a maximum age of 99). With each range we<br>store a count of the number of person tuples whose age values lie in that range. With-<br>out such histogram information, an optimizer would have to assume that the distri-<br>bution of values is uniform; that is, each range has the same count.<br>14.2.2<br>Selection Size Estimation<br>The size estimate of the result of a selection operation depends on the selection predi-<br>cate. We ﬁrst consider a single equality predicate, then a single comparison predicate,<br>and ﬁnally combinations of predicates.<br>• σA = a(r): If we assume uniform distribution of values (that is, each value ap-<br>pears with equal probability), the selection result can be estimated to have<br>nr/V (A, r) tuples, assuming that the value a appears in attribute A of some<br>record of r. The assumption that the value a in the selection appears in some<br>record is generally true, and cost estimates often make it implicitly. However,<br>it is often not realistic to assume that each value appears with equal proba-<br>bility. The branch-name attribute in the account relation is an example where<br>the assumption is not valid. There is one tuple in the account relation for<br>each account. It is reasonable to expect that the large branches have more ac-<br>counts than smaller branches. Therefore, certain branch-name values appear<br>with greater probability than do others. Despite the fact that the uniform-<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>IV. Data Storage and <br>Querying<br>14. Query Optimization<br>533<br>© The McGraw−Hill <br>Companies, 2001<br>14.2<br>Estimating Statistics of Expression Results<br>533<br>distribution assumption is often not correct, it is a reasonable approximation<br>of reality in many cases, and it helps us to keep our presentation relatively<br>simple.<br>• σA≤v(r): Consider a selection of the form σA≤v(r). If the actual value used<br>in the comparison (v) is available at the time of cost estimation, a more ac-<br>curate estimate can be made. The lowest and highest values (min(A, r) and<br>max(A, r)) for the attribute can be stored in the catalog. Assuming that values<br>are uniformly distributed, we can estimate the number of records that will<br>satisfy the condition A ≤v as 0 if v &lt; min(A, r), as nr if v ≥max(A, r), and<br>nr ·<br>v −min(A, r)<br>max(A, r) −min(A, r)<br>otherwise.<br>In some cases, such as when the query is part of a stored procedure, the<br>value v may not be available when the query is optimized. In such cases, we<br>will assume that approximately one-half the records will satisfy the compari-<br>son condition. That is, we assume the result has nr/2 tuples; the estimate may<br>be very inaccurate, but is the best we can do without any further information.<br>• Complex selections:<br>  Conjunction: A conjunctive selection is a selection of the form<br>σθ1∧θ2∧···∧θn(r)<br>We can estimate the result size of such a selection: For each θi, we esti-<br>mate the size of the selection σθi(r), denoted by si, as described previ-<br>ously. Thus, the probability that a tuple in the relation satisﬁes selection<br>condition θi is si/nr.<br>The preceding probability is called the selectivity of the selection σθi(r).<br>Assuming that the conditions are independent of each other, the probabil-<br>ity that a tuple satisﬁes all the conditions is simply the product of all these<br>probabilities. Thus, we estimate the number of tuples in the full selection<br>as<br>nr ∗s1 ∗s2 ∗· · · ∗sn<br>nnr<br>  Disjunction: A disjunctive selection is a selection of the form<br>σθ1∨θ2∨···∨θn(r)<br>A disjunctive condition is satisﬁed by t</span><br><br><span style="background-color: #CAFFBF;" title="Chunk 67 | Start: 1340134 | End: 1360134 | Tokens: 3405">he union of all records satisfying<br>the individual, simple conditions θi.<br>As before, let si/nr denote the probability that a tuple satisﬁes condi-<br>tion θi. The probability that the tuple will satisfy the disjunction is then 1<br>minus the probability that it will satisfy none of the conditions:<br>1 −(1 −s1<br>nr<br>) ∗(1 −s2<br>nr<br>) ∗· · · ∗(1 −sn<br>nr<br>)<br>Multiplying this value by nr gives us the estimated number of tuples that<br>satisfy the selection.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>IV. Data Storage and <br>Querying<br>14. Query Optimization<br>534<br>© The McGraw−Hill <br>Companies, 2001<br>534<br>Chapter 14<br>Query Optimization<br>  Negation: In the absence of nulls, the result of a selection σ¬θ(r) is simply<br>the tuples of r that are not in σθ(r). We already know how to estimate<br>the number of tuples in σθ(r). The number of tuples in σ¬θ(r) is therefore<br>estimated to be n(r) minus the estimated number of tuples in σθ(r).<br>We can account for nulls by estimating the number of tuples for which<br>the condition θ would evaluate to unknown, and subtracting that number<br>from the above estimate ignoring nulls. Estimating that number would<br>require extra statistics to be maintained in the catalog.<br>14.2.3<br>Join Size Estimation<br>In this section, we see how to estimate the size of the result of a join.<br>The Cartesian product r × s contains nr ∗ns tuples. Each tuple of r × s occupies<br>lr + ls bytes, from which we can calculate the size of the Cartesian product.<br>Estimating the size of a natural join is somewhat more complicated than estimat-<br>ing the size of a selection or of a Cartesian product. Let r(R) and s(S) be relations.<br>• If R ∩S = ∅—that is, the relations have no attribute in common—then r<br> s<br>is the same as r × s, and we can use our estimation technique for Cartesian<br>products.<br>• If R ∩S is a key for R, then we know that a tuple of s will join with at most<br>one tuple from r. Therefore, the number of tuples in r<br> s is no greater than<br>the number of tuples in s. The case where R ∩S is a key for S is symmetric<br>to the case just described. If R ∩S forms a foreign key of S, referencing R, the<br>number of tuples in r<br> s is exactly the same as the number of tuples in s.<br>• The most difﬁcult case is when R ∩S is a key for neither R nor S. In this<br>case, we assume, as we did for selections, that each value appears with equal<br>probability. Consider a tuple t of r, and assume R ∩S = {A}. We estimate<br>that tuple t produces<br>ns<br>V (A, s)<br>tuples in r<br> s, since this number is the average number of tuples in s with a<br>given value for the attributes A. Considering all the tuples in r, we estimate<br>that there are<br>nr ∗ns<br>V (A, s)<br>tuples in r<br> s. Observe that, if we reverse the roles of r and s in the preceding<br>estimate, we obtain an estimate of<br>nr ∗ns<br>V (A, r)<br>tuples in r<br> s. These two estimates differ if V (A, r) ̸= V (A, s). If this situation<br>occurs, there are likely to be dangling tuples that do not participate in the join.<br>Thus, the lower of the two estimates is probably the more accurate one.<br>The preceding estimate of join size may be too high if the V (A, r) values<br>for attribute A in r have few values in common with the V (A, s) values for<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>IV. Data Storage and <br>Querying<br>14. Query Optimization<br>535<br>© The McGraw−Hill <br>Companies, 2001<br>14.2<br>Estimating Statistics of Expression Results<br>535<br>attribute A in s. However, this situation is unlikely to happen in the real world,<br>since dangling tuples either do not exist, or constitute only a small fraction of<br>the tuples, in most real-world relations. More important, the preceding esti-<br>mate depends on the assumption that each value appears with equal proba-<br>bility. More sophisticated techniques for size estimation have to be used if this<br>assumption does not hold.<br>We can estimate the size of a theta join r<br>θ s by rewriting the join as σθ(r × s),<br>and using the size estimates for Cartesian products along with the size estimates for<br>selections, which we saw in Section 14.2.2.<br>To illustrate all these ways of estimating join sizes, consider the expression<br>depositor<br> customer<br>Assume the following catalog information about the two relations:<br>• ncustomer = 10000.<br>• fcustomer = 25, which implies that bcustomer = 10000/25 = 400.<br>• ndepositor = 5000.<br>• fdepositor = 50, which implies that bdepositor = 5000/50 = 100.<br>• V (customer-name, depositor) = 2500, which implies that, on average, each<br>customer has two accounts.<br>Also assume that customer-name in depositor is a foreign key on customer.<br>In our example of depositor<br> customer, customer-name in depositor is a foreign key<br>referencing customer; hence, the size of the result is exactly ndepositor, which is 5000.<br>Let us now compute the size estimates for depositor<br> customer without using infor-<br>mation about foreign keys. Since V (customer-name, depositor) = 2500 and V (customer-<br>name, customer) = 10000, the two estimates we get are 5000 ∗10000/2500 = 20, 000<br>and 5000 ∗10000/10000 = 5000, and we choose the lower one. In this case, the lower<br>of these estimates is the same as that which we computed earlier from information<br>about foreign keys.<br>14.2.4<br>Size Estimation for Other Operations<br>We outline below how to estimate the sizes of the results of other relational algebra<br>operations.<br>Projection: The estimated size (number of records or number of tuples) of a projec-<br>tion of the form ΠA(r) is V (A, r), since projection eliminates duplicates.<br>Aggregation: The size of AGF (r) is simply V (A, r), since there is one tuple in AGF (r)<br>for each distinct value of A.<br>Set operations: If the two inputs to a set operation are selections on the same rela-<br>tion, we can rewrite the set operation as disjunctions, conjunctions, or nega-<br>tions. For example, σθ1(r) ∪σθ2(r) can be rewritten as σθ1∨θ2(r). Similarly, we<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>IV. Data Storage and <br>Querying<br>14. Query Optimization<br>536<br>© The McGraw−Hill <br>Companies, 2001<br>536<br>Chapter 14<br>Query Optimization<br>can rewrite intersections as conjunctions, and we can rewrite set difference by<br>using negation, so long as the two relations participating in the set operations<br>are selections on the same relation. We can then use the estimates for selections<br>involving conjunctions, disjunctions, and negation in Section 14.2.2.<br>If the inputs are not selections on the same relation, we estimate the sizes<br>this way: The estimated size of r ∪s is the sum of the sizes of r and s. The<br>estimated size of r ∩s is the minimum of the sizes of r and s. The estimated<br>size of r −s is the same size as r. All three estimates may be inaccurate, but<br>provide upper bounds on the sizes.<br>Outer join: The estimated size of r<br> s is the size of r<br> s plus the size of r; that of<br>r<br> s is symmetric, while that of r<br> s is the size of r<br> s plus the sizes of<br>r and s. All three estimates may be inaccurate, but provide upper bounds on<br>the sizes.<br>14.2.5<br>Estimation of Number of Distinct Values<br>For selections, the number of distinct values of an attribute (or set of attributes) A in<br>the result of a selection, V (A, σθ(r)), can be estimated in these ways:<br>• If the selection condition θ forces A to take on a speciﬁed value (e.g., A = 3),<br>V (A, σθ(r)) = 1.<br>• If θ forces A to take on one of a speciﬁed set of values (e.g., (A = 1 ∨A =<br>3 ∨A = 4)), then V (A, σθ(r)) is set to the number of speciﬁed values.<br>• If the selection condition θ is of the form A op v, where op is a comparison<br>operator, V (A, σθ(r)) is estimated to be V (A, r) ∗s, where s is the selectivity<br>of the selection.<br>• In all other cases of selections, we assume that the distribution of A values<br>is independent of the distribution of the values on which selection conditions<br>are speciﬁed, and use an approximate estimate of min(V (A, r), nσθ(r)). A more<br>accurate estimate can be derived for this case using probability theory, but the<br>above approximation works fairly well.<br>For joins, the number of distinct values of an attribute (or set of attributes) A in the<br>result of a join, V (A, r<br> s), can be estimated in these ways:<br>• If all attributes in A are from r, V (A, r<br> s) is estimated as min(V (A, r), nr<br>s),<br>and similarly if all attributes in A are from s, V (A, r<br> s) is estimated to be<br>min(V (A, s), nr<br>s).<br>• If A contains attributes A1 from r and A2 from s, then V (A, r<br> s) is estimated<br>as<br>min(V (A1, r) ∗V (A2 −A1, s), V (A1 −A2, r) ∗V (A2, s), nr<br>s)<br>Note that some attributes may be in A1 as well as in A2, and A1 −A2 and<br>A2−A1 denote, respectively, attributes in A that are only from r and attributes<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>IV. Data Storage and <br>Querying<br>14. Query Optimization<br>537<br>© The McGraw−Hill <br>Companies, 2001<br>14.3<br>Transformation of Relational Expressions<br>537<br>in A that are only from s. Again, more accurate estimates can be derived by<br>using probability theory, but the above approximations work fairly well.<br>The estimates of distinct values are straightforward for projections: They are the<br>same in ΠA(r) as in r. The same holds for grouping attributes of aggregation. For<br>results of sum, count, and average, we can assume, for simplicity, that all aggregate<br>values are distinct. For min(A) and max(A), the number of distinct values can be es-<br>timated as min(V (A, r), V (G, r)), where G denotes the grouping attributes. We omit<br>details of estimating distinct values for other operations.<br>14.3<br>Transformation of Relational Expressions<br>So far, we have studied algorithms to evaluate extended relational-algebra opera-<br>tions, and have estimated their costs. As mentioned at the start of this chapter, a<br>query can be expressed in several different ways, with different costs of evaluation.<br>In this section, rather than take the relational expression as given, we consider alter-<br>native, equivalent expressions.<br>Two relational-algebra expressions are said to be equivalent if, on every legal data-<br>base instance, the two expressions generate the same set of tuples. (Recall that a legal<br>database instance is one that satisﬁes all the integrity constraints speciﬁed in the data-<br>base schema.) Note that the order of the tuples is irrelevant; the two expressions may<br>generate the tuples in different orders, but would be considered equivalent as long<br>as the set of tuples is the same.<br>In SQL, the inputs and outputs are multisets of tuples, and a multiset version of the<br>relational algebra is used for evaluating SQL queries. Two expressions in the multiset<br>version of the relational algebra are said to be equivalent if on every legal database<br>the two expressions generate the same multiset of tuples. The discussion in this chap-<br>ter is based on the relational algebra. We leave extensions to the multiset version of<br>the relational algebra to you as exercises.<br>14.3.1<br>Equivalence Rules<br>An equivalence rule says that expressions of two forms are equivalent. We can re-<br>place an expression of the ﬁrst form by an expression of the second form, or vice<br>versa—that is we can replace an expression of the second form by an expression<br>of the ﬁrst form—since the two expressions would generate the same result on any<br>valid database. The optimizer uses equivalence rules to transform expressions into<br>other logically equivalent expressions.<br>We now list a number of general equivalence rules on relational-algebra expres-<br>sions. Some of the equivalences listed appear in Figure 14.2. We use θ, θ1, θ2, and<br>so on to denote predicates, L1, L2, L3, and so on to denote lists of attributes, and<br>E, E1, E2, and so on to denote relational-algebra expressions. A relation name r is<br>simply a special case of a relational-algebra expression, and can be used wherever E<br>appears.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>IV. Data Storage and <br>Querying<br>14. Query Optimization<br>538<br>© The McGraw−Hill <br>Companies, 2001<br>538<br>Chapter 14<br>Query Optimization<br>θ<br>E1<br>E2<br>θ<br>E2<br>E1<br>Rule 5<br>E3<br>E1<br>E2<br>E2<br>E3<br>E1<br>Rule 6a<br>Rule 7a<br>If θ only has<br>attributes from E1<br>E1<br>E2<br>E1<br>E2<br>σθ<br>σθ<br>Figure 14.2<br>Pictorial representation of equivalences.<br>1. Conjunctive selection operations can be deconstructed into a sequence of in-<br>dividual selections. This transformation is referred to as a cascade of σ.<br>σθ1∧θ2(E) = σθ1(σθ2(E))<br>2. Selection operations are commutative.<br>σθ1(σθ2(E)) = σθ2(σθ1(E))<br>3. Only the ﬁnal operations in a sequence of projection operations are needed,<br>the others can be omitted. This transformation can also be referred to as a<br>cascade of Π.<br>ΠL1(ΠL2(. . . (ΠLn(E)) . . .)) = ΠL1(E)<br>4. Selections can be combined with Cartesian products and theta joins.<br>a. σθ(E1 × E2) = E1<br>θ E2<br>This expression is just the deﬁnition of the theta join.<br>b. σθ1(E1<br>θ2 E2) = E1<br>θ1∧θ2 E2<br>5. Theta-join operations are commutative.<br>E1<br>θ E2 = E2<br>θ E1<br>Actually, the order of attributes differs between the left-hand side and right-<br>hand side, so the equivalence does not hold if the order of attributes is taken<br>into account. A projection operation can be added to one of the sides of the<br>equivalence to appropriately reorder attributes, but for simplicity we omit the<br>projection and ignore the attribute order in most of our examples.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>IV. Data Storage and <br>Querying<br>14. Query Optimization<br>539<br>© The McGraw−Hill <br>Companies, 2001<br>14.3<br>Transformation of Relational Expressions<br>539<br>Recall that the natural-join operator is simply a special case of the theta-join<br>operator; hence, natural joins are also commutative.<br>6.<br>a. Natural-join operations are associative.<br>(E1<br> E2)<br> E3 = E1<br> (E2<br> E3)<br>b. Theta joins are associative in the following manner:<br>(E1<br>θ1 E2)<br>θ2∧θ3 E3 = E1<br>θ1∧θ3 (E2<br>θ2 E3)<br>where θ2 involves attributes from only E2 and E3. Any of these conditions<br>may be empty; hence, it follows that the Cartesian product (×) operation<br>is also associative. The commutativity and associativity of join operations<br>are important for join reordering in query optimization.<br>7. The selection operation distributes over the theta-join operation under the fol-<br>lowing two conditions:<br>a. It distributes when all the attributes in selection condition θ0 involve only<br>the attributes of one of the expressions (say, E1) being joined.<br>σθ0(E1<br>θ E2) = (σθ0(E1))<br>θ E2<br>b. It distributes when selection condition θ1 involves only the attributes of<br>E1 and θ2 involves only the attributes of E2.<br>σθ1∧θ2(E1<br>θ E2) = (σθ1(E1))<br>θ (σθ2(E2))<br>8. The projection operation distributes over the theta-join operation under the<br>following conditions.<br>a. Let L1 and L2 be attributes of E1 and E2, respectively. Suppose that the<br>join condition θ involves only attributes in L1 ∪L2. Then,<br>ΠL1∪L2(E1<br>θ E2) = (ΠL1(E1))<br>θ (ΠL2(E2))<br>b. Consider a join E1<br>θ E2. Let L1 and L2 be sets of attributes from E1<br>and E2, respectively. Let L3 be attributes of E1 that are involved in join<br>condition θ, but are not in L1 ∪L2, and let L4 be attributes of E2 that are<br>involved in join condition θ, but are not in L1 ∪L2. Then,<br>ΠL1∪L2(E1<br>θ E2) = ΠL1∪L2((ΠL1∪L3(E1))<br>θ (ΠL2∪L4(E2)))<br>9. The set operations union and intersection are commutative.<br>E1 ∪E2 = E2 ∪E1<br>E1 ∩E2 = E2 ∩E1<br>Set difference is not commutative.<br>10. Set union and intersection are associative.<br>(E1 ∪E2) ∪E3 = E1 ∪(E2 ∪E3)<br>(E1 ∩E2) ∩E3 = E1 ∩(E2 ∩E3)<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>IV. Data Storage and <br>Querying<br>14. Query Optimization<br>540<br>© The McGraw−Hill <br>Companies, 2001<br>540<br>Chapter 14<br>Query Optimization<br>11. The selection operation distributes over the union, intersection, and set–<br>difference operations.<br>σP (E1 −E2) = σP (E1) −σP (E2)<br>Similarly, the preceding equivalence, with −replaced with either ∪or ∩, also<br>holds. Further,<br>σP (E1 −E2) = σP (E1) −E2<br>The preceding equivalence, with −replaced by ∩, also holds, but does not<br>hold if −is replaced by ∪.<br>12. The projection operation distributes over the union operation.<br>ΠL(E1 ∪E2) = (ΠL(E1)) ∪(ΠL(E2))<br>This is only a partial list of equivalences. More equivalences involving extended<br>relational operators, such as the outer join and aggregation, are discussed in the ex-<br>ercises.<br>14.3.2<br>Examples of Transformations<br>We now illustrate the use of the equivalence rules. We use our bank example with the<br>relation schemas:<br>Branch-schema = (branch-name, branch-city, assets)<br>Account-schema = (account-number, branch-name, balance)<br>Depositor-schema = (customer-name, account-number)<br>The relations branch, account, and depositor are instances of these schemas.<br>In our example in Section 14.1, the expression<br>Πcustomer-name(σbranch-city = “Brooklyn”(branch<br> (account<br> depositor)))<br>was transformed into the following expression,<br>Πcustomer-name((σbranch-city = “Brooklyn”(branch))<br> (account<br> depositor))<br>which is equivalent to our original algebra expression, but generates smaller inter-<br>mediate relations. We can carry out this transformation by using rule 7.a. Remember<br>that the rule merely says that the two expressions are equivalent; it does not say that<br>one is better than the other.<br>Multiple equivalence rules can be used, one after the other, on a query or on parts<br>of the query. As an illustration, suppose that we modify our original query to restrict<br>attention to customers who have a balance over $1000. The new relational-algebra<br>query is<br>Πcustomer-name (σbranch-city = “Brooklyn” ∧balance &gt;1000<br>(branch<br> (account<br> depositor)))<br>We cannot apply the selection predicate directly to the branch relation, since the pred-<br>icate involves attributes of both the branch and account relation. However, we can ﬁrst<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>IV. Data Storage and <br>Querying<br>14. Query Optimization<br>541<br>© The McGraw−Hill <br>Companies, 2001<br>14.3<br>Transformation of Relational Expressions<br>541<br>apply rule 6.a (associativity of natural join) to transform the join branch<br> (account<br><br>depositor) into (branch<br> account)<br> depositor:<br>Πcustomer-name (σbranch-city = “Brooklyn” ∧balance &gt;1000<br>((branch<br> account)<br> depositor))<br>Then, using rule 7.a, we can rewrite our query as<br>Πcustomer-name ((σbranch-city = “Brooklyn”∧balance&gt;1000<br>(branch<br> account))<br> depositor)<br>Let us examine the selection subexpression within this expression. Using rule 1, we<br>can break the selection into two selections, to get the following subexpression:<br>σbranch-city = “Brooklyn” (σbalance &gt; 1000 (branch<br> account))<br>Both of the preceding expressions select tuples with branch-city = “Brooklyn” and<br>balance &gt; 1000. However, the latter form of the expression provides a new opportu-<br>nity to apply the “perform selections early” rule, resulting in the subexpression<br>σbranch-city = “Brooklyn” (branch)<br> σbalance&gt;1000 (account)<br>Figure 14.3 depicts the initial expression and the ﬁnal expression after all these<br>transformations. We could equally well have used rule 7.b to get the ﬁnal expression<br>directly, without using rule 1 to break the selection into two selections. In fact, rule 7.b<br>can itself be derived from rules 1 and 7.a<br>A set of equivalence rules is said to be minimal if no rule can be derived from any<br>combination of the others. The preceding example illustrates that the set of equiva-<br>lence rules in Section 14.3.1 is not minimal. An expression equivalent to the original<br>expression may be generated in different ways; the number of different ways of gen-<br>erating an expression increases when we use a nonminimal set of equivalence rules.<br>Query optimizers therefore use minimal sets of equivalence rules.<br>Π customer-name<br>branch<br>depositor<br>σbranch-city=Brooklyn<br>account<br>σbranch-city=Brooklyn<br>balance &lt; 1000<br>(a) Initial expression tree<br>(b) Tree after multiple transformations<br>branch<br>Π customer-name<br>account<br>depositor<br>σbalance &lt; 1000<br>^<br>Figure 14.3<br>Multiple transformations.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>IV. Data Storage and <br>Querying<br>14. Query Optimization<br>542<br>© The McGraw−Hill <br>Companies, 2001<br>542<br>Chapter 14<br>Query Optimization<br>Now consider the following form of our example query:<br>Πcustomer-name ((σbranch-city = “Brooklyn” (branch)<br> account)<br> dep</span><br><br><span style="background-color: #9BF6FF;" title="Chunk 68 | Start: 1360136 | End: 1380136 | Tokens: 3286">ositor)<br>When we compute the subexpression<br>(σbranch-city = “Brooklyn” (branch)<br> account)<br>we obtain a relation whose schema is<br>(branch-name, branch-city, assets, account-number, balance)<br>We can eliminate several attributes from the schema, by pushing projections based<br>on equivalence rules 8.a and 8.b. The only attributes that we must retain are those<br>that either appear in the result of the query or are needed to process subsequent<br>operations. By eliminating unneeded attributes, we reduce the number of columns<br>of the intermediate result. Thus, we reduce the size of the intermediate result. In our<br>example, the only attribute we need from the join of branch and account is account-<br>number. Therefore, we can modify the expression to<br>Πcustomer-name (<br>( Πaccount-number ((σbranch-city = “Brooklyn” (branch))<br> account))<br> depositor)<br>The projection Πaccount-number reduces the size of the intermediate join results.<br>14.3.3<br>Join Ordering<br>A good ordering of join operations is important for reducing the size of temporary<br>results; hence, most query optimizers pay a lot of attention to the join order. As men-<br>tioned in Chapter 3 and in equivalence rule 6.a, the natural-join operation is associa-<br>tive. Thus, for all relations r1, r2, and r3,<br>(r1<br> r2)<br> r3 = r1<br> (r2<br> r3)<br>Although these expressions are equivalent, the costs of computing them may differ.<br>Consider again the expression<br>Πcustomer-name ((σbranch-city = “Brooklyn” (branch))<br> account<br> depositor)<br>We could choose to compute account<br> depositor ﬁrst, and then to join the result with<br>σbranch-city = “Brooklyn” (branch)<br>However, account<br> depositor is likely to be a large relation, since it contains one tuple<br>for every account. In contrast,<br>σbranch-city = “Brooklyn” (branch)<br> account<br>is probably a small relation. To see that it is, we note that, since the bank has a large<br>number of widely distributed branches, it is likely that only a small fraction of the<br>bank’s customers have accounts in branches located in Brooklyn. Thus, the preced-<br>ing expression results in one tuple for each account held by a resident of Brooklyn.<br>Therefore, the temporary relation that we must store is smaller than it would have<br>been had we computed account<br> depositor ﬁrst.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>IV. Data Storage and <br>Querying<br>14. Query Optimization<br>543<br>© The McGraw−Hill <br>Companies, 2001<br>14.3<br>Transformation of Relational Expressions<br>543<br>There are other options to consider for evaluating our query. We do not care about<br>the order in which attributes appear in a join, since it is easy to change the order<br>before displaying the result. Thus, for all relations r1 and r2,<br>r1<br> r2 = r2<br> r1<br>That is, natural join is commutative (equivalence rule 5).<br>Using the associativity and commutativity of the natural join (rules 5 and 6), we<br>can consider rewriting our relational-algebra expression as<br>Πcustomer-name (((σbranch-city = “Brooklyn” (branch))<br> depositor)<br> account)<br>That is, we could compute<br>(σbranch-city = “Brooklyn” (branch))<br> depositor<br>ﬁrst, and, after that, join the result with account. Note, however, that there are no<br>attributes in common between Branch-schema and Depositor-schema, so the join is just<br>a Cartesian product. If there are b branches in Brooklyn and d tuples in the depositor<br>relation, this Cartesian product generates b ∗d tuples, one for every possible pair of<br>depositor tuple and branches (without regard for whether the account in depositor is<br>maintained at the branch). Thus, it appears that this Cartesian product will produce<br>a large temporary relation. As a result, we would reject this strategy. However, if<br>the user had entered the preceding expression, we could use the associativity and<br>commutativity of the natural join to transform this expression to the more efﬁcient<br>expression that we used earlier.<br>14.3.4<br>Enumeration of Equivalent Expressions<br>Query optimizers use equivalence rules to systematically generate expressions equiv-<br>alent to the given query expression. Conceptually, the process proceeds as follows.<br>Given an expression, if any subexpression matches one side of an equivalence rule,<br>the optimizer generates a new expression where the subexpression is transformed to<br>match the other side of the rule. This process continues until no more new expres-<br>sions can be generated.<br>The preceding process is costly both in space and in time. Here is how the space<br>requirement can be reduced: If we generate an expression E1 from an expression<br>E2 by using an equivalence rule, then E1 and E2 are similar in structure, and have<br>subexpressions that are identical. Expression-representation techniques that allow<br>both expressions to point to shared subexpressions can reduce the space requirement<br>signiﬁcantly, and many query optimizers use them.<br>Moreover, it is not always necessary to generate every expression that can be gen-<br>erated with the equivalence rules. If an optimizer takes cost estimates of evaluation<br>into account, it may be able to avoid examining some of the expressions, as we shall<br>see in Section 14.4. We can reduce the time required for optimization by using tech-<br>niques such as these.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>IV. Data Storage and <br>Querying<br>14. Query Optimization<br>544<br>© The McGraw−Hill <br>Companies, 2001<br>544<br>Chapter 14<br>Query Optimization<br>14.4<br>Choice of Evaluation Plans<br>Generation of expressions is only part of the query-optimization process, since each<br>operation in the expression can be implemented with different algorithms. An eval-<br>uation plan is therefore needed to deﬁne exactly what algorithm should be used for<br>each operation, and how the execution of the operations should be coordinated. Fig-<br>ure 14.4 illustrates one possible evaluation plan for the expression from Figure 14.3.<br>As we have seen, several different algorithms can be used for each relational opera-<br>tion, giving rise to alternative evaluation plans. Further, decisions about pipelining<br>have to be made. In the ﬁgure, the edges from the selection operations to the merge<br>join operation are marked as pipelined; pipelining is feasible if the selection oper-<br>ations generate their output sorted on the join attributes. They would do so if the<br>indices on branch and account store records with equal values for the index attributes<br>sorted by branch-name.<br>14.4.1<br>Interaction of Evaluation Techniques<br>One way to choose an evaluation plan for a query expression is simply to choose for<br>each operation the cheapest algorithm for evaluating it. We can choose any ordering<br>of the operations that ensures that operations lower in the tree are executed before<br>operations higher in the tree.<br>However, choosing the cheapest algorithm for each operation independently is not<br>necessarily a good idea. Although a merge join at a given level may be costlier than<br>a hash join, it may provide a sorted output that makes evaluating a later operation<br>(such as duplicate elimination, intersection, or another merge join) cheaper. Similarly,<br>a nested-loop join with indexing may provide opportunities for pipelining the results<br>to the next operation, and thus may be useful even if it is not the cheapest way of<br>σ branch-city=Brooklyn<br>branch<br>Π customer-name (sort to remove duplicates)<br>account<br>depositor<br>σ balance &lt; 1000<br>(use index 1)<br>(use linear scan)<br>(hash join)<br>(merge join)<br>pipeline<br>pipeline<br>Figure 14.4<br>An evaluation plan.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>IV. Data Storage and <br>Querying<br>14. Query Optimization<br>545<br>© The McGraw−Hill <br>Companies, 2001<br>14.4<br>Choice of Evaluation Plans<br>545<br>performing the join. To choose the best overall algorithm, we must consider even<br>nonoptimal algorithms for individual operations.<br>Thus, in addition to considering alternative expressions for a query, we must also<br>consider alternative algorithms for each operation in an expression. We can use rules<br>much like the equivalence rules to deﬁne what algorithms can be used for each op-<br>eration, and whether its result can be pipelined or must be materialized. We can use<br>these rules to generate all the query-evaluation plans for a given expression.<br>Given an evaluation plan, we can estimate its cost using statistics estimated by<br>the techniques in Section 14.2 coupled with cost estimates for various algorithms<br>and evaluation methods described in Chapter 13. That still leaves the problem of<br>choosing the best evaluation plan for a query. There are two broad approaches: The<br>ﬁrst searches all the plans, and chooses the best plan in a cost-based fashion. The<br>second uses heuristics to choose a plan. We discuss these approaches next. Practical<br>query optimizers incorporate elements of both approaches.<br>14.4.2<br>Cost-Based Optimization<br>A cost-based optimizer generates a range of query-evaluation plans from the given<br>query by using the equivalence rules, and chooses the one with the least cost. For<br>a complex query, the number of different query plans that are equivalent to a given<br>plan can be large. As an illustration, consider the expression<br>r1<br> r2<br> · · ·<br> rn<br>where the joins are expressed without any ordering. With n = 3, there are 12 different<br>join orderings:<br>r1<br> (r2<br> r3)<br>r1<br> (r3<br> r2)<br>(r2<br> r3)<br> r1<br>(r3<br> r2)<br> r1<br>r2<br> (r1<br> r3)<br>r2<br> (r3<br> r1)<br>(r1<br> r3)<br> r2<br>(r3<br> r1)<br> r2<br>r3<br> (r1<br> r2)<br>r3<br> (r2<br> r1)<br>(r1<br> r2)<br> r3<br>(r2<br> r1)<br> r3<br>In general, with n relations, there are (2(n −1))!/(n −1)! different join orders. (We<br>leave the computation of this expression for you to do in Exercise 14.10.) For joins<br>involving small numbers of relations, this number is acceptable; for example, with<br>n = 5, the number is 1680. However, as n increases, this number rises quickly. With<br>n = 7, the number is 665280; with n = 10, the number is greater than 17.6 billion!<br>Luckily, it is not necessary to generate all the expressions equivalent to a given<br>expression. For example, suppose we want to ﬁnd the best join order of the form<br>(r1<br> r2<br> r3)<br> r4<br> r5<br>which represents all join orders where r1, r2, and r3 are joined ﬁrst (in some order),<br>and the result is joined (in some order) with r4 and r5. There are 12 different join<br>orders for computing r1<br> r2<br> r3, and 12 orders for computing the join of this result<br>with r4 and r5. Thus, there appear to be 144 join orders to examine. However, once<br>we have found the best join order for the subset of relations {r1, r2, r3}, we can use<br>that order for further joins with r4 and r5, and can ignore all costlier join orders of<br>r1<br> r2<br> r3. Thus, instead of 144 choices to examine, we need to examine only<br>12 + 12 choices.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>IV. Data Storage and <br>Querying<br>14. Query Optimization<br>546<br>© The McGraw−Hill <br>Companies, 2001<br>546<br>Chapter 14<br>Query Optimization<br>procedure ﬁndbestplan(S)<br>if (bestplan[S].cost ̸= ∞)<br>return bestplan[S]<br>// else bestplan[S] has not been computed earlier, compute it now<br>for each non-empty subset S1 of S such that S1 ̸= S<br>P1 = ﬁndbestplan(S1)<br>P2 = ﬁndbestplan(S −S1)<br>A = best algorithm for joining results of P1 and P2<br>cost = P1.cost + P2.cost + cost of A<br>if cost &lt; bestplan[S].cost<br>bestplan[S].cost = cost<br>bestplan[S].plan = “execute P1.plan; execute P2.plan;<br>join results of P1 and P2 using A”<br>return bestplan[S]<br>Figure 14.5<br>Dynamic programming algorithm for join order optimization.<br>Using this idea, we can develop a dynamic-programming algorithm for ﬁnding op-<br>timal join orders. Dynamic programming algorithms store results of computations<br>and reuse them, a procedure that can reduce execution time greatly. A recursive pro-<br>cedure implementing the dynamic programming algorithm appears in Figure 14.5.<br>The procedure stores the evaluation plans it computes in an associative array<br>bestplan, which is indexed by sets of relations. Each element of the associative ar-<br>ray contains two components: the cost of the best plan of S, and the plan itself. The<br>value of bestplan[S].cost is assumed to be initialized to ∞if bestplan[S] has not yet<br>been computed.<br>The procedure ﬁrst checks if the best plan for computing the join of the given<br>set of relations S has been computed already (and stored in the associative array<br>bestplan); if so it returns the already computed plan. Otherwise, the procedure tries<br>every way of dividing S into two disjoint subsets. For each division, the procedure<br>recursively ﬁnds the best plans for each of the two subsets, and then computes the<br>cost of the overall plan by using that division. The procedure picks the cheapest plan<br>from among all the alternatives for dividing S into two sets. The cheapest plan and<br>its cost are stored in the array bestplan, and returned by the procedure. The time<br>complexity of the procedure can be shown to be O(3n) (see Exercise 14.11).<br>Actually, the order in which tuples are generated by the join of a set of relations<br>is also important for ﬁnding the best overall join order, since it can affect the cost of<br>further joins (for instance, if merge join is used). A particular sort order of the tuples<br>is said to be an interesting sort order if it could be useful for a later operation. For<br>instance, generating the result of r1<br> r2<br> r3 sorted on the attributes common with<br>r4 or r5 may be useful, but generating it sorted on the attributes common to only r1<br>and r2 is not useful. Using merge join for computing r1<br> r2<br> r3 may be costlier than<br>using some other join technique, but may provide an output sorted in an interesting<br>sort order.<br>Hence, it is not sufﬁcient to ﬁnd the best join order for each subset of the set of<br>n given relations. Instead, we have to ﬁnd the best join order for each subset, for<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>IV. Data Storage and <br>Querying<br>14. Query Optimization<br>547<br>© The McGraw−Hill <br>Companies, 2001<br>14.4<br>Choice of Evaluation Plans<br>547<br>each interesting sort order of the join result for that subset. The number of subsets of<br>n relations is 2n. The number of interesting sort orders is generally not large. Thus,<br>about 2n join expressions need to be stored. The dynamic-programming algorithm<br>for ﬁnding the best join order can be easily extended to handle sort orders. The cost of<br>the extended algorithm depends on the number of interesting orders for each subset<br>of relations; since this number has been found to be small in practice, the cost remains<br>at O(3n).<br>With n = 10, this number is around 59000, which is much better than the 17.6<br>billion different join orders. More important, the storage required is much less than<br>before, since we need to store only one join order for each interesting sort order of<br>each of 1024 subsets of r1, . . . , r10. Although both numbers still increase rapidly with<br>n, commonly occurring joins usually have less than 10 relations, and can be handled<br>easily.<br>We can use several techniques to reduce further the cost of searching through a<br>large number of plans. For instance, when examining the plans for an expression, we<br>can terminate after we examine only a part of the expression, if we determine that<br>the cheapest plan for that part is already costlier than the cheapest evaluation plan<br>for a full expression examined earlier. Similarly, suppose that we determine that the<br>cheapest way of evaluating a subexpression is costlier than the cheapest evaluation<br>plan for a full expression examined earlier. Then, no full expression involving that<br>subexpression needs to be examined. We can further reduce the number of evaluation<br>plans that need to be considered fully by ﬁrst making a heuristic guess of a good plan,<br>and estimating that plan’s cost. Then, only a few competing plans will require a full<br>analysis of cost. These optimizations can reduce the overhead of query optimization<br>signiﬁcantly.<br>14.4.3<br>Heuristic Optimization<br>A drawback of cost-based optimization is the cost of optimization itself. Although<br>the cost of query processing can be reduced by clever optimizations, cost-based opti-<br>mization is still expensive. Hence, many systems use heuristics to reduce the number<br>of choices that must be made in a cost-based fashion. Some systems even choose to<br>use only heuristics, and do not use cost-based optimization at all.<br>An example of a heuristic rule is the following rule for transforming relational-<br>algebra queries:<br>• Perform selection operations as early as possible.<br>A heuristic optimizer would use this rule without ﬁnding out whether the cost is<br>reduced by this transformation. In the ﬁrst transformation example in Section 14.3,<br>the selection operation was pushed into a join.<br>We say that the preceding rule is a heuristic because it usually, but not always,<br>helps to reduce the cost. For an example of where it can result in an increase in cost,<br>consider an expression σθ(r<br> s), where the condition θ refers to only attributes in s.<br>The selection can certainly be performed before the join. However, if r is extremely<br>small compared to s, and if there is an index on the join attributes of s, but no index<br>on the attributes used by θ, then it is probably a bad idea to perform the selection<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>IV. Data Storage and <br>Querying<br>14. Query Optimization<br>548<br>© The McGraw−Hill <br>Companies, 2001<br>548<br>Chapter 14<br>Query Optimization<br>early. Performing the selection early—that is, directly on s—would require doing a<br>scan of all tuples in s. It is probably cheaper, in this case, to compute the join by using<br>the index, and then to reject tuples that fail the selection.<br>The projection operation, like the selection operation, reduces the size of relations.<br>Thus, whenever we need to generate a temporary relation, it is advantageous to ap-<br>ply immediately any projections that are possible. This advantage suggests a com-<br>panion to the “perform selections early” heuristic:<br>• Perform projections early.<br>It is usually better to perform selections earlier than projections, since selections have<br>the potential to reduce the sizes of relations greatly, and selections enable the use of<br>indices to access tuples. An example similar to the one used for the selection heuristic<br>should convince you that this heuristic does not always reduce the cost.<br>Drawing on the equivalences discussed in Section 14.3.1, a heuristic optimization<br>algorithm will reorder the components of an initial query tree to achieve improved<br>query execution. We now present an overview of the steps in a typical heuristic op-<br>timization algorithm. You can understand the heuristics by visualizing a query ex-<br>pression as a tree, as illustrated in Figure 14.3<br>1. Deconstruct conjunctive selections into a sequence of single selection opera-<br>tions. This step, based on equivalence rule 1, facilitates moving selection op-<br>erations down the query tree.<br>2. Move selection operations down the query tree for the earliest possible exe-<br>cution. This step uses the commutativity and distributivity properties of the<br>selection operation noted in equivalence rules 2, 7.a, 7.b, and 11.<br>For instance, this step transforms σθ(r<br> s) into either σθ(r)<br> s or r<br> σθ(s)<br>whenever possible. Performing value-based selections as early as possible re-<br>duces the cost of sorting and merging intermediate results. The degree of re-<br>ordering permitted for a particular selection is determined by the attributes<br>involved in that selection condition.<br>3. Determine which selection operations and join operations will produce the<br>smallest relations—that is, will produce the relations with the least number<br>of tuples. Using associativity of the<br> operation, rearrange the tree so that the<br>leaf-node relations with these restrictive selections are executed ﬁrst.<br>This step considers the selectivity of a selection or join condition. Recall<br>that the most restrictive selection—that is, the condition with the smallest<br>selectivity—retrieves the fewest records. This step relies on the associativity<br>of binary operations given in equivalence rule 6.<br>4. Replace with join operations those Cartesian product operations that are fol-<br>lowed by a selection condition (rule</span><br><br><span style="background-color: #A0C4FF;" title="Chunk 69 | Start: 1380138 | End: 1400138 | Tokens: 3337"> 4.a). The Cartesian product operation is<br>often expensive to implement since r1 × r2 includes a record for each combi-<br>nation of records from r1 and r2. The selection may signiﬁcantly reduce the<br>number of records, making the join much less expensive than the Cartesian<br>product.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>IV. Data Storage and <br>Querying<br>14. Query Optimization<br>549<br>© The McGraw−Hill <br>Companies, 2001<br>14.4<br>Choice of Evaluation Plans<br>549<br>5. Deconstruct and move as far down the tree as possible lists of projection at-<br>tributes, creating new projections where needed. This step draws on the prop-<br>erties of the projection operation given in equivalence rules 3, 8.a, 8.b, and<br>12.<br>6. Identify those subtrees whose operations can be pipelined, and execute them<br>using pipelining.<br>In summary, the heuristics listed here reorder an initial query-tree representation<br>in such a way that the operations that reduce the size of intermediate results are ap-<br>plied ﬁrst; early selection reduces the number of tuples, and early projection reduces<br>the number of attributes. The heuristic transformations also restructure the tree so<br>that the system performs the most restrictive selection and join operations before<br>other similar operations.<br>Heuristic optimization further maps the heuristically transformed query expres-<br>sion into alternative sequences of operations to produce a set of candidate evalu-<br>ation plans. An evaluation plan includes not only the relational operations to be<br>performed, but also the indices to be used, the order in which tuples are to be ac-<br>cessed, and the order in which the operations are to be performed. The access-plan<br>–selection phase of a heuristic optimizer chooses the most efﬁcient strategy for each<br>operation.<br>14.4.4<br>Structure of Query Optimizers∗∗<br>So far, we have described the two basic approaches to choosing an evaluation plan;<br>as noted, most practical query optimizers combine elements of both approaches. For<br>example, certain query optimizers, such as the System R optimizer, do not consider<br>all join orders, but rather restrict the search to particular kinds of join orders. The<br>System R optimizer considers only those join orders where the right operand of each<br>join is one of the initial relations r1, . . . , rn. Such join orders are called left-deep join<br>orders. Left-deep join orders are particularly convenient for pipelined evaluation,<br>since the right operand is a stored relation, and thus only one input to each join is<br>pipelined.<br>Figure 14.6 illustrates the difference between left-deep join trees and non-left-deep<br>join trees. The time it takes to consider all left-deep join orders is O(n!), which is much<br>less than the time to consider all join orders. With the use of dynamic programming<br>optimizations, the System R optimizer can ﬁnd the best join order in time O(n2n).<br>Contrast this cost with the O(3n) time required to ﬁnd the best overall join order.<br>The System R optimizer uses heuristics to push selections and projections down the<br>query tree.<br>The cost estimate that we presented for scanning by secondary indices assumed<br>that every tuple access results in an I/O operation. The estimate is likely to be ac-<br>curate with small buffers; with large buffers, however, the page containing the tuple<br>may already be in the buffer. Some optimizers incorporate a better cost-estimation<br>technique for such scans: They take into account the probability that the page con-<br>taining the tuple is in the buffer.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>IV. Data Storage and <br>Querying<br>14. Query Optimization<br>550<br>© The McGraw−Hill <br>Companies, 2001<br>550<br>Chapter 14<br>Query Optimization<br>r4<br>r5<br>r3<br>r1<br>r2<br>r5<br>r4<br>r3<br>r2<br>r1<br>(a) Left-deep join tree<br>(b) Non-left-deep join tree<br>Figure 14.6<br>Left-deep join trees.<br>Query optimization approaches that integrate heuristic selection and the genera-<br>tion of alternative access plans have been adopted in several systems. The approach<br>used in System R and in its successor, the Starburst project, is a hierarchical procedure<br>based on the nested-block concept of SQL. The cost-based optimization techniques<br>described here are used for each block of the query separately.<br>The heuristic approach in some versions of Oracle works roughly this way: For<br>an n-way join, it considers n evaluation plans. Each plan uses a left-deep join order,<br>starting with a different one of the n relations. The heuristic constructs the join or-<br>der for each of the n evaluation plans by repeatedly selecting the “best” relation to<br>join next, on the basis of a ranking of the available access paths. Either nested-loop<br>or sort–merge join is chosen for each of the joins, depending on the available access<br>paths. Finally, the heuristic chooses one of the n evaluation plans in a heuristic man-<br>ner, based on minimizing the number of nested-loop joins that do not have an index<br>available on the inner relation, and on the number of sort–merge joins.<br>The intricacies of SQL introduce a good deal of complexity into query optimizers.<br>In particular, it is hard to translate nested subqueries in SQL into relational algebra.<br>We brieﬂy outline how to handle nested subqueries in Section 14.4.5. For compound<br>SQL queries (using the ∪, ∩, or −operation), the optimizer processes each component<br>separately, and combines the evaluation plans to form the overall evaluation plan.<br>Even with the use of heuristics, cost-based query optimization imposes a substan-<br>tial overhead on query processing. However, the added cost of cost-based query op-<br>timization is usually more than offset by the saving at query-execution time, which<br>is dominated by slow disk accesses. The difference in execution time between a good<br>plan and a bad one may be huge, making query optimization essential. The achieved<br>saving is magniﬁed in those applications that run on a regular basis, where the query<br>can be optimized once, and the selected query plan can be used on each run. There-<br>fore, most commercial systems include relatively sophisticated optimizers. The bib-<br>liographical notes give references to descriptions of the query optimizers of actual<br>database systems.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>IV. Data Storage and <br>Querying<br>14. Query Optimization<br>551<br>© The McGraw−Hill <br>Companies, 2001<br>14.4<br>Choice of Evaluation Plans<br>551<br>14.4.5<br>Optimizing Nested Subqueries∗∗<br>SQL conceptually treats nested subqueries in the where clause as functions that take<br>parameters and return either a single value or a set of values (possibly an empty set).<br>The parameters are the variables from outer level query that are used in the nested<br>subquery (these variables are called correlation variables). For instance, suppose we<br>have the following query.<br>select customer-name<br>from borrower<br>where exists (select *<br>from depositor<br>where depositor.customer-name = borrower.customer-name)<br>Conceptually, the subquery can be viewed as a function that takes a parameter (here,<br>borrower.customer-name) and returns the set of all depositors with the same name.<br>SQL evaluates the overall query (conceptually) by computing the Cartesian prod-<br>uct of the relations in the outer from clause and then testing the predicates in the<br>where clause for each tuple in the product. In the preceding example, the predicate<br>tests if the result of the subquery evaluation is empty.<br>This technique for evaluating a query with a nested subquery is called correlated<br>evaluation. Correlated evaluation is not very efﬁcient, since the subquery is sepa-<br>rately evaluated for each tuple in the outer level query. A large number of random<br>disk I/O operations may result.<br>SQL optimizers therefore attempt to transform nested subqueries into joins, where<br>possible. Efﬁcient join algorithms help avoid expensive random I/O. Where the trans-<br>formation is not possible, the optimizer keeps the subqueries as separate expressions,<br>optimizes them separately, and then evaluates them by correlated evaluation.<br>As an example of transforming a nested subquery into a join, the query in the<br>preceding example can be rewritten as<br>select customer-name<br>from borrower, depositor<br>where depositor.customer-name = borrower.customer-name<br>(To properly reﬂect SQL semantics, the number of duplicate derivations should not<br>change because of the rewriting; the rewritten query can be modiﬁed to ensure this<br>property, as we will see shortly.)<br>In the example, the nested subquery was very simple. In general, it may not be<br>possible to directly move the nested subquery relations into the from clause of the<br>outer query. Instead, we create a temporary relation that contains the results of the<br>nested query without the selections using correlation variables from the outer query,<br>and join the temporary table with the outer level query. For instance, a query of the<br>form<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>IV. Data Storage and <br>Querying<br>14. Query Optimization<br>552<br>© The McGraw−Hill <br>Companies, 2001<br>552<br>Chapter 14<br>Query Optimization<br>select . . .<br>from L1<br>where P1 and exists (select *<br>from L2<br>where P2)<br>where P2 is a conjunction of simpler predicates, can be rewritten as<br>create table t1 as<br>select distinct V<br>from L2<br>where P 1<br>2<br>select . . .<br>from L1, t1<br>where P1 and P 2<br>2<br>where P 1<br>2 contains predicates in P2 without selections involving correlation variables,<br>and P 2<br>2 reintroduces the selections involving correlation variables (with relations ref-<br>erenced in the predicate appropriately renamed). Here, V contains all attributes that<br>are used in selections with correlation variables in the nested subquery.<br>In our example, the original query would have been transformed to<br>create table t1 as<br>select distinct customer-name<br>from depositor<br>select customer-name<br>from borrower, t1<br>where t1.customer-name = borrower.customer-name<br>The query we rewrote to illustrate creation of a temporary relation can be obtained<br>by simplifying the above transformed query, assuming the number of duplicates of<br>each tuple does not matter.<br>The process of replacing a nested query by a query with a join (possibly with a<br>temporary relation) is called decorrelation.<br>Decorrelation is more complicated when the nested subquery uses aggregation,<br>or when the result of the nested subquery is used to test for equality, or when the<br>condition linking the nested subquery to the outer query is not exists, and so on.<br>We do not attempt to give algorithms for the general case, and instead refer you to<br>relevant items in the bibliographical notes.<br>Optimization of complex nested subqueries is a difﬁcult task, as you can infer from<br>the above discussion, and many optimizers do only a limited amount of decorrela-<br>tion. It is best to avoid using complex nested subqueries, where possible, since we<br>cannot be sure that the query optimizer will succeed in converting them to a form<br>that can be evaluated efﬁciently.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>IV. Data Storage and <br>Querying<br>14. Query Optimization<br>553<br>© The McGraw−Hill <br>Companies, 2001<br>14.5<br>Materialized Views∗∗<br>553<br>14.5<br>Materialized Views∗∗<br>When a view is deﬁned, normally the database stores only the query deﬁning the<br>view. In contrast, a materialized view is a view whose contents are computed and<br>stored. Materialized views constitute redundant data, in that their contents can be<br>inferred from the view deﬁnition and the rest of the database contents. However, it<br>is much cheaper in many cases to read the contents of a materialized view than to<br>compute the contents of the view by executing the query deﬁning the view.<br>Materialized views are important for improving performance in some applica-<br>tions. Consider this view, which gives the total loan amount at each branch:<br>create view branch-total-loan(branch-name, total-loan) as<br>select branch-name, sum(amount)<br>from loan<br>groupby branch-name<br>Suppose the total loan amount at the branch is required frequently (before making<br>a new loan, for example). Computing the view requires reading every loan tuple<br>pertaining to the branch, and summing up the loan amounts, which can be time-<br>consuming.<br>In contrast, if the view deﬁnition of the total loan amount were materialized, the<br>total loan amount could be found by looking up a single tuple in the materialized<br>view.<br>14.5.1<br>View Maintenance<br>A problem with materialized views is that they must be kept up-to-date when the<br>data used in the view deﬁnition changes. For instance, if the amount value of a loan<br>is updated, the materialized view would become inconsistent with the underlying<br>data, and must be updated. The task of keeping a materialized view up-to-date with<br>the underlying data is known as view maintenance.<br>Views can be maintained by manually written code: That is, every piece of code<br>that updates the amount value of a loan can be modiﬁed to also update the total loan<br>amount for the corresponding branch.<br>Another option for maintaining materialized views is to deﬁne triggers on insert,<br>delete, and update of each relation in the view deﬁnition. The triggers must modify<br>the contents of the materialized view, to take into account the change that caused the<br>trigger to ﬁre. A simplistic way of doing so is to completely recompute the material-<br>ized view on every update.<br>A better option is to modify only the affected parts of the materialized view, which<br>is known as incremental view maintenance. We describe how to perform incremen-<br>tal view maintenance in Section 14.5.2.<br>Modern database systems provide more direct support for incremental view main-<br>tenance. Database system programmers no longer need to deﬁne triggers for view<br>maintenance. Instead, once a view is declared to be materialized, the database sys-<br>tem computes the contents of the view, and incrementally updates the contents when<br>the underlying data changes.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>IV. Data Storage and <br>Querying<br>14. Query Optimization<br>554<br>© The McGraw−Hill <br>Companies, 2001<br>554<br>Chapter 14<br>Query Optimization<br>14.5.2<br>Incremental View Maintenance<br>To understand how to incrementally maintain materialized views, we start off by<br>considering individual operations, and then see how to handle a complete expres-<br>sion.<br>The changes to a relation that can cause a materialized view to become out-of-date<br>are inserts, deletes, and updates. To simplify our description, we replace updates to<br>a tuple by deletion of the tuple followed by insertion of the updated tuple. Thus,<br>we need to consider only inserts and deletes. The changes (inserts and deletes) to a<br>relation or expression are referred to as its differential.<br>14.5.2.1<br>Join Operation<br>Consider the materialized view v = r<br> s. Suppose we modify r by inserting a set of<br>tuples denoted by ir. If the old value of r is denoted by rold, and the new value of r<br>by rnew, rnew = rold ∪ir. Now, the old value of the view, vold is given by rold<br> s, and<br>the new value vnew is given by rnew<br> s. We can rewrite rnew<br> s as (rold ∪ir)<br> s,<br>which we can again rewrite as (rold<br> s) ∪(ir<br> s). In other words,<br>vnew = vold ∪(ir<br> s)<br>Thus, to update the materialized view v, we simply need to add the tuples ir<br> s<br>to the old contents of the materialized view. Inserts to s are handled in an exactly<br>symmetric fashion.<br>Now suppose r is modiﬁed by deleting a set of tuples denoted by dr. Using the<br>same reasoning as above, we get<br>vnew = vold −(dr<br> s)<br>Deletes on s are handled in an exactly symmetric fashion.<br>14.5.2.2<br>Selection and Projection Operations<br>Consider a view v = σθ(r). If we modify r by inserting a set of tuples ir, the new<br>value of v can be computed as<br>vnew = vold ∪σθ(ir)<br>Similarly, if r is modiﬁed by deleting a set of tuples dr, the new value of v can be<br>computed as<br>vnew = vold −σθ(dr)<br>Projection is a more difﬁcult operation with which to deal. Consider a materialized<br>view v = ΠA(r). Suppose the relation r is on the schema R = (A, B), and r contains<br>two tuples (a, 2) and (a, 3). Then, ΠA(r) has a single tuple (a). If we delete the tuple<br>(a, 2) from r, we cannot delete the tuple (a) from ΠA(r): If we did so, the result would<br>be an empty relation, whereas in reality ΠA(r) still has a single tuple (a). The reason is<br>that the same tuple (a) is derived in two ways, and deleting one tuple from r removes<br>only one of the ways of deriving (a); the other is still present.<br>This reason also gives us the intuition for solution: For each tuple in a projection<br>such as ΠA(r), we will keep a count of how many times it was derived.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>IV. Data Storage and <br>Querying<br>14. Query Optimization<br>555<br>© The McGraw−Hill <br>Companies, 2001<br>14.5<br>Materialized Views∗∗<br>555<br>When a set of tuples dr is deleted from r, for each tuple t in dr we do the following.<br>Let t.A denote the projection of t on the attribute A. We ﬁnd (t.A) in the materialized<br>view, and decrease the count stored with it by 1. If the count becomes 0, (t.A) is<br>deleted from the materialized view.<br>Handling insertions is relatively straightforward. When a set of tuples ir is in-<br>serted into r, for each tuple t in ir we do the following. If (t.A) is already present in<br>the materialized view, we increase the count stored with it by 1. If not, we add (t.A)<br>to the materialized view, with the count set to 1.<br>14.5.2.3<br>Aggregation Operations<br>Aggregation operations proceed somewhat like projections. The aggregate opera-<br>tions in SQL are count, sum, avg, min, and max:<br>• count: Consider a materialized view v =<br>AGcount(B)(r), which computes the<br>count of the attribute B, after grouping r by attribute A.<br>When a set of tuples ir is inserted into r, for each tuple t in ir we do the fol-<br>lowing. We look for the group t.A in the materialized view. If it is not present,<br>we add (t.A, 1) to the materialized view. If the group t.A is present, we add 1<br>to the count of the group.<br>When a set of tuples dr is deleted from r, for each tuple t in dr we do the<br>following. We look for the group t.A in the materialized view, and subtract 1<br>from the count for the group. If the count becomes 0, we delete the tuple for<br>the group t.A from the materialized view.<br>• sum: Consider a materialized view v = AGsum(B)(r).<br>When a set of tuples ir is inserted into r, for each tuple t in ir we do the fol-<br>lowing. We look for the group t.A in the materialized view. If it is not present,<br>we add (t.A, t.B) to the materialized view; in addition, we store a count of<br>1 associated with (t.A, t.B), just as we did for projection. If the group t.A is<br>present, we add the value of t.B to the aggregate value for the group, and add<br>1 to the count of the group.<br>When a set of tuples dr is deleted from r, for each tuple t in dr we do the<br>following. We look for the group t.A in the materialized view, and subtract<br>t.B from the aggregate value for the group. We also subtract 1 from the count<br>for the group, and if the count becomes 0, we delete the tuple for the group<br>t.A from the materialized view.<br>Without keeping the extra count value, we would not be able to distinguish<br>a case where the sum for a group is 0 from the case where the last tuple in a<br>group is deleted.<br>• avg: Consider a materialized view v = AGavg(B)(r).<br>Directly updating the average on an insert or delete is not possible, since<br>it depends not only on the old average and the tuple being inserted/deleted,<br>but also on the number of tuples in the group.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>IV. Data Storage and <br>Querying<br>14. Query Optimization<br>556<br>© The McGraw−Hill <br>Companies, 2001<br>556<br>Chapter 14<br>Query Optimization<br>Instead, to handle the case of avg, we maintain the sum and count aggre-<br>gate values as described earlier, and compute the average as the sum divided<br>by the count.<br>• min, max: Consider a materialized view v = AGmin(B)(r). (The case of max is<br>exactly equivalent.)<br>Handling insertions on r is straightforward. Maintaining the aggregate val-<br>ues min and max on deletions may be more expensive. For example, if the<br>tuple corresponding to the minim</span><br><br><span style="background-color: #BDB2FF;" title="Chunk 70 | Start: 1400140 | End: 1420140 | Tokens: 3250">um value for a group is deleted from r, we<br>have to look at the other tuples of r that are in the same group to ﬁnd the new<br>minimum value.<br>14.5.2.4<br>Other Operations<br>The set operation intersection is maintained as follows. Given materialized view v =<br>r ∩s, when a tuple is inserted in r we check if it is present in s, and if so we add<br>it to v. If a tuple is deleted from r, we delete it from the intersection if it is present.<br>The other set operations, union and set difference, are handled in a similar fashion; we<br>leave details to you.<br>Outer joins are handled in much the same way as joins, but with some extra work.<br>In the case of deletion from r we have to handle tuples in s that no longer match any<br>tuple in r. In the case of insertion to r, we have to handle tuples in s that did not<br>match any tuple in r. Again we leave details to you.<br>14.5.2.5<br>Handling Expressions<br>So far we have seen how to update incrementally the result of a single operation. To<br>handle an entire expression, we can derive expressions for computing the incremen-<br>tal change to the result of each subexpression, starting from the smallest subexpres-<br>sions.<br>For example, suppose we wish to incrementally update a materialized view E1<br><br>E2 when a set of tuples ir is inserted into relation r. Let us assume r is used in E1<br>alone. Suppose the set of tuples to be inserted into E1 is given by expression D1. Then<br>the expression D1<br> E2 gives the set of tuples to be inserted into E1<br> E2.<br>See the bibliographical notes for further details on incremental view maintenance<br>with expressions.<br>14.5.3<br>Query Optimization and Materialized Views<br>Query optimization can be performed by treating materialized views just like regular<br>relations. However, materialized views offer further opportunities for optimization:<br>• Rewriting queries to use materialized views:<br>Suppose a materialized view v = r<br> s is available, and a user submits a<br>query r<br> s<br> t. Rewriting the query as v<br> t may provide a more efﬁcient<br>query plan than optimizing the query as submitted. Thus, it is the job of the<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>IV. Data Storage and <br>Querying<br>14. Query Optimization<br>557<br>© The McGraw−Hill <br>Companies, 2001<br>14.6<br>Summary<br>557<br>query optimizer to recognize when a materialized view can be used to speed<br>up a query.<br>• Replacing a use of a materialized view by the view deﬁnition:<br>Suppose a materialized view v = r<br> s is available, but without any index<br>on it, and a user submits a query σA=10(v). Suppose also that s has an index<br>on the common attribute B, and r has an index on attribute A. The best plan<br>for this query may be to replace v by r<br> s, which can lead to the query plan<br>σA=10(r)<br> s; the selection and join can be performed efﬁciently by using<br>the indices on r.A and s.B, respectively. In contrast, evaluating the selection<br>directly on v may require a full scan of v, which may be more expensive.<br>The bibliographical notes give pointers to research showing how to efﬁciently per-<br>form query optimization with materialized views.<br>Another related optimization problem is that of materialized view selection,<br>namely, “What is the best set of views to materialize?” This decision must be made<br>on the basis of the system workload, which is a sequence of queries and updates that<br>reﬂects the typical load on the system. One simple criterion would be to select a set<br>of materialized views that minimizes the overall execution time of the workload of<br>queries and updates, including the time taken to maintain the materialized views.<br>Database administrators usually modify this criterion to take into account the im-<br>portance of different queries and updates: Fast response may be required for some<br>queries and updates, but a slow response may be acceptable for others.<br>Indices are just like materialized views, in that they too are derived data, can speed<br>up queries, and may slow down updates. Thus, the problem of index selection is<br>closely related, to that of materialized view selection, although it is simpler.<br>We examine these issues in more detail in Sections 21.2.5 and 21.2.6.<br>Some database systems, such as Microsoft SQL Server 7.5, and the RedBrick Data<br>Warehouse from Informix, provide tools to help the database administrator with in-<br>dex and materialized view selection. These tools examine the history of queries and<br>updates, and suggest indices and views to be materialized.<br>14.6<br>Summary<br>• Given a query, there are generally a variety of methods for computing the<br>answer. It is the responsibility of the system to transform the query as entered<br>by the user into an equivalent query that can be computed more efﬁciently.<br>The process of ﬁnding a good strategy for processing a query, is called query<br>optimization.<br>• The evaluation of complex queries involves many accesses to disk. Since the<br>transfer of data from disk is slow relative to the speed of main memory and<br>the CPU of the computer system, it is worthwhile to allocate a considerable<br>amount of processing to choose a method that minimizes disk accesses.<br>• The strategy that the database system chooses for evaluating an operation de-<br>pends on the size of each relation and on the distribution of values within<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>IV. Data Storage and <br>Querying<br>14. Query Optimization<br>558<br>© The McGraw−Hill <br>Companies, 2001<br>558<br>Chapter 14<br>Query Optimization<br>columns. So that they can base the strategy choice on reliable information,<br>database systems may store statistics for each relation r. These statistics in-<br>clude<br>  The number of tuples in the relation r<br>  The size of a record (tuple) of relation r in bytes<br>  The number of distinct values that appear in the relation r for a particular<br>attribute<br>• These statistics allow us to estimate the sizes of the results of various oper-<br>ations, as well as the cost of executing the operations. Statistical information<br>about relations is particularly useful when several indices are available to as-<br>sist in the processing of a query. The presence of these structures has a signif-<br>icant inﬂuence on the choice of a query-processing strategy.<br>• Each relational-algebra expression represents a particular sequence of opera-<br>tions. The ﬁrst step in selecting a query-processing strategy is to ﬁnd a relation-<br>al-algebra expression that is equivalent to the given expression and is esti-<br>mated to cost less to execute.<br>• There are a number of equivalence rules that we can use to transform an ex-<br>pression into an equivalent one. We use these rules to generate systematically<br>all expressions equivalent to the given query.<br>• Alternative evaluation plans for each expression can be generated by simi-<br>lar rules, and the cheapest plan across all expressions can be chosen. Several<br>optimization techniques are available to reduce the number of alternative ex-<br>pressions and plans that need to be generated.<br>• We use heuristics to reduce the number of plans considered, and thereby to<br>reduce the cost of optimization. Heuristic rules for transforming relational-<br>algebra queries include “Perform selection operations as early as possible,”<br>“Perform projections early,” and “Avoid Cartesian products.”<br>• Materialized views can be used to speed up query processing. Incremental<br>view maintenance is needed to efﬁciently update materialized views when<br>the underlying relations are modiﬁed. The differential of an operation can be<br>computed by means of algebraic expressions involving differentials of the in-<br>puts of the operation. Other issues related to materialized views include how<br>to optimize queries by making use of available materialized views, and how<br>to select views to be materialized.<br>Review Terms<br>• Query optimization<br>• Statistics estimation<br>• Catalog information<br>• Size estimation<br>  Selection<br>  Selectivity<br>  Join<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>IV. Data Storage and <br>Querying<br>14. Query Optimization<br>559<br>© The McGraw−Hill <br>Companies, 2001<br>Exercises<br>559<br>• Distinct value estimation<br>• Transformation of expressions<br>• Cost-based optimization<br>• Equivalence of expressions<br>• Equivalence rules<br>  Join commutativity<br>  Join associativity<br>• Minimal set of equivalence rules<br>• Enumeration of equivalent<br>expressions<br>• Choice of evaluation plans<br>• Interaction of evaluation<br>techniques<br>• Join order optimization<br>  Dynamic-programming<br>algorithm<br>  Left-deep join order<br>• Heuristic optimization<br>• Access-plan selection<br>• Correlated evaluation<br>• Decorrelation<br>• Materialized views<br>• Materialized view maintenance<br>  Recomputation<br>  Incremental maintenance<br>  Insertion,<br>  Deletion<br>  Updates<br>• Query optimization with<br>materialized views<br>• Index selection<br>• Materialized view selection<br>Exercises<br>14.1 Clustering indices may allow faster access to data than a nonclustering index<br>affords. When must we create a nonclustering index, despite the advantages of<br>a clustering index? Explain your answer.<br>14.2 Consider the relations r1(A, B, C), r2(C, D, E), and r3(E, F), with primary keys<br>A, C, and E, respectively. Assume that r1 has 1000 tuples, r2 has 1500 tuples,<br>and r3 has 750 tuples. Estimate the size of r1<br> r2<br> r3, and give an efﬁcient<br>strategy for computing the join.<br>14.3 Consider the relations r1(A, B, C), r2(C, D, E), and r3(E, F) of Exercise 14.2.<br>Assume that there are no primary keys, except the entire schema. Let V (C, r1)<br>be 900, V (C, r2) be 1100, V (E, r2) be 50, and V (E, r3) be 100. Assume that r1<br>has 1000 tuples, r2 has 1500 tuples, and r3 has 750 tuples. Estimate the size of<br>r1<br> r2<br> r3, and give an efﬁcient strategy for computing the join.<br>14.4 Suppose that a B+-tree index on branch-city is available on relation branch, and<br>that no other index is available. What would be the best way to handle the<br>following selections that involve negation?<br>a. σ¬(branch-city&lt;“Brooklyn”)(branch)<br>b. σ¬(branch-city=“Brooklyn”)(branch)<br>c. σ¬(branch-city&lt;“Brooklyn” ∨assets&lt;5000)(branch)<br>14.5 Suppose that a B+-tree index on (branch-name, branch-city) is available on rela-<br>tion branch. What would be the best way to handle the following selection?<br>σ(branch-city&lt;“Brooklyn”) ∧(assets&lt;5000)∧(branch-name=“Downtown”)(branch)<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>IV. Data Storage and <br>Querying<br>14. Query Optimization<br>560<br>© The McGraw−Hill <br>Companies, 2001<br>560<br>Chapter 14<br>Query Optimization<br>14.6 Show that the following equivalences hold. Explain how you can apply then<br>to improve the efﬁciency of certain queries:<br>a. E1<br>θ (E2 −E3) = (E1<br>θ E2 −E1<br>θ E3).<br>b. σθ( AGF (E)) =<br>AGF (σθ(E)), where θ uses only attributes from A.<br>c. σθ(E1<br> E2) = σθ(E1)<br> E2 where θ uses only attributes from E1.<br>14.7 Show how to derive the following equivalences by a sequence of transforma-<br>tions using the equivalence rules in Section 14.3.1.<br>a. σθ1∧θ2∧θ3(E) = σθ1(σθ2(σθ3(E)))<br>b. σθ1∧θ2(E1<br>θ3 E2) = σθ1(E1<br>θ3 (σθ2(E2))), where θ2 involves only at-<br>tributes from E2<br>14.8 For each of the following pairs of expressions, give instances of relations that<br>show the expressions are not equivalent.<br>a. ΠA(R −S) and ΠA(R) −ΠA(S)<br>b. σB&lt;4( AGmax(B)(R)) and AGmax(B)(σB&lt;4(R))<br>c. In the preceding expressions, if both occurrences of max were replaced by<br>min would the expressions be equivalent?<br>d. (R<br> S)<br> T and R<br> (S<br> T)<br>In other words, the natural left outer join is not associative.<br>(Hint: Assume that the schemas of the three relations are R(a, b1), S(a, b2),<br>and T(a, b3), respectively.)<br>e. σθ(E1<br> E2) and E1<br> σθ(E2), where θ uses only attributes from E2<br>14.9 SQL allows relations with duplicates (Chapter 4).<br>a. Deﬁne versions of the basic relational-algebra operations σ, Π, ×,<br>, −, ∪,<br>and ∩that work on relations with duplicates, in a way consistent with SQL.<br>b. Check which of the equivalence rules 1 through 7.b hold for the multiset<br>version of the relational-algebra deﬁned in part a.<br>14.10 ∗∗Show that, with n relations, there are (2(n−1))!/(n−1)! different join orders.<br>Hint: A complete binary tree is one where every internal node has exactly<br>two children. Use the fact that the number of different complete binary trees<br>with n leaf nodes is 1<br>n<br>2(n−1)<br>(n−1)<br><br>.<br>If you wish, you can derive the formula for the number of complete binary<br>trees with n nodes from the formula for the number of binary trees with n<br>nodes. The number of binary trees with n nodes is<br>1<br>n+1<br>2n<br>n<br><br>; this number is<br>known as the Catalan number, and its derivation can be found in any standard<br>textbook on data structures or algorithms.<br>14.11 ∗∗Show that the lowest-cost join order can be computed in time O(3n). As-<br>sume that you can store and look up information about a set of relations (such<br>as the optimal join order for the set, and the cost of that join order) in constant<br>time. (If you ﬁnd this exercise difﬁcult, at least show the looser time bound of<br>O(22n).)<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>IV. Data Storage and <br>Querying<br>14. Query Optimization<br>561<br>© The McGraw−Hill <br>Companies, 2001<br>Bibliographical Notes<br>561<br>14.12 Show that, if only left-deep join trees are considered, as in the System R opti-<br>mizer, the time taken to ﬁnd the most efﬁcient join order is around n2n. Assume<br>that there is only one interesting sort order.<br>14.13 A set of equivalence rules is said to be complete if, whenever two expressions<br>are equivalent, one can be derived from the other by a sequence of uses of the<br>equivalence rules. Is the set of equivalence rules that we considered in Sec-<br>tion 14.3.1 complete? Hint: Consider the equivalence σ3=5(r) = { }.<br>14.14 Decorrelation:<br>a. Write a nested query on the relation account to ﬁnd for each branch with<br>name starting with “B”, all accounts with the maximum balance at the<br>branch.<br>b. Rewrite the preceding query, without using a nested subquery; in other<br>words, decorrelate the query.<br>c. Give a procedure (similar that that described in Section 14.4.5) for decorre-<br>lating such queries.<br>14.15 Describe how to incrementally maintain the results of the following operations,<br>on both insertions and deletions.<br>a. Union and set difference<br>b. Left outer join<br>14.16 Give an example of an expression deﬁning a materialized view and two situ-<br>ations (sets of statistics for the input relations and the differentials) such that<br>incremental view maintenance is better than recomputation in one situation,<br>and recomputation is better in the other situation.<br>Bibliographical Notes<br>The seminal work of Selinger et al. [1979] describes access-path selection in the Sys-<br>tem R optimizer, which was one of the earliest relational-query optimizers. Graefe<br>and McKenna [1993] describe Volcano, an equivalence-rule based query optimizer.<br>Query processing in Starburst is described in Haas et al. [1989]. Query optimization<br>in Oracle is brieﬂy outlined in Oracle [1997].<br>Estimation of statistics of query results, such as result size, is addressed by Ioanni-<br>dis and Poosala [1995], Poosala et al. [1996], and Ganguly et al. [1996], among others.<br>Nonuniform distributions of values causes problems for estimation of query size and<br>cost. Cost-estimation techniques that use histograms of value distributions have been<br>proposed to tackle the problem. Ioannidis and Christodoulakis [1993], Ioannidis and<br>Poosala [1995], and Poosala et al. [1996] present results in this area.<br>Exhaustive searching of all query plans is impractical for optimization of joins<br>involving many relations, and techniques based on randomized searching, which do<br>not examine all alternatives, have been proposed. Ioannidis and Wong [1987], Swami<br>and Gupta [1988], and Ioannidis and Kang [1990] present results in this area.<br>Parametric query-optimization techniques have been proposed by Ioannidis et al.<br>[1992] and Ganguly [1998], to handle query processing when the selectivity of query<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>IV. Data Storage and <br>Querying<br>14. Query Optimization<br>562<br>© The McGraw−Hill <br>Companies, 2001<br>562<br>Chapter 14<br>Query Optimization<br>parameters is not known at optimization time. A set of plans—one for each of several<br>different query selectivities—is computed, and is stored by the optimizer, at compile<br>time. One of these plans is chosen at run time, on the basis of the actual selectivities,<br>avoiding the cost of full optimization at run time.<br>Klug [1982] was an early work on optimization of relational-algebra expressions<br>with aggregate functions. More recent work in this area includes Yan and Larson<br>[1995] and Chaudhuri and Shim [1994]. Optimization of queries containing outer<br>joins is described in Rosenthal and Reiner [1984], Galindo-Legaria and Rosenthal<br>[1992], and Galindo-Legaria [1994].<br>The SQL language poses several challenges for query optimization, including the<br>presence of duplicates and nulls, and the semantics of nested subqueries. Extension<br>of relational algebra to duplicates is described in Dayal et al. [1982]. Optimization of<br>nested subqueries is discussed in Kim [1982], Ganski and Wong [1987], Dayal [1987],<br>and more recently, in Seshadri et al. [1996].<br>When queries are generated through views, more relations often are joined than is<br>necessary for computation of the query. A collection of techniques for join minimiza-<br>tion has been grouped under the name tableau optimization. The notion of a tableau<br>was introduced by Aho et al. [1979b] and Aho et al. [1979a], and was further extended<br>by Sagiv and Yannakakis [1981]. Ullman [1988] andMaier [1983] provide a textbook<br>coverage of tableaux.<br>Sellis [1988] and Roy et al. [2000] describe multiquery optimization, which is the<br>problem of optimizing the execution of several queries as a group. If an entire group<br>of queries is considered, it is possible to discover common subexpressions that can be<br>evaluated once for the entire group. Finkelstein [1982] and Hall [1976] consider op-<br>timization of a group of queries and the use of common subexpressions. Dalvi et al.<br>[2001] discuss optimization issues in pipelining with limited buffer space combined<br>with sharing of common subexpressions.<br>Query optimization can make use of semantic information, such as functional de-<br>pendencies and other integrity constraints. Semantic query-optimization in relational<br>databases is covered by King [1981], Chakravarthy et al. [1990], and in the context of<br>aggregation, by Sudarshan and Ramakrishnan [1991].<br>Query-processing and optimization techniques for Datalog, in particular techni-<br>ques to handle queries on recursive views, are described in Bancilhon and Ramakr-<br>ishnan [1986], Beeri and Ramakrishnan [1991], Ramakrishnan et al. [1992c], Srivas-<br>tava et al. [1995] and Mumick et al. [1996]. Query processing and optimization tech-<br>niques for object-oriented databases are discussed in Maier and Stein [1986], Beech<br>[1988], Bertino and Kim [1989], and Blakeley et al. [1993].<br>Blakeley et al. [1986], Blakeley et al. [1989], and Grifﬁn and Libkin [1995] describe<br>techniques for maintenance of materialized views. Gupta and Mumick [1995] pro-<br>vides a survey of materialized view maintenance. Optimization of materialized view<br>maintenance plans is described by Vista [1998] and Mistry et al. [2001]. Query op-<br>timization in the presence of materialized views is addressed by Larson and Yang<br>[1985], Chaudhuri et al. [1995], Dar et al. [1996], and Roy et al. [2000]. Index selec-<br>tion and materialized view selection are addressed by Ross et al. [1996], Labio et al.<br>[1997], Gupta [1997], Chaudhuri and Narasayya [1997], and Roy et al. [2000].<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>V. Transaction <br>Management<br>Introduction<br>563<br>© The McGraw−Hill <br>Companies, 2001<br>P<br>A<br>R<br>T<br>5<br>Transaction Management<br>The term transaction refers to a collection of operations that form a single logical unit<br>of work. For instance, transfer of money from one account to another is a transaction<br>consisting of two updates, one to each account.<br>It is important that either all actions of a transaction be executed completely, or, in<br>case of some failure, partial effects of a transaction be undone. This prope</span><br><br><span style="background-color: #FFC6FF;" title="Chunk 71 | Start: 1420142 | End: 1440142 | Tokens: 3171">rty is called<br>atomicity. Further, once a transaction is successfully executed, its effects must persist<br>in the database—a system failure should not result in the database forgetting about<br>a transaction that successfully completed. This property is called durability.<br>In a database system where multiple transactions are executing concurrently, if<br>updates to shared data are not controlled there is potential for transactions to see<br>inconsistent intermediate states created by updates of other transactions. Such a sit-<br>uation can result in erroneous updates to data stored in the database. Thus, database<br>systems must provide mechanisms to isolate transactions from the effects of other<br>concurrently executing transactions. This property is called isolation.<br>Chapter 15 describes the concept of a transaction in detail, including the properties<br>of atomicity, durability, isolation, and other properties provided by the transaction<br>abstraction. In particular, the chapter makes precise the notion of isolation by means<br>of a concept called serializability.<br>Chapter 16 describes several concurrency control techniques that help implement<br>the isolation property.<br>Chapter 17 describes the recovery management component of a database, which<br>implements the atomicity and durability properties.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>V. Transaction <br>Management<br>15. Transactions<br>564<br>© The McGraw−Hill <br>Companies, 2001<br>C<br>H<br>A<br>P<br>T<br>E<br>R<br>1<br>5<br>Transactions<br>Often, a collection of several operations on the database appears to be a single unit<br>from the point of view of the database user. For example, a transfer of funds from<br>a checking account to a savings account is a single operation from the customer’s<br>standpoint; within the database system, however, it consists of several operations.<br>Clearly, it is essential that all these operations occur, or that, in case of a failure, none<br>occur. It would be unacceptable if the checking account were debited, but the savings<br>account were not credited.<br>Collections of operations that form a single logical unit of work are called transac-<br>tions. A database system must ensure proper execution of transactions despite fail-<br>ures—either the entire transaction executes, or none of it does. Furthermore, it must<br>manage concurrent execution of transactions in a way that avoids the introduction of<br>inconsistency. In our funds-transfer example, a transaction computing the customer’s<br>total money might see the checking-account balance before it is debited by the funds-<br>transfer transaction, but see the savings balance after it is credited. As a result, it<br>would obtain an incorrect result.<br>This chapter introduces the basic concepts of transaction processing. Details on<br>concurrent transaction processing and recovery from failures are in Chapters 16 and<br>17, respectively. Further topics in transaction processing are discussed in Chapter 24.<br>15.1<br>Transaction Concept<br>A transaction is a unit of program execution that accesses and possibly updates var-<br>ious data items. Usually, a transaction is initiated by a user program written in a<br>high-level data-manipulation language or programming language (for example, SQL,<br>COBOL, C, C++, or Java), where it is delimited by statements (or function calls) of the<br>form begin transaction and end transaction. The transaction consists of all opera-<br>tions executed between the begin transaction and end transaction.<br>To ensure integrity of the data, we require that the database system maintain the<br>following properties of the transactions:<br>565<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>V. Transaction <br>Management<br>15. Transactions<br>565<br>© The McGraw−Hill <br>Companies, 2001<br>566<br>Chapter 15<br>Transactions<br>• Atomicity. Either all operations of the transaction are reﬂected properly in the<br>database, or none are.<br>• Consistency. Execution of a transaction in isolation (that is, with no other<br>transaction executing concurrently) preserves the consistency of the database.<br>• Isolation. Even though multiple transactions may execute concurrently, the<br>system guarantees that, for every pair of transactions Ti and Tj, it appears<br>to Ti that either Tj ﬁnished execution before Ti started, or Tj started execu-<br>tion after Ti ﬁnished. Thus, each transaction is unaware of other transactions<br>executing concurrently in the system.<br>• Durability. After a transaction completes successfully, the changes it has made<br>to the database persist, even if there are system failures.<br>These properties are often called the ACID properties; the acronym is derived from<br>the ﬁrst letter of each of the four properties.<br>To gain a better understanding of ACID properties and the need for them, con-<br>sider a simpliﬁed banking system consisting of several accounts and a set of trans-<br>actions that access and update those accounts. For the time being, we assume that<br>the database permanently resides on disk, but that some portion of it is temporarily<br>residing in main memory.<br>Transactions access data using two operations:<br>• read(X), which transfers the data item X from the database to a local buffer<br>belonging to the transaction that executed the read operation.<br>• write(X), which transfers the data item X from the the local buffer of the trans-<br>action that executed the write back to the database.<br>In a real database system, the write operation does not necessarily result in the imme-<br>diate update of the data on the disk; the write operation may be temporarily stored<br>in memory and executed on the disk later. For now, however, we shall assume that<br>the write operation updates the database immediately. We shall return to this subject<br>in Chapter 17.<br>Let Ti be a transaction that transfers $50 from account A to account B. This trans-<br>action can be deﬁned as<br>Ti: read(A);<br>A := A −50;<br>write(A);<br>read(B);<br>B := B + 50;<br>write(B).<br>Let us now consider each of the ACID requirements. (For ease of presentation, we<br>consider them in an order different from the order A-C-I-D).<br>• Consistency: The consistency requirement here is that the sum of A and B<br>be unchanged by the execution of the transaction. Without the consistency<br>requirement, money could be created or destroyed by the transaction! It can<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>V. Transaction <br>Management<br>15. Transactions<br>566<br>© The McGraw−Hill <br>Companies, 2001<br>15.1<br>Transaction Concept<br>567<br>be veriﬁed easily that, if the database is consistent before an execution of the<br>transaction, the database remains consistent after the execution of the transac-<br>tion.<br>Ensuring consistency for an individual transaction is the responsibility of<br>the application programmer who codes the transaction. This task may be facil-<br>itated by automatic testing of integrity constraints, as we discussed in Chap-<br>ter 6.<br>• Atomicity: Suppose that, just before the execution of transaction Ti the values<br>of accounts A and B are $1000 and $2000, respectively. Now suppose that, dur-<br>ing the execution of transaction Ti, a failure occurs that prevents Ti from com-<br>pleting its execution successfully. Examples of such failures include power<br>failures, hardware failures, and software errors. Further, suppose that the fail-<br>ure happened after the write(A) operation but before the write(B) operation. In<br>this case, the values of accounts A and B reﬂected in the database are $950 and<br>$2000. The system destroyed $50 as a result of this failure. In particular, we<br>note that the sum A + B is no longer preserved.<br>Thus, because of the failure, the state of the system no longer reﬂects a real<br>state of the world that the database is supposed to capture. We term such a<br>state an inconsistent state. We must ensure that such inconsistencies are not<br>visible in a database system. Note, however, that the system must at some<br>point be in an inconsistent state. Even if transaction Ti is executed to comple-<br>tion, there exists a point at which the value of account A is $950 and the value<br>of account B is $2000, which is clearly an inconsistent state. This state, how-<br>ever, is eventually replaced by the consistent state where the value of account<br>A is $950, and the value of account B is $2050. Thus, if the transaction never<br>started or was guaranteed to complete, such an inconsistent state would not<br>be visible except during the execution of the transaction. That is the reason for<br>the atomicity requirement: If the atomicity property is present, all actions of<br>the transaction are reﬂected in the database, or none are.<br>The basic idea behind ensuring atomicity is this: The database system keeps<br>track (on disk) of the old values of any data on which a transaction performs a<br>write, and, if the transaction does not complete its execution, the database sys-<br>tem restores the old values to make it appear as though the transaction never<br>executed. We discuss these ideas further in Section 15.2. Ensuring atomicity<br>is the responsibility of the database system itself; speciﬁcally, it is handled by<br>a component called the transaction-management component, which we de-<br>scribe in detail in Chapter 17.<br>• Durability: Once the execution of the transaction completes successfully, and<br>the user who initiated the transaction has been notiﬁed that the transfer of<br>funds has taken place, it must be the case that no system failure will result in<br>a loss of data corresponding to this transfer of funds.<br>The durability property guarantees that, once a transaction completes suc-<br>cessfully, all the updates that it carried out on the database persist, even if<br>there is a system failure after the transaction completes execution.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>V. Transaction <br>Management<br>15. Transactions<br>567<br>© The McGraw−Hill <br>Companies, 2001<br>568<br>Chapter 15<br>Transactions<br>We assume for now that a failure of the computer system may result in<br>loss of data in main memory, but data written to disk are never lost. We can<br>guarantee durability by ensuring that either<br>1. The updates carried out by the transaction have been written to disk be-<br>fore the transaction completes.<br>2. Information about the updates carried out by the transaction and written<br>to disk is sufﬁcient to enable the database to reconstruct the updates when<br>the database system is restarted after the failure.<br>Ensuring durability is the responsibility of a component of the database sys-<br>tem called the recovery-management component. The transaction-manage-<br>ment component and the recovery-management component are closely re-<br>lated, and we describe them in Chapter 17.<br>• Isolation: Even if the consistency and atomicity properties are ensured for<br>each transaction, if several transactions are executed concurrently, their oper-<br>ations may interleave in some undesirable way, resulting in an inconsistent<br>state.<br>For example, as we saw earlier, the database is temporarily inconsistent<br>while the transaction to transfer funds from A to B is executing, with the de-<br>ducted total written to A and the increased total yet to be written to B. If a<br>second concurrently running transaction reads A and B at this intermediate<br>point and computes A+B, it will observe an inconsistent value. Furthermore,<br>if this second transaction then performs updates on A and B based on the in-<br>consistent values that it read, the database may be left in an inconsistent state<br>even after both transactions have completed.<br>A way to avoid the problem of concurrently executing transactions is to<br>execute transactions serially—that is, one after the other. However, concur-<br>rent execution of transactions provides signiﬁcant performance beneﬁts, as<br>we shall see in Section 15.4. Other solutions have therefore been developed;<br>they allow multiple transactions to execute concurrently.<br>We discuss the problems caused by concurrently executing transactions in<br>Section 15.4. The isolation property of a transaction ensures that the concur-<br>rent execution of transactions results in a system state that is equivalent to a<br>state that could have been obtained had these transactions executed one at a<br>time in some order. We shall discuss the principles of isolation further in Sec-<br>tion 15.5. Ensuring the isolation property is the responsibility of a component<br>of the database system called the concurrency-control component, which we<br>discuss later, in Chapter 16.<br>15.2<br>Transaction State<br>In the absence of failures, all transactions complete successfully. However, as we<br>noted earlier, a transaction may not always complete its execution successfully. Such<br>a transaction is termed aborted. If we are to ensure the atomicity property, an aborted<br>transaction must have no effect on the state of the database. Thus, any changes that<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>V. Transaction <br>Management<br>15. Transactions<br>568<br>© The McGraw−Hill <br>Companies, 2001<br>15.2<br>Transaction State<br>569<br>the aborted transaction made to the database must be undone. Once the changes<br>caused by an aborted transaction have been undone, we say that the transaction has<br>been rolled back. It is part of the responsibility of the recovery scheme to manage<br>transaction aborts.<br>A transaction that completes its execution successfully is said to be committed.<br>A committed transaction that has performed updates transforms the database into a<br>new consistent state, which must persist even if there is a system failure.<br>Once a transaction has committed, we cannot undo its effects by aborting it. The<br>only way to undo the effects of a committed transaction is to execute a compensating<br>transaction. For instance, if a transaction added $20 to an account, the compensating<br>transaction would subtract $20 from the account. However, it is not always possible<br>to create such a compensating transaction. Therefore, the responsibility of writing<br>and executing a compensating transaction is left to the user, and is not handled by<br>the database system. Chapter 24 includes a discussion of compensating transactions.<br>We need to be more precise about what we mean by successful completion of a trans-<br>action. We therefore establish a simple abstract transaction model. A transaction must<br>be in one of the following states:<br>• Active, the initial state; the transaction stays in this state while it is executing<br>• Partially committed, after the ﬁnal statement has been executed<br>• Failed, after the discovery that normal execution can no longer proceed<br>• Aborted, after the transaction has been rolled back and the database has been<br>restored to its state prior to the start of the transaction<br>• Committed, after successful completion<br>The state diagram corresponding to a transaction appears in Figure 15.1. We say<br>that a transaction has committed only if it has entered the committed state. Simi-<br>larly, we say that a transaction has aborted only if it has entered the aborted state. A<br>transaction is said to have terminated if has either committed or aborted.<br>A transaction starts in the active state. When it ﬁnishes its ﬁnal statement, it enters<br>the partially committed state. At this point, the transaction has completed its exe-<br>cution, but it is still possible that it may have to be aborted, since the actual output<br>may still be temporarily residing in main memory, and thus a hardware failure may<br>preclude its successful completion.<br>The database system then writes out enough information to disk that, even in the<br>event of a failure, the updates performed by the transaction can be re-created when<br>the system restarts after the failure. When the last of this information is written out,<br>the transaction enters the committed state.<br>As mentioned earlier, we assume for now that failures do not result in loss of data<br>on disk. Chapter 17 discusses techniques to deal with loss of data on disk.<br>A transaction enters the failed state after the system determines that the transac-<br>tion can no longer proceed with its normal execution (for example, because of hard-<br>ware or logical errors). Such a transaction must be rolled back. Then, it enters the<br>aborted state. At this point, the system has two options:<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>V. Transaction <br>Management<br>15. Transactions<br>569<br>© The McGraw−Hill <br>Companies, 2001<br>570<br>Chapter 15<br>Transactions<br>committed<br>aborted<br>failed<br>active<br>partially<br>committed<br>Figure 15.1<br>State diagram of a transaction.<br>• It can restart the transaction, but only if the transaction was aborted as a result<br>of some hardware or software error that was not created through the inter-<br>nal logic of the transaction. A restarted transaction is considered to be a new<br>transaction.<br>• It can kill the transaction. It usually does so because of some internal logical<br>error that can be corrected only by rewriting the application program, or be-<br>cause the input was bad, or because the desired data were not found in the<br>database.<br>We must be cautious when dealing with observable external writes, such as writes<br>to a terminal or printer. Once such a write has occurred, it cannot be erased, since it<br>may have been seen external to the database system. Most systems allow such writes<br>to take place only after the transaction has entered the committed state. One way to<br>implement such a scheme is for the database system to store any value associated<br>with such external writes temporarily in nonvolatile storage, and to perform the ac-<br>tual writes only after the transaction enters the committed state. If the system should<br>fail after the transaction has entered the committed state, but before it could complete<br>the external writes, the database system will carry out the external writes (using the<br>data in nonvolatile storage) when the system is restarted.<br>Handling external writes can be more complicated in some situations. For example<br>suppose the external action is that of dispensing cash at an automated teller machine,<br>and the system fails just before the cash is actually dispensed (we assume that cash<br>can be dispensed atomically). It makes no sense to dispense cash when the system<br>is restarted, since the user may have left the machine. In such a case a compensat-<br>ing transaction, such as depositing the cash back in the users account, needs to be<br>executed when the system is restarted.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>V. Transaction <br>Management<br>15. Transactions<br>570<br>© The McGraw−Hill <br>Companies, 2001<br>15.3<br>Implementation of Atomicity and Durability<br>571<br>For certain applications, it may be desirable to allow active transactions to dis-<br>play data to users, particularly for long-duration transactions that run for minutes<br>or hours. Unfortunately, we cannot allow such output of observable data unless we<br>are willing to compromise transaction atomicity. Most current transaction systems<br>ensure atomicity and, therefore, forbid this form of interaction with users. In Chapter<br>24, we discuss alternative transaction models that support long-duration, interactive<br>transactions.<br>15.3<br>Implementation of Atomicity and Durability<br>The recovery-management component of a database system can support atomicity<br>and durability by a variety of schemes. We ﬁrst consider a simple, but extremely in-<br>efﬁcient, scheme called the shadow copy scheme. This scheme, which is based on<br>making copies of the database, called shadow copies, assumes that only one transac-<br>tion is active at a time. The scheme also assumes that the database is simply a ﬁle on<br>disk. A pointer called db-pointer is maintained on disk; it points to the current copy<br>of the database.<br>In the shadow-copy scheme, a transaction that wants to update the database ﬁrst<br>creates a complete copy of the database. All updates are done on the new database<br>copy, leaving the original copy, the shadow copy, untouched. If at any point the trans-<br>action has to be aborted, the system merely deletes the new copy. The old copy of the<br>database has not been affected.<br>If the transaction completes, it is committed as follows. First, the operating system<br>is asked to make sure that all pages of the new copy of the database have been written<br>out to disk</span><br><br><span style="background-color: #FFADAD;" title="Chunk 72 | Start: 1440144 | End: 1460144 | Tokens: 3321">. (Unix systems use the ﬂush command for this purpose.) After the operat-<br>ing system has written all the pages to disk, the database system updates the pointer<br>db-pointer to point to the new copy of the database; the new copy then becomes<br>the current copy of the database. The old copy of the database is then deleted. Fig-<br>ure 15.2 depicts the scheme, showing the database state before and after the update.<br>db-pointer<br>(a) Before update<br>old copy of<br>database<br>(to be deleted)<br>old copy of<br>database<br>db-pointer<br>(b) After update<br>new copy of<br>database<br>Figure 15.2<br>Shadow-copy technique for atomicity and durability.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>V. Transaction <br>Management<br>15. Transactions<br>571<br>© The McGraw−Hill <br>Companies, 2001<br>572<br>Chapter 15<br>Transactions<br>The transaction is said to have been committed at the point where the updated db-<br>pointer is written to disk.<br>We now consider how the technique handles transaction and system failures. First,<br>consider transaction failure. If the transaction fails at any time before db-pointer is<br>updated, the old contents of the database are not affected. We can abort the trans-<br>action by just deleting the new copy of the database. Once the transaction has been<br>committed, all the updates that it performed are in the database pointed to by db-<br>pointer. Thus, either all updates of the transaction are reﬂected, or none of the effects<br>are reﬂected, regardless of transaction failure.<br>Now consider the issue of system failure. Suppose that the system fails at any time<br>before the updated db-pointer is written to disk. Then, when the system restarts, it<br>will read db-pointer and will thus see the original contents of the database, and none<br>of the effects of the transaction will be visible on the database. Next, suppose that the<br>system fails after db-pointer has been updated on disk. Before the pointer is updated,<br>all updated pages of the new copy of the database were written to disk. Again, we<br>assume that, once a ﬁle is written to disk, its contents will not be damaged even if<br>there is a system failure. Therefore, when the system restarts, it will read db-pointer<br>and will thus see the contents of the database after all the updates performed by the<br>transaction.<br>The implementation actually depends on the write to db-pointer being atomic;<br>that is, either all its bytes are written or none of its bytes are written. If some of the<br>bytes of the pointer were updated by the write, but others were not, the pointer is<br>meaningless, and neither old nor new versions of the database may be found when<br>the system restarts. Luckily, disk systems provide atomic updates to entire blocks, or<br>at least to a disk sector. In other words, the disk system guarantees that it will update<br>db-pointer atomically, as long as we make sure that db-pointer lies entirely in a single<br>sector, which we can ensure by storing db-pointer at the beginning of a block.<br>Thus, the atomicity and durability properties of transactions are ensured by the<br>shadow-copy implementation of the recovery-management component.<br>As a simple example of a transaction outside the database domain, consider a text-<br>editing session. An entire editing session can be modeled as a transaction. The actions<br>executed by the transaction are reading and updating the ﬁle. Saving the ﬁle at the<br>end of editing corresponds to a commit of the editing transaction; quitting the editing<br>session without saving the ﬁle corresponds to an abort of the editing transaction.<br>Many text editors use essentially the implementation just described, to ensure that<br>an editing session is transactional. A new ﬁle is used to store the updated ﬁle. At the<br>end of the editing session, if the updated ﬁle is to be saved, the text editor uses a ﬁle<br>rename command to rename the new ﬁle to have the actual ﬁle name. The rename,<br>assumed to be implemented as an atomic operation by the underlying ﬁle system,<br>deletes the old ﬁle as well.<br>Unfortunately, this implementation is extremely inefﬁcient in the context of large<br>databases, since executing a single transaction requires copying the entire database.<br>Furthermore, the implementation does not allow transactions to execute concurrently<br>with one another. There are practical ways of implementing atomicity and durability<br>that are much less expensive and more powerful. We study these recovery techniques<br>in Chapter 17.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>V. Transaction <br>Management<br>15. Transactions<br>572<br>© The McGraw−Hill <br>Companies, 2001<br>15.4<br>Concurrent Executions<br>573<br>15.4<br>Concurrent Executions<br>Transaction-processing systems usually allow multiple transactions to run concur-<br>rently. Allowing multiple transactions to update data concurrently causes several<br>complications with consistency of the data, as we saw earlier. Ensuring consistency<br>in spite of concurrent execution of transactions requires extra work; it is far easier to<br>insist that transactions run serially—that is, one at a time, each starting only after<br>the previous one has completed. However, there are two good reasons for allowing<br>concurrency:<br>• Improved throughput and resource utilization. A transaction consists of many<br>steps. Some involve I/O activity; others involve CPU activity. The CPU and the<br>disks in a computer system can operate in parallel. Therefore, I/O activity can<br>be done in parallel with processing at the CPU. The parallelism of the CPU<br>and the I/O system can therefore be exploited to run multiple transactions in<br>parallel. While a read or write on behalf of one transaction is in progress on<br>one disk, another transaction can be running in the CPU, while another disk<br>may be executing a read or write on behalf of a third transaction. All of this<br>increases the throughput of the system—that is, the number of transactions<br>executed in a given amount of time. Correspondingly, the processor and disk<br>utilization also increase; in other words, the processor and disk spend less<br>time idle, or not performing any useful work.<br>• Reduced waiting time. There may be a mix of transactions running on a sys-<br>tem, some short and some long. If transactions run serially, a short transaction<br>may have to wait for a preceding long transaction to complete, which can lead<br>to unpredictable delays in running a transaction. If the transactions are oper-<br>ating on different parts of the database, it is better to let them run concurrently,<br>sharing the CPU cycles and disk accesses among them. Concurrent execution<br>reduces the unpredictable delays in running transactions. Moreover, it also<br>reduces the average response time: the average time for a transaction to be<br>completed after it has been submitted.<br>The motivation for using concurrent execution in a database is essentially the same<br>as the motivation for using multiprogramming in an operating system.<br>When several transactions run concurrently, database consistency can be destroyed<br>despite the correctness of each individual transaction. In this section, we present the<br>concept of schedules to help identify those executions that are guaranteed to ensure<br>consistency.<br>The database system must control the interaction among the concurrent trans-<br>actions to prevent them from destroying the consistency of the database. It does<br>so through a variety of mechanisms called concurrency-control schemes. We study<br>concurrency-control schemes in Chapter 16; for now, we focus on the concept of cor-<br>rect concurrent execution.<br>Consider again the simpliﬁed banking system of Section 15.1, which has several<br>accounts, and a set of transactions that access and update those accounts. Let T1 and<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>V. Transaction <br>Management<br>15. Transactions<br>573<br>© The McGraw−Hill <br>Companies, 2001<br>574<br>Chapter 15<br>Transactions<br>T2 be two transactions that transfer funds from one account to another. Transaction T1<br>transfers $50 from account A to account B. It is deﬁned as<br>T1: read(A);<br>A := A −50;<br>write(A);<br>read(B);<br>B := B + 50;<br>write(B).<br>Transaction T2 transfers 10 percent of the balance from account A to account B. It is<br>deﬁned as<br>T2: read(A);<br>temp := A * 0.1;<br>A := A −temp;<br>write(A);<br>read(B);<br>B := B + temp;<br>write(B).<br>Suppose the current values of accounts A and B are $1000 and $2000, respectively.<br>Suppose also that the two transactions are executed one at a time in the order T1<br>followed by T2. This execution sequence appears in Figure 15.3. In the ﬁgure, the<br>sequence of instruction steps is in chronological order from top to bottom, with in-<br>structions of T1 appearing in the left column and instructions of T2 appearing in the<br>right column. The ﬁnal values of accounts A and B, after the execution in Figure 15.3<br>takes place, are $855 and $2145, respectively. Thus, the total amount of money in<br>T1<br>T2<br>read(A)<br>A := A – 50<br>write (A)<br>read(B)<br>B := B + 50<br>write(B)<br>read(A)<br>temp := A * 0.1<br>A := A – temp<br>write(A)<br>read(B)<br>B := B + temp<br>write(B)<br>Figure 15.3<br>Schedule 1—a serial schedule in which T1 is followed by T2.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>V. Transaction <br>Management<br>15. Transactions<br>574<br>© The McGraw−Hill <br>Companies, 2001<br>15.4<br>Concurrent Executions<br>575<br>accounts A and B—that is, the sum A + B—is preserved after the execution of both<br>transactions.<br>Similarly, if the transactions are executed one at a time in the order T2 followed<br>by T1, then the corresponding execution sequence is that of Figure 15.4. Again, as<br>expected, the sum A + B is preserved, and the ﬁnal values of accounts A and B are<br>$850 and $2150, respectively.<br>The execution sequences just described are called schedules. They represent the<br>chronological order in which instructions are executed in the system. Clearly, a sched-<br>ule for a set of transactions must consist of all instructions of those transactions, and<br>must preserve the order in which the instructions appear in each individual transac-<br>tion. For example, in transaction T1, the instruction write(A) must appear before the<br>instruction read(B), in any valid schedule. In the following discussion, we shall refer<br>to the ﬁrst execution sequence (T1 followed by T2) as schedule 1, and to the second<br>execution sequence (T2 followed by T1) as schedule 2.<br>These schedules are serial: Each serial schedule consists of a sequence of instruc-<br>tions from various transactions, where the instructions belonging to one single trans-<br>action appear together in that schedule. Thus, for a set of n transactions, there exist<br>n! different valid serial schedules.<br>When the database system executes several transactions concurrently, the corre-<br>sponding schedule no longer needs to be serial. If two transactions are running con-<br>currently, the operating system may execute one transaction for a little while, then<br>perform a context switch, execute the second transaction for some time, and then<br>switch back to the ﬁrst transaction for some time, and so on. With multiple transac-<br>tions, the CPU time is shared among all the transactions.<br>Several execution sequences are possible, since the various instructions from both<br>transactions may now be interleaved. In general, it is not possible to predict exactly<br>how many instructions of a transaction will be executed before the CPU switches to<br>T1<br>T2<br>read(A)<br>temp := A * 0.1<br>A := A –<br>–<br>temp<br>write(A)<br>read(B)<br>B := B + temp<br>write(B)<br>read(A)<br>A := A<br>50<br>write(A)<br>read(B)<br>B := B + 50<br>write(B)<br>Figure 15.4<br>Schedule 2—a serial schedule in which T2 is followed by T1.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>V. Transaction <br>Management<br>15. Transactions<br>575<br>© The McGraw−Hill <br>Companies, 2001<br>576<br>Chapter 15<br>Transactions<br>T1<br>T2<br>read(A)<br>A := A –<br>–<br>50<br>write(A)<br>read(A)<br>temp := A * 0.1<br>A := A<br>temp<br>write(A )<br>read(B)<br>B := B + 50<br>write(B)<br>read(B)<br>B := B + temp<br>write(B)<br>Figure 15.5<br>Schedule 3—a concurrent schedule equivalent to schedule 1.<br>another transaction. Thus, the number of possible schedules for a set of n transactions<br>is much larger than n!.<br>Returning to our previous example, suppose that the two transactions are exe-<br>cuted concurrently. One possible schedule appears in Figure 15.5. After this execu-<br>tion takes place, we arrive at the same state as the one in which the transactions are<br>executed serially in the order T1 followed by T2. The sum A + B is indeed preserved.<br>Not all concurrent executions result in a correct state. To illustrate, consider the<br>schedule of Figure 15.6. After the execution of this schedule, we arrive at a state<br>where the ﬁnal values of accounts A and B are $950 and $2100, respectively. This ﬁnal<br>state is an inconsistent state, since we have gained $50 in the process of the concur-<br>rent execution. Indeed, the sum A + B is not preserved by the execution of the two<br>transactions.<br>If control of concurrent execution is left entirely to the operating system, many<br>possible schedules, including ones that leave the database in an inconsistent state,<br>such as the one just described, are possible. It is the job of the database system to<br>ensure that any schedule that gets executed will leave the database in a consistent<br>state. The concurrency-control component of the database system carries out this<br>task.<br>We can ensure consistency of the database under concurrent execution by making<br>sure that any schedule that executed has the same effect as a schedule that could<br>have occurred without any concurrent execution. That is, the schedule should, in<br>some sense, be equivalent to a serial schedule. We examine this idea in Section 15.5.<br>15.5<br>Serializability<br>The database system must control concurrent execution of transactions, to ensure<br>that the database state remains consistent. Before we examine how the database<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>V. Transaction <br>Management<br>15. Transactions<br>576<br>© The McGraw−Hill <br>Companies, 2001<br>15.5<br>Serializability<br>577<br>T1<br>T2<br>read(A)<br>A := A –<br>–<br>50<br>read(A)<br>temp := A * 0.1<br>A := A<br>temp<br>write(A)<br>read(B)<br>write(A)<br>read(B)<br>B := B + 50<br>write(B)<br>B := B + temp<br>write(B)<br>Figure 15.6<br>Schedule 4—a concurrent schedule.<br>system can carry out this task, we must ﬁrst understand which schedules will en-<br>sure consistency, and which schedules will not.<br>Since transactions are programs, it is computationally difﬁcult to determine ex-<br>actly what operations a transaction performs and how operations of various trans-<br>actions interact. For this reason, we shall not interpret the type of operations that a<br>transaction can perform on a data item. Instead, we consider only two operations:<br>read and write. We thus assume that, between a read(Q) instruction and a write(Q)<br>instruction on a data item Q, a transaction may perform an arbitrary sequence of op-<br>erations on the copy of Q that is residing in the local buffer of the transaction. Thus,<br>the only signiﬁcant operations of a transaction, from a scheduling point of view, are<br>its read and write instructions. We shall therefore usually show only read and write<br>instructions in schedules, as we do in schedule 3 in Figure 15.7.<br>In this section, we discuss different forms of schedule equivalence; they lead to the<br>notions of conﬂict serializability and view serializability.<br>T1<br>T2<br>read(A)<br>write(A)<br>read(A)<br>write(A)<br>read(B)<br>write(B)<br>read(B)<br>write(B)<br>Figure 15.7<br>Schedule 3—showing only the read and write instructions.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>V. Transaction <br>Management<br>15. Transactions<br>577<br>© The McGraw−Hill <br>Companies, 2001<br>578<br>Chapter 15<br>Transactions<br>15.5.1<br>Conﬂict Serializability<br>Let us consider a schedule S in which there are two consecutive instructions Ii and<br>Ij, of transactions Ti and Tj, respectively (i ̸= j). If Ii and Ij refer to different data<br>items, then we can swap Ii and Ij without affecting the results of any instruction in<br>the schedule. However, if Ii and Ij refer to the same data item Q, then the order of<br>the two steps may matter. Since we are dealing with only read and write instructions,<br>there are four cases that we need to consider:<br>1. Ii = read(Q), Ij = read(Q). The order of Ii and Ij does not matter, since the<br>same value of Q is read by Ti and Tj, regardless of the order.<br>2. Ii = read(Q), Ij = write(Q). If Ii comes before Ij, then Ti does not read the value<br>of Q that is written by Tj in instruction Ij. If Ij comes before Ii, then Ti reads<br>the value of Q that is written by Tj. Thus, the order of Ii and Ij matters.<br>3. Ii = write(Q), Ij = read(Q). The order of Ii and Ij matters for reasons similar<br>to those of the previous case.<br>4. Ii = write(Q), Ij = write(Q). Since both instructions are write operations, the<br>order of these instructions does not affect either Ti or Tj. However, the value<br>obtained by the next read(Q) instruction of S is affected, since the result of<br>only the latter of the two write instructions is preserved in the database. If<br>there is no other write(Q) instruction after Ii and Ij in S, then the order of Ii<br>and Ij directly affects the ﬁnal value of Q in the database state that results<br>from schedule S.<br>Thus, only in the case where both Ii and Ij are read instructions does the relative<br>order of their execution not matter.<br>We say that Ii and Ij conﬂict if they are operations by different transactions on the<br>same data item, and at least one of these instructions is a write operation.<br>To illustrate the concept of conﬂicting instructions, we consider schedule 3, in Fig-<br>ure 15.7. The write(A) instruction of T1 conﬂicts with the read(A) instruction of T2.<br>However, the write(A) instruction of T2 does not conﬂict with the read(B) instruction<br>of T1, because the two instructions access different data items.<br>Let Ii and Ij be consecutive instructions of a schedule S. If Ii and Ij are instructions<br>of different transactions and Ii and Ij do not conﬂict, then we can swap the order of<br>Ii and Ij to produce a new schedule S′. We expect S to be equivalent to S′, since all<br>instructions appear in the same order in both schedules except for Ii and Ij, whose<br>order does not matter.<br>Since the write(A) instruction of T2 in schedule 3 of Figure 15.7 does not conﬂict<br>with the read(B) instruction of T1, we can swap these instructions to generate an<br>equivalent schedule, schedule 5, in Figure 15.8. Regardless of the initial system state,<br>schedules 3 and 5 both produce the same ﬁnal system state.<br>We continue to swap nonconﬂicting instructions:<br>• Swap the read(B) instruction of T1 with the read(A) instruction of T2.<br>• Swap the write(B) instruction of T1 with the write(A) instruction of T2.<br>• Swap the write(B) instruction of T1 with the read(A) instruction of T2.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>V. Transaction <br>Management<br>15. Transactions<br>578<br>© The McGraw−Hill <br>Companies, 2001<br>15.5<br>Serializability<br>579<br>T1<br>T2<br>read(A)<br>write(A)<br>read(A)<br>read(B)<br>write(A)<br>write(B)<br>read(B)<br>write(B)<br>Figure 15.8<br>Schedule 5—schedule 3 after swapping of a pair of instructions.<br>The ﬁnal result of these swaps, schedule 6 of Figure 15.9, is a serial schedule. Thus,<br>we have shown that schedule 3 is equivalent to a serial schedule. This equivalence<br>implies that, regardless of the initial system state, schedule 3 will produce the same<br>ﬁnal state as will some serial schedule.<br>If a schedule S can be transformed into a schedule S′ by a series of swaps of non-<br>conﬂicting instructions, we say that S and S′ are conﬂict equivalent.<br>In our previous examples, schedule 1 is not conﬂict equivalent to schedule 2. How-<br>ever, schedule 1 is conﬂict equivalent to schedule 3, because the read(B) and write(B)<br>instruction of T1 can be swapped with the read(A) and write(A) instruction of T2.<br>The concept of conﬂict equivalence leads to the concept of conﬂict serializability.<br>We say that a schedule S is conﬂict serializable if it is conﬂict equivalent to a serial<br>schedule. Thus, schedule 3 is conﬂict serializable, since it is conﬂict equivalent to the<br>serial schedule 1.<br>Finally, consider schedule 7 of Figure 15.10; it consists of only the signiﬁcant op-<br>erations (that is, the read and write) of transactions T3 and T4. </span><br><br><span style="background-color: #FFD6A5;" title="Chunk 73 | Start: 1460146 | End: 1480146 | Tokens: 3160">This schedule is not<br>conﬂict serializable, since it is not equivalent to either the serial schedule &lt;T3,T4&gt; or<br>the serial schedule &lt;T4,T3&gt;.<br>It is possible to have two schedules that produce the same outcome, but that are<br>not conﬂict equivalent. For example, consider transaction T5, which transfers $10<br>T1<br>T2<br>read(A)<br>write(A)<br>read(B)<br>write(B)<br>read(A)<br>write(A)<br>read(B)<br>write(B)<br>Figure 15.9<br>Schedule 6—a serial schedule that is equivalent to schedule 3.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>V. Transaction <br>Management<br>15. Transactions<br>579<br>© The McGraw−Hill <br>Companies, 2001<br>580<br>Chapter 15<br>Transactions<br>T3<br>T4<br>read(Q)<br>write(Q)<br>write(Q)<br>Figure 15.10<br>Schedule 7.<br>from account B to account A. Let schedule 8 be as deﬁned in Figure 15.11. We claim<br>that schedule 8 is not conﬂict equivalent to the serial schedule &lt;T1,T5&gt;, since, in<br>schedule 8, the write(B) instruction of T5 conﬂicts with the read(B) instruction of T1.<br>Thus, we cannot move all the instructions of T1 before those of T5 by swapping con-<br>secutive nonconﬂicting instructions. However, the ﬁnal values of accounts A and B<br>after the execution of either schedule 8 or the serial schedule &lt;T1,T5&gt; are the same<br>—$960 and $2040, respectively.<br>We can see from this example that there are less stringent deﬁnitions of schedule<br>equivalence than conﬂict equivalence. For the system to determine that schedule 8<br>produces the same outcome as the serial schedule &lt;T1,T5&gt;, it must analyze the com-<br>putation performed by T1 and T5, rather than just the read and write operations. In<br>general, such analysis is hard to implement and is computationally expensive. How-<br>ever, there are other deﬁnitions of schedule equivalence based purely on the read and<br>write operations. We will consider one such deﬁnition in the next section.<br>15.5.2<br>View Serializability<br>In this section, we consider a form of equivalence that is less stringent than conﬂict<br>equivalence, but that, like conﬂict equivalence, is based on only the read and write<br>operations of transactions.<br>T1<br>T5<br>read(A)<br>A := A – 50<br>write(A)<br>read(B)<br>B := B<br>10<br>write(B)<br>read(B)<br>B := B + 50<br>write(B)<br>read(A)<br>A := A + 10<br>write(A)<br>–<br>Figure 15.11<br>Schedule 8.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>V. Transaction <br>Management<br>15. Transactions<br>580<br>© The McGraw−Hill <br>Companies, 2001<br>15.5<br>Serializability<br>581<br>Consider two schedules S and S′, where the same set of transactions participates<br>in both schedules. The schedules S and S′ are said to be view equivalent if three<br>conditions are met:<br>1. For each data item Q, if transaction Ti reads the initial value of Q in schedule<br>S, then transaction Ti must, in schedule S′, also read the initial value of Q.<br>2. For each data item Q, if transaction Ti executes read(Q) in schedule S, and if<br>that value was produced by a write(Q) operation executed by transaction Tj,<br>then the read(Q) operation of transaction Ti must, in schedule S′, also read the<br>value of Q that was produced by the same write(Q) operation of transaction Tj.<br>3. For each data item Q, the transaction (if any) that performs the ﬁnal write(Q)<br>operation in schedule S must perform the ﬁnal write(Q) operation in sched-<br>ule S′.<br>Conditions 1 and 2 ensure that each transaction reads the same values in both<br>schedules and, therefore, performs the same computation. Condition 3, coupled with<br>conditions 1 and 2, ensures that both schedules result in the same ﬁnal system state.<br>In our previous examples, schedule 1 is not view equivalent to schedule 2, since,<br>in schedule 1, the value of account A read by transaction T2 was produced by T1,<br>whereas this case does not hold in schedule 2. However, schedule 1 is view equivalent<br>to schedule 3, because the values of account A and B read by transaction T2 were<br>produced by T1 in both schedules.<br>The concept of view equivalence leads to the concept of view serializability. We<br>say that a schedule S is view serializable if it is view equivalent to a serial schedule.<br>As an illustration, suppose that we augment schedule 7 with transaction T6, and<br>obtain schedule 9 in Figure 15.12. Schedule 9 is view serializable. Indeed, it is view<br>equivalent to the serial schedule &lt;T3, T4, T6&gt;, since the one read(Q) instruction reads<br>the initial value of Q in both schedules, and T6 performs the ﬁnal write of Q in both<br>schedules.<br>Every conﬂict-serializable schedule is also view serializable, but there are view-<br>serializable schedules that are not conﬂict serializable. Indeed, schedule 9 is not con-<br>ﬂict serializable, since every pair of consecutive instructions conﬂicts, and, thus, no<br>swapping of instructions is possible.<br>Observe that, in schedule 9, transactions T4 and T6 perform write(Q) operations<br>without having performed a read(Q) operation. Writes of this sort are called blind<br>writes. Blind writes appear in any view-serializable schedule that is not conﬂict seri-<br>alizable.<br>T3<br>T4<br>T6<br>read(Q)<br>write(Q)<br>write(Q)<br>write(Q)<br>Figure 15.12<br>Schedule 9—a view-serializable schedule.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>V. Transaction <br>Management<br>15. Transactions<br>581<br>© The McGraw−Hill <br>Companies, 2001<br>582<br>Chapter 15<br>Transactions<br>15.6<br>Recoverability<br>So far, we have studied what schedules are acceptable from the viewpoint of consis-<br>tency of the database, assuming implicitly that there are no transaction failures. We<br>now address the effect of transaction failures during concurrent execution.<br>If a transaction Ti fails, for whatever reason, we need to undo the effect of this<br>transaction to ensure the atomicity property of the transaction. In a system that allows<br>concurrent execution, it is necessary also to ensure that any transaction Tj that is<br>dependent on Ti (that is, Tj has read data written by Ti) is also aborted. To achieve<br>this surety, we need to place restrictions on the type of schedules permitted in the<br>system.<br>In the following two subsections, we address the issue of what schedules are<br>acceptable from the viewpoint of recovery from transaction failure. We describe in<br>Chapter 16 how to ensure that only such acceptable schedules are generated.<br>15.6.1<br>Recoverable Schedules<br>Consider schedule 11 in Figure 15.13, in which T9 is a transaction that performs only<br>one instruction: read(A). Suppose that the system allows T9 to commit immediately<br>after executing the read(A) instruction. Thus, T9 commits before T8 does. Now sup-<br>pose that T8 fails before it commits. Since T9 has read the value of data item A writ-<br>ten by T8, we must abort T9 to ensure transaction atomicity. However, T9 has already<br>committed and cannot be aborted. Thus, we have a situation where it is impossible<br>to recover correctly from the failure of T8.<br>Schedule 11, with the commit happening immediately after the read(A) instruc-<br>tion, is an example of a nonrecoverable schedule, which should not be allowed. Most<br>database system require that all schedules be recoverable. A recoverable schedule is<br>one where, for each pair of transactions Ti and Tj such that Tj reads a data item previ-<br>ously written by Ti, the commit operation of Ti appears before the commit operation<br>of Tj.<br>15.6.2<br>Cascadeless Schedules<br>Even if a schedule is recoverable, to recover correctly from the failure of a transac-<br>tion Ti, we may have to roll back several transactions. Such situations occur if trans-<br>actions have read data written by Ti. As an illustration, consider the partial schedule<br>T8<br>T9<br>read(A)<br>write(A)<br>read(A)<br>read(B)<br>Figure 15.13<br>Schedule 11.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>V. Transaction <br>Management<br>15. Transactions<br>582<br>© The McGraw−Hill <br>Companies, 2001<br>15.7<br>Implementation of Isolation<br>583<br>T10<br>T11<br>T12<br>read(A)<br>read(B)<br>write(A)<br>read(A)<br>write(A)<br>read(A)<br>Figure 15.14<br>Schedule 12.<br>of Figure 15.14. Transaction T10 writes a value of A that is read by transaction T11.<br>Transaction T11 writes a value of A that is read by transaction T12. Suppose that,<br>at this point, T10 fails. T10 must be rolled back. Since T11 is dependent on T10, T11<br>must be rolled back. Since T12 is dependent on T11, T12 must be rolled back. This<br>phenomenon, in which a single transaction failure leads to a series of transaction<br>rollbacks, is called cascading rollback.<br>Cascading rollback is undesirable, since it leads to the undoing of a signiﬁcant<br>amount of work. It is desirable to restrict the schedules to those where cascading<br>rollbacks cannot occur. Such schedules are called cascadeless schedules. Formally, a<br>cascadeless schedule is one where, for each pair of transactions Ti and Tj such that<br>Tj reads a data item previously written by Ti, the commit operation of Ti appears<br>before the read operation of Tj. It is easy to verify that every cascadeless schedule is<br>also recoverable.<br>15.7<br>Implementation of Isolation<br>So far, we have seen what properties a schedule must have if it is to leave the database<br>in a consistent state and allow transaction failures to be handled in a safe manner.<br>Speciﬁcally, schedules that are conﬂict or view serializable and cascadeless satisfy<br>these requirements.<br>There are various concurrency-control schemes that we can use to ensure that,<br>even when multiple transactions are executed concurrently, only acceptable sched-<br>ules are generated, regardless of how the operating-system time-shares resources<br>(such as CPU time) among the transactions.<br>As a trivial example of a concurrency-control scheme, consider this scheme: A<br>transaction acquires a lock on the entire database before it starts and releases the<br>lock after it has committed. While a transaction holds a lock, no other transaction is<br>allowed to acquire the lock, and all must therefore wait for the lock to be released. As<br>a result of the locking policy, only one transaction can execute at a time. Therefore,<br>only serial schedules are generated. These are trivially serializable, and it is easy to<br>verify that they are cascadeless as well.<br>A concurrency-control scheme such as this one leads to poor performance, since it<br>forces transactions to wait for preceding transactions to ﬁnish before they can start. In<br>other words, it provides a poor degree of concurrency. As explained in Section 15.4,<br>concurrent execution has several performance beneﬁts.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>V. Transaction <br>Management<br>15. Transactions<br>583<br>© The McGraw−Hill <br>Companies, 2001<br>584<br>Chapter 15<br>Transactions<br>The goal of concurrency-control schemes is to provide a high degree of concur-<br>rency, while ensuring that all schedules that can be generated are conﬂict or view<br>serializable, and are cascadeless.<br>We study a number of concurrency-control schemes in Chapter 16. The schemes<br>have different trade-offs in terms of the amount of concurrency they allow and the<br>amount of overhead that they incur. Some of them allow only conﬂict serializable<br>schedules to be generated; others allow certain view-serializable schedules that are<br>not conﬂict-serializable to be generated.<br>15.8<br>Transaction Deﬁnition in SQL<br>A data-manipulation language must include a construct for specifying the set of ac-<br>tions that constitute a transaction.<br>The SQL standard speciﬁes that a transaction begins implicitly. Transactions are<br>ended by one of these SQL statements:<br>• Commit work commits the current transaction and begins a new one.<br>• Rollback work causes the current transaction to abort.<br>The keyword work is optional in both the statements. If a program terminates with-<br>out either of these commands, the updates are either committed or rolled back—<br>which of the two happens is not speciﬁed by the standard and depends on the im-<br>plementation.<br>The standard also speciﬁes that the system must ensure both serializability and<br>freedom from cascading rollback. The deﬁnition of serializability used by the stan-<br>dard is that a schedule must have the same effect as would some serial schedule. Thus,<br>conﬂict and view serializability are both acceptable.<br>The SQL-92 standard also allows a transaction to specify that it may be executed in<br>a manner that causes it to become nonserializable with respect to other transactions.<br>We study such weaker levels of consistency in Section 16.8.<br>15.9<br>Testing for Serializability<br>When designing concurrency control schemes, we must show that schedules gen-<br>erated by the scheme are serializable. To do that, we must ﬁrst understand how to<br>determine, given a particular schedule S, whether the schedule is serializable.<br>We now present a simple and efﬁcient method for determining conﬂict serializ-<br>ability of a schedule. Consider a schedule S. We construct a directed graph, called a<br>precedence graph, from S. This graph consists of a pair G = (V, E), where V is a set<br>of vertices and E is a set of edges. The set of vertices consists of all the transactions<br>participating in the schedule. The set of edges consists of all edges Ti →Tj for which<br>one of three conditions holds:<br>1. Ti executes write(Q) before Tj executes read(Q).<br>2. Ti executes read(Q) before Tj executes write(Q).<br>3. Ti executes write(Q) before Tj executes write(Q).<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>V. Transaction <br>Management<br>15. Transactions<br>584<br>© The McGraw−Hill <br>Companies, 2001<br>15.9<br>Testing for Serializability<br>585<br>T1<br>T2<br>T2<br>T1<br>(a)<br>(b)<br>Figure 15.15<br>Precedence graph for (a) schedule 1 and (b) schedule 2.<br>If an edge Ti →Tj exists in the precedence graph, then, in any serial schedule S′<br>equivalent to S, Ti must appear before Tj.<br>For example, the precedence graph for schedule 1 in Figure 15.15a contains the<br>single edge T1 →T2, since all the instructions of T1 are executed before the ﬁrst in-<br>struction of T2 is executed. Similarly, Figure 15.15b shows the precedence graph for<br>schedule 2 with the single edge T2 →T1, since all the instructions of T2 are executed<br>before the ﬁrst instruction of T1 is executed.<br>The precedence graph for schedule 4 appears in Figure 15.16. It contains the edge<br>T1 →T2, because T1 executes read(A) before T2 executes write(A). It also contains the<br>edge T2 →T1, because T2 executes read(B) before T1 executes write(B).<br>If the precedence graph for S has a cycle, then schedule S is not conﬂict serializable.<br>If the graph contains no cycles, then the schedule S is conﬂict serializable.<br>A serializability order of the transactions can be obtained through topological<br>sorting, which determines a linear order consistent with the partial order of the<br>precedence graph. There are, in general, several possible linear orders that can be<br>obtained through a topological sorting. For example, the graph of Figure 15.17a has<br>the two acceptable linear orderings shown in Figures 15.17b and 15.17c.<br>Thus, to test for conﬂict serializability, we need to construct the precedence graph<br>and to invoke a cycle-detection algorithm. Cycle-detection algorithms can be found<br>in standard textbooks on algorithms. Cycle-detection algorithms, such as those based<br>on depth-ﬁrst search, require on the order of n2 operations, where n is the number of<br>vertices in the graph (that is, the number of transactions). Thus, we have a practical<br>scheme for determining conﬂict serializability.<br>Returning to our previous examples, note that the precedence graphs for sched-<br>ules 1 and 2 (Figure 15.15) indeed do not contain cycles. The precedence graph for<br>schedule 4 (Figure 15.16), on the other hand, contains a cycle, indicating that this<br>schedule is not conﬂict serializable.<br>Testing for view serializability is rather complicated. In fact, it has been shown<br>that the problem of testing for view serializability is itself NP-complete. Thus, al-<br>most certainly there exists no efﬁcient algorithm to test for view serializability. See<br>T1<br>T2<br>Figure 15.16<br>Precedence graph for schedule 4.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>V. Transaction <br>Management<br>15. Transactions<br>585<br>© The McGraw−Hill <br>Companies, 2001<br>586<br>Chapter 15<br>Transactions<br>Tj<br>Tk<br>Ti<br>Tm<br>(a)<br>Ti<br>Tj<br>Tk<br>Tm<br>(b)<br>Ti<br>Tk<br>Tj<br>Tm<br>(c)<br>Figure 15.17<br>Illustration of topological sorting.<br>the bibliographical notes for references on testing for view serializability. However,<br>concurrency-control schemes can still use sufﬁcient conditions for view serializability.<br>That is, if the sufﬁcient conditions are satisﬁed, the schedule is view serializable, but<br>there may be view-serializable schedules that do not satisfy the sufﬁcient conditions.<br>15.10<br>Summary<br>• A transaction is a unit of program execution that accesses and possibly updates<br>various data items. Understanding the concept of a transaction is critical for<br>understanding and implementing updates of data in a database, in such a way<br>that concurrent executions and failures of various forms do not result in the<br>database becoming inconsistent.<br>• Transactions are required to have the ACID properties: atomicity, consistency,<br>isolation, and durability.<br>  Atomicity ensures that either all the effects of a transaction are reﬂected<br>in the database, or none are; a failure cannot leave the database in a state<br>where a transaction is partially executed.<br>  Consistency ensures that, if the database is initially consistent, the execu-<br>tion of the transaction (by itself) leaves the database in a consistent state.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>V. Transaction <br>Management<br>15. Transactions<br>586<br>© The McGraw−Hill <br>Companies, 2001<br>15.10<br>Summary<br>587<br>  Isolation ensures that concurrently executing transactions are isolated from<br>one another, so that each has the impression that no other transaction is<br>executing concurrently with it.<br>  Durability ensures that, once a transaction has been committed, that trans-<br>action’s updates do not get lost, even if there is a system failure.<br>• Concurrent execution of transactions improves throughput of transactions and<br>system utilization, and also reduces waiting time of transactions.<br>• When several transactions execute concurrently in the database, the consis-<br>tency of data may no longer be preserved. It is therefore necessary for the<br>system to control the interaction among the concurrent transactions.<br>  Since a transaction is a unit that preserves consistency, a serial execution<br>of transactions guarantees that consistency is preserved.<br>  A schedule captures the key actions of transactions that affect concurrent<br>execution, such as read and write operations, while abstracting away in-<br>ternal details of the execution of the transaction.<br>  We require that any schedule produced by concurrent processing of a<br>set of transactions will have an effect equivalent to a schedule produced<br>when these transactions are run serially in some order.<br>  A system that guarantees this property is said to ensure serializability.<br>  There are several different notions of equivalence leading to the concepts<br>of conﬂict serializability and view serializability.<br>• Serializability of schedules generated by concurrently executing transactions<br>can be ensured through one of a variety of mechanisms called concurrency-<br>control schemes.<br>• Schedules must be recoverable, to make sure that if transaction a sees the ef-<br>fects of transaction b, and b then aborts, then a also gets aborted.<br>• Schedules should preferably be cascadeless, so that the abort of a transaction<br>does not result in cascading aborts of other transactions. Cascadelessness is<br>ensured by allowing transactions to only read committed data.<br>• The concurrency-control–management component of the database is respon-<br>sible for handling the concurrency-control schemes. Chapter 16 describes<br>concurrency-control schemes.<br>• The recovery-management component of a database is responsible for ensur-<br>ing the atomicity and durability properties of transactions.<br>The shadow copy scheme is used for ensuring atomicity and durability in<br>text editors; however, it has extremely high overheads when used for database<br>systems, and, moreover, it does not support concurrent execution. Chapter 17<br>covers better schemes.<br>• We can test a given schedule for conﬂict serializability by constructing a prece-<br>dence graph for the schedule, and by searching </span><br><br><span style="background-color: #FDFFB6;" title="Chunk 74 | Start: 1480148 | End: 1500148 | Tokens: 3236">for absence of cycles in the<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>V. Transaction <br>Management<br>15. Transactions<br>587<br>© The McGraw−Hill <br>Companies, 2001<br>588<br>Chapter 15<br>Transactions<br>graph. However, there are more efﬁcient concurrency control schemes for en-<br>suring serializability.<br>Review Terms<br>• Transaction<br>• ACID properties<br>  Atomicity<br>  Consistency<br>  Isolation<br>  Durability<br>• Inconsistent state<br>• Transaction state<br>  Active<br>  Partially committed<br>  Failed<br>  Aborted<br>  Committed<br>  Terminated<br>• Transaction<br>  Restart<br>  Kill<br>• Observable external writes<br>• Shadow copy scheme<br>• Concurrent executions<br>• Serial execution<br>• Schedules<br>• Conﬂict of operations<br>• Conﬂict equivalence<br>• Conﬂict serializability<br>• View equivalence<br>• View serializability<br>• Blind writes<br>• Recoverability<br>• Recoverable schedules<br>• Cascading rollback<br>• Cascadeless schedules<br>• Concurrency-control scheme<br>• Lock<br>• Serializability testing<br>• Precedence graph<br>• Serializability order<br>Exercises<br>15.1 List the ACID properties. Explain the usefulness of each.<br>15.2 Suppose that there is a database system that never fails. Is a recovery manager<br>required for this system?<br>15.3 Consider a ﬁle system such as the one on your favorite operating system.<br>a. What are the steps involved in creation and deletion of ﬁles, and in writing<br>data to a ﬁle?<br>b. Explain how the issues of atomicity and durability are relevant to the cre-<br>ation and deletion of ﬁles, and to writing data to ﬁles.<br>15.4 Database-system implementers have paid much more attention to the ACID<br>properties than have ﬁle-system implementers. Why might this be the case?<br>15.5 During its execution, a transaction passes through several states, until it ﬁnally<br>commits or aborts. List all possible sequences of states through which a trans-<br>action may pass. Explain why each state transition may occur.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>V. Transaction <br>Management<br>15. Transactions<br>588<br>© The McGraw−Hill <br>Companies, 2001<br>Exercises<br>589<br>15.6 Justify the following statement: Concurrent execution of transactions is more<br>important when data must be fetched from (slow) disk or when transactions<br>are long, and is less important when data is in memory and transactions are<br>very short.<br>15.7 Explain the distinction between the terms serial schedule and serializable schedule.<br>15.8 Consider the following two transactions:<br>T1: read(A);<br>read(B);<br>if A = 0 then B := B + 1;<br>write(B).<br>T2: read(B);<br>read(A);<br>if B = 0 then A := A + 1;<br>write(A).<br>Let the consistency requirement be A = 0 ∨B = 0, with A = B = 0 the<br>initial values.<br>a. Show that every serial execution involving these two transactions pre-<br>serves the consistency of the database.<br>b. Show a concurrent execution of T1 and T2 that produces a nonserializable<br>schedule.<br>c. Is there a concurrent execution of T1 and T2 that produces a serializable<br>schedule?<br>15.9 Since every conﬂict-serializable schedule is view serializable, why do we em-<br>phasize conﬂict serializability rather than view serializability?<br>15.10 Consider the precedence graph of Figure 15.18. Is the corresponding schedule<br>conﬂict serializable? Explain your answer.<br>15.11 What is a recoverable schedule? Why is recoverability of schedules desirable?<br>Are there any circumstances under which it would be desirable to allow non-<br>recoverable schedules? Explain your answer.<br>T4<br>T2<br>T5<br>T1<br>T3<br>Figure 15.18<br>Precedence graph.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>V. Transaction <br>Management<br>15. Transactions<br>589<br>© The McGraw−Hill <br>Companies, 2001<br>590<br>Chapter 15<br>Transactions<br>15.12 What is a cascadeless schedule? Why is cascadelessness of schedules desir-<br>able? Are there any circumstances under which it would be desirable to allow<br>noncascadeless schedules? Explain your answer.<br>Bibliographical Notes<br>Gray and Reuter [1993] provides detailed textbook coverage of transaction-processing<br>concepts, techniques and implementation details, including concurrency control and<br>recovery issues. Bernstein and Newcomer [1997] provides textbook coverage of var-<br>ious aspects of transaction processing.<br>Early textbook discussions of concurrency control and recovery included Papadim-<br>itriou [1986] and Bernstein et al. [1987]. An early survey paper on implementation<br>issues in concurrency control and recovery is presented by Gray [1978].<br>The concept of serializability was formalized by Eswaran et al. [1976] in connection<br>to work on concurrency control for System R. The results concerning serializability<br>testing and NP-completeness of testing for view serializability are from Papadim-<br>itriou et al. [1977] and Papadimitriou [1979]. Cycle-detection algorithms as well as an<br>introduction to NP-completeness can be found in standard algorithm textbooks such<br>as Cormen et al. [1990].<br>References covering speciﬁc aspects of transaction processing, such as concurrency<br>control and recovery, are cited in Chapters 16, 17, and 24.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>V. Transaction <br>Management<br>16. Concurrency Control<br>590<br>© The McGraw−Hill <br>Companies, 2001<br>C<br>H<br>A<br>P<br>T<br>E<br>R<br>1<br>6<br>Concurrency Control<br>We saw in Chapter 15 that one of the fundamental properties of a transaction is iso-<br>lation. When several transactions execute concurrently in the database, however, the<br>isolation property may no longer be preserved. To ensure that it is, the system must<br>control the interaction among the concurrent transactions; this control is achieved<br>through one of a variety of mechanisms called concurrency-control schemes.<br>The concurrency-control schemes that we discuss in this chapter are all based on<br>the serializability property. That is, all the schemes presented here ensure that the<br>schedules are serializable. In Chapter 24, we discuss concurrency control schemes<br>that admit nonserializable schedules. In this chapter, we consider the management of<br>concurrently executing transactions, and we ignore failures. In Chapter 17, we shall<br>see how the system can recover from failures.<br>16.1<br>Lock-Based Protocols<br>One way to ensure serializability is to require that data items be accessed in a mutu-<br>ally exclusive manner; that is, while one transaction is accessing a data item, no other<br>transaction can modify that data item. The most common method used to implement<br>this requirement is to allow a transaction to access a data item only if it is currently<br>holding a lock on that item.<br>16.1.1<br>Locks<br>There are various modes in which a data item may be locked. In this section, we<br>restrict our attention to two modes:<br>1. Shared. If a transaction Ti has obtained a shared-mode lock (denoted by S)<br>on item Q, then Ti can read, but cannot write, Q.<br>2. Exclusive. If a transaction Ti has obtained an exclusive-mode lock (denoted<br>by X) on item Q, then Ti can both read and write Q.<br>591<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>V. Transaction <br>Management<br>16. Concurrency Control<br>591<br>© The McGraw−Hill <br>Companies, 2001<br>592<br>Chapter 16<br>Concurrency Control<br>S<br>X<br>S<br>true<br>false<br>X<br>false<br>false<br>Figure 16.1<br>Lock-compatibility matrix comp.<br>We require that every transaction request a lock in an appropriate mode on data<br>item Q, depending on the types of operations that it will perform on Q. The trans-<br>action makes the request to the concurrency-control manager. The transaction can<br>proceed with the operation only after the concurrency-control manager grants the<br>lock to the transaction.<br>Given a set of lock modes, we can deﬁne a compatibility function on them as<br>follows. Let A and B represent arbitrary lock modes. Suppose that a transaction Ti<br>requests a lock of mode A on item Q on which transaction Tj (Ti ̸= Tj) currently holds<br>a lock of mode B. If transaction Ti can be granted a lock on Q immediately, in spite<br>of the presence of the mode B lock, then we say mode A is compatible with mode<br>B. Such a function can be represented conveniently by a matrix. The compatibility<br>relation between the two modes of locking discussed in this section appears in the<br>matrix comp of Figure 16.1. An element comp(A, B) of the matrix has the value true if<br>and only if mode A is compatible with mode B.<br>Note that shared mode is compatible with shared mode, but not with exclusive<br>mode. At any time, several shared-mode locks can be held simultaneously (by differ-<br>ent transactions) on a particular data item. A subsequent exclusive-mode lock request<br>has to wait until the currently held shared-mode locks are released.<br>A transaction requests a shared lock on data item Q by executing the lock-S(Q)<br>instruction. Similarly, a transaction requests an exclusive lock through the lock-X(Q)<br>instruction. A transaction can unlock a data item Q by the unlock(Q) instruction.<br>To access a data item, transaction Ti must ﬁrst lock that item. If the data item is<br>already locked by another transaction in an incompatible mode, the concurrency-<br>control manager will not grant the lock until all incompatible locks held by other<br>transactions have been released. Thus, Ti is made to wait until all incompatible locks<br>held by other transactions have been released.<br>T1: lock-X(B);<br>read(B);<br>B := B −50;<br>write(B);<br>unlock(B);<br>lock-X(A);<br>read(A);<br>A := A + 50;<br>write(A);<br>unlock(A).<br>Figure 16.2<br>Transaction T1.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>V. Transaction <br>Management<br>16. Concurrency Control<br>592<br>© The McGraw−Hill <br>Companies, 2001<br>16.1<br>Lock-Based Protocols<br>593<br>T2: lock-S(A);<br>read(A);<br>unlock(A);<br>lock-S(B);<br>read(B);<br>unlock(B);<br>display(A + B).<br>Figure 16.3<br>Transaction T2.<br>Transaction Ti may unlock a data item that it had locked at some earlier point.<br>Note that a transaction must hold a lock on a data item as long as it accesses that item.<br>Moreover, for a transaction to unlock a data item immediately after its ﬁnal access of<br>that data item is not always desirable, since serializability may not be ensured.<br>As an illustration, consider again the simpliﬁed banking system that we intro-<br>duced in Chapter 15. Let A and B be two accounts that are accessed by transactions<br>T1 and T2. Transaction T1 transfers $50 from account B to account A (Figure 16.2).<br>Transaction T2 displays the total amount of money in accounts A and B—that is, the<br>sum A + B (Figure 16.3).<br>anager<br>T1<br>T2<br>concurrency-control<br>lock-X(B)<br>grant-X(B, T1)<br>read(B)<br>B := B<br>50<br>write(B)<br>unlock(B)<br>lock-S(A)<br>grant-S(A, T2)<br>read(A)<br>unlock(A)<br>lock-S(B)<br>grant-S(B, T2)<br>read(B)<br>unlock(B)<br>display(A + B)<br>lock-X(A)<br>grant-X(A, T2)<br>read(A)<br>A := A + 50<br>write(A)<br>unlock(A)<br>––<br>m<br>Figure 16.4<br>Schedule 1.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>V. Transaction <br>Management<br>16. Concurrency Control<br>593<br>© The McGraw−Hill <br>Companies, 2001<br>594<br>Chapter 16<br>Concurrency Control<br>T3: lock-X(B);<br>read(B);<br>B := B −50;<br>write(B);<br>lock-X(A);<br>read(A);<br>A := A + 50;<br>write(A);<br>unlock(B);<br>unlock(A).<br>Figure 16.5<br>Transaction T3.<br>Suppose that the values of accounts A and B are $100 and $200, respectively. If these<br>two transactions are executed serially, either in the order T1, T2 or the order T2, T1,<br>then transaction T2 will display the value $300. If, however, these transactions are<br>executed concurrently, then schedule 1, in Figure 16.4 is possible. In this case, trans-<br>action T2 displays $250, which is incorrect. The reason for this mistake is that the<br>transaction T1 unlocked data item B too early, as a result of which T2 saw an incon-<br>sistent state.<br>The schedule shows the actions executed by the transactions, as well as the points<br>at which the concurrency-control manager grants the locks. The transaction mak-<br>ing a lock request cannot execute its next action until the concurrency-control man-<br>ager grants the lock. Hence, the lock must be granted in the interval of time between<br>the lock-request operation and the following action of the transaction. Exactly when<br>within this interval the lock is granted is not important; we can safely assume that the<br>lock is granted just before the following action of the transaction. We shall therefore<br>drop the column depicting the actions of the concurrency-control manager from all<br>schedules depicted in the rest of the chapter. We let you infer when locks are granted.<br>Suppose now that unlocking is delayed to the end of the transaction. Transac-<br>tion T3 corresponds to T1 with unlocking delayed (Figure 16.5). Transaction T4 corre-<br>sponds to T2 with unlocking delayed (Figure 16.6).<br>You should verify that the sequence of reads and writes in schedule 1, which lead<br>to an incorrect total of $250 being displayed, is no longer possible with T3 and T4.<br>T4: lock-S(A);<br>read(A);<br>lock-S(B);<br>read(B);<br>display(A + B);<br>unlock(A);<br>unlock(B).<br>Figure 16.6<br>Transaction T4.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>V. Transaction <br>Management<br>16. Concurrency Control<br>594<br>© The McGraw−Hill <br>Companies, 2001<br>16.1<br>Lock-Based Protocols<br>595<br>T3<br>T4<br>lock-X(B)<br>read(B)<br>B := B<br>50<br>write(B)<br>lock-S(A)<br>read(A)<br>lock-S(B)<br>lock-X(A)<br>–<br>Figure 16.7<br>Schedule 2.<br>Other schedules are possible. T4 will not print out an inconsistent result in any of<br>them; we shall see why later.<br>Unfortunately, locking can lead to an undesirable situation. Consider the partial<br>schedule of Figure 16.7 for T3 and T4. Since T3 is holding an exclusive-mode lock<br>on B and T4 is requesting a shared-mode lock on B, T4 is waiting for T3 to unlock<br>B. Similarly, since T4 is holding a shared-mode lock on A and T3 is requesting an<br>exclusive-mode lock on A, T3 is waiting for T4 to unlock A. Thus, we have arrived at<br>a state where neither of these transactions can ever proceed with its normal execution.<br>This situation is called deadlock. When deadlock occurs, the system must roll back<br>one of the two transactions. Once a transaction has been rolled back, the data items<br>that were locked by that transaction are unlocked. These data items are then available<br>to the other transaction, which can continue with its execution. We shall return to the<br>issue of deadlock handling in Section 16.6.<br>If we do not use locking, or if we unlock data items as soon as possible after read-<br>ing or writing them, we may get inconsistent states. On the other hand, if we do not<br>unlock a data item before requesting a lock on another data item, deadlocks may<br>occur. There are ways to avoid deadlock in some situations, as we shall see in Sec-<br>tion 16.1.5. However, in general, deadlocks are a necessary evil associated with lock-<br>ing, if we want to avoid inconsistent states. Deadlocks are deﬁnitely preferable to in-<br>consistent states, since they can be handled by rolling back of transactions, whereas<br>inconsistent states may lead to real-world problems that cannot be handled by the<br>database system.<br>We shall require that each transaction in the system follow a set of rules, called a<br>locking protocol, indicating when a transaction may lock and unlock each of the data<br>items. Locking protocols restrict the number of possible schedules. The set of all such<br>schedules is a proper subset of all possible serializable schedules. We shall present<br>several locking protocols that allow only conﬂict-serializable schedules. Before doing<br>so, we need a few deﬁnitions.<br>Let {T0, T1, . . ., Tn} be a set of transactions participating in a schedule S. We say<br>that Ti precedes Tj in S, written Ti →Tj, if there exists a data item Q such that Ti<br>has held lock mode A on Q, and Tj has held lock mode B on Q later, and comp(A,B)<br>= false. If Ti →Tj, then that precedence implies that in any equivalent serial sched-<br>ule, Ti must appear before Tj. Observe that this graph is similar to the precedence<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>V. Transaction <br>Management<br>16. Concurrency Control<br>595<br>© The McGraw−Hill <br>Companies, 2001<br>596<br>Chapter 16<br>Concurrency Control<br>graph that we used in Section 15.9 to test for conﬂict serializability. Conﬂicts between<br>instructions correspond to noncompatibility of lock modes.<br>We say that a schedule S is legal under a given locking protocol if S is a possible<br>schedule for a set of transactions that follow the rules of the locking protocol. We say<br>that a locking protocol ensures conﬂict serializability if and only if all legal sched-<br>ules are conﬂict serializable; in other words, for all legal schedules the associated →<br>relation is acyclic.<br>16.1.2<br>Granting of Locks<br>When a transaction requests a lock on a data item in a particular mode, and no other<br>transaction has a lock on the same data item in a conﬂicting mode, the lock can be<br>granted. However, care must be taken to avoid the following scenario. Suppose a<br>transaction T2 has a shared-mode lock on a data item, and another transaction T1<br>requests an exclusive-mode lock on the data item. Clearly, T1 has to wait for T2 to re-<br>lease the shared-mode lock. Meanwhile, a transaction T3 may request a shared-mode<br>lock on the same data item. The lock request is compatible with the lock granted to<br>T2, so T3 may be granted the shared-mode lock. At this point T2 may release the lock,<br>but still T1 has to wait for T3 to ﬁnish. But again, there may be a new transaction<br>T4 that requests a shared-mode lock on the same data item, and is granted the lock<br>before T3 releases it. In fact, it is possible that there is a sequence of transactions that<br>each requests a shared-mode lock on the data item, and each transaction releases the<br>lock a short while after it is granted, but T1 never gets the exclusive-mode lock on the<br>data item. The transaction T1 may never make progress, and is said to be starved.<br>We can avoid starvation of transactions by granting locks in the following manner:<br>When a transaction Ti requests a lock on a data item Q in a particular mode M, the<br>concurrency-control manager grants the lock provided that<br>1. There is no other other transaction holding a lock on Q in a mode that conﬂicts<br>with M.<br>2. There is no other transaction that is waiting for a lock on Q, and that made its<br>lock request before Ti.<br>Thus, a lock request will never get blocked by a lock request that is made later.<br>16.1.3<br>The Two-Phase Locking Protocol<br>One protocol that ensures serializability is the two-phase locking protocol. This pro-<br>tocol requires that each transaction issue lock and unlock requests in two phases:<br>1. Growing phase. A transaction may obtain locks, but may not release any lock.<br>2. Shrinking phase. A transaction may release locks, but may not obtain any<br>new locks.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>V. Transaction <br>Management<br>16. Concurrency Control<br>596<br>© The McGraw−Hill <br>Companies, 2001<br>16.1<br>Lock-Based Protocols<br>597<br>Initially, a transaction is in the growing phase. The transaction acquires locks as<br>needed. Once the transaction releases a lock, it enters the shrinking phase, and it<br>can issue no more lock requests.<br>For example, transactions T3 and T4 are two phase. On the other hand, transactions<br>T1 and T2 are not two phase. Note that the unlock instructions do not need to appear<br>at the end of the transaction. For example, in the case of transaction T3, we could<br>move the unlock(B) instruction to just after the lock-X(A) instruction, and still retain<br>the two-phase locking property.<br>We can show that the two-phase locking protocol ensures conﬂict serializability.<br>Consider any transaction. The point in the schedule where the transaction has ob-<br>tained its ﬁnal lock (the end of its growing phase) is called the lock point of the<br>transaction. Now, transactions can be ordered according to their lock points—this or-<br>dering is, in fact, a serializability ordering for the transactions. We leave the proof as<br>an exercise for you to do (see Exercise 16.1).<br>Two-phase locking does not ensure freedom from deadlock. Observe that transac-<br>tions T3 and T4 are two phase, but, in schedule 2 (Figure 16.7), they are deadlocked.<br>Recall from Section 15.6.2 that, in addition to being serializable, schedules should<br>be cascadeless. Cascading rollback may occur under two-phase locking. As an illus-<br>tration, consider the partial schedule of Figure 16.8. Each transaction observes the<br>two-phase locki</span><br><br><span style="background-color: #CAFFBF;" title="Chunk 75 | Start: 1500150 | End: 1520150 | Tokens: 3182">ng protocol, but the failure of T5 after the read(A) step of T7 leads to<br>cascading rollback of T6 and T7.<br>Cascading rollbacks can be avoided by a modiﬁcation of two-phase locking called<br>the strict two-phase locking protocol. This protocol requires not only that locking<br>be two phase, but also that all exclusive-mode locks taken by a transaction be held<br>until that transaction commits. This requirement ensures that any data written by an<br>uncommitted transaction are locked in exclusive mode until the transaction commits,<br>preventing any other transaction from reading the data.<br>Another variant of two-phase locking is the rigorous two-phase locking proto-<br>col, which requires that all locks be held until the transaction commits. We can easily<br>read<br>T5<br>T6<br>T7<br>lock-X(A)<br>read(A)<br>lock-S(B)<br>read(B)<br>write(A)<br>unlock(A)<br>lock-X(A)<br>read(A)<br>write(A)<br>unlock(A)<br>lock-S(A)<br>read(A)<br>Figure 16.8<br>Partial schedule under two-phase locking.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>V. Transaction <br>Management<br>16. Concurrency Control<br>597<br>© The McGraw−Hill <br>Companies, 2001<br>598<br>Chapter 16<br>Concurrency Control<br>verify that, with rigorous two-phase locking, transactions can be serialized in the or-<br>der in which they commit. Most database systems implement either strict or rigorous<br>two-phase locking.<br>Consider the following two transactions, for which we have shown only some of<br>the signiﬁcant read and write operations:<br>T8: read(a1);<br>read(a2);<br>. . .<br>read(an);<br>write(a1).<br>T9: read(a1);<br>read(a2);<br>display(a1 + a2).<br>If we employ the two-phase locking protocol, then T8 must lock a1 in exclusive<br>mode. Therefore, any concurrent execution of both transactions amounts to a serial<br>execution. Notice, however, that T8 needs an exclusive lock on a1 only at the end of<br>its execution, when it writes a1. Thus, if T8 could initially lock a1 in shared mode, and<br>then could later change the lock to exclusive mode, we could get more concurrency,<br>since T8 and T9 could access a1 and a2 simultaneously.<br>This observation leads us to a reﬁnement of the basic two-phase locking protocol,<br>in which lock conversions are allowed. We shall provide a mechanism for upgrading<br>a shared lock to an exclusive lock, and downgrading an exclusive lock to a shared<br>lock. We denote conversion from shared to exclusive modes by upgrade, and from<br>exclusive to shared by downgrade. Lock conversion cannot be allowed arbitrarily.<br>Rather, upgrading can take place in only the growing phase, whereas downgrading<br>can take place in only the shrinking phase.<br>Returning to our example, transactions T8 and T9 can run concurrently under<br>the reﬁned two-phase locking protocol, as shown in the incomplete schedule of Fig-<br>ure 16.9, where only some of the locking instructions are shown.<br>T8<br>T9<br>lock-S(a<br>a<br>a<br>a<br>a<br>a<br>a<br>a<br>a<br>a<br>1)<br>lock-S( 1)<br>lock-S( 2)<br>lock-S( 2)<br>lock-S( 3)<br>lock-S( 4)<br>unlock( 1)<br>unlock( 2)<br>lock-S( n )<br>upgrade( 1)<br>Figure 16.9<br>Incomplete schedule with a lock conversion.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>V. Transaction <br>Management<br>16. Concurrency Control<br>598<br>© The McGraw−Hill <br>Companies, 2001<br>16.1<br>Lock-Based Protocols<br>599<br>Note that a transaction attempting to upgrade a lock on an item Q may be forced<br>to wait. This enforced wait occurs if Q is currently locked by another transaction in<br>shared mode.<br>Just like the basic two-phase locking protocol, two-phase locking with lock conver-<br>sion generates only conﬂict-serializable schedules, and transactions can be serialized<br>by their lock points. Further, if exclusive locks are held until the end of the transac-<br>tion, the schedules are cascadeless.<br>For a set of transactions, there may be conﬂict-serializable schedules that cannot<br>be obtained through the two-phase locking protocol. However, to obtain conﬂict-<br>serializable schedules through non-two-phase locking protocols, we need either to<br>have additional information about the transactions or to impose some structure or<br>ordering on the set of data items in the database. In the absence of such information,<br>two-phase locking is necessary for conﬂict serializability—if Ti is a non-two-phase<br>transaction, it is always possible to ﬁnd another transaction Tj that is two-phase so<br>that there is a schedule possible for Ti and Tj that is not conﬂict serializable.<br>Strict two-phase locking and rigorous two-phase locking (with lock conversions)<br>are used extensively in commercial database systems.<br>A simple but widely used scheme automatically generates the appropriate lock<br>and unlock instructions for a transaction, on the basis of read and write requests<br>from the transaction:<br>• When a transaction Ti issues a read(Q) operation, the system issues a lock-<br>S(Q) instruction followed by the read(Q) instruction.<br>• When Ti issues a write(Q) operation, the system checks to see whether Ti<br>already holds a shared lock on Q. If it does, then the system issues an up-<br>grade(Q) instruction, followed by the write(Q) instruction. Otherwise, the sys-<br>tem issues a lock-X(Q) instruction, followed by the write(Q) instruction.<br>• All locks obtained by a transaction are unlocked after that transaction commits<br>or aborts.<br>16.1.4<br>Implementation of Locking∗∗<br>A lock manager can be implemented as a process that receives messages from trans-<br>actions and sends messages in reply. The lock-manager process replies to lock-request<br>messages with lock-grant messages, or with messages requesting rollback of the trans-<br>action (in case of deadlocks). Unlock messages require only an acknowledgment in<br>response, but may result in a grant message to another waiting transaction.<br>The lock manager uses this data structure: For each data item that is currently<br>locked, it maintains a linked list of records, one for each request, in the order in which<br>the requests arrived. It uses a hash table, indexed on the name of a data item, to<br>ﬁnd the linked list (if any) for a data item; this table is called the lock table. Each<br>record of the linked list for a data item notes which transaction made the request,<br>and what lock mode it requested. The record also notes if the request has currently<br>been granted.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>V. Transaction <br>Management<br>16. Concurrency Control<br>599<br>© The McGraw−Hill <br>Companies, 2001<br>600<br>Chapter 16<br>Concurrency Control<br>T8<br>T23<br>T1<br>T2<br>I7<br>I23<br>I912<br>T23<br>I4<br>T1<br>T23<br>T8<br>I44<br>Figure 16.10<br>Lock table.<br>Figure 16.10 shows an example of a lock table. The table contains locks for ﬁve<br>different data items, I4, I7, I23, I44, and I912. The lock table uses overﬂow chaining,<br>so there is a linked list of data items for each entry in the lock table. There is also a list<br>of transactions that have been granted locks, or are waiting for locks, for each of the<br>data items. Granted locks are the ﬁlled-in (black) rectangles, while waiting requests<br>are the empty rectangles. We have omitted the lock mode to keep the ﬁgure simple.<br>It can be seen, for example, that T23 has been granted locks on I912 and I7, and is<br>waiting for a lock on I4.<br>Although the ﬁgure does not show it, the lock table should also maintain an index<br>on transaction identiﬁers, so that it is possible to determine efﬁciently the set of locks<br>held by a given transaction.<br>The lock manager processes requests this way:<br>• When a lock request message arrives, it adds a record to the end of the linked<br>list for the data item, if the linked list is present. Otherwise it creates a new<br>linked list, containing only the record for the request.<br>It always grants the ﬁrst lock request on a data item. But if the transaction<br>requests a lock on an item on which a lock has already been granted, the lock<br>manager grants the request only if it is compatible with all earlier requests,<br>and all earlier requests have been granted already. Otherwise the request has<br>to wait.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>V. Transaction <br>Management<br>16. Concurrency Control<br>600<br>© The McGraw−Hill <br>Companies, 2001<br>16.1<br>Lock-Based Protocols<br>601<br>• When the lock manager receives an unlock message from a transaction, it<br>deletes the record for that data item in the linked list corresponding to that<br>transaction. It tests the record that follows, if any, as described in the previous<br>paragraph, to see if that request can now be granted. If it can, the lock man-<br>ager grants that request, and processes the record following it, if any, similarly,<br>and so on.<br>• If a transaction aborts, the lock manager deletes any waiting request made<br>by the transaction. Once the database system has taken appropriate actions to<br>undo the transaction (see Section 17.3), it releases all locks held by the aborted<br>transaction.<br>This algorithm guarantees freedom from starvation for lock requests, since a re-<br>quest can never be granted while a request received earlier is waiting to be granted.<br>We study how to detect and handle deadlocks later, in Section 16.6.3. Section 18.2.1<br>describes an alternative implementation—one that uses shared memory instead of<br>message passing for lock request/grant.<br>16.1.5<br>Graph-Based Protocols<br>As noted in Section 16.1.3, the two-phase locking protocol is both necessary and suf-<br>ﬁcient for ensuring serializability in the absence of information concerning the man-<br>ner in which data items are accessed. But, if we wish to develop protocols that are<br>not two phase, we need additional information on how each transaction will access<br>the database. There are various models that can give us the additional information,<br>each differing in the amount of information provided. The simplest model requires<br>that we have prior knowledge about the order in which the database items will be<br>accessed. Given such information, it is possible to construct locking protocols that are<br>not two phase, but that, nevertheless, ensure conﬂict serializability.<br>To acquire such prior knowledge, we impose a partial ordering →on the set<br>D = {d1, d2, . . ., dh} of all data items. If di →dj, then any transaction accessing both<br>di and dj must access di before accessing dj. This partial ordering may be the result<br>of either the logical or the physical organization of the data, or it may be imposed<br>solely for the purpose of concurrency control.<br>The partial ordering implies that the set D may now be viewed as a directed acyclic<br>graph, called a database graph. In this section, for the sake of simplicity, we will<br>restrict our attention to only those graphs that are rooted trees. We will present a<br>simple protocol, called the tree protocol, which is restricted to employ only exclusive<br>locks. References to other, more complex, graph-based locking protocols are in the<br>bibliographical notes.<br>In the tree protocol, the only lock instruction allowed is lock-X. Each transaction<br>Ti can lock a data item at most once, and must observe the following rules:<br>1. The ﬁrst lock by Ti may be on any data item.<br>2. Subsequently, a data item Q can be locked by Ti only if the parent of Q is<br>currently locked by Ti.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>V. Transaction <br>Management<br>16. Concurrency Control<br>601<br>© The McGraw−Hill <br>Companies, 2001<br>602<br>Chapter 16<br>Concurrency Control<br>3. Data items may be unlocked at any time.<br>4. A data item that has been locked and unlocked by Ti cannot subsequently be<br>relocked by Ti.<br>All schedules that are legal under the tree protocol are conﬂict serializable.<br>To illustrate this protocol, consider the database graph of Figure 16.11. The follow-<br>ing four transactions follow the tree protocol on this graph. We show only the lock<br>and unlock instructions:<br>T10: lock-X(B); lock-X(E); lock-X(D); unlock(B); unlock(E); lock-X(G);<br>unlock(D); unlock(G).<br>T11: lock-X(D); lock-X(H); unlock(D); unlock(H).<br>T12: lock-X(B); lock-X(E); unlock(E); unlock(B).<br>T13: lock-X(D); lock-X(H); unlock(D); unlock(H).<br>One possible schedule in which these four transactions participated appears in<br>Figure 16.12. Note that, during its execution, transaction T10 holds locks on two dis-<br>joint subtrees.<br>Observe that the schedule of Figure 16.12 is conﬂict serializable. It can be shown<br>not only that the tree protocol ensures conﬂict serializability, but also that this proto-<br>col ensures freedom from deadlock.<br>The tree protocol in Figure 16.12 does not ensure recoverability and cascadeless-<br>ness. To ensure recoverability and cascadelessness, the protocol can be modiﬁed to<br>not permit release of exclusive locks until the end of the transaction. Holding exclu-<br>sive locks until the end of the transaction reduces concurrency. Here is an alterna-<br>tive that improves concurrency, but ensures only recoverability: For each data item<br>with an uncommitted write we record which transaction performed the last write to<br>the data item. Whenever a transaction Ti performs a read of an uncommitted data<br>item, we record a commit dependency of Ti on the transaction that performed the<br>E<br>A<br>I<br>B<br>C<br>D<br>J<br>H<br>F<br>G<br>Figure 16.11<br>Tree-structured database graph.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>V. Transaction <br>Management<br>16. Concurrency Control<br>602<br>© The McGraw−Hill <br>Companies, 2001<br>16.1<br>Lock-Based Protocols<br>603<br>T10<br>T11<br>T12<br>T13<br>lock-X(B)<br>lock-X(D)<br>lock-X(H)<br>unlock(D)<br>lock-X(E)<br>lock-X(D)<br>unlock(B)<br>unlock(E)<br>lock-X(B)<br>lock-X(E)<br>unlock<br>unlock<br>unlock<br>(H)<br>lock-X(G)<br>unlock<br>unlock<br>(D)<br>lock-X(D)<br>lock-X(H)<br>unlock(D)<br>unlock(H)<br>(E)<br>(B)<br>(G)<br>Figure 16.12<br>Serializable schedule under the tree protocol.<br>last write to the data item. Transaction Ti is then not permitted to commit until the<br>commit of all transactions on which it has a commit dependency. If any of these trans-<br>actions aborts, Ti must also be aborted.<br>The tree-locking protocol has an advantage over the two-phase locking protocol in<br>that, unlike two-phase locking, it is deadlock-free, so no rollbacks are required. The<br>tree-locking protocol has another advantage over the two-phase locking protocol in<br>that unlocking may occur earlier. Earlier unlocking may lead to shorter waiting times,<br>and to an increase in concurrency.<br>However, the protocol has the disadvantage that, in some cases, a transaction may<br>have to lock data items that it does not access. For example, a transaction that needs<br>to access data items A and J in the database graph of Figure 16.11 must lock not only<br>A and J, but also data items B, D, and H. This additional locking results in increased<br>locking overhead, the possibility of additional waiting time, and a potential decrease<br>in concurrency. Further, without prior knowledge of what data items will need to<br>be locked, transactions will have to lock the root of the tree, and that can reduce<br>concurrency greatly.<br>For a set of transactions, there may be conﬂict-serializable schedules that cannot<br>be obtained through the tree protocol. Indeed, there are schedules possible under the<br>two-phase locking protocol that are not possible under the tree protocol, and vice<br>versa. Examples of such schedules are explored in the exercises.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>V. Transaction <br>Management<br>16. Concurrency Control<br>603<br>© The McGraw−Hill <br>Companies, 2001<br>604<br>Chapter 16<br>Concurrency Control<br>16.2<br>Timestamp-Based Protocols<br>The locking protocols that we have described thus far determine the order between<br>every pair of conﬂicting transactions at execution time by the ﬁrst lock that both<br>members of the pair request that involves incompatible modes. Another method<br>for determining the serializability order is to select an ordering among transactions<br>in advance. The most common method for doing so is to use a timestamp-ordering<br>scheme.<br>16.2.1<br>Timestamps<br>With each transaction Ti in the system, we associate a unique ﬁxed timestamp, de-<br>noted by TS(Ti). This timestamp is assigned by the database system before the trans-<br>action Ti starts execution. If a transaction Ti has been assigned timestamp TS(Ti), and<br>a new transaction Tj enters the system, then TS(Ti) &lt; TS(Tj). There are two simple<br>methods for implementing this scheme:<br>1. Use the value of the system clock as the timestamp; that is, a transaction’s time-<br>stamp is equal to the value of the clock when the transaction enters the system.<br>2. Use a logical counter that is incremented after a new timestamp has been<br>assigned; that is, a transaction’s timestamp is equal to the value of the counter<br>when the transaction enters the system.<br>The timestamps of the transactions determine the serializability order. Thus, if<br>TS(Ti) &lt; TS(Tj), then the system must ensure that the produced schedule is equiva-<br>lent to a serial schedule in which transaction Ti appears before transaction Tj.<br>To implement this scheme, we associate with each data item Q two timestamp<br>values:<br>• W-timestamp(Q) denotes the largest timestamp of any transaction that exe-<br>cuted write(Q) successfully.<br>• R-timestamp(Q) denotes the largest timestamp of any transaction that exe-<br>cuted read(Q) successfully.<br>These timestamps are updated whenever a new read(Q) or write(Q) instruction is<br>executed.<br>16.2.2<br>The Timestamp-Ordering Protocol<br>The timestamp-ordering protocol ensures that any conﬂicting read and write opera-<br>tions are executed in timestamp order. This protocol operates as follows:<br>1. Suppose that transaction Ti issues read(Q).<br>a. If TS(Ti) &lt; W-timestamp(Q), then Ti needs to read a value of Q that was<br>already overwritten. Hence, the read operation is rejected, and Ti is rolled<br>back.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>V. Transaction <br>Management<br>16. Concurrency Control<br>604<br>© The McGraw−Hill <br>Companies, 2001<br>16.2<br>Timestamp-Based Protocols<br>605<br>b. If TS(Ti) ≥W-timestamp(Q), then the read operation is executed, and R-<br>timestamp(Q) is set to the maximum of R-timestamp(Q) and TS(Ti).<br>2. Suppose that transaction Ti issues write(Q).<br>a. If TS(Ti) &lt; R-timestamp(Q), then the value of Q that Ti is producing was<br>needed previously, and the system assumed that that value would never<br>be produced. Hence, the system rejects the write operation and rolls Ti<br>back.<br>b. If TS(Ti) &lt; W-timestamp(Q), then Ti is attempting to write an obsolete<br>value of Q. Hence, the system rejects this write operation and rolls Ti back.<br>c. Otherwise, the system executes the write operation and sets W-time-<br>stamp(Q) to TS(Ti).<br>If a transaction Ti is rolled back by the concurrency-control scheme as result of is-<br>suance of either a read or write operation, the system assigns it a new timestamp and<br>restarts it.<br>To illustrate this protocol, we consider transactions T14 and T15. Transaction T14<br>displays the contents of accounts A and B:<br>T14: read(B);<br>read(A);<br>display(A + B).<br>Transaction T15 transfers $50 from account A to account B, and then displays the<br>contents of both:<br>T15: read(B);<br>B := B −50;<br>write(B);<br>read(A);<br>A := A + 50;<br>write(A);<br>display(A + B).<br>In presenting schedules under the timestamp protocol, we shall assume that a trans-<br>action is assigned a timestamp immediately before its ﬁrst instruction. Thus, in sched-<br>ule 3 of Figure 16.13, TS(T14) &lt; TS(T15), and the schedule is possible under the time-<br>stamp protocol.<br>We note that the preceding execution can also be produced by the two-phase lock-<br>ing protocol. There are, however, schedules that are possible under the two-phase<br>locking protocol, but are not possible under the timestamp protocol, and vice versa<br>(see Exercise 16.20).<br>The timestamp-ordering protocol ensures conﬂict serializability. This is because<br>conﬂicting operations are processed in timestamp order.<br>The protocol ensures freedom from deadlock, since no transaction ever waits.<br>However, there is a possibility of starvation of long transactions if a sequence of<br>conﬂicting short transactions causes repeated restarting of the long transaction. If<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>V. Transaction <br>Management<br>16. Concurrency Control<br>605<br>© The McGraw−Hill <br>Companies, 2001<br>606<br>Chapter 16<br>Concurrency Control<br>T14<br>T15<br>read(B)<br>read (B)<br>B := B  – 50<br>write(B)<br>read(A)<br>read(A)<br>display(A + B)<br>A := A + 50<br>write(A)<br>display(A + B)<br>Figure 16.13<br>Schedule 3.<br>a transaction is found to be gettin</span><br><br><span style="background-color: #9BF6FF;" title="Chunk 76 | Start: 1520152 | End: 1540152 | Tokens: 3281">g restarted repeatedly, conﬂicting transactions need<br>to be temporarily blocked to enable the transaction to ﬁnish.<br>The protocol can generate schedules that are not recoverable. However, it can be<br>extended to make the schedules recoverable, in one of several ways:<br>• Recoverability and cascadelessness can be ensured by performing all writes<br>together at the end of the transaction. The writes must be atomic in the fol-<br>lowing sense: While the writes are in progress, no transaction is permitted to<br>access any of the data items that have been written.<br>• Recoverability and cascadelessness can also be guaranteed by using a limited<br>form of locking, whereby reads of uncommitted items are postponed until the<br>transaction that updated the item commits (see Exercise 16.22).<br>• Recoverability alone can be ensured by tracking uncommitted writes, and al-<br>lowing a transaction Ti to commit only after the commit of any transaction that<br>wrote a value that Ti read. Commit dependencies, outlined in Section 16.1.5,<br>can be used for this purpose.<br>16.2.3<br>Thomas’ Write Rule<br>We now present a modiﬁcation to the timestamp-ordering protocol that allows greater<br>potential concurrency than does the protocol of Section 16.2.2. Let us consider sched-<br>ule 4 of Figure 16.14, and apply the timestamp-ordering protocol. Since T16 starts<br>before T17, we shall assume that TS(T16) &lt; TS(T17). The read(Q) operation of T16 suc-<br>ceeds, as does the write(Q) operation of T17. When T16 attempts its write(Q) operation,<br>we ﬁnd that TS(T16) &lt; W-timestamp(Q), since W-timestamp(Q) = TS(T17). Thus, the<br>write(Q) by T16 is rejected and transaction T16 must be rolled back.<br>Although the rollback of T16 is required by the timestamp-ordering protocol, it<br>is unnecessary. Since T17 has already written Q, the value that T16 is attempting to<br>write is one that will never need to be read. Any transaction Ti with TS(Ti) &lt; TS(T17)<br>that attempts a read(Q) will be rolled back, since TS(Ti) &lt; W-timestamp(Q). Any<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>V. Transaction <br>Management<br>16. Concurrency Control<br>606<br>© The McGraw−Hill <br>Companies, 2001<br>16.3<br>Validation-Based Protocols<br>607<br>T16<br>T17<br>read(Q)<br>write(Q)<br>write(Q)<br>Figure 16.14<br>Schedule 4.<br>transaction Tj with TS(Tj) &gt; TS(T17) must read the value of Q written by T17, rather<br>than the value written by T16.<br>This observation leads to a modiﬁed version of the timestamp-ordering protocol<br>in which obsolete write operations can be ignored under certain circumstances. The<br>protocol rules for read operations remain unchanged. The protocol rules for write<br>operations, however, are slightly different from the timestamp-ordering protocol of<br>Section 16.2.2.<br>The modiﬁcation to the timestamp-ordering protocol, called Thomas’ write rule,<br>is this: Suppose that transaction Ti issues write(Q).<br>1. If TS(Ti) &lt; R-timestamp(Q), then the value of Q that Ti is producing was pre-<br>viously needed, and it had been assumed that the value would never be pro-<br>duced. Hence, the system rejects the write operation and rolls Ti back.<br>2. If TS(Ti) &lt; W-timestamp(Q), then Ti is attempting to write an obsolete value<br>of Q. Hence, this write operation can be ignored.<br>3. Otherwise, the system executes the write operation and sets W-timestamp(Q)<br>to TS(Ti).<br>The difference between these rules and those of Section 16.2.2 lies in the second<br>rule. The timestamp-ordering protocol requires that Ti be rolled back if Ti issues<br>write(Q) and TS(Ti) &lt; W-timestamp(Q). However, here, in those cases where TS(Ti)<br>≥R-timestamp(Q), we ignore the obsolete write.<br>Thomas’ write rule makes use of view serializability by, in effect, deleting obsolete<br>write operations from the transactions that issue them. This modiﬁcation of transac-<br>tions makes it possible to generate serializable schedules that would not be possible<br>under the other protocols presented in this chapter. For example, schedule 4 of Fig-<br>ure 16.14 is not conﬂict serializable and, thus, is not possible under any of two-phase<br>locking, the tree protocol, or the timestamp-ordering protocol. Under Thomas’ write<br>rule, the write(Q) operation of T16 would be ignored. The result is a schedule that is<br>view equivalent to the serial schedule &lt;T16, T17&gt;.<br>16.3<br>Validation-Based Protocols<br>In cases where a majority of transactions are read-only transactions, the rate of con-<br>ﬂicts among transactions may be low. Thus, many of these transactions, if executed<br>without the supervision of a concurrency-control scheme, would nevertheless leave<br>the system in a consistent state. A concurrency-control scheme imposes overhead of<br>code execution and possible delay of transactions. It may be better to use an alterna-<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>V. Transaction <br>Management<br>16. Concurrency Control<br>607<br>© The McGraw−Hill <br>Companies, 2001<br>608<br>Chapter 16<br>Concurrency Control<br>tive scheme that imposes less overhead. A difﬁculty in reducing the overhead is that<br>we do not know in advance which transactions will be involved in a conﬂict. To gain<br>that knowledge, we need a scheme for monitoring the system.<br>We assume that each transaction Ti executes in two or three different phases in its<br>lifetime, depending on whether it is a read-only or an update transaction. The phases<br>are, in order,<br>1. Read phase. During this phase, the system executes transaction Ti. It reads<br>the values of the various data items and stores them in variables local to Ti.<br>It performs all write operations on temporary local variables, without updates<br>of the actual database.<br>2. Validation phase. Transaction Ti performs a validation test to determine whe-<br>ther it can copy to the database the temporary local variables that hold the<br>results of write operations without causing a violation of serializability.<br>3. Write phase. If transaction Ti succeeds in validation (step 2), then the system<br>applies the actual updates to the database. Otherwise, the system rolls back<br>Ti.<br>Each transaction must go through the three phases in the order shown. However, all<br>three phases of concurrently executing transactions can be interleaved.<br>To perform the validation test, we need to know when the various phases of trans-<br>actions Ti took place. We shall, therefore, associate three different timestamps with<br>transaction Ti:<br>1. Start(Ti), the time when Ti started its execution.<br>2. Validation(Ti), the time when Ti ﬁnished its read phase and started its vali-<br>dation phase.<br>3. Finish(Ti), the time when Ti ﬁnished its write phase.<br>We determine the serializability order by the timestamp-ordering technique, using<br>the value of the timestamp Validation(Ti). Thus, the value TS(Ti) = Validation(Ti)<br>and, if TS(Tj) &lt; TS(Tk), then any produced schedule must be equivalent to a serial<br>schedule in which transaction Tj appears before transaction Tk. The reason we have<br>chosen Validation(Ti), rather than Start(Ti), as the timestamp of transaction Ti is that<br>we can expect faster response time provided that conﬂict rates among transactions<br>are indeed low.<br>The validation test for transaction Tj requires that, for all transactions Ti with<br>TS(Ti) &lt; TS(Tj), one of the following two conditions must hold:<br>1. Finish(Ti) &lt; Start(Tj). Since Ti completes its execution before Tj started, the<br>serializability order is indeed maintained.<br>2. The set of data items written by Ti does not intersect with the set of data items<br>read by Tj, and Ti completes its write phase before Tj starts its validation<br>phase (Start(Tj) &lt; Finish(Ti) &lt; Validation(Tj)). This condition ensures that<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>V. Transaction <br>Management<br>16. Concurrency Control<br>608<br>© The McGraw−Hill <br>Companies, 2001<br>16.4<br>Multiple Granularity<br>609<br>T<br>B<br>A<br>14<br>T15<br>read(B)<br>read(B)<br>B := B – 50<br>read(A)<br>A := A + 50<br>read(A)<br>display(A + B)<br>validate<br>write(<br>)<br>write( )<br>validate<br>Figure 16.15<br>Schedule 5, a schedule produced by using validation.<br>the writes of Ti and Tj do not overlap. Since the writes of Ti do not affect the<br>read of Tj, and since Tj cannot affect the read of Ti, the serializability order is<br>indeed maintained.<br>As an illustration, consider again transactions T14 and T15. Suppose that TS(T14)<br>&lt; TS(T15). Then, the validation phase succeeds in the schedule 5 in Figure 16.15. Note<br>that the writes to the actual variables are performed only after the validation phase<br>of T15. Thus, T14 reads the old values of B and A, and this schedule is serializable.<br>The validation scheme automatically guards against cascading rollbacks, since the<br>actual writes take place only after the transaction issuing the write has committed.<br>However, there is a possibility of starvation of long transactions, due to a sequence<br>of conﬂicting short transactions that cause repeated restarts of the long transaction.<br>To avoid starvation, conﬂicting transactions must be temporarily blocked, to enable<br>the long transaction to ﬁnish.<br>This validation scheme is called the optimistic concurrency control scheme since<br>transactions execute optimistically, assuming they will be able to ﬁnish execution<br>and validate at the end. In contrast, locking and timestamp ordering are pessimistic<br>in that they force a wait or a rollback whenever a conﬂict is detected, even though<br>there is a chance that the schedule may be conﬂict serializable.<br>16.4<br>Multiple Granularity<br>In the concurrency-control schemes described thus far, we have used each individual<br>data item as the unit on which synchronization is performed.<br>There are circumstances, however, where it would be advantageous to group sev-<br>eral data items, and to treat them as one individual synchronization unit. For exam-<br>ple, if a transaction Ti needs to access the entire database, and a locking protocol is<br>used, then Ti must lock each item in the database. Clearly, executing these locks is<br>time consuming. It would be better if Ti could issue a single lock request to lock the<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>V. Transaction <br>Management<br>16. Concurrency Control<br>609<br>© The McGraw−Hill <br>Companies, 2001<br>610<br>Chapter 16<br>Concurrency Control<br>DB<br>A1<br>ra1<br>A2<br>Fa<br>Fb<br>Fc<br>ra2<br>ran<br>rb1<br>rbk<br>rc1<br>rcm<br>…<br>…<br>…<br>Figure 16.16<br>Granularity hierarchy.<br>entire database. On the other hand, if transaction Tj needs to access only a few data<br>items, it should not be required to lock the entire database, since otherwise concur-<br>rency is lost.<br>What is needed is a mechanism to allow the system to deﬁne multiple levels of<br>granularity. We can make one by allowing data items to be of various sizes and deﬁn-<br>ing a hierarchy of data granularities, where the small granularities are nested within<br>larger ones. Such a hierarchy can be represented graphically as a tree. Note that the<br>tree that we describe here is signiﬁcantly different from that used by the tree protocol<br>(Section 16.1.5). A nonleaf node of the multiple-granularity tree represents the data<br>associated with its descendants. In the tree protocol, each node is an independent<br>data item.<br>As an illustration, consider the tree of Figure 16.16, which consists of four levels<br>of nodes. The highest level represents the entire database. Below it are nodes of type<br>area; the database consists of exactly these areas. Each area in turn has nodes of type<br>ﬁle as its children. Each area contains exactly those ﬁles that are its child nodes. No<br>ﬁle is in more than one area. Finally, each ﬁle has nodes of type record. As before, the<br>ﬁle consists of exactly those records that are its child nodes, and no record can be<br>present in more than one ﬁle.<br>Each node in the tree can be locked individually. As we did in the two-phase lock-<br>ing protocol, we shall use shared and exclusive lock modes. When a transaction locks<br>a node, in either shared or exclusive mode, the transaction also has implicitly locked<br>all the descendants of that node in the same lock mode. For example, if transaction<br>Ti gets an explicit lock on ﬁle Fc of Figure 16.16, in exclusive mode, then it has an<br>implicit lock in exclusive mode all the records belonging to that ﬁle. It does not need<br>to lock the individual records of Fc explicitly.<br>Suppose that transaction Tj wishes to lock record rb6 of ﬁle Fb. Since Ti has locked<br>Fb explicitly, it follows that rb6 is also locked (implicitly). But, when Tj issues a lock<br>request for rb6, rb6 is not explicitly locked! How does the system determine whether<br>Tj can lock rb6? Tj must traverse the tree from the root to record rb6. If any node in<br>that path is locked in an incompatible mode, then Tj must be delayed.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>V. Transaction <br>Management<br>16. Concurrency Control<br>610<br>© The McGraw−Hill <br>Companies, 2001<br>16.4<br>Multiple Granularity<br>611<br>IS<br>IX<br>S<br>SIX<br>X<br>IS<br>true<br>true<br>true<br>true<br>false<br>IX<br>true<br>true<br>false<br>false<br>false<br>S<br>true<br>false<br>true<br>false<br>false<br>SIX<br>true<br>false<br>false<br>false<br>false<br>X<br>false<br>false<br>false<br>false<br>false<br>Figure 16.17<br>Compatibility matrix.<br>Suppose now that transaction Tk wishes to lock the entire database. To do so, it<br>simply must lock the root of the hierarchy. Note, however, that Tk should not suc-<br>ceed in locking the root node, since Ti is currently holding a lock on part of the tree<br>(speciﬁcally, on ﬁle Fb). But how does the system determine if the root node can be<br>locked? One possibility is for it to search the entire tree. This solution, however, de-<br>feats the whole purpose of the multiple-granularity locking scheme. A more efﬁcient<br>way to gain this knowledge is to introduce a new class of lock modes, called inten-<br>tion lock modes. If a node is locked in an intention mode, explicit locking is being<br>done at a lower level of the tree (that is, at a ﬁner granularity). Intention locks are put<br>on all the ancestors of a node before that node is locked explicitly. Thus, a transaction<br>does not need to search the entire tree to determine whether it can lock a node suc-<br>cessfully. A transaction wishing to lock a node—say, Q—must traverse a path in the<br>tree from the root to Q. While traversing the tree, the transaction locks the various<br>nodes in an intention mode.<br>There is an intention mode associated with shared mode, and there is one with<br>exclusive mode. If a node is locked in intention-shared (IS) mode, explicit locking is<br>being done at a lower level of the tree, but with only shared-mode locks. Similarly,<br>if a node is locked in intention-exclusive (IX) mode, then explicit locking is being<br>done at a lower level, with exclusive-mode or shared-mode locks. Finally, if a node<br>is locked in shared and intention-exclusive (SIX) mode, the subtree rooted by that<br>node is locked explicitly in shared mode, and that explicit locking is being done at<br>a lower level with exclusive-mode locks. The compatibility function for these lock<br>modes is in Figure 16.17.<br>The multiple-granularity locking protocol, which ensures serializability, is this:<br>Each transaction Ti can lock a node Q by following these rules:<br>1. It must observe the lock-compatibility function of Figure 16.17.<br>2. It must lock the root of the tree ﬁrst, and can lock it in any mode.<br>3. It can lock a node Q in S or IS mode only if it currently has the parent of Q<br>locked in either IX or IS mode.<br>4. It can lock a node Q in X, SIX, or IX mode only if it currently has the parent of<br>Q locked in either IX or SIX mode.<br>5. It can lock a node only if it has not previously unlocked any node (that is, Ti<br>is two phase).<br>6. It can unlock a node Q only if it currently has none of the children of Q locked.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>V. Transaction <br>Management<br>16. Concurrency Control<br>611<br>© The McGraw−Hill <br>Companies, 2001<br>612<br>Chapter 16<br>Concurrency Control<br>Observe that the multiple-granularity protocol requires that locks be acquired in top-<br>down (root-to-leaf) order, whereas locks must be released in bottom-up (leaf-to-root)<br>order.<br>As an illustration of the protocol, consider the tree of Figure 16.16 and these trans-<br>actions:<br>• Suppose that transaction T18 reads record ra2 in ﬁle Fa. Then, T18 needs to<br>lock the database, area A1, and Fa in IS mode (and in that order), and ﬁnally<br>to lock ra2 in S mode.<br>• Suppose that transaction T19 modiﬁes record ra9 in ﬁle Fa. Then, T19 needs to<br>lock the database, area A1, and ﬁle Fa in IX mode, and ﬁnally to lock ra9 in X<br>mode.<br>• Suppose that transaction T20 reads all the records in ﬁle Fa. Then, T20 needs<br>to lock the database and area A1 (in that order) in IS mode, and ﬁnally to lock<br>Fa in S mode.<br>• Suppose that transaction T21 reads the entire database. It can do so after lock-<br>ing the database in S mode.<br>We note that transactions T18, T20, and T21 can access the database concurrently.<br>Transaction T19 can execute concurrently with T18, but not with either T20 or T21.<br>This protocol enhances concurrency and reduces lock overhead. It is particularly<br>useful in applications that include a mix of<br>• Short transactions that access only a few data items<br>• Long transactions that produce reports from an entire ﬁle or set of ﬁles<br>There is a similar locking protocol that is applicable to database systems in which<br>data granularities are organized in the form of a directed acyclic graph. See the bib-<br>liographical notes for additional references. Deadlock is possible in the protocol that<br>we have, as it is in the two-phase locking protocol. There are techniques to reduce<br>deadlock frequency in the multiple-granularity protocol, and also to eliminate dead-<br>lock entirely. These techniques are referenced in the bibliographical notes.<br>16.5<br>Multiversion Schemes<br>The concurrency-control schemes discussed thus far ensure serializability by either<br>delaying an operation or aborting the transaction that issued the operation. For ex-<br>ample, a read operation may be delayed because the appropriate value has not been<br>written yet; or it may be rejected (that is, the issuing transaction must be aborted)<br>because the value that it was supposed to read has already been overwritten. These<br>difﬁculties could be avoided if old copies of each data item were kept in a system.<br>In multiversion concurrency control schemes, each write(Q) operation creates a<br>new version of Q. When a transaction issues a read(Q) operation, the concurrency-<br>control manager selects one of the versions of Q to be read. The concurrency-control<br>scheme must ensure that the version to be read is selected in a manner that ensures<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>V. Transaction <br>Management<br>16. Concurrency Control<br>612<br>© The McGraw−Hill <br>Companies, 2001<br>16.5<br>Multiversion Schemes<br>613<br>serializability. It is also crucial, for performance reasons, that a transaction be able to<br>determine easily and quickly which version of the data item should be read.<br>16.5.1<br>Multiversion Timestamp Ordering<br>The most common transaction ordering technique used by multiversion schemes is<br>timestamping. With each transaction Ti in the system, we associate a unique static<br>timestamp, denoted by TS(Ti). The database system assigns this timestamp before<br>the transaction starts execution, as described in Section 16.2.<br>With each data item Q, a sequence of versions &lt;Q1, Q2, . . ., Qm&gt; is associated.<br>Each version Qk contains three data ﬁelds:<br>• Content is the value of version Qk.<br>• W-timestamp(Qk) is the timestamp of the transaction that created version Qk.<br>• R-timestamp(Qk) is the largest timestamp of any transaction that successfully<br>read version Qk.<br>A transaction—say, Ti—creates a new version Qk of data item Q by issuing a<br>write(Q) operation. The content ﬁeld of the version holds the value written by Ti.<br>The system initializes the W-timestamp and R-timestamp to TS(Ti). It updates the<br>R-timestamp value of Qk whenever a transaction Tj reads the content of Qk, and<br>R-timestamp(Qk) &lt; TS(Tj).<br>The multiversion timestamp-ordering scheme presented next ensures serializ-<br>ability. The scheme operates as follows. Suppose that transaction Ti issues a read(Q)<br>or write(Q) operation. Let Qk denote the version of Q whose write timestamp is the<br>largest write timestamp less than or equal to TS(Ti).<br>1. If tran</span><br><br><span style="background-color: #A0C4FF;" title="Chunk 77 | Start: 1540154 | End: 1560154 | Tokens: 3311">saction Ti issues a read(Q), then the value returned is the content of<br>version Qk.<br>2. If transaction Ti issues write(Q), and if TS(Ti) &lt; R-timestamp(Qk), then the sys-<br>tem rolls back transaction Ti. On the other hand, if TS(Ti) = W-timestamp(Qk),<br>the system overwrites the contents of Qk; otherwise it creates a new version<br>of Q.<br>The justiﬁcation for rule 1 is clear. A transaction reads the most recent version that<br>comes before it in time. The second rule forces a transaction to abort if it is “too late”<br>in doing a write. More precisely, if Ti attempts to write a version that some other<br>transaction would have read, then we cannot allow that write to succeed.<br>Versions that are no longer needed are removed according to the following rule.<br>Suppose that there are two versions, Qk and Qj, of a data item, and that both versions<br>have a W-timestamp less than the timestamp of the oldest transaction in the system.<br>Then, the older of the two versions Qk and Qj will not be used again, and can be<br>deleted.<br>The multiversion timestamp-ordering scheme has the desirable property that a<br>read request never fails and is never made to wait. In typical database systems, where<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>V. Transaction <br>Management<br>16. Concurrency Control<br>613<br>© The McGraw−Hill <br>Companies, 2001<br>614<br>Chapter 16<br>Concurrency Control<br>reading is a more frequent operation than is writing, this advantage may be of major<br>practical signiﬁcance.<br>The scheme, however, suffers from two undesirable properties. First, the reading<br>of a data item also requires the updating of the R-timestamp ﬁeld, resulting in two<br>potential disk accesses, rather than one. Second, the conﬂicts between transactions<br>are resolved through rollbacks, rather than through waits. This alternative may be<br>expensive. Section 16.5.2 describes an algorithm to alleviate this problem.<br>This multiversion timestamp-ordering scheme does not ensure recoverability and<br>cascadelessness. It can be extended in the same manner as the basic timestamp-<br>ordering scheme, to make it recoverable and cascadeless.<br>16.5.2<br>Multiversion Two-Phase Locking<br>The multiversion two-phase locking protocol attempts to combine the advantages<br>of multiversion concurrency control with the advantages of two-phase locking. This<br>protocol differentiates between read-only transactions and update transactions.<br>Update transactions perform rigorous two-phase locking; that is, they hold all<br>locks up to the end of the transaction. Thus, they can be serialized according to their<br>commit order. Each version of a data item has a single timestamp. The timestamp in<br>this case is not a real clock-based timestamp, but rather is a counter, which we will<br>call the ts-counter, that is incremented during commit processing.<br>Read-only transactions are assigned a timestamp by reading the current value<br>of ts-counter before they start execution; they follow the multiversion timestamp-<br>ordering protocol for performing reads. Thus, when a read-only transaction Ti issues<br>a read(Q), the value returned is the contents of the version whose timestamp is the<br>largest timestamp less than TS(Ti).<br>When an update transaction reads an item, it gets a shared lock on the item, and<br>reads the latest version of that item. When an update transaction wants to write an<br>item, it ﬁrst gets an exclusive lock on the item, and then creates a new version of<br>the data item. The write is performed on the new version, and the timestamp of the<br>new version is initially set to a value ∞, a value greater than that of any possible<br>timestamp.<br>When the update transaction Ti completes its actions, it carries out commit pro-<br>cessing: First, Ti sets the timestamp on every version it has created to 1 more than the<br>value of ts-counter; then, Ti increments ts-counter by 1. Only one update transaction<br>is allowed to perform commit processing at a time.<br>As a result, read-only transactions that start after Ti increments ts-counter will see<br>the values updated by Ti, whereas those that start before Ti increments ts-counter will<br>see the value before the updates by Ti. In either case, read-only transactions never<br>need to wait for locks. Multiversion two-phase locking also ensures that schedules<br>are recoverable and cascadeless.<br>Versions are deleted in a manner like that of multiversion timestamp ordering.<br>Suppose there are two versions, Qk and Qj, of a data item, and that both versions<br>have a timestamp less than the timestamp of the oldest read-only transaction in the<br>system. Then, the older of the two versions Qk and Qj will not be used again and can<br>be deleted.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>V. Transaction <br>Management<br>16. Concurrency Control<br>614<br>© The McGraw−Hill <br>Companies, 2001<br>16.6<br>Deadlock Handling<br>615<br>Multiversion two-phase locking or variations of it are used in some commercial<br>database systems.<br>16.6<br>Deadlock Handling<br>A system is in a deadlock state if there exists a set of transactions such that every<br>transaction in the set is waiting for another transaction in the set. More precisely,<br>there exists a set of waiting transactions {T0, T1, . . ., Tn} such that T0 is waiting for a<br>data item that T1 holds, and T1 is waiting for a data item that T2 holds, and . . ., and<br>Tn−1 is waiting for a data item that Tn holds, and Tn is waiting for a data item that<br>T0 holds. None of the transactions can make progress in such a situation.<br>The only remedy to this undesirable situation is for the system to invoke some<br>drastic action, such as rolling back some of the transactions involved in the deadlock.<br>Rollback of a transaction may be partial: That is, a transaction may be rolled back to<br>the point where it obtained a lock whose release resolves the deadlock.<br>There are two principal methods for dealing with the deadlock problem. We can<br>use a deadlock prevention protocol to ensure that the system will never enter a dead-<br>lock state. Alternatively, we can allow the system to enter a deadlock state, and then<br>try to recover by using a deadlock detection and deadlock recovery scheme. As we<br>shall see, both methods may result in transaction rollback. Prevention is commonly<br>used if the probability that the system would enter a deadlock state is relatively high;<br>otherwise, detection and recovery are more efﬁcient.<br>Note that a detection and recovery scheme requires overhead that includes not<br>only the run-time cost of maintaining the necessary information and of executing the<br>detection algorithm, but also the potential losses inherent in recovery from a dead-<br>lock.<br>16.6.1<br>Deadlock Prevention<br>There are two approaches to deadlock prevention. One approach ensures that no<br>cyclic waits can occur by ordering the requests for locks, or requiring all locks to be<br>acquired together. The other approach is closer to deadlock recovery, and performs<br>transaction rollback instead of waiting for a lock, whenever the wait could potentially<br>result in a deadlock.<br>The simplest scheme under the ﬁrst approach requires that each transaction locks<br>all its data items before it begins execution. Moreover, either all are locked in one step<br>or none are locked. There are two main disadvantages to this protocol: (1) it is often<br>hard to predict, before the transaction begins, what data items need to be locked;<br>(2) data-item utilization may be very low, since many of the data items may be locked<br>but unused for a long time.<br>Another approach for preventing deadlocks is to impose an ordering of all data<br>items, and to require that a transaction lock data items only in a sequence consistent<br>with the ordering. We have seen one such scheme in the tree protocol, which uses a<br>partial ordering of data items.<br>A variation of this approach is to use a total order of data items, in conjunction<br>with two-phase locking. Once a transaction has locked a particular item, it cannot<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>V. Transaction <br>Management<br>16. Concurrency Control<br>615<br>© The McGraw−Hill <br>Companies, 2001<br>616<br>Chapter 16<br>Concurrency Control<br>request locks on items that precede that item in the ordering. This scheme is easy<br>to implement, as long as the set of data items accessed by a transaction is known<br>when the transaction starts execution. There is no need to change the underlying<br>concurrency-control system if two-phase locking is used: All that is needed it to en-<br>sure that locks are requested in the right order.<br>The second approach for preventing deadlocks is to use preemption and transac-<br>tion rollbacks. In preemption, when a transaction T2 requests a lock that transaction<br>T1 holds, the lock granted to T1 may be preempted by rolling back of T1, and granting<br>of the lock to T2. To control the preemption, we assign a unique timestamp to each<br>transaction. The system uses these timestamps only to decide whether a transaction<br>should wait or roll back. Locking is still used for concurrency control. If a transaction<br>is rolled back, it retains its old timestamp when restarted. Two different deadlock-<br>prevention schemes using timestamps have been proposed:<br>1. The wait–die scheme is a nonpreemptive technique. When transaction Ti re-<br>quests a data item currently held by Tj, Ti is allowed to wait only if it has a<br>timestamp smaller than that of Tj (that is, Ti is older than Tj). Otherwise, Ti is<br>rolled back (dies).<br>For example, suppose that transactions T22, T23, and T24 have timestamps<br>5, 10, and 15, respectively. If T22 requests a data item held by T23, then T22 will<br>wait. If T24 requests a data item held by T23, then T24 will be rolled back.<br>2. The wound–wait scheme is a preemptive technique. It is a counterpart to the<br>wait–die scheme. When transaction Ti requests a data item currently held by<br>Tj, Ti is allowed to wait only if it has a timestamp larger than that of Tj (that<br>is, Ti is younger than Tj). Otherwise, Tj is rolled back (Tj is wounded by Ti).<br>Returning to our example, with transactions T22, T23, and T24, if T22 re-<br>quests a data item held by T23, then the data item will be preempted from T23,<br>and T23 will be rolled back. If T24 requests a data item held by T23, then T24<br>will wait.<br>Whenever the system rolls back transactions, it is important to ensure that there<br>is no starvation—that is, no transaction gets rolled back repeatedly and is never al-<br>lowed to make progress.<br>Both the wound–wait and the wait–die schemes avoid starvation: At any time,<br>there is a transaction with the smallest timestamp. This transaction cannot be required<br>to roll back in either scheme. Since timestamps always increase, and since transac-<br>tions are not assigned new timestamps when they are rolled back, a transaction that<br>is rolled back repeatedly will eventually have the smallest timestamp, at which point<br>it will not be rolled back again.<br>There are, however, signiﬁcant differences in the way that the two schemes oper-<br>ate.<br>• In the wait–die scheme, an older transaction must wait for a younger one to<br>release its data item. Thus, the older the transaction gets, the more it tends to<br>wait. By contrast, in the wound–wait scheme, an older transaction never waits<br>for a younger transaction.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>V. Transaction <br>Management<br>16. Concurrency Control<br>616<br>© The McGraw−Hill <br>Companies, 2001<br>16.6<br>Deadlock Handling<br>617<br>• In the wait–die scheme, if a transaction Ti dies and is rolled back because it<br>requested a data item held by transaction Tj, then Ti may reissue the same<br>sequence of requests when it is restarted. If the data item is still held by Tj,<br>then Ti will die again. Thus, Ti may die several times before acquiring the<br>needed data item. Contrast this series of events with what happens in the<br>wound–wait scheme. Transaction Ti is wounded and rolled back because Tj<br>requested a data item that it holds. When Ti is restarted and requests the data<br>item now being held by Tj, Ti waits. Thus, there may be fewer rollbacks in the<br>wound–wait scheme.<br>The major problem with both of these schemes is that unnecessary rollbacks may<br>occur.<br>16.6.2<br>Timeout-Based Schemes<br>Another simple approach to deadlock handling is based on lock timeouts. In this ap-<br>proach, a transaction that has requested a lock waits for at most a speciﬁed amount of<br>time. If the lock has not been granted within that time, the transaction is said to time<br>out, and it rolls itself back and restarts. If there was in fact a deadlock, one or more<br>transactions involved in the deadlock will time out and roll back, allowing the oth-<br>ers to proceed. This scheme falls somewhere between deadlock prevention, where a<br>deadlock will never occur, and deadlock detection and recovery, which Section 16.6.3<br>discusses.<br>The timeout scheme is particularly easy to implement, and works well if transac-<br>tions are short and if long waits are likely to be due to deadlocks. However, in general<br>it is hard to decide how long a transaction must wait before timing out. Too long a<br>wait results in unnecessary delays once a deadlock has occurred. Too short a wait<br>results in transaction rollback even when there is no deadlock, leading to wasted re-<br>sources. Starvation is also a possibility with this scheme. Hence, the timeout-based<br>scheme has limited applicability.<br>16.6.3<br>Deadlock Detection and Recovery<br>If a system does not employ some protocol that ensures deadlock freedom, then a<br>detection and recovery scheme must be used. An algorithm that examines the state<br>of the system is invoked periodically to determine whether a deadlock has occurred.<br>If one has, then the system must attempt to recover from the deadlock. To do so, the<br>system must:<br>• Maintain information about the current allocation of data items to transac-<br>tions, as well as any outstanding data item requests.<br>• Provide an algorithm that uses this information to determine whether the sys-<br>tem has entered a deadlock state.<br>• Recover from the deadlock when the detection algorithm determines that a<br>deadlock exists.<br>In this section, we elaborate on these issues.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>V. Transaction <br>Management<br>16. Concurrency Control<br>617<br>© The McGraw−Hill <br>Companies, 2001<br>618<br>Chapter 16<br>Concurrency Control<br>T26<br>T28<br>T25<br>T27<br>Figure 16.18<br>Wait-for graph with no cycle.<br>16.6.3.1<br>Deadlock Detection<br>Deadlocks can be described precisely in terms of a directed graph called a wait-for<br>graph. This graph consists of a pair G = (V, E), where V is a set of vertices and E is<br>a set of edges. The set of vertices consists of all the transactions in the system. Each<br>element in the set E of edges is an ordered pair Ti →Tj. If Ti →Tj is in E, then there<br>is a directed edge from transaction Ti to Tj, implying that transaction Ti is waiting<br>for transaction Tj to release a data item that it needs.<br>When transaction Ti requests a data item currently being held by transaction Tj,<br>then the edge Ti →Tj is inserted in the wait-for graph. This edge is removed only<br>when transaction Tj is no longer holding a data item needed by transaction Ti.<br>A deadlock exists in the system if and only if the wait-for graph contains a cycle.<br>Each transaction involved in the cycle is said to be deadlocked. To detect deadlocks,<br>the system needs to maintain the wait-for graph, and periodically to invoke an algo-<br>rithm that searches for a cycle in the graph.<br>To illustrate these concepts, consider the wait-for graph in Figure 16.18, which<br>depicts the following situation:<br>• Transaction T25 is waiting for transactions T26 and T27.<br>• Transaction T27 is waiting for transaction T26.<br>• Transaction T26 is waiting for transaction T28.<br>Since the graph has no cycle, the system is not in a deadlock state.<br>Suppose now that transaction T28 is requesting an item held by T27. The edge<br>T28<br>→<br>T27 is added to the wait-for graph, resulting in the new system state in<br>Figure 16.19. This time, the graph contains the cycle<br>T26 →T28 →T27 →T26<br>implying that transactions T26, T27, and T28 are all deadlocked.<br>Consequently, the question arises: When should we invoke the detection algo-<br>rithm? The answer depends on two factors:<br>1. How often does a deadlock occur?<br>2. How many transactions will be affected by the deadlock?<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>V. Transaction <br>Management<br>16. Concurrency Control<br>618<br>© The McGraw−Hill <br>Companies, 2001<br>16.6<br>Deadlock Handling<br>619<br>T26<br>T28<br>T25<br>T27<br>Figure 16.19<br>Wait-for graph with a cycle.<br>If deadlocks occur frequently, then the detection algorithm should be invoked<br>more frequently than usual. Data items allocated to deadlocked transactions will be<br>unavailable to other transactions until the deadlock can be broken. In addition, the<br>number of cycles in the graph may also grow. In the worst case, we would invoke the<br>detection algorithm every time a request for allocation could not be granted immedi-<br>ately.<br>16.6.3.2<br>Recovery from Deadlock<br>When a detection algorithm determines that a deadlock exists, the system must re-<br>cover from the deadlock. The most common solution is to roll back one or more trans-<br>actions to break the deadlock. Three actions need to be taken:<br>1. Selection of a victim. Given a set of deadlocked transactions, we must deter-<br>mine which transaction (or transactions) to roll back to break the deadlock. We<br>should roll back those transactions that will incur the minimum cost. Unfortu-<br>nately, the term minimum cost is not a precise one. Many factors may determine<br>the cost of a rollback, including<br>a. How long the transaction has computed, and how much longer the trans-<br>action will compute before it completes its designated task.<br>b. How many data items the transaction has used.<br>c. How many more data items the transaction needs for it to complete.<br>d. How many transactions will be involved in the rollback.<br>2. Rollback. Once we have decided that a particular transaction must be rolled<br>back, we must determine how far this transaction should be rolled back.<br>The simplest solution is a total rollback: Abort the transaction and then<br>restart it. However, it is more effective to roll back the transaction only as far<br>as necessary to break the deadlock. Such partial rollback requires the system<br>to maintain additional information about the state of all the running trans-<br>actions. Speciﬁcally, the sequence of lock requests/grants and updates per-<br>formed by the transaction needs to be recorded. The deadlock detection mech-<br>anism should decide which locks the selected transaction needs to release in<br>order to break the deadlock. The selected transaction must be rolled back to<br>the point where it obtained the ﬁrst of these locks, undoing all actions it took<br>after that point. The recovery mechanism must be capable of performing such<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>V. Transaction <br>Management<br>16. Concurrency Control<br>619<br>© The McGraw−Hill <br>Companies, 2001<br>620<br>Chapter 16<br>Concurrency Control<br>partial rollbacks. Furthermore, the transactions must be capable of resuming<br>execution after a partial rollback. See the bibliographical notes for relevant<br>references.<br>3. Starvation. In a system where the selection of victims is based primarily on<br>cost factors, it may happen that the same transaction is always picked as a<br>victim. As a result, this transaction never completes its designated task, thus<br>there is starvation. We must ensure that transaction can be picked as a victim<br>only a (small) ﬁnite number of times. The most common solution is to include<br>the number of rollbacks in the cost factor.<br>16.7<br>Insert and Delete Operations<br>Until now, we have restricted our attention to read and write operations. This re-<br>striction limits transactions to data items already in the database. Some transactions<br>require not only access to existing data items, but also the ability to create new data<br>items. Others require the ability to delete data items. To examine how such transac-<br>tions affect concurrency control, we introduce these additional operations:<br>• delete(Q) deletes data item Q from the database.<br>• insert(Q) i</span><br><br><span style="background-color: #BDB2FF;" title="Chunk 78 | Start: 1560156 | End: 1580156 | Tokens: 3260">nserts a new data item Q into the database and assigns Q an initial<br>value.<br>An attempt by a transaction Ti to perform a read(Q) operation after Q has been<br>deleted results in a logical error in Ti. Likewise, an attempt by a transaction Ti to<br>perform a read(Q) operation before Q has been inserted results in a logical error in<br>Ti. It is also a logical error to attempt to delete a nonexistent data item.<br>16.7.1<br>Deletion<br>To understand how the presence of delete instructions affects concurrency control,<br>we must decide when a delete instruction conﬂicts with another instruction. Let Ii<br>and Ij be instructions of Ti and Tj, respectively, that appear in schedule S in consec-<br>utive order. Let Ii = delete(Q). We consider several instructions Ij.<br>• Ij = read(Q). Ii and Ij conﬂict. If Ii comes before Ij, Tj will have a logical<br>error. If Ij comes before Ii, Tj can execute the read operation successfully.<br>• Ij = write(Q). Ii and Ij conﬂict. If Ii comes before Ij, Tj will have a logical<br>error. If Ij comes before Ii, Tj can execute the write operation successfully.<br>• Ij = delete(Q). Ii and Ij conﬂict. If Ii comes before Ij, Ti will have a logical<br>error. If Ij comes before Ii, Ti will have a logical error.<br>• Ij = insert(Q). Ii and Ij conﬂict. Suppose that data item Q did not exist prior<br>to the execution of Ii and Ij. Then, if Ii comes before Ij, a logical error results<br>for Ti. If Ij comes before Ii, then no logical error results. Likewise, if Q existed<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>V. Transaction <br>Management<br>16. Concurrency Control<br>620<br>© The McGraw−Hill <br>Companies, 2001<br>16.7<br>Insert and Delete Operations<br>621<br>prior to the execution of Ii and Ij, then a logical error results if Ij comes before<br>Ii, but not otherwise.<br>We can conclude the following:<br>• Under the two-phase locking protocol, an exclusive lock is required on a data<br>item before that item can be deleted.<br>• Under the timestamp-ordering protocol, a test similar to that for a write must<br>be performed. Suppose that transaction Ti issues delete(Q).<br>  If TS(Ti) &lt; R-timestamp(Q), then the value of Q that Ti was to delete has<br>already been read by a transaction Tj with TS(Tj) &gt; TS(Ti). Hence, the<br>delete operation is rejected, and Ti is rolled back.<br>  If TS(Ti) &lt; W-timestamp(Q), then a transaction Tj with TS(Tj) &gt; TS(Ti)<br>has written Q. Hence, this delete operation is rejected, and Ti is rolled<br>back.<br>  Otherwise, the delete is executed.<br>16.7.2<br>Insertion<br>We have already seen that an insert(Q) operation conﬂicts with a delete(Q) operation.<br>Similarly, insert(Q) conﬂicts with a read(Q) operation or a write(Q) operation; no read<br>or write can be performed on a data item before it exists.<br>Since an insert(Q) assigns a value to data item Q, an insert is treated similarly to a<br>write for concurrency-control purposes:<br>• Under the two-phase locking protocol, if Ti performs an insert(Q) operation,<br>Ti is given an exclusive lock on the newly created data item Q.<br>• Under the timestamp-ordering protocol, if Ti performs an insert(Q) operation,<br>the values R-timestamp(Q) and W-timestamp(Q) are set to TS(Ti).<br>16.7.3<br>The Phantom Phenomenon<br>Consider transaction T29 that executes the following SQL query on the bank database:<br>select sum(balance)<br>from account<br>where branch-name = ’Perryridge’<br>Transaction T29 requires access to all tuples of the account relation pertaining to the<br>Perryridge branch.<br>Let T30 be a transaction that executes the following SQL insertion:<br>insert into account<br>values (A-201, ’Perryridge’, 900)<br>Let S be a schedule involving T29 and T30. We expect there to be potential for a<br>conﬂict for the following reasons:<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>V. Transaction <br>Management<br>16. Concurrency Control<br>621<br>© The McGraw−Hill <br>Companies, 2001<br>622<br>Chapter 16<br>Concurrency Control<br>• If T29 uses the tuple newly inserted by T30 in computing sum(balance), then<br>T29 read a value written by T30. Thus, in a serial schedule equivalent to S, T30<br>must come before T29.<br>• If T29 does not use the tuple newly inserted by T30 in computing sum(balance),<br>then in a serial schedule equivalent to S, T29 must come before T30.<br>The second of these two cases is curious. T29 and T30 do not access any tuple in<br>common, yet they conﬂict with each other! In effect, T29 and T30 conﬂict on a phantom<br>tuple. If concurrency control is performed at the tuple granularity, this conﬂict would<br>go undetected. This problem is called the phantom phenomenon.<br>To prevent the phantom phenomenon, we allow T29 to prevent other transactions<br>from creating new tuples in the account relation with branch-name = “Perryridge.”<br>To ﬁnd all account tuples with branch-name = “Perryridge”, T29 must search either<br>the whole account relation, or at least an index on the relation. Up to now, we have as-<br>sumed implicitly that the only data items accessed by a transaction are tuples. How-<br>ever, T29 is an example of a transaction that reads information about what tuples are<br>in a relation, and T30 is an example of a transaction that updates that information.<br>Clearly, it is not sufﬁcient merely to lock the tuples that are accessed; the informa-<br>tion used to ﬁnd the tuples that are accessed by the transaction must also be locked.<br>The simplest solution to this problem is to associate a data item with the relation;<br>the data item represents the information used to ﬁnd the tuples in the relation. Trans-<br>actions, such as T29, that read the information about what tuples are in a relation<br>would then have to lock the data item corresponding to the relation in shared mode.<br>Transactions, such as T30, that update the information about what tuples are in a re-<br>lation would have to lock the data item in exclusive mode. Thus, T29 and T30 would<br>conﬂict on a real data item, rather than on a phantom.<br>Do not confuse the locking of an entire relation, as in multiple granularity lock-<br>ing, with the locking of the data item corresponding to the relation. By locking the<br>data item, a transaction only prevents other transactions from updating information<br>about what tuples are in the relation. Locking is still required on tuples. A transaction<br>that directly accesses a tuple can be granted a lock on the tuples even when another<br>transaction has an exclusive lock on the data item corresponding to the relation itself.<br>The major disadvantage of locking a data item corresponding to the relation is<br>the low degree of concurrency— two transactions that insert different tuples into a<br>relation are prevented from executing concurrently.<br>A better solution is the index-locking technique. Any transaction that inserts a<br>tuple into a relation must insert information into every index maintained on the re-<br>lation. We eliminate the phantom phenomenon by imposing a locking protocol for<br>indices. For simplicity we shall only consider B+-tree indices.<br>As we saw in Chapter 12, every search-key value is associated with an index leaf<br>node. A query will usually use one or more indices to access a relation. An insert<br>must insert the new tuple in all indices on the relation. In our example, we assume<br>that there is an index on account for branch-name. Then, T30 must modify the leaf<br>containing the key Perryridge. If T29 reads the same leaf node to locate all tuples<br>pertaining to the Perryridge branch, then T29 and T30 conﬂict on that leaf node.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>V. Transaction <br>Management<br>16. Concurrency Control<br>622<br>© The McGraw−Hill <br>Companies, 2001<br>16.8<br>Weak Levels of Consistency<br>623<br>The index-locking protocol takes advantage of the availability of indices on a re-<br>lation, by turning instances of the phantom phenomenon into conﬂicts on locks on<br>index leaf nodes. The protocol operates as follows:<br>• Every relation must have at least one index.<br>• A transaction Ti can access tuples of a relation only after ﬁrst ﬁnding them<br>through one or more of the indices on the relation.<br>• A transaction Ti that performs a lookup (whether a range lookup or a point<br>lookup) must acquire a shared lock on all the index leaf nodes that it accesses.<br>• A transaction Ti may not insert, delete, or update a tuple ti in a relation r<br>without updating all indices on r. The transaction must obtain exclusive locks<br>on all index leaf nodes that are affected by the insertion, deletion, or update.<br>For insertion and deletion, the leaf nodes affected are those that contain (after<br>insertion) or contained (before deletion) the search-key value of the tuple. For<br>updates, the leaf nodes affected are those that (before the modiﬁcation) con-<br>tained the old value of the search-key, and nodes that (after the modiﬁcation)<br>contain the new value of the search-key.<br>• The rules of the two-phase locking protocol must be observed.<br>Variants of the index-locking technique exist for eliminating the phantom phe-<br>nomenon under the other concurrency-control protocols presented in this chapter.<br>16.8<br>Weak Levels of Consistency<br>Serializability is a useful concept because it allows programmers to ignore issues<br>related to concurrency when they code transactions. If every transaction has the<br>property that it maintains database consistency if executed alone, then serializabil-<br>ity ensures that concurrent executions maintain consistency. However, the protocols<br>required to ensure serializability may allow too little concurrency for certain applica-<br>tions. In these cases, weaker levels of consistency are used. The use of weaker levels<br>of consistency places additional burdens on programmers for ensuring database cor-<br>rectness.<br>16.8.1<br>Degree-Two Consistency<br>The purpose of degree-two consistency is to avoid cascading aborts without neces-<br>sarily ensuring serializability. The locking protocol for degree-two consistency uses<br>the same two lock modes that we used for the two-phase locking protocol: shared<br>(S) and exclusive (X). A transaction must hold the appropriate lock mode when it<br>accesses a data item.<br>In contrast to the situation in two-phase locking, S-locks may be released at any<br>time, and locks may be acquired at any time. Exclusive locks cannot be released until<br>the transaction either commits or aborts. Serializability is not ensured by this pro-<br>tocol. Indeed, a transaction may read the same data item twice and obtain different<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>V. Transaction <br>Management<br>16. Concurrency Control<br>623<br>© The McGraw−Hill <br>Companies, 2001<br>624<br>Chapter 16<br>Concurrency Control<br>T3<br>T4<br>lock-S(Q)<br>lock-X(Q)<br>read(Q)<br>write(Q)<br>unlock(Q)<br>lock-S(Q)<br>read(Q)<br>unlock(Q)<br>read(Q)<br>unlock(Q)<br>Figure 16.20<br>Nonserializable schedule with degree-two consistency.<br>results. In Figure 16.20, T3 reads the value of Q before and after that value is written<br>by T4.<br>The potential for inconsistency due to nonserializable schedules under degree-two<br>consistency makes this approach undesirable for many applications.<br>16.8.2<br>Cursor Stability<br>Cursor stability is a form of degree-two consistency designed for programs written<br>in host languages, which iterate over tuples of a relation by using cursors. Instead of<br>locking the entire relation, cursor stability ensures that<br>• The tuple that is currently being processed by the iteration is locked in shared<br>mode.<br>• Any modiﬁed tuples are locked in exclusive mode until the transaction com-<br>mits.<br>These rules ensure that degree-two consistency is obtained. Two-phase locking is<br>not required. Serializability is not guaranteed. Cursor stability is used in practice<br>on heavily accessed relations as a means of increasing concurrency and improving<br>system performance. Applications that use cursor stability must be coded in a way<br>that ensures database consistency despite the possibility of nonserializable sched-<br>ules. Thus, the use of cursor stability is limited to specialized situations with simple<br>consistency constraints.<br>16.8.3<br>Weak Levels of Consistency in SQL<br>The SQL standard also allows a transaction to specify that it may be executed in such a<br>way that it becomes nonserializable with respect to other transactions. For instance, a<br>transaction may operate at the level of read uncommitted, which permits the transac-<br>tion to read records even if they have not been committed. SQL provides such features<br>for long transactions whose results do not need to be precise. For instance, approx-<br>imate information is usually sufﬁcient for statistics used for query optimization. If<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>V. Transaction <br>Management<br>16. Concurrency Control<br>624<br>© The McGraw−Hill <br>Companies, 2001<br>16.9<br>Concurrency in Index Structures∗∗<br>625<br>these transactions were to execute in a serializable fashion, they could interfere with<br>other transactions, causing the others’ execution to be delayed.<br>The levels of consistency speciﬁed by SQL-92 are as follows:<br>• Serializable is the default.<br>• Repeatable read allows only committed records to be read, and further re-<br>quires that, between two reads of a record by a transaction, no other trans-<br>action is allowed to update the record. However, the transaction may not be<br>serializable with respect to other transactions. For instance, when it is search-<br>ing for records satisfying some conditions, a transaction may ﬁnd some of the<br>records inserted by a committed transaction, but may not ﬁnd others.<br>• Read committed allows only committed records to be read, but does not re-<br>quire even repeatable reads. For instance, between two reads of a record by the<br>transaction, the records may have been updated by other committed transac-<br>tions. This is basically the same as degree-two consistency; most systems sup-<br>porting this level of consistency would actually implement cursor stability,<br>which is a special case of degree-two consistency.<br>• Read uncommitted allows even uncommitted records to be read. It is the low-<br>est level of consistency allowed by SQL-92.<br>16.9<br>Concurrency in Index Structures∗∗<br>It is possible to treat access to index structures like any other database structure, and<br>to apply the concurrency-control techniques discussed earlier. However, since indices<br>are accessed frequently, they would become a point of great lock contention, leading<br>to a low degree of concurrency. Luckily, indices do not have to be treated like other<br>database structures. It is perfectly acceptable for a transaction to perform a lookup<br>on an index twice, and to ﬁnd that the structure of the index has changed in between,<br>as long as the index lookup returns the correct set of tuples. Thus, it is acceptable<br>to have nonserializable concurrent access to an index, as long as the accuracy of the<br>index is maintained.<br>We outline two techniques for managing concurrent access to B+-trees. The bib-<br>liographical notes reference other techniques for B+-trees, as well as techniques for<br>other index structures.<br>The techniques that we present for concurrency control on B+-trees are based on<br>locking, but neither two-phase locking nor the tree protocol is employed. The algo-<br>rithms for lookup, insertion, and deletion are those used in Chapter 12, with only<br>minor modiﬁcations.<br>The ﬁrst technique is called the crabbing protocol:<br>• When searching for a key value, the crabbing protocol ﬁrst locks the root node<br>in shared mode. When traversing down the tree, it acquires a shared lock on<br>the child node to be traversed further. After acquiring the lock on the child<br>node, it releases the lock on the parent node. It repeats this process until it<br>reaches a leaf node.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>V. Transaction <br>Management<br>16. Concurrency Control<br>625<br>© The McGraw−Hill <br>Companies, 2001<br>626<br>Chapter 16<br>Concurrency Control<br>• When inserting or deleting a key value, the crabbing protocol takes these ac-<br>tions:<br>  It follows the same protocol as for searching until it reaches the desired<br>leaf node. Up to this point, it obtains only shared locks.<br>  It locks the leaf node in exclusive mode and inserts or deletes the key<br>value.<br>  If it needs to split a node or coalesce it with its siblings, or redistribute<br>key values between siblings, the crabbing protocol locks the parent of the<br>node in exclusive mode. After performing these actions, it releases the<br>locks on the node and siblings.<br>If the parent requires splitting, coalescing, or redistribution of key val-<br>ues, the protcol retains the lock on the parent, and splitting, coalescing,<br>or redistribution propagates further in the same manner. Otherwise, it re-<br>leases the lock on the parent.<br>The protocol gets its name from the way in which crabs advance by moving side-<br>ways, moving the legs on one side, then the legs on the other, and so on alternately.<br>The progress of locking while the protocol both goes down the tree and goes back up<br>(in case of splits, coalescing, or redistribution) proceeds in a similar crab-like manner.<br>Once a particular operation releases a lock on a node, other operations can ac-<br>cess that node. There is a possibility of deadlocks between search operations coming<br>down the tree, and splits, coalescing or redistribution propagating up the tree. The<br>system can easily handle such deadlocks by restarting the search operation from the<br>root, after releasing the locks held by the operation.<br>The second technique achieves even more concurrency, avoiding even holding the<br>lock on one node while acquiring the lock on another node, by using a modiﬁed ver-<br>sion of B+-trees called B-link trees; B-link trees require that every node (including in-<br>ternal nodes, not just the leaves) maintain a pointer to its right sibling. This pointer is<br>required because a lookup that occurs while a node is being split may have to search<br>not only that node but also that node’s right sibling (if one exists). We shall illustrate<br>this technique with an example later, but we ﬁrst present the modiﬁed procedures of<br>the B-link-tree locking protocol.<br>• Lookup. Each node of the B+-tree must be locked in shared mode before it is<br>accessed. A lock on a nonleaf node is released before any lock on any other<br>node in the B+-tree is requested. If a split occurs concurrently with a lookup,<br>the desired search-key value may no longer appear within the range of values<br>represented by a node accessed during lookup. In such a case, the search-key<br>value is in the range represented by a sibling node, which the system locates<br>by following the pointer to the right sibling. However, the system locks leaf<br>nodes following the two-phase locking protocol, as Section 16.7.3 describes,<br>to avoid the phantom phenomenon.<br>• Insertion and deletion. The system follows the rules for lookup to locate the<br>leaf node into which it will make the insertion or deletion. It upgrades the<br>shared-mode lock on this node to exclusive mode, and performs the insertion<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>V. Transaction <br>Management<br>16. Concurrency Control<br>626<br>© The McGraw−Hill <br>Companies, 2001<br>16.9<br>Concurrency in Index Structures∗∗<br>627<br>or deletion. It locks leaf nodes affected by insertion or deletion following the<br>two-phase locking protocol, as Section 16.7.3 describes, to avoid the phantom<br>phenomenon.<br>• Split. If the transaction splits a node, it creates a new node according to the<br>algorithm of Section 12.3 and makes it the right sibling of the original node.<br>The right-sibling pointers of both the original node and the new node are set.<br>Following this, the transaction releases the exclusive lock on the original node<br>and requests an exclusive lock on the parent, so that it can insert a pointer to<br>the new node.<br>• Coalescence. If a node has too few search-key values after a deletion, the node<br>with which it will be coalesced must be locked in exclusive mode. Once the<br>transaction has coalesced these two nodes, it requests an exclusive lock on the<br>parent so that the deleted node can be removed. At this point, the transaction<br>releases the locks on the coalesced nodes. Unless the parent node must be<br>coalesced also, its lock is released.<br>Observe this important fact: An insertion or dele</span><br><br><span style="background-color: #FFC6FF;" title="Chunk 79 | Start: 1580158 | End: 1600158 | Tokens: 3146">tion may lock a node, unlock it, and<br>subsequently relock it. Furthermore, a lookup that runs concurrently with a split or<br>coalescence operation may ﬁnd that the desired search key has been moved to the<br>right-sibling node by the split or coalescence operation.<br>As an illustration, consider the B+-tree in Figure 16.21. Assume that there are two<br>concurrent operations on this B+-tree:<br>1. Insert “Clearview”<br>2. Look up “Downtown”<br>Let us assume that the insertion operation begins ﬁrst. It does a lookup on “Clear-<br>view,” and ﬁnds that the node into which “Clearview” should be inserted is full.<br>It therefore converts its shared lock on the node to exclusive mode, and creates a<br>new node. The original node now contains the search-key values “Brighton” and<br>“Clearview.” The new node contains the search-key value “Downtown.”<br>Now assume that a context switch occurs that results in control passing to the<br>lookup operation. This lookup operation accesses the root, and follows the pointer<br>Perryridge<br>Redwood<br>Redwood<br>Perryridge<br>Brighton<br>Mianus<br>Downtown<br>Downtown<br>Mianus<br>Clearview<br>Round Hill<br>Figure 16.21<br>B+-tree for account ﬁle with n = 3.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>V. Transaction <br>Management<br>16. Concurrency Control<br>627<br>© The McGraw−Hill <br>Companies, 2001<br>628<br>Chapter 16<br>Concurrency Control<br>Perryridge<br>Perryridge<br>Redwood<br>Redwood<br>Brighton<br>Mianus<br>Downtown<br>Downtown<br>Mianus<br>Clearview<br>Round Hill<br>Figure 16.22<br>Insertion of “Clearview” into the B+-tree of Figure 16.21.<br>to the left child of the root. It then accesses that node, and obtains a pointer to the<br>left child. This left-child node originally contained the search-key values “Brighton”<br>and “Downtown.” Since this node is currently locked by the insertion operation in<br>exclusive mode, the lookup operation must wait. Note that, at this point, the lookup<br>operation holds no locks at all!<br>The insertion operation now unlocks the leaf node and relocks its parent, this time<br>in exclusive mode. It completes the insertion, leaving the B+-tree as in Figure 16.22.<br>The lookup operation proceeds. However, it is holding a pointer to an incorrect leaf<br>node. It therefore follows the right-sibling pointer to locate the next node. If this node,<br>too, turns out to be incorrect, the lookup follows that node’s right-sibling pointer. It<br>can be shown that, if a lookup holds a pointer to an incorrect node, then, by following<br>right-sibling pointers, the lookup must eventually reach the correct node.<br>Lookup and insertion operations cannot lead to deadlock. Coalescing of nodes<br>during deletion can cause inconsistencies, since a lookup may have read a pointer<br>to a deleted node from its parent, before the parent node was updated, and may<br>then try to access the deleted node. The lookup would then have to restart from the<br>root. Leaving nodes uncoalesced avoids such inconsistencies. This solution results<br>in nodes that contain too few search-key values and that violate some properties of<br>B+-trees. In most databases, however, insertions are more frequent than deletions, so<br>it is likely that nodes that have too few search-key values will gain additional values<br>relatively quickly.<br>Instead of locking index leaf nodes in a two-phase manner, some index concur-<br>rency control schemes use key-value locking on individual key values, allowing<br>other key values to be inserted or deleted from the same leaf. Key-value locking thus<br>provides increased concurrency. Using key-value locking naively, however, would<br>allow the phantom phenomenon to occur; to prevent the phantom phenomenon, the<br>next-key locking technique is used. In this technique, every index lookup must lock<br>not only the keys found within the range (or the single key, in case of a point lookup)<br>but also the next key value—that is, the key value just greater than the last key value<br>that was within the range. Also, every insert must lock not only the value that is in-<br>serted, but also the next key value. Thus, if a transaction attempts to insert a value<br>that was within the range of the index lookup of another transaction, the two transac-<br>tions would conﬂict on the key value next to the inserted key value. Similarly, deletes<br>must also lock the next key value to the value being deleted, to ensure that conﬂicts<br>with subsequent range lookups of other queries are detected.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>V. Transaction <br>Management<br>16. Concurrency Control<br>628<br>© The McGraw−Hill <br>Companies, 2001<br>16.10<br>Summary<br>629<br>16.10<br>Summary<br>• When several transactions execute concurrently in the database, the consis-<br>tency of data may no longer be preserved. It is necessary for the system to<br>control the interaction among the concurrent transactions, and this control is<br>achieved through one of a variety of mechanisms called concurrency-control<br>schemes.<br>• To ensure serializability, we can use various concurrency-control schemes.<br>All these schemes either delay an operation or abort the transaction that is-<br>sued the operation. The most common ones are locking protocols, timestamp-<br>ordering schemes, validation techniques, and multiversion schemes.<br>• A locking protocol is a set of rules that state when a transaction may lock and<br>unlock each of the data items in the database.<br>• The two-phase locking protocol allows a transaction to lock a new data item<br>only if that transaction has not yet unlocked any data item. The protocol en-<br>sures serializability, but not deadlock freedom. In the absence of information<br>concerning the manner in which data items are accessed, the two-phase lock-<br>ing protocol is both necessary and sufﬁcient for ensuring serializability.<br>• The strict two-phase locking protocol permits release of exclusive locks only<br>at the end of transaction, in order to ensure recoverability and cascadelessness<br>of the resulting schedules. The rigorous two-phase locking protocol releases<br>all locks only at the end of the transaction.<br>• A timestamp-ordering scheme ensures serializability by selecting an ordering<br>in advance between every pair of transactions. A unique ﬁxed timestamp is<br>associated with each transaction in the system. The timestamps of the transac-<br>tions determine the serializability order. Thus, if the timestamp of transaction<br>Ti is smaller than the timestamp of transaction Tj, then the scheme ensures<br>that the produced schedule is equivalent to a serial schedule in which trans-<br>action Ti appears before transaction Tj. It does so by rolling back a transaction<br>whenever such an order is violated.<br>• A validation scheme is an appropriate concurrency-control method in cases<br>where a majority of transactions are read-only transactions, and thus the rate<br>of conﬂicts among these transactions is low. A unique ﬁxed timestamp is as-<br>sociated with each transaction in the system. The serializability order is de-<br>termined by the timestamp of the transaction. A transaction in this scheme is<br>never delayed. It must, however, pass a validation test to complete. If it does<br>not pass the validation test, the system rolls it back to its initial state.<br>• There are circumstances where it would be advantageous to group several<br>data items, and to treat them as one aggregate data item for purposes of work-<br>ing, resulting in multiple levels of granularity. We allow data items of various<br>sizes, and deﬁne a hierarchy of data items, where the small items are nested<br>within larger ones. Such a hierarchy can be represented graphically as a tree.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>V. Transaction <br>Management<br>16. Concurrency Control<br>629<br>© The McGraw−Hill <br>Companies, 2001<br>630<br>Chapter 16<br>Concurrency Control<br>Locks are acquired in root-to-leaf order; they are released in leaf-to-root order.<br>The protocol ensures serializability, but not freedom from deadlock.<br>• A multiversion concurrency-control scheme is based on the creation of a new<br>version of a data item for each transaction that writes that item. When a read<br>operation is issued, the system selects one of the versions to be read. The<br>concurrency-control scheme ensures that the version to be read is selected in<br>a manner that ensures serializability, by using timestamps. A read operation<br>always succeeds.<br>  In multiversion timestamp ordering, a write operation may result in the<br>rollback of the transaction.<br>  In multiversion two-phase locking, write operations may result in a lock<br>wait or, possibly, in deadlock.<br>• Various locking protocols do not guard against deadlocks. One way to prevent<br>deadlock is to use an ordering of data items, and to request locks in a sequence<br>consistent with the ordering.<br>• Another way to prevent deadlock is to use preemption and transaction roll-<br>backs. To control the preemption, we assign a unique timestamp to each trans-<br>action. The system uses these timestamps to decide whether a transaction<br>should wait or roll back. If a transaction is rolled back, it retains its old time-<br>stamp when restarted. The wound–wait scheme is a preemptive scheme.<br>• If deadlocks are not prevented, the system must deal with them by using a<br>deadlock detection and recovery scheme. To do so, the system constructs a<br>wait-for graph. A system is in a deadlock state if and only if the wait-for graph<br>contains a cycle. When the deadlock detection algorithm determines that a<br>deadlock exists, the system must recover from the deadlock. It does so by<br>rolling back one or more transactions to break the deadlock.<br>• A delete operation may be performed only if the transaction deleting the tuple<br>has an exclusive lock on the tuple to be deleted. A transaction that inserts a<br>new tuple into the database is given an exclusive lock on the tuple.<br>• Insertions can lead to the phantom phenomenon, in which an insertion logi-<br>cally conﬂicts with a query even though the two transactions may access no<br>tuple in common. Such conﬂict cannot be detected if locking is done only on<br>tuples accessed by the transactions. Locking is required on the data used to<br>ﬁnd the tuples in the relation. The index-locking technique solves this prob-<br>lem by requiring locks on certain index buckets. These locks ensure that all<br>conﬂicting transactions conﬂict on a real data item, rather than on a phantom.<br>• Weak levels of consistency are used in some applications where consistency<br>of query results is not critical, and using serializability would result in queries<br>adversely affecting transaction processing. Degree-two consistency is one such<br>weaker level of consistency; cursor stability is a special case of degree-two<br>consistency, and is widely used. SQL:1999 allows queries to specify the level of<br>consistency that they require.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>V. Transaction <br>Management<br>16. Concurrency Control<br>630<br>© The McGraw−Hill <br>Companies, 2001<br>16.10<br>Summary<br>631<br>• Special concurrency-control techniques can be developed for special data<br>structures. Often, special techniques are applied in B+-trees to allow greater<br>concurrency. These techniques allow nonserializable access to the B+-tree, but<br>they ensure that the B+-tree structure is correct, and ensure that accesses to<br>the database itself are serializable.<br>Review Terms<br>• Concurrency control<br>• Lock types<br>  Shared-mode (S) lock<br>  Exclusive-mode (X) lock<br>• Lock<br>  Compatibility<br>  Request<br>  Wait<br>  Grant<br>• Deadlock<br>• Starvation<br>• Locking protocol<br>• Legal schedule<br>• Two-phase locking protocol<br>  Growing phase<br>  Shrinking phase<br>  Lock point<br>  Strict two-phase locking<br>  Rigorous two-phase locking<br>• Lock conversion<br>  Upgrade<br>  Downgrade<br>• Graph-based protocols<br>  Tree protocol<br>  Commit dependency<br>• Timestamp-based protocols<br>• Timestamp<br>  System clock<br>  Logical counter<br>  W-timestamp(Q)<br>  R-timestamp(Q)<br>• Timestamp-ordering protocol<br>  Thomas’ write rule<br>• Validation-based protocols<br>  Read phase<br>  Validation phase<br>  Write phase<br>  Validation test<br>• Multiple granularity<br>  Explicit locks<br>  Implicit locks<br>  Intention locks<br>• Intention lock modes<br>  Intention-shared (IS)<br>  Intention-exclusive (IX)<br>  Shared and intention-<br>exclusive (SIX)<br>• Multiple-granularity locking<br>protocol<br>• Multiversion concurrency control<br>• Versions<br>• Multiversion timestamp ordering<br>• Multiversion two-phase locking<br>  Read-only transactions<br>  Update transactions<br>• Deadlock handling<br>  Prevention<br>  Detection<br>  Recovery<br>• Deadlock prevention<br>  Ordered locking<br>  Preemption of locks<br>  Wait–die scheme<br>  Wound–wait scheme<br>  Timeout-based schemes<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>V. Transaction <br>Management<br>16. Concurrency Control<br>631<br>© The McGraw−Hill <br>Companies, 2001<br>632<br>Chapter 16<br>Concurrency Control<br>• Deadlock detection<br>  Wait-for graph<br>• Deadlock recovery<br>  Total rollback<br>  Partial rollback<br>• Insert and delete operations<br>• Phantom phenomenon<br>  Index-locking protocol<br>• Weak levels of consistency<br>  Degree-two consistency<br>  Cursor stability<br>  Repeatable read<br>  Read committed<br>  Read uncommitted<br>• Concurrency in indices<br>  Crabbing<br>  B-link trees<br>  B-link-tree locking protocol<br>  Next-key locking<br>Exercises<br>16.1 Show that the two-phase locking protocol ensures conﬂict serializability, and<br>that transactions can be serialized according to their lock points.<br>16.2 Consider the following two transactions:<br>T31: read(A);<br>read(B);<br>if A = 0 then B := B + 1;<br>write(B).<br>T32: read(B);<br>read(A);<br>if B = 0 then A := A + 1;<br>write(A).<br>Add lock and unlock instructions to transactions T31 and T32, so that they ob-<br>serve the two-phase locking protocol. Can the execution of these transactions<br>result in a deadlock?<br>16.3 What beneﬁt does strict two-phase locking provide? What disadvantages re-<br>sult?<br>16.4 What beneﬁt does rigorous two-phase locking provide? How does it compare<br>with other forms of two-phase locking?<br>16.5 Most implementations of database systems use strict two-phase locking. Sug-<br>gest three reasons for the popularity of this protocol.<br>16.6 Consider a database organized in the form of a rooted tree. Suppose that we<br>insert a dummy vertex between each pair of vertices. Show that, if we follow<br>the tree protocol on the new tree, we get better concurrency than if we follow<br>the tree protocol on the original tree.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>V. Transaction <br>Management<br>16. Concurrency Control<br>632<br>© The McGraw−Hill <br>Companies, 2001<br>Exercises<br>633<br>16.7 Show by example that there are schedules possible under the tree protocol that<br>are not possible under the two-phase locking protocol, and vice versa.<br>16.8 Consider the following extension to the tree-locking protocol, which allows<br>both shared and exclusive locks:<br>• A transaction can be either a read-only transaction, in which case it can<br>request only shared locks, or an update transaction, in which case it can<br>request only exclusive locks.<br>• Each transaction must follow the rules of the tree protocol. Read-only trans-<br>actions may lock any data item ﬁrst, whereas update transactions must<br>lock the root ﬁrst.<br>Show that the protocol ensures serializability and deadlock freedom.<br>16.9 Consider the following graph-based locking protocol, which allows only ex-<br>clusive lock modes, and which operates on data graphs that are in the form of<br>a rooted directed acyclic graph.<br>• A transaction can lock any vertex ﬁrst.<br>• To lock any other vertex, the transaction must be holding a lock on the<br>majority of the parents of that vertex.<br>Show that the protocol ensures serializability and deadlock freedom.<br>16.10 Consider the following graph-based locking protocol that allows only exclu-<br>sive lock modes, and that operates on data graphs that are in the form of a<br>rooted directed acyclic graph.<br>• A transaction can lock any vertex ﬁrst.<br>• To lock any other vertex, the transaction must have visited all the parents<br>of that vertex, and must be holding a lock on one of the parents of the<br>vertex.<br>Show that the protocol ensures serializability and deadlock freedom.<br>16.11 Consider a variant of the tree protocol called the forest protocol. The database<br>is organized as a forest of rooted trees. Each transaction Ti must follow the<br>following rules:<br>• The ﬁrst lock in each tree may be on any data item.<br>• The second, and all subsequent, locks in a tree may be requested only if<br>the parent of the requested node is currently locked.<br>• Data items may be unlocked at any time.<br>• A data item may not be relocked by Ti after it has been unlocked by Ti.<br>Show that the forest protocol does not ensure serializability.<br>16.12 Locking is not done explicitly in persistent programming languages. Rather,<br>objects (or the corresponding pages) must be locked when the objects are<br>accessed. Most modern operating systems allow the user to set access pro-<br>tections (no access, read, write) on pages, and memory access that violate the<br>access protections result in a protection violation (see the Unix mprotect com-<br>mand, for example). Describe how the access-protection mechanism can be<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>V. Transaction <br>Management<br>16. Concurrency Control<br>633<br>© The McGraw−Hill <br>Companies, 2001<br>634<br>Chapter 16<br>Concurrency Control<br>S<br>X<br>I<br>S<br>true<br>false<br>false<br>X<br>false<br>false<br>false<br>I<br>false<br>false<br>true<br>Figure 16.23<br>Lock-compatibility matrix.<br>used for page-level locking in a persistent programming language. (Hint: The<br>technique is similar to that used for hardware swizzling in Section 11.9.4).<br>16.13 Consider a database system that includes an atomic increment operation, in<br>addition to the read and write operations. Let V be the value of data item X.<br>The operation<br>increment(X) by C<br>sets the value of X to V + C in an atomic step. The value of X is not available to<br>the transaction unless the latter executes a read(X). Figure 16.23 shows a lock-<br>compatibility matrix for three lock modes: share mode, exclusive mode, and<br>incrementation mode.<br>a. Show that, if all transactions lock the data that they access in the corre-<br>sponding mode, then two-phase locking ensures serializability.<br>b. Show that the inclusion of increment mode locks allows for increased con-<br>currency. (Hint: Consider check-clearing transactions in our bank exam-<br>ple.)<br>16.14 In timestamp ordering, W-timestamp(Q) denotes the largest timestamp of any<br>transaction that executed write(Q) successfully. Suppose that, instead, we de-<br>ﬁned it to be the timestamp of the most recent transaction to execute write(Q)<br>successfully. Would this change in wording make any difference? Explain your<br>answer.<br>16.15 When a transaction is rolled back under timestamp ordering, it is assigned a<br>new timestamp. Why can it not simply keep its old timestamp?<br>16.16 In multiple-granularity locking, what is the difference between implicit and<br>explicit locking?<br>16.17 Although SIX mode is useful in multiple-granularity locking, an exclusive and<br>intend-shared (XIS) mode is of no use. Why is it useless?<br>16.18 Use of multiple-granularity locking may require more or fewer locks than an<br>equivalent system with a single lock granularity. Provide examples of both sit-<br>uations, and compare the relative amount of concurrency allowed.<br>16.19 Consider the validation-based concurrency-control scheme of Section 16.3.<br>Show that by choosing Validation(Ti), rather than Start(Ti), as the timestamp of<br>transaction Ti, we can expect better response time provided that conﬂict rates<br>among transactions are indeed low.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>V. Transaction <br>Management<br>16. Concurrency Control<br>634<br>© The McGraw−Hill <br>Companies, 2001<br>Exercises<br>635<br>16.20 Show that there are schedules that are possible under the two-phase locking<br>protocol, but are not possible under the timestamp protocol, and vice versa.<br>16.21 For each of the following protocols, describe aspects of practical applications<br>that would lead you to suggest using the protocol, and aspects that would<br>suggest not using the protocol:<br>• Two-phase locking<br>• Two-phase locking with multiple-granularity locking<br>• The tree protocol<br>• Timestamp ordering<br>• Validation<br>• Multiversion timestamp ordering<br>• Multiversion two-phase l</span><br><br><span style="background-color: #FFADAD;" title="Chunk 80 | Start: 1600160 | End: 1620160 | Tokens: 3158">ocking<br>16.22 Under a modiﬁed version of the timestamp protocol, we require that a commit<br>bit be tested to see whether a read request must wait. Explain how the com-<br>mit bit can prevent cascading abort. Why is this test not necessary for write<br>requests?<br>16.23 Explain why the following technique for transaction execution may provide<br>better performance than just using strict two-phase locking: First execute the<br>transaction without acquiring any locks and without performing any writes to<br>the database as in the validation based techniques, but unlike in the validation<br>techniques do not perform either validation or perform writes on the database.<br>Instead, rerun the transaction using strict two-phase locking. (Hint: Consider<br>waits for disk I/O.)<br>16.24 Under what conditions is it less expensive to avoid deadlock than to allow<br>deadlocks to occur and then to detect them?<br>16.25 If deadlock is avoided by deadlock avoidance schemes, is starvation still pos-<br>sible? Explain your answer.<br>16.26 Consider the timestamp ordering protocol, and two transactions, one that<br>writes two data items p and q, and another that reads the same two data items.<br>Give a schedule whereby the timestamp test for a write operation fails and<br>causes the ﬁrst transaction to be restarted, in turn causing a cascading abort of<br>the other transaction. Show how this could result in starvation of both transac-<br>tions. (Such a situation, where two or more processes carry out actions, but are<br>unable to complete their task because of interaction with the other processes,<br>is called a livelock.)<br>16.27 Explain the phantom phenomenon. Why may this phenomenon lead to an in-<br>correct concurrent execution despite the use of the two-phase locking protocol?<br>16.28 Devise a timestamp-based protocol that avoids the phantom phenomenon.<br>16.29 Explain the reason for the use of degree-two consistency. What disadvantages<br>does this approach have?<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>V. Transaction <br>Management<br>16. Concurrency Control<br>635<br>© The McGraw−Hill <br>Companies, 2001<br>636<br>Chapter 16<br>Concurrency Control<br>16.30 Suppose that we use the tree protocol of Section 16.1.5 to manage concurrent<br>access to a B+-tree. Since a split may occur on an insert that affects the root, it<br>appears that an insert operation cannot release any locks until it has completed<br>the entire operation. Under what circumstances is it possible to release a lock<br>earlier?<br>16.31 Give example schedules to show that if any of lookup, insert or delete do not<br>lock the next key value, the phantom phenomemon could go undetected.<br>Bibliographical Notes<br>Gray and Reuter [1993] provides detailed textbook coverage of transaction-processing<br>concepts, including concurrency control concepts and implementation details. Bern-<br>stein and Newcomer [1997] provides textbook coverage of various aspects of trans-<br>action processing including concurrency control.<br>Early textbook discussions of concurrency control and recovery included Papadim-<br>itriou [1986] and Bernstein et al. [1987]. An early survey paper on implementation<br>issues in concurrency control and recovery is presented by Gray [1978].<br>The two-phase locking protocol was introduced by Eswaran et al. [1976]. The tree-<br>locking protocol is from Silberschatz and Kedem [1980]. Other non-two-phase lock-<br>ing protocols that operate on more general graphs are described in Yannakakis et al.<br>[1979], Kedem and Silberschatz [1983], and Buckley and Silberschatz [1985]. Gen-<br>eral discussions concerning locking protocols are offered by Lien and Weinberger<br>[1978], Yannakakis et al. [1979], Yannakakis [1981], and Papadimitriou [1982]. Korth<br>[1983] explores various lock modes that can be obtained from the basic shared and<br>exclusive lock modes.<br>Exercise 16.6 is from Buckley and Silberschatz [1984]. Exercise 16.8 is from Kedem<br>and Silberschatz [1983]. Exercise 16.9 is from Kedem and Silberschatz [1979]. Exercise<br>16.10 is from Yannakakis et al. [1979]. Exercise 16.13 is from Korth [1983].<br>The timestamp-based concurrency-control scheme is from Reed [1983]. An expo-<br>sition of various timestamp-based concurrency-control algorithms is presented by<br>Bernstein and Goodman [1980]. A timestamp algorithm that does not require any<br>rollback to ensure serializability is presented by Buckley and Silberschatz [1983]. The<br>validation concurrency-control scheme is from Kung and Robinson [1981].<br>The locking protocol for multiple-granularity data items is from Gray et al. [1975].<br>A detailed description is presented by Gray et al. [1976]. The effects of locking granu-<br>larity are discussed by Ries and Stonebraker [1977]. Korth [1983] formalizes multiple-<br>granularity locking for an arbitrary collection of lock modes (allowing for more se-<br>mantics than simply read and write). This approach includes a class of lock modes<br>called update modes to deal with lock conversion. Carey [1983] extends the multiple-<br>granularity idea to timestamp-based concurrency control. An extension of the pro-<br>tocol to ensure deadlock freedom is presented by Korth [1982]. Multiple-granularity<br>locking for object-oriented database systems is discussed in Lee and Liou [1996].<br>Discussions concerning multiversion concurrency control are offered by Bernstein<br>et al. [1983]. A multiversion tree-locking algorithm appears in Silberschatz [1982].<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>V. Transaction <br>Management<br>16. Concurrency Control<br>636<br>© The McGraw−Hill <br>Companies, 2001<br>Bibliographical Notes<br>637<br>Multiversion timestamp order was introduced in Reed [1978] and Reed [1983]. Lai<br>and Wilkinson [1984] describes a multiversion two-phase locking certiﬁer.<br>Dijkstra [1965] was one of the ﬁrst and most inﬂuential contributors in the dead-<br>lock area. Holt [1971] and Holt [1972] were the ﬁrst to formalize the notion of dead-<br>locks in terms of a graph model similar to the one presented in this chapter. An anal-<br>ysis of the probability of waiting and deadlock is presented by Gray et al. [1981a].<br>Theoretical results concerning deadlocks and serializability are presented by Fussell<br>et al. [1981] and Yannakakis [1981]. Cycle-detection algorithms can be found in stan-<br>dard algorithm textbooks, such as Cormen et al. [1990].<br>Degree-two consistency was introduced in Gray et al. [1975]. The levels of consis-<br>tency—or isolation—offered in SQL are explained and critiqued in Berenson et al.<br>[1995].<br>Concurrency in B+-trees was studied by Bayer and Schkolnick [1977] and Johnson<br>and Shasha [1993]. The techniques presented in Section 16.9 are based on Kung and<br>Lehman [1980] and Lehman and Yao [1981]. The technique of key-value locking used<br>in ARIES provides for very high concurrency on B+-tree access, and is described in<br>Mohan [1990a] and Mohan and Levine [1992].<br>Shasha and Goodman [1988] presents a good characterization of concurrency pro-<br>tocols for index structures. Ellis [1987] presents a concurrency-control technique for<br>linear hashing. Lomet and Salzberg [1992] present some extensions of B-link trees.<br>Concurrency-control algorithms for other index structures appear in Ellis [1980a] and<br>Ellis [1980b].<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>V. Transaction <br>Management<br>17. Recovery System<br>637<br>© The McGraw−Hill <br>Companies, 2001<br>C<br>H<br>A<br>P<br>T<br>E<br>R<br>1<br>7<br>Recovery System<br>A computer system, like any other device, is subject to failure from a variety of<br>causes: disk crash, power outage, software error, a ﬁre in the machine room, even<br>sabotage. In any failure, information may be lost. Therefore, the database system<br>must take actions in advance to ensure that the atomicity and durability properties of<br>transactions, introduced in Chapter 15, are preserved. An integral part of a database<br>system is a recovery scheme that can restore the database to the consistent state that<br>existed before the failure. The recovery scheme must also provide high availability;<br>that is, it must minimize the time for which the database is not usable after a crash.<br>17.1<br>Failure Classiﬁcation<br>There are various types of failure that may occur in a system, each of which needs to<br>be dealt with in a different manner. The simplest type of failure is one that does not<br>result in the loss of information in the system. The failures that are more difﬁcult to<br>deal with are those that result in loss of information. In this chapter, we shall consider<br>only the following types of failure:<br>• Transaction failure. There are two types of errors that may cause a transaction<br>to fail:<br>  Logical error. The transaction can no longer continue with its normal ex-<br>ecution because of some internal condition, such as bad input, data not<br>found, overﬂow, or resource limit exceeded.<br>  System error. The system has entered an undesirable state (for example,<br>deadlock), as a result of which a transaction cannot continue with its nor-<br>mal execution. The transaction, however, can be reexecuted at a later time.<br>• System crash. There is a hardware malfunction, or a bug in the database soft-<br>ware or the operating system, that causes the loss of the content of volatile<br>639<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>V. Transaction <br>Management<br>17. Recovery System<br>638<br>© The McGraw−Hill <br>Companies, 2001<br>640<br>Chapter 17<br>Recovery System<br>storage, and brings transaction processing to a halt. The content of nonvolatile<br>storage remains intact, and is not corrupted.<br>The assumption that hardware errors and bugs in the software bring the<br>system to a halt, but do not corrupt the nonvolatile storage contents, is known<br>as the fail-stop assumption. Well-designed systems have numerous internal<br>checks, at the hardware and the software level, that bring the system to a halt<br>when there is an error. Hence, the fail-stop assumption is a reasonable one.<br>• Disk failure. A disk block loses its content as a result of either a head crash<br>or failure during a data transfer operation. Copies of the data on other disks,<br>or archival backups on tertiary media, such as tapes, are used to recover from<br>the failure.<br>To determine how the system should recover from failures, we need to identify<br>the failure modes of those devices used for storing data. Next, we must consider<br>how these failure modes affect the contents of the database. We can then propose<br>algorithms to ensure database consistency and transaction atomicity despite failures.<br>These algorithms, known as recovery algorithms, have two parts:<br>1. Actions taken during normal transaction processing to ensure that enough<br>information exists to allow recovery from failures.<br>2. Actions taken after a failure to recover the database contents to a state that<br>ensures database consistency, transaction atomicity, and durability.<br>17.2<br>Storage Structure<br>As we saw in Chapter 11, the various data items in the database may be stored and<br>accessed in a number of different storage media. To understand how to ensure the<br>atomicity and durability properties of a transaction, we must gain a better under-<br>standing of these storage media and their access methods.<br>17.2.1<br>Storage Types<br>In Chapter 11 we saw that storage media can be distinguished by their relative speed,<br>capacity, and resilience to failure, and classiﬁed as volatile storage or nonvolatile stor-<br>age. We review these terms, and introduce another class of storage, called stable stor-<br>age.<br>• Volatile storage. Information residing in volatile storage does not usually sur-<br>vive system crashes. Examples of such storage are main memory and cache<br>memory. Access to volatile storage is extremely fast, both because of the speed<br>of the memory access itself, and because it is possible to access any data item<br>in volatile storage directly.<br>• Nonvolatile storage. Information residing in nonvolatile storage survives sys-<br>tem crashes. Examples of such storage are disk and magnetic tapes. Disks are<br>used for online storage, whereas tapes are used for archival storage. Both,<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>V. Transaction <br>Management<br>17. Recovery System<br>639<br>© The McGraw−Hill <br>Companies, 2001<br>17.2<br>Storage Structure<br>641<br>however, are subject to failure (for example, head crash), which may result<br>in loss of information. At the current state of technology, nonvolatile stor-<br>age is slower than volatile storage by several orders of magnitude. This is<br>because disk and tape devices are electromechanical, rather than based en-<br>tirely on chips, as is volatile storage. In database systems, disks are used for<br>most nonvolatile storage. Other nonvolatile media are normally used only for<br>backup data. Flash storage (see Section 11.1), though nonvolatile, has insufﬁ-<br>cient capacity for most database systems.<br>• Stable storage. Information residing in stable storage is never lost (never should<br>be taken with a grain of salt, since theoretically never cannot be guaranteed—<br>for example, it is possible, although extremely unlikely, that a black hole may<br>envelop the earth and permanently destroy all data!). Although stable stor-<br>age is theoretically impossible to obtain, it can be closely approximated by<br>techniques that make data loss extremely unlikely. Section 17.2.2 discusses<br>stable-storage implementation.<br>The distinctions among the various storage types are often less clear in practice than<br>in our presentation. Certain systems provide battery backup, so that some main<br>memory can survive system crashes and power failures. Alternative forms of non-<br>volatile storage, such as optical media, provide an even higher degree of reliability<br>than do disks.<br>17.2.2<br>Stable-Storage Implementation<br>To implement stable storage, we need to replicate the needed information in sev-<br>eral nonvolatile storage media (usually disk) with independent failure modes, and<br>to update the information in a controlled manner to ensure that failure during data<br>transfer does not damage the needed information.<br>Recall (from Chapter 11) that RAID systems guarantee that the failure of a single<br>disk (even during data transfer) will not result in loss of data. The simplest and fastest<br>form of RAID is the mirrored disk, which keeps two copies of each block, on separate<br>disks. Other forms of RAID offer lower costs, but at the expense of lower performance.<br>RAID systems, however, cannot guard against data loss due to disasters such as<br>ﬁres or ﬂooding. Many systems store archival backups of tapes off-site to guard<br>against such disasters. However, since tapes cannot be carried off-site continually,<br>updates since the most recent time that tapes were carried off-site could be lost in<br>such a disaster. More secure systems keep a copy of each block of stable storage at a<br>remote site, writing it out over a computer network, in addition to storing the block<br>on a local disk system. Since the blocks are output to a remote system as and when<br>they are output to local storage, once an output operation is complete, the output is<br>not lost, even in the event of a disaster such as a ﬁre or ﬂood. We study such remote<br>backup systems in Section 17.10.<br>In the remainder of this section, we discuss how storage media can be protected<br>from failure during data transfer. Block transfer between memory and disk storage<br>can result in<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>V. Transaction <br>Management<br>17. Recovery System<br>640<br>© The McGraw−Hill <br>Companies, 2001<br>642<br>Chapter 17<br>Recovery System<br>• Successful completion. The transferred information arrived safely at its des-<br>tination.<br>• Partial failure. A failure occurred in the midst of transfer, and the destination<br>block has incorrect information.<br>• Total failure. The failure occurred sufﬁciently early during the transfer that<br>the destination block remains intact.<br>We require that, if a data-transfer failure occurs, the system detects it and invokes<br>a recovery procedure to restore the block to a consistent state. To do so, the system<br>must maintain two physical blocks for each logical database block; in the case of<br>mirrored disks, both blocks are at the same location; in the case of remote backup,<br>one of the blocks is local, whereas the other is at a remote site. An output operation<br>is executed as follows:<br>1. Write the information onto the ﬁrst physical block.<br>2. When the ﬁrst write completes successfully, write the same information onto<br>the second physical block.<br>3. The output is completed only after the second write completes successfully.<br>During recovery, the system examines each pair of physical blocks. If both are the<br>same and no detectable error exists, then no further actions are necessary. (Recall that<br>errors in a disk block, such as a partial write to the block, are detected by storing a<br>checksum with each block.) If the system detects an error in one block, then it replaces<br>its content with the content of the other block. If both blocks contain no detectable<br>error, but they differ in content, then the system replaces the content of the ﬁrst block<br>with the value of the second. This recovery procedure ensures that a write to stable<br>storage either succeeds completely (that is, updates all copies) or results in no change.<br>The requirement of comparing every corresponding pair of blocks during recovery<br>is expensive to meet. We can reduce the cost greatly by keeping track of block writes<br>that are in progress, using a small amount of nonvolatile RAM. On recovery, only<br>blocks for which writes were in progress need to be compared.<br>The protocols for writing out a block to a remote site are similar to the protocols<br>for writing blocks to a mirrored disk system, which we examined in Chapter 11, and<br>particularly in Exercise 11.4.<br>We can extend this procedure easily to allow the use of an arbitrarily large number<br>of copies of each block of stable storage. Although a large number of copies reduces<br>the probability of a failure to even lower than two copies do, it is usually reasonable<br>to simulate stable storage with only two copies.<br>17.2.3<br>Data Access<br>As we saw in Chapter 11, the database system resides permanently on nonvolatile<br>storage (usually disks), and is partitioned into ﬁxed-length storage units called blocks.<br>Blocks are the units of data transfer to and from disk, and may contain several data<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>V. Transaction <br>Management<br>17. Recovery System<br>641<br>© The McGraw−Hill <br>Companies, 2001<br>17.2<br>Storage Structure<br>643<br>A<br>B<br>input(A)<br>output(B)<br>B<br>main memory<br>disk<br>Figure 17.1<br>Block storage operations.<br>items. We shall assume that no data item spans two or more blocks. This assumption<br>is realistic for most data-processing applications, such as our banking example.<br>Transactions input information from the disk to main memory, and then output the<br>information back onto the disk. The input and output operations are done in block<br>units. The blocks residing on the disk are referred to as physical blocks; the blocks<br>residing temporarily in main memory are referred to as buffer blocks. The area of<br>memory where blocks reside temporarily is called the disk buffer.<br>Block movements between disk and main memory are initiated through the fol-<br>lowing two operations:<br>1. input(B) transfers the physical block B to main memory.<br>2. output(B) transfers the buffer block B to the disk, and replaces the appropriate<br>physical block there.<br>Figure 17.1 illustrates this scheme.<br>Each transaction Ti has a private work area in which copies of all the data items<br>accessed and updated by Ti are kept. The system creates this work area when the<br>transaction is initiated; the system removes it when the transaction either commits<br>or aborts. Each data item X kept in the work area of transaction Ti is denoted by xi.<br>Transaction Ti interacts with the database system by transferring data to and from its<br>work area to the system buffer. We transfer data by these two operations:<br>1. read(X) assigns the value of data item X to the local variable xi. It executes<br>this operation as follows:<br>a. If block BX on which X resides is not in main memory, it issues input(BX).<br>b. It assigns to xi the value of X </span><br><br><span style="background-color: #FFD6A5;" title="Chunk 81 | Start: 1620162 | End: 1640162 | Tokens: 3442">from the buffer block.<br>2. write(X) assigns the value of local variable xi to data item X in the buffer block.<br>It executes this operation as follows:<br>a. If block BX on which X resides is not in main memory, it issues input(BX).<br>b. It assigns the value of xi to X in buffer BX.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>V. Transaction <br>Management<br>17. Recovery System<br>642<br>© The McGraw−Hill <br>Companies, 2001<br>644<br>Chapter 17<br>Recovery System<br>Note that both operations may require the transfer of a block from disk to main mem-<br>ory. They do not, however, speciﬁcally require the transfer of a block from main mem-<br>ory to disk.<br>A buffer block is eventually written out to the disk either because the buffer man-<br>ager needs the memory space for other purposes or because the database system<br>wishes to reﬂect the change to B on the disk. We shall say that the database system<br>performs a force-output of buffer B if it issues an output(B).<br>When a transaction needs to access a data item X for the ﬁrst time, it must execute<br>read(X). The system then performs all updates to X on xi. After the transaction ac-<br>cesses X for the ﬁnal time, it must execute write(X) to reﬂect the change to X in the<br>database itself.<br>The output(BX) operation for the buffer block BX on which X resides does not<br>need to take effect immediately after write(X) is executed, since the block BX may<br>contain other data items that are still being accessed. Thus, the actual output may<br>take place later. Notice that, if the system crashes after the write(X) operation was<br>executed but before output(BX) was executed, the new value of X is never written to<br>disk and, thus, is lost.<br>17.3<br>Recovery and Atomicity<br>Consider again our simpliﬁed banking system and transaction Ti that transfers $50<br>from account A to account B, with initial values of A and B being $1000 and $2000,<br>respectively. Suppose that a system crash has occurred during the execution of Ti,<br>after output(BA) has taken place, but before output(BB) was executed, where BA and<br>BB denote the buffer blocks on which A and B reside. Since the memory contents<br>were lost, we do not know the fate of the transaction; thus, we could invoke one of<br>two possible recovery procedures:<br>• Reexecute Ti. This procedure will result in the value of A becoming $900,<br>rather than $950. Thus, the system enters an inconsistent state.<br>• Do not reexecute Ti. The current system state has values of $950 and $2000<br>for A and B, respectively. Thus, the system enters an inconsistent state.<br>In either case, the database is left in an inconsistent state, and thus this simple re-<br>covery scheme does not work. The reason for this difﬁculty is that we have modiﬁed<br>the database without having assurance that the transaction will indeed commit. Our<br>goal is to perform either all or no database modiﬁcations made by Ti. However, if<br>Ti performed multiple database modiﬁcations, several output operations may be re-<br>quired, and a failure may occur after some of these modiﬁcations have been made,<br>but before all of them are made.<br>To achieve our goal of atomicity, we must ﬁrst output information describing the<br>modiﬁcations to stable storage, without modifying the database itself. As we shall<br>see, this procedure will allow us to output all the modiﬁcations made by a commit-<br>ted transaction, despite failures. There are two ways to perform such outputs; we<br>study them in Sections 17.4 and 17.5. In these two sections, we shall assume that<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>V. Transaction <br>Management<br>17. Recovery System<br>643<br>© The McGraw−Hill <br>Companies, 2001<br>17.4<br>Log-Based Recovery<br>645<br>transactions are executed serially; in other words, only a single transaction is active at<br>a time. We shall describe how to handle concurrently executing transactions later, in<br>Section 17.6.<br>17.4<br>Log-Based Recovery<br>The most widely used structure for recording database modiﬁcations is the log. The<br>log is a sequence of log records, recording all the update activities in the database.<br>There are several types of log records. An update log record describes a single data-<br>base write. It has these ﬁelds:<br>• Transaction identiﬁer is the unique identiﬁer of the transaction that performed<br>the write operation.<br>• Data-item identiﬁer is the unique identiﬁer of the data item written. Typically,<br>it is the location on disk of the data item.<br>• Old value is the value of the data item prior to the write.<br>• New value is the value that the data item will have after the write.<br>Other special log records exist to record signiﬁcant events during transaction pro-<br>cessing, such as the start of a transaction and the commit or abort of a transaction.<br>We denote the various types of log records as:<br>• &lt;Ti start&gt;. Transaction Ti has started.<br>• &lt;Ti, Xj, V1, V2&gt;. Transaction Ti has performed a write on data item Xj. Xj<br>had value V1 before the write, and will have value V2 after the write.<br>• &lt;Ti commit&gt;. Transaction Ti has committed.<br>• &lt;Ti abort&gt;. Transaction Ti has aborted.<br>Whenever a transaction performs a write, it is essential that the log record for that<br>write be created before the database is modiﬁed. Once a log record exists, we can<br>output the modiﬁcation to the database if that is desirable. Also, we have the ability<br>to undo a modiﬁcation that has already been output to the database. We undo it by<br>using the old-value ﬁeld in log records.<br>For log records to be useful for recovery from system and disk failures, the log<br>must reside in stable storage. For now, we assume that every log record is written to<br>the end of the log on stable storage as soon as it is created. In Section 17.7, we shall<br>see when it is safe to relax this requirement so as to reduce the overhead imposed by<br>logging. In Sections 17.4.1 and 17.4.2, we shall introduce two techniques for using the<br>log to ensure transaction atomicity despite failures. Observe that the log contains a<br>complete record of all database activity. As a result, the volume of data stored in the<br>log may become unreasonably large. In Section 17.4.3, we shall show when it is safe<br>to erase log information.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>V. Transaction <br>Management<br>17. Recovery System<br>644<br>© The McGraw−Hill <br>Companies, 2001<br>646<br>Chapter 17<br>Recovery System<br>17.4.1<br>Deferred Database Modiﬁcation<br>The deferred-modiﬁcation technique ensures transaction atomicity by recording all<br>database modiﬁcations in the log, but deferring the execution of all write operations<br>of a transaction until the transaction partially commits. Recall that a transaction is<br>said to be partially committed once the ﬁnal action of the transaction has been ex-<br>ecuted. The version of the deferred-modiﬁcation technique that we describe in this<br>section assumes that transactions are executed serially.<br>When a transaction partially commits, the information on the log associated with<br>the transaction is used in executing the deferred writes. If the system crashes before<br>the transaction completes its execution, or if the transaction aborts, then the informa-<br>tion on the log is simply ignored.<br>The execution of transaction Ti proceeds as follows. Before Ti starts its execution,<br>a record &lt;Ti start&gt; is written to the log. A write(X) operation by Ti results in the<br>writing of a new record to the log. Finally, when Ti partially commits, a record &lt;Ti<br>commit&gt; is written to the log.<br>When transaction Ti partially commits, the records associated with it in the log are<br>used in executing the deferred writes. Since a failure may occur while this updating is<br>taking place, we must ensure that, before the start of these updates, all the log records<br>are written out to stable storage. Once they have been written, the actual updating<br>takes place, and the transaction enters the committed state.<br>Observe that only the new value of the data item is required by the deferred-<br>modiﬁcation technique. Thus, we can simplify the general update-log record struc-<br>ture that we saw in the previous section, by omitting the old-value ﬁeld.<br>To illustrate, reconsider our simpliﬁed banking system. Let T0 be a transaction that<br>transfers $50 from account A to account B:<br>T0: read(A);<br>A := A −50;<br>write(A);<br>read(B);<br>B := B + 50;<br>write(B).<br>Let T1 be a transaction that withdraws $100 from account C:<br>T1: read(C);<br>C := C −100;<br>write(C).<br>Suppose that these transactions are executed serially, in the order T0 followed by T1,<br>and that the values of accounts A, B, and C before the execution took place were<br>$1000, $2000, and $700, respectively. The portion of the log containing the relevant<br>information on these two transactions appears in Figure 17.2.<br>There are various orders in which the actual outputs can take place to both the<br>database system and the log as a result of the execution of T0 and T1. One such order<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>V. Transaction <br>Management<br>17. Recovery System<br>645<br>© The McGraw−Hill <br>Companies, 2001<br>17.4<br>Log-Based Recovery<br>647<br>&lt;T0  start&gt;<br>&lt;T0 , A, 950&gt;<br>&lt;T0 , B, 2050&gt;<br>&lt;T0  commit&gt;<br>&lt;T1  start&gt;<br>&lt;T1 , C, 600&gt;<br>&lt;T1  commit&gt;<br>Figure 17.2<br>Portion of the database log corresponding to T0 and T1.<br>appears in Figure 17.3. Note that the value of A is changed in the database only after<br>the record &lt;T0, A, 950&gt; has been placed in the log.<br>Using the log, the system can handle any failure that results in the loss of informa-<br>tion on volatile storage. The recovery scheme uses the following recovery procedure:<br>• redo(Ti) sets the value of all data items updated by transaction Ti to the new<br>values.<br>The set of data items updated by Ti and their respective new values can be found in<br>the log.<br>The redo operation must be idempotent; that is, executing it several times must be<br>equivalent to executing it once. This characteristic is required if we are to guarantee<br>correct behavior even if a failure occurs during the recovery process.<br>After a failure, the recovery subsystem consults the log to determine which trans-<br>actions need to be redone. Transaction Ti needs to be redone if and only if the log<br>contains both the record &lt;Ti start&gt; and the record &lt;Ti commit&gt;. Thus, if the system<br>crashes after the transaction completes its execution, the recovery scheme uses the<br>information in the log to restore the system to a previous consistent state after the<br>transaction had completed.<br>As an illustration, let us return to our banking example with transactions T0 and<br>T1 executed one after the other in the order T0 followed by T1. Figure 17.2 shows the<br>log that results from the complete execution of T0 and T1. Let us suppose that the<br>Log<br>Database<br>A = 950<br>B = 2050<br>C = 600<br>&lt;T0  start&gt;<br>&lt;T0 ,  A,  950&gt;<br>&lt;T0 ,  B,  2050&gt;<br>&lt;T0  commit&gt;<br>&lt;T1  start&gt;<br>&lt;T1 ,  C,  600&gt;<br>&lt;T1  commit&gt;<br>Figure 17.3<br>State of the log and database corresponding to T0 and T1.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>V. Transaction <br>Management<br>17. Recovery System<br>646<br>© The McGraw−Hill <br>Companies, 2001<br>648<br>Chapter 17<br>Recovery System<br>&lt;T0  start&gt;<br>&lt;T0 ,  A,  950&gt;<br>&lt;T0 ,  B,  2050&gt;<br>&lt;T0  start&gt;<br>&lt;T0 ,  A,  950&gt;<br>&lt;T0 ,  B,  2050&gt;<br>&lt;T0  commit&gt;<br>&lt;T1  start&gt;<br>&lt;T1 ,  C,  600&gt;<br>&lt;T0  start&gt;<br>&lt;T0 ,  A,  950&gt;<br>&lt;T0 ,  B,  2050&gt;<br>&lt;T0  commit&gt;<br>&lt;T1  start&gt;<br>&lt;T1 ,  C,  600&gt;<br>&lt;T1  commit&gt;<br>(a)<br>(b)<br>(c)<br>Figure 17.4<br>The same log as that in Figure 17.3, shown at three different times.<br>system crashes before the completion of the transactions, so that we can see how the<br>recovery technique restores the database to a consistent state. Assume that the crash<br>occurs just after the log record for the step<br>write(B)<br>of transaction T0 has been written to stable storage. The log at the time of the crash<br>appears in Figure 17.4a. When the system comes back up, no redo actions need to<br>be taken, since no commit record appears in the log. The values of accounts A and B<br>remain $1000 and $2000, respectively. The log records of the incomplete transaction<br>T0 can be deleted from the log.<br>Now, let us assume the crash comes just after the log record for the step<br>write(C)<br>of transaction T1 has been written to stable storage. In this case, the log at the time<br>of the crash is as in Figure 17.4b. When the system comes back up, the operation<br>redo(T0) is performed, since the record<br>&lt;T0 commit&gt;<br>appears in the log on the disk. After this operation is executed, the values of accounts<br>A and B are $950 and $2050, respectively. The value of account C remains $700. As<br>before, the log records of the incomplete transaction T1 can be deleted from the log.<br>Finally, assume that a crash occurs just after the log record<br>&lt;T1 commit&gt;<br>is written to stable storage. The log at the time of this crash is as in Figure 17.4c. When<br>the system comes back up, two commit records are in the log: one for T0 and one<br>for T1. Therefore, the system must perform operations redo(T0) and redo(T1), in the<br>order in which their commit records appear in the log. After the system executes these<br>operations, the values of accounts A, B, and C are $950, $2050, and $600, respectively.<br>Finally, let us consider a case in which a second system crash occurs during re-<br>covery from the ﬁrst crash. Some changes may have been made to the database as a<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>V. Transaction <br>Management<br>17. Recovery System<br>647<br>© The McGraw−Hill <br>Companies, 2001<br>17.4<br>Log-Based Recovery<br>649<br>result of the redo operations, but all changes may not have been made. When the sys-<br>tem comes up after the second crash, recovery proceeds exactly as in the preceding<br>examples. For each commit record<br>&lt;Ti commit&gt;<br>found in the log, the the system performs the operation redo(Ti). In other words,<br>it restarts the recovery actions from the beginning. Since redo writes values to the<br>database independent of the values currently in the database, the result of a success-<br>ful second attempt at redo is the same as though redo had succeeded the ﬁrst time.<br>17.4.2<br>Immediate Database Modiﬁcation<br>The immediate-modiﬁcation technique allows database modiﬁcations to be output<br>to the database while the transaction is still in the active state. Data modiﬁcations<br>written by active transactions are called uncommitted modiﬁcations. In the event<br>of a crash or a transaction failure, the system must use the old-value ﬁeld of the<br>log records described in Section 17.4 to restore the modiﬁed data items to the value<br>they had prior to the start of the transaction. The undo operation, described next,<br>accomplishes this restoration.<br>Before a transaction Ti starts its execution, the system writes the record &lt;Ti start&gt;<br>to the log. During its execution, any write(X) operation by Ti is preceded by the writ-<br>ing of the appropriate new update record to the log. When Ti partially commits, the<br>system writes the record &lt;Ti commit&gt; to the log.<br>Since the information in the log is used in reconstructing the state of the database,<br>we cannot allow the actual update to the database to take place before the corre-<br>sponding log record is written out to stable storage. We therefore require that, before<br>execution of an output(B) operation, the log records corresponding to B be written<br>onto stable storage. We shall return to this issue in Section 17.7.<br>As an illustration, let us reconsider our simpliﬁed banking system, with transac-<br>tions T0 and T1 executed one after the other in the order T0 followed by T1. The por-<br>tion of the log containing the relevant information concerning these two transactions<br>appears in Figure 17.5.<br>Figure 17.6 shows one possible order in which the actual outputs took place in both<br>the database system and the log as a result of the execution of T0 and T1. Notice that<br>&lt;T0  start&gt;<br>&lt;T0 ,  A,  1000,  950&gt;<br>&lt;T0 ,  B,  2000,  2050&gt;<br>&lt;T0  commit&gt;<br>&lt;T1  start&gt;<br>&lt;T1 ,  C,  700,  600&gt;<br>&lt;T1  commit&gt;<br>Figure 17.5<br>Portion of the system log corresponding to T0 and T1.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>V. Transaction <br>Management<br>17. Recovery System<br>648<br>© The McGraw−Hill <br>Companies, 2001<br>650<br>Chapter 17<br>Recovery System<br>Log<br>Database<br>A = 950<br>B = 2050<br>C = 600<br>&lt;T0  start&gt;<br>&lt;T0 ,  A,  1000,  950&gt;<br>&lt;T0 ,  B,  2000,  2050&gt;<br>&lt;T0  commit&gt;<br>&lt;T1  start&gt;<br>&lt;T1 ,  C,  700,  600&gt;<br>&lt;T1  commit&gt;<br>Figure 17.6<br>State of system log and database corresponding to T0 and T1.<br>this order could not be obtained in the deferred-modiﬁcation technique of Section<br>17.4.1.<br>Using the log, the system can handle any failure that does not result in the loss<br>of information in nonvolatile storage. The recovery scheme uses two recovery proce-<br>dures:<br>• undo(Ti) restores the value of all data items updated by transaction Ti to the<br>old values.<br>• redo(Ti) sets the value of all data items updated by transaction Ti to the new<br>values.<br>The set of data items updated by Ti and their respective old and new values can be<br>found in the log.<br>The undo and redo operations must be idempotent to guarantee correct behavior<br>even if a failure occurs during the recovery process.<br>After a failure has occurred, the recovery scheme consults the log to determine<br>which transactions need to be redone, and which need to be undone:<br>• Transaction Ti needs to be undone if the log contains the record &lt;Ti start&gt;,<br>but does not contain the record &lt;Ti commit&gt;.<br>• Transaction Ti needs to be redone if the log contains both the record &lt;Ti start&gt;<br>and the record &lt;Ti commit&gt;.<br>As an illustration, return to our banking example, with transaction T0 and T1 ex-<br>ecuted one after the other in the order T0 followed by T1. Suppose that the system<br>crashes before the completion of the transactions. We shall consider three cases. The<br>state of the logs for each of these cases appears in Figure 17.7.<br>First, let us assume that the crash occurs just after the log record for the step<br>write(B)<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>V. Transaction <br>Management<br>17. Recovery System<br>649<br>© The McGraw−Hill <br>Companies, 2001<br>17.4<br>Log-Based Recovery<br>651<br>&lt;T0  start&gt;<br>&lt;T0 ,  A,  1000,  950&gt;<br>&lt;T0 ,  B,  2000,  2050&gt;<br>&lt;T0  start&gt;<br>&lt;T0 ,  A,  1000,  950&gt;<br>&lt;T0 ,  B,  2000,  2050&gt;<br>&lt;T0  commit&gt;<br>&lt;T1  start&gt;<br>&lt;T1 ,  C,  700,  600&gt;<br>&lt;T0  start&gt;<br>&lt;T0 ,  A,  1000,  950&gt;<br>&lt;T0 ,  B,  2000,  2050&gt;<br>&lt;T0  commit&gt;<br>&lt;T1  start&gt;<br>&lt;T1 ,  C,  700,  600&gt;<br>&lt;T1  commit&gt;<br>(a)<br>(b)<br>(c)<br>Figure 17.7<br>The same log, shown at three different times.<br>of transaction T0 has been written to stable storage (Figure 17.7a). When the system<br>comes back up, it ﬁnds the record &lt;T0 start&gt; in the log, but no corresponding &lt;T0<br>commit&gt; record. Thus, transaction T0 must be undone, so an undo(T0) is performed.<br>As a result, the values in accounts A and B (on the disk) are restored to $1000 and<br>$2000, respectively.<br>Next, let us assume that the crash comes just after the log record for the step<br>write(C)<br>of transaction T1 has been written to stable storage (Figure 17.7b). When the system<br>comes back up, two recovery actions need to be taken. The operation undo(T1) must<br>be performed, since the record &lt;T1 start&gt; appears in the log, but there is no record<br>&lt;T1 commit&gt;. The operation redo(T0) must be performed, since the log contains both<br>the record &lt;T0 start&gt; and the record &lt;T0 commit&gt;. At the end of the entire recovery<br>procedure, the values of accounts A, B, and C are $950, $2050, and $700, respectively.<br>Note that the undo(T1) operation is performed before the redo(T0). In this example,<br>the same outcome would result if the order were reversed. However, the order of<br>doing undo operations ﬁrst, and then redo operations, is important for the recovery<br>algorithm that we shall see in Section 17.6.<br>Finally, let us assume that the crash occurs just after the log record<br>&lt;T1 commit&gt;<br>has been written to stable storage (Figure 17.7c). When the system comes back up,<br>both T0 and T1 need to be redone, since the records &lt;T0 start&gt; and &lt;T0 commit&gt;<br>appear in the log, as do the records &lt;T1 start&gt; and &lt;T1 commit&gt;. After the system<br>performs the recovery procedures redo(T0) and redo(T1), the values in accounts A, B,<br>and C are $950, $2050, and $600, respectively.<br>17.4.3<br>Checkpoints<br>When a system failure occurs, we must consult the log to determine those transac-</span><br><br><span style="background-color: #FDFFB6;" title="Chunk 82 | Start: 1640164 | End: 1660164 | Tokens: 3381"><br>tions that need to be redone and those that need to be undone. In principle, we need<br>to search the entire log to determine this information. There are two major difﬁculties<br>with this approach:<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>V. Transaction <br>Management<br>17. Recovery System<br>650<br>© The McGraw−Hill <br>Companies, 2001<br>652<br>Chapter 17<br>Recovery System<br>1. The search process is time consuming.<br>2. Most of the transactions that, according to our algorithm, need to be redone<br>have already written their updates into the database. Although redoing them<br>will cause no harm, it will nevertheless cause recovery to take longer.<br>To reduce these types of overhead, we introduce checkpoints. During execution, the<br>system maintains the log, using one of the two techniques described in Sections 17.4.1<br>and 17.4.2. In addition, the system periodically performs checkpoints, which require<br>the following sequence of actions to take place:<br>1. Output onto stable storage all log records currently residing in main memory.<br>2. Output to the disk all modiﬁed buffer blocks.<br>3. Output onto stable storage a log record &lt;checkpoint&gt;.<br>Transactions are not allowed to perform any update actions, such as writing to a<br>buffer block or writing a log record, while a checkpoint is in progress.<br>The presence of a &lt;checkpoint&gt; record in the log allows the system to streamline<br>its recovery procedure. Consider a transaction Ti that committed prior to the check-<br>point. For such a transaction, the &lt;Ti commit&gt; record appears in the log before the<br>&lt;checkpoint&gt; record. Any database modiﬁcations made by Ti must have been writ-<br>ten to the database either prior to the checkpoint or as part of the checkpoint itself.<br>Thus, at recovery time, there is no need to perform a redo operation on Ti.<br>This observation allows us to reﬁne our previous recovery schemes. (We continue<br>to assume that transactions are run serially.) After a failure has occurred, the recov-<br>ery scheme examines the log to determine the most recent transaction Ti that started<br>executing before the most recent checkpoint took place. It can ﬁnd such a transac-<br>tion by searching the log backward, from the end of the log, until it ﬁnds the ﬁrst<br>&lt;checkpoint&gt; record (since we are searching backward, the record found is the ﬁnal<br>&lt;checkpoint&gt; record in the log); then it continues the search backward until it ﬁnds<br>the next &lt;Ti start&gt; record. This record identiﬁes a transaction Ti.<br>Once the system has identiﬁed transaction Ti, the redo and undo operations need<br>to be applied to only transaction Ti and all transactions Tj that started executing<br>after transaction Ti. Let us denote these transactions by the set T. The remainder<br>(earlier part) of the log can be ignored, and can be erased whenever desired. The<br>exact recovery operations to be performed depend on the modiﬁcation technique<br>being used. For the immediate-modiﬁcation technique, the recovery operations are:<br>• For all transactions Tk in T that have no &lt;Tk commit&gt; record in the log, exe-<br>cute undo(Tk).<br>• For all transactions Tk in T such that the record &lt;Tk commit&gt; appears in the<br>log, execute redo(Tk).<br>Obviously, the undo operation does not need to be applied when the deferred-modiﬁ-<br>cation technique is being employed.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>V. Transaction <br>Management<br>17. Recovery System<br>651<br>© The McGraw−Hill <br>Companies, 2001<br>17.5<br>Shadow Paging<br>653<br>As an illustration, consider the set of transactions {T0, T1, . . ., T100} executed in the<br>order of the subscripts. Suppose that the most recent checkpoint took place during<br>the execution of transaction T67. Thus, only transactions T67, T68, . . ., T100 need to be<br>considered during the recovery scheme. Each of them needs to be redone if it has<br>committed; otherwise, it needs to be undone.<br>In Section 17.6.3, we consider an extension of the checkpoint technique for concur-<br>rent transaction processing.<br>17.5<br>Shadow Paging<br>An alternative to log-based crash-recovery techniques is shadow paging. The<br>shadow-paging technique is essentially an improvement on the shadow-copy tech-<br>nique that we saw in Section 15.3. Under certain circumstances, shadow paging may<br>require fewer disk accesses than do the log-based methods discussed previously.<br>There are, however, disadvantages to the shadow-paging approach, as we shall see,<br>that limit its use. For example, it is hard to extend shadow paging to allow multiple<br>transactions to execute concurrently.<br>As before, the database is partitioned into some number of ﬁxed-length blocks,<br>which are referred to as pages. The term page is borrowed from operating systems,<br>since we are using a paging scheme for memory management. Assume that there are<br>n pages, numbered 1 through n. (In practice, n may be in the hundreds of thousands.)<br>These pages do not need to be stored in any particular order on disk (there are many<br>reasons why they do not, as we saw in Chapter 11). However, there must be a way to<br>ﬁnd the ith page of the database for any given i. We use a page table, as in Figure 17.8,<br>for this purpose. The page table has n entries—one for each database page. Each<br>entry contains a pointer to a page on disk. The ﬁrst entry contains a pointer to the<br>ﬁrst page of the database, the second entry points to the second page, and so on. The<br>example in Figure 17.8 shows that the logical order of database pages does not need<br>to correspond to the physical order in which the pages are placed on disk.<br>The key idea behind the shadow-paging technique is to maintain two page tables<br>during the life of a transaction: the current page table and the shadow page table.<br>When the transaction starts, both page tables are identical. The shadow page table is<br>never changed over the duration of the transaction. The current page table may be<br>changed when a transaction performs a write operation. All input and output opera-<br>tions use the current page table to locate database pages on disk.<br>Suppose that the transaction Tj performs a write(X) operation, and that X resides<br>on the ith page. The system executes the write operation as follows:<br>1. If the ith page (that is, the page on which X resides) is not already in main<br>memory, then the system issues input(X).<br>2. If this is the write ﬁrst performed on the ith page by this transaction, then the<br>system modiﬁes the current page table as follows:<br>a. It ﬁnds an unused page on disk. Usually, the database system has access<br>to a list of unused (free) pages, as we saw in Chapter 11.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>V. Transaction <br>Management<br>17. Recovery System<br>652<br>© The McGraw−Hill <br>Companies, 2001<br>654<br>Chapter 17<br>Recovery System<br> page table<br>pages on disk<br>…<br>…<br>…<br>1<br>2<br>3<br>4<br>5<br>6<br>7<br>n<br>Figure 17.8<br>Sample page table.<br>b. It deletes the page found in step 2a from the list of free page frames; it<br>copies the contents of the ith page to the page found in step 2a.<br>c. It modiﬁes the current page table so that the ith entry points to the page<br>found in step 2a.<br>3. It assigns the value of xj to X in the buffer page.<br>Compare this action for a write operation with that described in Section 17.2.3 The<br>only difference is that we have added a new step. Steps 1 and 3 here correspond<br>to steps 1 and 2 in Section 17.2.3. The added step, step 2, manipulates the current<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>V. Transaction <br>Management<br>17. Recovery System<br>653<br>© The McGraw−Hill <br>Companies, 2001<br>17.5<br>Shadow Paging<br>655<br>1<br>2<br>3<br>4<br>5<br>6<br>7<br>8<br>9<br>10<br>shadow page table<br>current page table<br>1<br>2<br>3<br>4<br>5<br>6<br>7<br>8<br>9<br>10<br>pages on disk<br>Figure 17.9<br>Shadow and current page tables.<br>page table. Figure 17.9 shows the shadow and current page tables for a transaction<br>performing a write to the fourth page of a database consisting of 10 pages.<br>Intuitively, the shadow-page approach to recovery is to store the shadow page ta-<br>ble in nonvolatile storage, so that the state of the database prior to the execution of<br>the transaction can be recovered in the event of a crash, or transaction abort. When<br>the transaction commits, the system writes the current page table to nonvolatile stor-<br>age. The current page table then becomes the new shadow page table, and the next<br>transaction is allowed to begin execution. It is important that the shadow page table<br>be stored in nonvolatile storage, since it provides the only means of locating database<br>pages. The current page table may be kept in main memory (volatile storage). We do<br>not care whether the current page table is lost in a crash, since the system recovers by<br>using the shadow page table.<br>Successful recovery requires that we ﬁnd the shadow page table on disk after a<br>crash. A simple way of ﬁnding it is to choose one ﬁxed location in stable storage that<br>contains the disk address of the shadow page table. When the system comes back<br>up after a crash, it copies the shadow page table into main memory and uses it for<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>V. Transaction <br>Management<br>17. Recovery System<br>654<br>© The McGraw−Hill <br>Companies, 2001<br>656<br>Chapter 17<br>Recovery System<br>subsequent transaction processing. Because of our deﬁnition of the write operation,<br>we are guaranteed that the shadow page table will point to the database pages cor-<br>responding to the state of the database prior to any transaction that was active at the<br>time of the crash. Thus, aborts are automatic. Unlike our log-based schemes, shadow<br>paging needs to invoke no undo operations.<br>To commit a transaction, we must do the following:<br>1. Ensure that all buffer pages in main memory that have been changed by the<br>transaction are output to disk. (Note that these output operations will not<br>change database pages pointed to by some entry in the shadow page table.)<br>2. Output the current page table to disk. Note that we must not overwrite the<br>shadow page table, since we may need it for recovery from a crash.<br>3. Output the disk address of the current page table to the ﬁxed location in sta-<br>ble storage containing the address of the shadow page table. This action over-<br>writes the address of the old shadow page table. Therefore, the current page<br>table has become the shadow page table, and the transaction is committed.<br>If a crash occurs prior to the completion of step 3, we revert to the state just prior to<br>the execution of the transaction. If the crash occurs after the completion of step 3, the<br>effects of the transaction will be preserved; no redo operations need to be invoked.<br>Shadow paging offers several advantages over log-based techniques. The over-<br>head of log-record output is eliminated, and recovery from crashes is signiﬁcantly<br>faster (since no undo or redo operations are needed). However, there are drawbacks<br>to the shadow-page technique:<br>• Commit overhead. The commit of a single transaction using shadow paging<br>requires multiple blocks to be output—the actual data blocks, the current page<br>table, and the disk address of the current page table. Log-based schemes need<br>to output only the log records, which, for typical small transactions, ﬁt within<br>one block.<br>The overhead of writing an entire page table can be reduced by implement-<br>ing the page table as a tree structure, with page table entries at the leaves. We<br>outline the idea below, and leave it to the reader to ﬁll in missing details. The<br>nodes of the tree are pages and have a high fanout, like B+-trees. The current<br>page table’s tree is initially the same as the shadow page table’s tree. When a<br>page is to be updated for the ﬁrst time, the system changes the entry in the cur-<br>rent page table to point to the copy of the page. If the leaf page containing the<br>entry has been copied already, the system directly updates it. Otherwise, the<br>system ﬁrst copies it, and updates the copy. In turn, the parent of the copied<br>page needs to be updated to point to the new copy, which the system does<br>by applying the same procedure to its parent, copying it if it was not already<br>copied. The process of copying proceeds up to the root of the tree. Changes<br>are made only to the copied nodes, so the shadow page table’s tree does not<br>get modiﬁed.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>V. Transaction <br>Management<br>17. Recovery System<br>655<br>© The McGraw−Hill <br>Companies, 2001<br>17.6<br>Recovery with Concurrent Transactions<br>657<br>The beneﬁt of the tree representation is that the only pages that need to be<br>copied are the leaf pages that are updated, and all their ancestors in the tree.<br>All the other parts of the tree are shared between the shadow and the current<br>page table, and do not need to be copied. The reduction in copying costs can be<br>very signiﬁcant for large databases. However, several pages of the page table<br>still need to copied for each transaction, and the log-based schemes continue<br>to be superior as long as most transactions update only small parts of the<br>database.<br>• Data fragmentation. In Chapter 11, we considered strategies to ensure locality<br>—that is, to keep related database pages close physically on the disk. Local-<br>ity allows for faster data transfer. Shadow paging causes database pages to<br>change location when they are updated. As a result, either we lose the locality<br>property of the pages or we must resort to more complex, higher-overhead<br>schemes for physical storage management. (See the bibliographical notes for<br>references.)<br>• Garbage collection. Each time that a transaction commits, the database pages<br>containing the old version of data changed by the transaction become inac-<br>cessible. In Figure 17.9, the page pointed to by the fourth entry of the shadow<br>page table will become inaccessible once the transaction of that example com-<br>mits. Such pages are considered garbage, since they are not part of free space<br>and do not contain usable information. Garbage may be created also as a side<br>effect of crashes. Periodically, it is necessary to ﬁnd all the garbage pages, and<br>to add them to the list of free pages. This process, called garbage collection,<br>imposes additional overhead and complexity on the system. There are several<br>standard algorithms for garbage collection. (See the bibliographical notes for<br>references.)<br>In addition to the drawbacks of shadow paging just mentioned, shadow paging is<br>more difﬁcult than logging to adapt to systems that allow several transactions to exe-<br>cute concurrently. In such systems, some logging is usually required, even if shadow<br>paging is used. The System R prototype, for example, used a combination of shadow<br>paging and a logging scheme similar to that presented in Section 17.4.2. It is relatively<br>easy to extend the log-based recovery schemes to allow concurrent transactions, as<br>we shall see in Section 17.6. For these reasons, shadow paging is not widely used.<br>17.6<br>Recovery with Concurrent Transactions<br>Until now, we considered recovery in an environment where only a single trans-<br>action at a time is executing. We now discuss how we can modify and extend the<br>log-based recovery scheme to deal with multiple concurrent transactions. Regardless<br>of the number of concurrent transactions, the system has a single disk buffer and a<br>single log. All transactions share the buffer blocks. We allow immediate modiﬁcation,<br>and permit a buffer block to have data items updated by one or more transactions.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>V. Transaction <br>Management<br>17. Recovery System<br>656<br>© The McGraw−Hill <br>Companies, 2001<br>658<br>Chapter 17<br>Recovery System<br>17.6.1<br>Interaction with Concurrency Control<br>The recovery scheme depends greatly on the concurrency-control scheme that is<br>used. To roll back a failed transaction, we must undo the updates performed by the<br>transaction. Suppose that a transaction T0 has to be rolled back, and a data item Q that<br>was updated by T0 has to be restored to its old value. Using the log-based schemes<br>for recovery, we restore the value by using the undo information in a log record. Sup-<br>pose now that a second transaction T1 has performed yet another update on Q before<br>T0 is rolled back. Then, the update performed by T1 will be lost if T0 is rolled back.<br>Therefore, we require that, if a transaction T has updated a data item Q, no other<br>transaction may update the same data item until T has committed or been rolled<br>back. We can ensure this requirement easily by using strict two-phase locking—that<br>is, two-phase locking with exclusive locks held until the end of the transaction.<br>17.6.2<br>Transaction Rollback<br>We roll back a failed transaction, Ti, by using the log. The system scans the log back-<br>ward; for every log record of the form &lt;Ti, Xj, V1, V2&gt; found in the log, the system<br>restores the data item Xj to its old value V1. Scanning of the log terminates when the<br>log record &lt;Ti, start&gt; is found.<br>Scanning the log backward is important, since a transaction may have updated a<br>data item more than once. As an illustration, consider the pair of log records<br>&lt;Ti, A, 10, 20&gt;<br>&lt;Ti, A, 20, 30&gt;<br>The log records represent a modiﬁcation of data item A by Ti, followed by another<br>modiﬁcation of A by Ti. Scanning the log backward sets A correctly to 10. If the log<br>were scanned in the forward direction, A would be set to 20, which is incorrect.<br>If strict two-phase locking is used for concurrency control, locks held by a transac-<br>tion T may be released only after the transaction has been rolled back as described.<br>Once transaction T (that is being rolled back) has updated a data item, no other trans-<br>action could have updated the same data item, because of the concurrency-control<br>requirements mentioned in Section 17.6.1. Therefore, restoring the old value of the<br>data item will not erase the effects of any other transaction.<br>17.6.3<br>Checkpoints<br>In Section 17.4.3, we used checkpoints to reduce the number of log records that the<br>system must scan when it recovers from a crash. Since we assumed no concurrency,<br>it was necessary to consider only the following transactions during recovery:<br>• Those transactions that started after the most recent checkpoint<br>• The one transaction, if any, that was active at the time of the most recent check-<br>point<br>The situation is more complex when transactions can execute concurrently, since sev-<br>eral transactions may have been active at the time of the most recent checkpoint.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>V. Transaction <br>Management<br>17. Recovery System<br>657<br>© The McGraw−Hill <br>Companies, 2001<br>17.6<br>Recovery with Concurrent Transactions<br>659<br>In a concurrent transaction-processing system, we require that the checkpoint log<br>record be of the form &lt;checkpoint L&gt;, where L is a list of transactions active at the<br>time of the checkpoint. Again, we assume that transactions do not perform updates<br>either on the buffer blocks or on the log while the checkpoint is in progress.<br>The requirement that transactions must not perform any updates to buffer blocks<br>or to the log during checkpointing can be bothersome, since transaction processing<br>will have to halt while a checkpoint is in progress. A fuzzy checkpoint is a check-<br>point where transactions are allowed to perform updates even while buffer blocks<br>are being written out. Section 17.9.5 describes fuzzy checkpointing schemes.<br>17.6.4<br>Restart Recovery<br>When the system recovers from a crash, it constructs two lists: The undo-list consists<br>of transactions to be undone, and the redo-list consists of transactions to be redone.<br>The system constructs the two lists as follows: Initially, they are both empty.<br>The system scans the log backward, examining each record, until it ﬁnds the ﬁrst<br>&lt;checkpoint&gt; record:<br>• For each record found of the form &lt;Ti commit&gt;, it adds Ti to redo-list.<br>• For each record found of the form &lt;Ti start&gt;, if Ti is not in redo-list, then it<br>adds Ti to undo-list.<br>When the system has examined all the appropriate log records, it checks the list L in<br>the checkpoint record. For each transaction Ti in L, if Ti is not in redo-list then it adds<br>Ti to the undo-list.<br>Once the redo-list and undo-list have</span><br><br><span style="background-color: #CAFFBF;" title="Chunk 83 | Start: 1660166 | End: 1680166 | Tokens: 3315"> have been constructed, the recovery pro-<br>ceeds as follows:<br>1. The system rescans the log from the most recent record backward, and per-<br>forms an undo for each log record that belongs transaction Ti on the undo-list.<br>Log records of transactions on the redo-list are ignored in this phase. The scan<br>stops when the &lt;Ti start&gt; records have been found for every transaction Ti<br>in the undo-list.<br>2. The system locates the most recent &lt;checkpoint L&gt; record on the log. Notice<br>that this step may involve scanning the log forward, if the checkpoint record<br>was passed in step 1.<br>3. The system scans the log forward from the most recent &lt;checkpoint L&gt; record,<br>and performs redo for each log record that belongs to a transaction Ti that is<br>on the redo-list. It ignores log records of transactions on the undo-list in this<br>phase.<br>It is important in step 1 to process the log backward, to ensure that the resulting<br>state of the database is correct.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>V. Transaction <br>Management<br>17. Recovery System<br>658<br>© The McGraw−Hill <br>Companies, 2001<br>660<br>Chapter 17<br>Recovery System<br>After the system has undone all transactions on the undo-list, it redoes those trans-<br>actions on the redo-list. It is important, in this case, to process the log forward. When<br>the recovery process has completed, transaction processing resumes.<br>It is important to undo the transaction in the undo-list before redoing transactions<br>in the redo-list, using the algorithm in steps 1 to 3; otherwise, a problem may occur.<br>Suppose that data item A initially has the value 10. Suppose that a transaction Ti<br>updated data item A to 20 and aborted; transaction rollback would restore A to the<br>value 10. Suppose that another transaction Tj then updated data item A to 30 and<br>committed, following which the system crashed. The state of the log at the time of<br>the crash is<br>&lt;Ti, A, 10, 20&gt;<br>&lt;Tj, A, 10, 30&gt;<br>&lt;Tj commit&gt;<br>If the redo pass is performed ﬁrst, A will be set to 30; then, in the undo pass, A will<br>be set to 10, which is wrong. The ﬁnal value of Q should be 30, which we can ensure<br>by performing undo before performing redo.<br>17.7<br>Buffer Management<br>In this section, we consider several subtle details that are essential to the implementa-<br>tion of a crash-recovery scheme that ensures data consistency and imposes a minimal<br>amount of overhead on interactions with the database.<br>17.7.1<br>Log-Record Buffering<br>So far, we have assumed that every log record is output to stable storage at the time it<br>is created. This assumption imposes a high overhead on system execution for several<br>reasons: Typically, output to stable storage is in units of blocks. In most cases, a log<br>record is much smaller than a block. Thus, the output of each log record translates to<br>a much larger output at the physical level. Furthermore, as we saw in Section 17.2.2,<br>the output of a block to stable storage may involve several output operations at the<br>physical level.<br>The cost of performing the output of a block to stable storage is sufﬁciently high<br>that it is desirable to output multiple log records at once. To do so, we write log<br>records to a log buffer in main memory, where they stay temporarily until they are<br>output to stable storage. Multiple log records can be gathered in the log buffer, and<br>output to stable storage in a single output operation. The order of log records in the<br>stable storage must be exactly the same as the order in which they were written to<br>the log buffer.<br>As a result of log buffering, a log record may reside in only main memory (volatile<br>storage) for a considerable time before it is output to stable storage. Since such log<br>records are lost if the system crashes, we must impose additional requirements on<br>the recovery techniques to ensure transaction atomicity:<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>V. Transaction <br>Management<br>17. Recovery System<br>659<br>© The McGraw−Hill <br>Companies, 2001<br>17.7<br>Buffer Management<br>661<br>• Transaction Ti enters the commit state after the &lt;Ti commit&gt; log record has<br>been output to stable storage.<br>• Before the &lt;Ti commit&gt; log record can be output to stable storage, all log<br>records pertaining to transaction Ti must have been output to stable storage.<br>• Before a block of data in main memory can be output to the database (in non-<br>volatile storage), all log records pertaining to data in that block must have<br>been output to stable storage.<br>This rule is called the write-ahead logging (WAL) rule. (Strictly speaking,<br>the WAL rule requires only that the undo information in the log have been<br>output to stable storage, and permits the redo information to be written later.<br>The difference is relevant in systems where undo information and redo infor-<br>mation are stored in separate log records.)<br>The three rules state situations in which certain log records must have been output<br>to stable storage. There is no problem resulting from the output of log records earlier<br>than necessary. Thus, when the system ﬁnds it necessary to output a log record to<br>stable storage, it outputs an entire block of log records, if there are enough log records<br>in main memory to ﬁll a block. If there are insufﬁcient log records to ﬁll the block, all<br>log records in main memory are combined into a partially full block, and are output<br>to stable storage.<br>Writing the buffered log to disk is sometimes referred to as a log force.<br>17.7.2<br>Database Buffering<br>In Section 17.2, we described the use of a two-level storage hierarchy. The system<br>stores the database in nonvolatile storage (disk), and brings blocks of data into main<br>memory as needed. Since main memory is typically much smaller than the entire<br>database, it may be necessary to overwrite a block B1 in main memory when another<br>block B2 needs to be brought into memory. If B1 has been modiﬁed, B1 must be<br>output prior to the input of B2. As discussed in Section 11.5.1 in Chapter 11, this<br>storage hierarchy is the standard operating system concept of virtual memory.<br>The rules for the output of log records limit the freedom of the system to output<br>blocks of data. If the input of block B2 causes block B1 to be chosen for output, all log<br>records pertaining to data in B1 must be output to stable storage before B1 is output.<br>Thus, the sequence of actions by the system would be:<br>• Output log records to stable storage until all log records pertaining to block<br>B1 have been output.<br>• Output block B1 to disk.<br>• Input block B2 from disk to main memory.<br>It is important that no writes to the block B1 be in progress while the system car-<br>ries out this sequence of actions. We can ensure that there are no writes in progress<br>by using a special means of locking: Before a transaction performs a write on a data<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>V. Transaction <br>Management<br>17. Recovery System<br>660<br>© The McGraw−Hill <br>Companies, 2001<br>662<br>Chapter 17<br>Recovery System<br>item, it must acquire an exclusive lock on the block in which the data item resides.<br>The lock can be released immediately after the update has been performed. Before<br>a block is output, the system obtains an exclusive lock on the block, to ensure that<br>no transaction is updating the block. It releases the lock once the block output has<br>completed. Locks that are held for a short duration are often called latches. Latches<br>are treated as distinct from locks used by the concurrency-control system. As a re-<br>sult, they may be released without regard to any locking protocol, such as two-phase<br>locking, required by the concurrency-control system.<br>To illustrate the need for the write-ahead logging requirement, consider our bank-<br>ing example with transactions T0 and T1. Suppose that the state of the log is<br>&lt;T0 start&gt;<br>&lt;T0, A, 1000, 950&gt;<br>and that transaction T0 issues a read(B). Assume that the block on which B resides is<br>not in main memory, and that main memory is full. Suppose that the block on which<br>A resides is chosen to be output to disk. If the system outputs this block to disk and<br>then a crash occurs, the values in the database for accounts A, B, and C are $950,<br>$2000, and $700, respectively. This database state is inconsistent. However, because<br>of the WAL requirements, the log record<br>&lt;T0, A, 1000, 950&gt;<br>must be output to stable storage prior to output of the block on which A resides.<br>The system can use the log record during recovery to bring the database back to a<br>consistent state.<br>17.7.3<br>Operating System Role in Buffer Management<br>We can manage the database buffer by using one of two approaches:<br>1. The database system reserves part of main memory to serve as a buffer that<br>it, rather than the operating system, manages. The database system manages<br>data-block transfer in accordance with the requirements in Section 17.7.2.<br>This approach has the drawback of limiting ﬂexibility in the use of main<br>memory. The buffer must be kept small enough that other applications have<br>sufﬁcient main memory available for their needs. However, even when the<br>other applications are not running, the database will not be able to make use<br>of all the available memory. Likewise, nondatabase applications may not use<br>that part of main memory reserved for the database buffer, even if some of the<br>pages in the database buffer are not being used.<br>2. The database system implements its buffer within the virtual memory pro-<br>vided by the operating system. Since the operating system knows about the<br>memory requirements of all processes in the system, ideally it should be in<br>charge of deciding what buffer blocks must be force-output to disk, and when.<br>But, to ensure the write-ahead logging requirements in Section 17.7.1, the op-<br>erating system should not write out the database buffer pages itself, but in-<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>V. Transaction <br>Management<br>17. Recovery System<br>661<br>© The McGraw−Hill <br>Companies, 2001<br>17.8<br>Failure with Loss of Nonvolatile Storage<br>663<br>stead should request the database system to force-output the buffer blocks.<br>The database system in turn would force-output the buffer blocks to the data-<br>base, after writing relevant log records to stable storage.<br>Unfortunately, almost all current-generation operating systems retain com-<br>plete control of virtual memory. The operating system reserves space on disk<br>for storing virtual-memory pages that are not currently in main memory; this<br>space is called swap space. If the operating system decides to output a block<br>Bx, that block is output to the swap space on disk, and there is no way for the<br>database system to get control of the output of buffer blocks.<br>Therefore, if the database buffer is in virtual memory, transfers between<br>database ﬁles and the buffer in virtual memory must be managed by the<br>database system, which enforces the write-ahead logging requirements that<br>we discussed.<br>This approach may result in extra output of data to disk. If a block Bx is<br>output by the operating system, that block is not output to the database. In-<br>stead, it is output to the swap space for the operating system’s virtual mem-<br>ory. When the database system needs to output Bx, the operating system may<br>need ﬁrst to input Bx from its swap space. Thus, instead of a single output of<br>Bx, there may be two outputs of Bx (one by the operating system, and one by<br>the database system) and one extra input of Bx.<br>Although both approaches suffer from some drawbacks, one or the other must<br>be chosen unless the operating system is designed to support the requirements of<br>database logging. Only a few current operating systems, such as the Mach operating<br>system, support these requirements.<br>17.8<br>Failure with Loss of Nonvolatile Storage<br>Until now, we have considered only the case where a failure results in the loss of<br>information residing in volatile storage while the content of the nonvolatile storage<br>remains intact. Although failures in which the content of nonvolatile storage is lost<br>are rare, we nevertheless need to be prepared to deal with this type of failure. In<br>this section, we discuss only disk storage. Our discussions apply as well to other<br>nonvolatile storage types.<br>The basic scheme is to dump the entire content of the database to stable storage<br>periodically—say, once per day. For example, we may dump the database to one or<br>more magnetic tapes. If a failure occurs that results in the loss of physical database<br>blocks, the system uses the most recent dump in restoring the database to a previous<br>consistent state. Once this restoration has been accomplished, the system uses the log<br>to bring the database system to the most recent consistent state.<br>More precisely, no transaction may be active during the dump procedure, and a<br>procedure similar to checkpointing must take place:<br>1. Output all log records currently residing in main memory onto stable storage.<br>2. Output all buffer blocks onto the disk.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>V. Transaction <br>Management<br>17. Recovery System<br>662<br>© The McGraw−Hill <br>Companies, 2001<br>664<br>Chapter 17<br>Recovery System<br>3. Copy the contents of the database to stable storage.<br>4. Output a log record &lt;dump&gt; onto the stable storage.<br>Steps 1, 2, and 4 correspond to the three steps used for checkpoints in Section 17.4.3.<br>To recover from the loss of nonvolatile storage, the system restores the database<br>to disk by using the most recent dump. Then, it consults the log and redoes all the<br>transactions that have committed since the most recent dump occurred. Notice that<br>no undo operations need to be executed.<br>A dump of the database contents is also referred to as an archival dump, since<br>we can archive the dumps and use them later to examine old states of the database.<br>Dumps of a database and checkpointing of buffers are similar.<br>The simple dump procedure described here is costly for the following two reasons.<br>First, the entire database must be be copied to stable storage, resulting in considerable<br>data transfer. Second, since transaction processing is halted during the dump proce-<br>dure, CPU cycles are wasted. Fuzzy dump schemes have been developed, which al-<br>low transactions to be active while the dump is in progress. They are similar to fuzzy<br>checkpointing schemes; see the bibliographical notes for more details.<br>17.9<br>Advanced Recovery Techniques∗∗<br>The recovery techniques described in Section 17.6 require that, once a transaction up-<br>dates a data item, no other transaction may update the same data item until the ﬁrst<br>commits or is rolled back. We ensure the condition by using strict two-phase locking.<br>Although strict two-phase locking is acceptable for records in relations, as discussed<br>in Section 16.9, it causes a signiﬁcant decrease in concurrency when applied to certain<br>specialized structures, such as B+-tree index pages.<br>To increase concurrency, we can use the B+-tree concurrency-control algorithm de-<br>scribed in Section 16.9 to allow locks to be released early, in a non-two-phase manner.<br>As a result, however, the recovery techniques from Section 17.6 will become inap-<br>plicable. Several alternative recovery techniques, applicable even with early lock re-<br>lease, have been proposed. These schemes can be used in a variety of applications, not<br>just for recovery of B+-trees. We ﬁrst describe an advanced recovery scheme support-<br>ing early lock release. We then outline the ARIES recovery scheme, which is widely<br>used in the industry. ARIES is more complex than our advanced recovery scheme, but<br>incorporates a number of optimizations to minimize recovery time, and provides a<br>number of other useful features.<br>17.9.1<br>Logical Undo Logging<br>For operations where locks are released early, we cannot perform the undo actions<br>by simply writing back the old value of the data items. Consider a transaction T<br>that inserts an entry into a B+-tree, and, following the B+-tree concurrency-control<br>protocol, releases some locks after the insertion operation completes, but before the<br>transaction commits. After the locks are released, other transactions may perform<br>further insertions or deletions, thereby causing further changes to the B+-tree nodes.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>V. Transaction <br>Management<br>17. Recovery System<br>663<br>© The McGraw−Hill <br>Companies, 2001<br>17.9<br>Advanced Recovery Techniques∗∗<br>665<br>Even though the operation releases some locks early, it must retain enough locks<br>to ensure that no other transaction is allowed to execute any conﬂicting operation<br>(such as reading the inserted value or deleting the inserted value). For this reason,<br>the B+-tree concurrency-control protocol in Section 16.9 holds locks on the leaf level<br>of the B+-tree until the end of the transaction.<br>Now let us consider how to perform transaction rollback. If physical undo is used,<br>that is, the old values of the internal B+-tree nodes (before the insertion operation<br>was executed) are written back during transaction rollback, some of the updates per-<br>formed by later insertion or deletion operations executed by other transactions could<br>be lost. Instead, the insertion operation has to be undone by a logical undo—that is,<br>in this case, by the execution of a delete operation.<br>Therefore, when the insertion operation completes, before it releases any locks, it<br>writes a log record &lt;Ti, Oj, operation-end, U&gt;, where the U denotes undo informa-<br>tion and Oj denotes a unique identiﬁer for (the instance of) the operation. For exam-<br>ple, if the operation inserted an entry in a B+-tree, the undo information U would<br>indicate that a deletion operation is to be performed, and would identify the B+-tree<br>and what to delete from the tree. Such logging of information about operations is<br>called logical logging. In contrast, logging of old-value and new-value information<br>is called physical logging, and the corresponding log records are called physical log<br>records.<br>The insertion and deletion operations are examples of a class of operations that re-<br>quire logical undo operations since they release locks early; we call such operations<br>logical operations. Before a logical operation begins, it writes a log record &lt;Ti, Oj,<br>operation-begin&gt;, where Oj is the unique identiﬁer for the operation. While the sys-<br>tem is executing the operation, it does physical logging in the normal fashion for all<br>updates performed by the operation. Thus, the usual old-value and new-value in-<br>formation is written out for each update. When the operation ﬁnishes, it writes an<br>operation-end log record as described earlier.<br>17.9.2<br>Transaction Rollback<br>First consider transaction rollback during normal operation (that is, not during re-<br>covery from system failure). The system scans the log backward and uses log records<br>belonging to the transaction to restore the old values of data items. Unlike rollback<br>in normal operation, however, rollback in our advanced recovery scheme writes out<br>special redo-only log records of the form &lt;Ti, Xj, V &gt; containing the value V being<br>restored to data item Xj during the rollback. These log records are sometimes called<br>compensation log records. Such records do not need undo information, since we will<br>never need to undo such an undo operation.<br>Whenever the system ﬁnds a log record &lt;Ti, Oj, operation-end, U&gt;, it takes spe-<br>cial actions:<br>1. It rolls back the operation by using the undo information U in the log record.<br>It logs the updates performed during the rollback of the operation just like<br>updates performed when the operation was ﬁrst executed. In other words,<br>the system logs physical undo information for the updates performed during<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>V. Transaction <br>Management<br>17. Recovery System<br>664<br>© The McGraw−Hill <br>Companies, 2001<br>666<br>Chapter 17<br>Recovery System<br>rollback, instead of using compensation log records. This is because a crash<br>may occur while a logical undo is in progress, and on recovery the system<br>has to complete the logical undo; to do so, restart recovery will undo the par-<br>tial effects of the earlier undo, using the phy</span><br><br><span style="background-color: #9BF6FF;" title="Chunk 84 | Start: 1680168 | End: 1700168 | Tokens: 3329">sical undo information, and then<br>perform the logical undo again, as we will see in Section 17.9.4.<br>At the end of the operation rollback, instead of generating a log record<br>&lt; Ti, Oj, operation-end, U &gt;, the system generates a log record &lt; Ti, Oj,<br>operation-abort&gt;.<br>2. When the backward scan of the log continues, the system skips all log records<br>of the transaction until it ﬁnds the log record &lt;Ti, Oj, operation-begin&gt;. After<br>it ﬁnds the operation-begin log record, it processes log records of the transac-<br>tion in the normal manner again.<br>Observe that skipping over physical log records when the operation-end log record<br>is found during rollback ensures that the old values in the physical log record are not<br>used for rollback, once the operation completes.<br>If the system ﬁnds a record &lt; Ti, Oj, operation-abort&gt;, it skips all preceding re-<br>cords until it ﬁnds the record &lt; Ti, Oj, operation-begin&gt;. These preceding log records<br>must be skipped to prevent multiple rollback of the same operation, in case there had<br>been a crash during an earlier rollback, and the transaction had already been partly<br>rolled back. When the transaction Ti has been rolled back, the system adds a record<br>&lt;Ti abort&gt; to the log.<br>If failures occur while a logical operation is in progress, the operation-end log<br>record for the operation will not be found when the transaction is rolled back. How-<br>ever, for every update performed by the operation, undo information—in the form<br>of the old value in the physical log records—is available in the log. The physical log<br>records will be used to roll back the incomplete operation.<br>17.9.3<br>Checkpoints<br>Checkpointing is performed as described in Section 17.6. The system suspends up-<br>dates to the database temporarily and carries out these actions:<br>1. It outputs to stable storage all log records currently residing in main memory.<br>2. It outputs to the disk all modiﬁed buffer blocks.<br>3. It outputs onto stable storage a log record &lt;checkpoint L&gt;, where L is a list of<br>all active transactions.<br>17.9.4<br>Restart Recovery<br>Recovery actions, when the database system is restarted after a failure, take place in<br>two phases:<br>1. In the redo phase, the system replays updates of all transactions by scan-<br>ning the log forward from the last checkpoint. The log records that are re-<br>played include log records for transactions that were rolled back before sys-<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>V. Transaction <br>Management<br>17. Recovery System<br>665<br>© The McGraw−Hill <br>Companies, 2001<br>17.9<br>Advanced Recovery Techniques∗∗<br>667<br>tem crash, and those that had not committed when the system crash occurred.<br>The records are the usual log records of the form &lt;Ti, Xj, V1, V2&gt; as well<br>as the special log records of the form &lt;Ti, Xj, V2&gt;; the value V2 is written<br>to data item Xj in either case. This phase also determines all transactions that<br>are either in the transaction list in the checkpoint record, or started later, but<br>did not have either a &lt;Ti abort&gt; or a &lt;Ti commit&gt; record in the log. All these<br>transactions have to be rolled back, and the system puts their transaction iden-<br>tiﬁers in an undo-list.<br>2. In the undo phase, the system rolls back all transactions in the undo-list. It<br>performs rollback by scanning the log backward from the end. Whenever<br>it ﬁnds a log record belonging to a transaction in the undo-list, it performs<br>undo actions just as if the log record had been found during the rollback of a<br>failed transaction. Thus, log records of a transaction preceding an operation-<br>end record, but after the corresponding operation-begin record, are ignored.<br>When the system ﬁnds a &lt;Ti start&gt; log record for a transaction Ti in undo-<br>list, it writes a &lt;Ti abort&gt; log record to the log. Scanning of the log stops<br>when the system has found &lt;Ti start&gt; log records for all transactions in the<br>undo-list.<br>The redo phase of restart recovery replays every physical log record since the most<br>recent checkpoint record. In other words, this phase of restart recovery repeats all<br>the update actions that were executed after the checkpoint, and whose log records<br>reached the stable log. The actions include actions of incomplete transactions and the<br>actions carried out to roll failed transactions back. The actions are repeated in the<br>same order in which they were carried out; hence, this process is called repeating<br>history. Repeating history simpliﬁes recovery schemes greatly.<br>Note that if an operation undo was in progress when the system crash occurred,<br>the physical log records written during operation undo would be found, and the par-<br>tial operation undo would itself be undone on the basis of these physical log records.<br>After that the original operation’s operation-end record would be found during re-<br>covery, and the operation undo would be executed again.<br>17.9.5<br>Fuzzy Checkpointing<br>The checkpointing technique described in Section 17.6.3 requires that all updates to<br>the database be temporarily suspended while the checkpoint is in progress. If the<br>number of pages in the buffer is large, a checkpoint may take a long time to ﬁnish,<br>which can result in an unacceptable interruption in processing of transactions.<br>To avoid such interruptions, the checkpointing technique can be modiﬁed to per-<br>mit updates to start once the checkpoint record has been written, but before the modi-<br>ﬁed buffer blocks are written to disk. The checkpoint thus generated is a fuzzy check-<br>point.<br>Since pages are output to disk only after the checkpoint record has been written, it<br>is possible that the system could crash before all pages are written. Thus, a checkpoint<br>on disk may be incomplete. One way to deal with incomplete checkpoints is this:<br>The location in the log of the checkpoint record of the last completed checkpoint<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>V. Transaction <br>Management<br>17. Recovery System<br>666<br>© The McGraw−Hill <br>Companies, 2001<br>668<br>Chapter 17<br>Recovery System<br>is stored in a ﬁxed position, last-checkpoint, on disk. The system does not update<br>this information when it writes the checkpoint record. Instead, before it writes the<br>checkpoint record, it creates a list of all modiﬁed buffer blocks. The last-checkpoint<br>information is updated only after all buffer blocks in the list of modiﬁed buffer blocks<br>have been output to disk.<br>Even with fuzzy checkpointing, a buffer block must not be updated while it is<br>being output to disk, although other buffer blocks may be updated concurrently. The<br>write-ahead log protocol must be followed so that (undo) log records pertaining to a<br>block are on stable storage before the block is output.<br>Note that, in our scheme, logical logging is used only for undo purposes, whereas<br>physical logging is used for redo and undo purposes. There are recovery schemes that<br>use logical logging for redo purposes. To perform logical redo, the database state on<br>disk must be operation consistent, that is, it should not have partial effects of any<br>operation. It is difﬁcult to guarantee operation consistency of the database on disk<br>if an operation can affect more than one page, since it is not possible to write two<br>or more pages atomically. Therefore, logical redo logging is usually restricted only<br>to operations that affect a single page; we will see how to handle such logical redos<br>in Section 17.9.6. In contrast, logical undos are performed on an operation-consistent<br>database state achieved by repeating history, and then performing physical undo of<br>partially completed operations.<br>17.9.6<br>ARIES<br>The state of the art in recovery methods is best illustrated by the ARIES recovery<br>method. The advanced recovery technique which we have described is modeled af-<br>ter ARIES, but has been simpliﬁed signiﬁcantly to bring out key concepts and make<br>it easier to understand. In contrast, ARIES uses a number of techniques to reduce the<br>time taken for recovery, and to reduce the overheads of checkpointing. In particu-<br>lar, ARIES is able to avoid redoing many logged operations that have already been<br>applied and to reduce the amount of information logged. The price paid is greater<br>complexity; the beneﬁts are worth the price.<br>The major differences between ARIES and our advanced recovery algorithm are<br>that ARIES:<br>1. Uses a log sequence number (LSN) to identify log records, and the use of<br>LSNs in database pages to identify which operations have been applied to a<br>database page.<br>2. Supports physiological redo operations, which are physical in that the af-<br>fected page is physically identiﬁed, but can be logical within the page.<br>For instance, the deletion of a record from a page may result in many other<br>records in the page being shifted, if a slotted page structure is used. With phys-<br>ical redo logging, all bytes of the page affected by the shifting of records must<br>be logged. With physiological logging, the deletion operation can be logged,<br>resulting in a much smaller log record. Redo of the deletion operation would<br>delete the record and shift other records as required.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>V. Transaction <br>Management<br>17. Recovery System<br>667<br>© The McGraw−Hill <br>Companies, 2001<br>17.9<br>Advanced Recovery Techniques∗∗<br>669<br>3. Uses a dirty page table to minimize unnecessary redos during recovery. Dirty<br>pages are those that have been updated in memory, and the disk version is<br>not up-to-date.<br>4. Uses fuzzy checkpointing scheme that only records information about dirty<br>pages and associated information, and does not even require writing of dirty<br>pages to disk. It ﬂushes dirty pages in the background, continuously, instead<br>of writing them during checkpoints.<br>In the rest of this section we provide an overview of ARIES. The bibliographical notes<br>list references that provide a complete description of ARIES.<br>17.9.6.1<br>Data Structures<br>Each log record in ARIES has a log sequence number (LSN) that uniquely identiﬁes<br>the record. The number is conceptually just a logical identiﬁer whose value is greater<br>for log records that occur later in the log. In practice, the LSN is generated in such a<br>way that it can also be used to locate the log record on disk. Typically, ARIES splits a<br>log into multiple log ﬁles, each of which has a ﬁle number. When a log ﬁle grows to<br>some limit, ARIES appends further log records to a new log ﬁle; the new log ﬁle has a<br>ﬁle number that is higher by 1 than the previous log ﬁle. The LSN then consists of a<br>ﬁle number and an offset within the ﬁle.<br>Each page also maintains an identiﬁer called the PageLSN. Whenever an opera-<br>tion (whether physical or logical) occurs on a page, the operation stores the LSN of<br>its log record in the PageLSN ﬁeld of the page. During the redo phase of recovery,<br>any log records with LSN less than or equal to the PageLSN of a page should not be<br>executed on the page, since their actions are already reﬂected on the page. In com-<br>bination with a scheme for recording PageLSNs as part of checkpointing, which we<br>present later, ARIES can avoid even reading many pages for which logged operations<br>are already reﬂected on disk. Thereby recovery time is reduced signiﬁcantly.<br>The PageLSN is essential for ensuring idempotence in the presence of physiologi-<br>cal redo operations, since reapplying a physiological redo that has already been ap-<br>plied to a page could cause incorrect changes to a page.<br>Pages should not be ﬂushed to disk while an update is in progress, since physi-<br>ological operations cannot be redone on the partially updated state of the page on<br>disk. Therefore, ARIES uses latches on buffer pages to prevent them from being writ-<br>ten to disk while they are being updated. It releases the buffer page latch only after<br>the update is completed, and the log record for the update has been written to the<br>log.<br>Each log record also contains the LSN of the previous log record of the same trans-<br>action. This value, stored in the PrevLSN ﬁeld, permits log records of a transaction<br>to be fetched backward, without reading the whole log. There are special redo-only<br>log records generated during transaction rollback, called compensation log records<br>(CLRs) in ARIES. These serve the same purpose as the redo-only log records in our<br>advanced recovery scheme. In addition CLRs serve the role of the operation-abort<br>log records in our scheme. The CLRs have an extra ﬁeld, called the UndoNextLSN,<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>V. Transaction <br>Management<br>17. Recovery System<br>668<br>© The McGraw−Hill <br>Companies, 2001<br>670<br>Chapter 17<br>Recovery System<br>that records the LSN of the log that needs to be undone next, when the transaction is<br>being rolled back. This ﬁeld serves the same purpose as the operation identiﬁer in the<br>operation-abort log record in our scheme, which helps to skip over log records that<br>have already been rolled back. The DirtyPageTable contains a list of pages that have<br>been updated in the database buffer. For each page, it stores the PageLSN and a ﬁeld<br>called the RecLSN which helps identify log records that have been applied already<br>to the version of the page on disk. When a page is inserted into the DirtyPageTable<br>(when it is ﬁrst modiﬁed in the buffer pool) the value of RecLSN is set to the cur-<br>rent end of log. Whenever the page is ﬂushed to disk, the page is removed from the<br>DirtyPageTable.<br>A checkpoint log record contains the DirtyPageTable and a list of active transac-<br>tions. For each transaction, the checkpoint log record also notes LastLSN, the LSN of<br>the last log record written by the transaction. A ﬁxed position on disk also notes the<br>LSN of the last (complete) checkpoint log record.<br>17.9.6.2<br>Recovery Algorithm<br>ARIES recovers from a system crash in three passes.<br>• Analysis pass: This pass determines which transactions to undo, which pages<br>were dirty at the time of the crash, and the LSN from which the redo pass<br>should start.<br>• Redo pass: This pass starts from a position determined during analysis, and<br>performs a redo, repeating history, to bring the database to a state it was in<br>before the crash.<br>• Undo pass: This pass rolls back all transactions that were incomplete at the<br>time of crash.<br>Analysis Pass: The analysis pass ﬁnds the last complete checkpoint log record, and<br>reads in the DirtyPageTable from this record. It then sets RedoLSN to the minimum<br>of the RecLSNs of the pages in the DirtyPageTable. If there are no dirty pages, it<br>sets RedoLSN to the LSN of the checkpoint log record. The redo pass starts its scan<br>of the log from RedoLSN. All the log records earlier than this point have already<br>been applied to the database pages on disk. The analysis pass initially sets the list of<br>transactions to be undone, undo-list, to the list of transactions in the checkpoint log<br>record. The analysis pass also reads from the checkpoint log record the LSNs of the<br>last log record for each transaction in undo-list.<br>The analysis pass continues scanning forward from the checkpoint. Whenever it<br>ﬁnds a log record for a transaction not in the undo-list, it adds the transaction to<br>undo-list. Whenever it ﬁnds a transaction end log record, it deletes the transaction<br>from undo-list. All transactions left in undo-list at the end of analysis have to be<br>rolled back later, in the undo pass. The analysis pass also keeps track of the last record<br>of each transaction in undo-list, which is used in the undo pass.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>V. Transaction <br>Management<br>17. Recovery System<br>669<br>© The McGraw−Hill <br>Companies, 2001<br>17.9<br>Advanced Recovery Techniques∗∗<br>671<br>The analysis pass also updates DirtyPageTable whenever it ﬁnds a log record for<br>an update on a page. If the page is not in DirtyPageTable, the analysis pass adds it to<br>DirtyPageTable, and sets the RecLSN of the page to the LSN of the log record.<br>Redo Pass: The redo pass repeats history by replaying every action that is not already<br>reﬂected in the page on disk. The redo pass scans the log forward from RedoLSN.<br>Whenever it ﬁnds an update log record, it takes this action:<br>1. If the page is not in DirtyPageTable or the LSN of the update log record is less<br>than the RecLSN of the page in DirtyPageTable, then the redo pass skips the<br>log record.<br>2. Otherwise the redo pass fetches the page from disk, and if the PageLSN is less<br>than the LSN of the log record, it redoes the log record.<br>Note that if either of the tests is negative, then the effects of the log record have<br>already appeared on the page. If the ﬁrst test is negative, it is not even necessary to<br>fetch the page from disk.<br>Undo Pass and Transaction Rollback: The undo pass is relatively straightforward. It<br>performs a backward scan of the log, undoing all transactions in undo-list. If a CLR<br>is found, it uses the UndoNextLSN ﬁeld to skip log records that have already been<br>rolled back. Otherwise, it uses the PrevLSN ﬁeld of the log record to ﬁnd the next log<br>record to be undone.<br>Whenever an update log record is used to perform an undo (whether for transac-<br>tion rollback during normal processing, or during the restart undo pass), the undo<br>pass generates a CLR containing the undo action performed (which must be physio-<br>logical). It sets the UndoNextLSN of the CLR to the PrevLSN value of the update log<br>record.<br>17.9.6.3<br>Other Features<br>Among other key features that ARIES provides are:<br>• Recovery independence: Some pages can be recovered independently from<br>others, so that they can be used even while other pages are being recovered. If<br>some pages of a disk fail, they can be recovered without stopping transaction<br>processing on other pages.<br>• Savepoints: Transactions can record savepoints, and can be rolled back par-<br>tially, up to a savepoint. This can be quite useful for deadlock handling, since<br>transactions can be rolled back up to a point that permits release of required<br>locks, and then restarted from that point.<br>• Fine-grained locking: The ARIES recovery algorithm can be used with index<br>concurrency control algorithms that permit tuple level locking on indices, in-<br>stead of page level locking, which improves concurrency signiﬁcantly.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>V. Transaction <br>Management<br>17. Recovery System<br>670<br>© The McGraw−Hill <br>Companies, 2001<br>672<br>Chapter 17<br>Recovery System<br>• Recovery optimizations: The DirtyPageTable can be used to prefetch pages<br>during redo, instead of fetching a page only when the system ﬁnds a log<br>record to be applied to the page. Out-of-order redo is also possible: Redo can<br>be postponed on a page being fetched from disk, and performed when the<br>page is fetched. Meanwhile, other log records can continue to be processed.<br>In summary, the ARIES algorithm is a state-of-the-art recovery algorithm, incorpo-<br>rating a variety of optimizations designed to improve concurrency, reduce logging<br>overhead, and reduce recovery time.<br>17.10<br>Remote Backup Systems<br>Traditional transaction-processing systems are centralized or client–server systems.<br>Such systems are vulnerable to environmental disasters such as ﬁre, ﬂooding, or<br>earthquakes. Increasingly, there is a need for transaction-processing systems that can<br>function in spite of system failures or environmental disasters. Such systems must<br>provide high availability, that is, the time for which the system is unusable must be<br>extremely small.<br>We can achieve high availability by performing transaction processing at one site,<br>called the primary site, and having a remote backup site where all the data from<br>the primary site are replicated. The remote backup site is sometimes also called the<br>secondary site. The remote site must be kept synchronized with the primary site, as<br>updates are performed at the primary. We achieve synchronization by sending all log<br>records from primary site to the remote backup site. The remote backup site must be<br>physically separated from the primary—for example, we can locate it in a different<br>state—so that a disaster at the primary does not damage the remote backup site.<br>Figure 17.10 shows the architecture of a remote backup system.</span><br><br><span style="background-color: #A0C4FF;" title="Chunk 85 | Start: 1700170 | End: 1720170 | Tokens: 3229"><br>When the primary site fails, the remote backup site takes over processing. First,<br>however, it performs recovery, using its (perhaps outdated) copy of the data from the<br>primary, and the log records received from the primary. In effect, the remote backup<br>site is performing recovery actions that would have been performed at the primary<br>site when the latter recovered. Standard recovery algorithms, with minor modiﬁca-<br>tions, can be used for recovery at the remote backup site. Once recovery has been<br>performed, the remote backup site starts processing transactions.<br>log<br>records<br>backup<br>network<br>primary<br>Figure 17.10<br>Architecture of remote backup system.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>V. Transaction <br>Management<br>17. Recovery System<br>671<br>© The McGraw−Hill <br>Companies, 2001<br>17.10<br>Remote Backup Systems<br>673<br>Availability is greatly increased over a single-site system, since the system can<br>recover even if all data at the primary site are lost. The performance of a remote<br>backup system is better than the performance of a distributed system with two-phase<br>commit.<br>Several issues must be addressed in designing a remote backup system:<br>• Detection of failure. As in failure-handling protocols for distributed system,<br>it is important for the remote backup system to detect when the primary has<br>failed. Failure of communication lines can fool the remote backup into believ-<br>ing that the primary has failed. To avoid this problem, we maintain several<br>communication links with independent modes of failure between the primary<br>and the remote backup. For example, in addition to the network connection,<br>there may be a separate modem connection over a telephone line, with ser-<br>vices provided by different telecommunication companies. These connections<br>may be backed up via manual intervention by operators, who can communi-<br>cate over the telephone system.<br>• Transfer of control. When the primary fails, the backup site takes over pro-<br>cessing and becomes the new primary. When the original primary site recov-<br>ers, it can either play the role of remote backup, or take over the role of pri-<br>mary site again. In either case, the old primary must receive a log of updates<br>carried out by the backup site while the old primary was down.<br>The simplest way of transferring control is for the old primary to receive<br>redo logs from the old backup site, and to catch up with the updates by ap-<br>plying them locally. The old primary can then act as a remote backup site.<br>If control must be transferred back, the old backup site can pretend to have<br>failed, resulting in the old primary taking over.<br>• Time to recover. If the log at the remote backup grows large, recovery will<br>take a long time. The remote backup site can periodically process the redo log<br>records that it has received, and can perform a checkpoint, so that earlier parts<br>of the log can be deleted. The delay before the remote backup takes over can<br>be signiﬁcantly reduced as a result.<br>A hot-spare conﬁguration can make takeover by the backup site almost<br>instantaneous. In this conﬁguration, the remote backup site continually pro-<br>cesses redo log records as they arrive, applying the updates locally. As soon<br>as the failure of the primary is detected, the backup site completes recovery<br>by rolling back incomplete transactions; it is then ready to process new trans-<br>actions.<br>• Time to commit. To ensure that the updates of a committed transaction are<br>durable, a transaction must not be declared committed until its log records<br>have reached the backup site. This delay can result in a longer wait to commit<br>a transaction, and some systems therefore permit lower degrees of durability.<br>The degrees of durability can be classiﬁed as follows.<br>  One-safe. A transaction commits as soon as its commit log record is writ-<br>ten to stable storage at the primary site.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>V. Transaction <br>Management<br>17. Recovery System<br>672<br>© The McGraw−Hill <br>Companies, 2001<br>674<br>Chapter 17<br>Recovery System<br>The problem with this scheme is that the updates of a committed trans-<br>action may not have made it to the backup site, when the backup site<br>takes over processing. Thus, the updates may appear to be lost. When the<br>primary site recovers, the lost updates cannot be merged in directly, since<br>the updates may conﬂict with later updates performed at the backup site.<br>Thus, human intervention may be required to bring the database to a con-<br>sistent state.<br>  Two-very-safe. A transaction commits as soon as its commit log record is<br>written to stable storage at the primary and the backup site.<br>The problem with this scheme is that transaction processing cannot<br>proceed if either the primary or the backup site is down. Thus, availabil-<br>ity is actually less than in the single-site case, although the probability of<br>data loss is much less.<br>  Two-safe. This scheme is the same as two-very-safe if both primary and<br>backup sites are active. If only the primary is active, the transaction is<br>allowed to commit as soon as its commit log record is written to stable<br>storage at the primary site.<br>This scheme provides better availability than does two-very-safe, while<br>avoiding the problem of lost transactions faced by the one-safe scheme.<br>It results in a slower commit than the one-safe scheme, but the beneﬁts<br>generally outweigh the cost.<br>Several commercial shared-disk systems provide a level of fault tolerance that is<br>intermediate between centralized and remote backup systems. In these systems, the<br>failure of a CPU does not result in system failure. Instead, other CPUs take over, and<br>they carry out recovery. Recovery actions include rollback of transactions running<br>on the failed CPU, and recovery of locks held by those transactions. Since data are<br>on a shared disk, there is no need for transfer of log records. However, we should<br>safeguard the data from disk failure by using, for example, a RAID disk organization.<br>An alternative way of achieving high availability is to use a distributed database,<br>with data replicated at more than one site. Transactions are then required to update<br>all replicas of any data item that they update. We study distributed databases, includ-<br>ing replication, in Chapter 19.<br>17.11<br>Summary<br>• A computer system, like any other mechanical or electrical device, is subject<br>to failure. There are a variety of causes of such failure, including disk crash,<br>power failure, and software errors. In each of these cases, information con-<br>cerning the database system is lost.<br>• In addition to system failures, transactions may also fail for various reasons,<br>such as violation of integrity constraints or deadlocks.<br>• An integral part of a database system is a recovery scheme that is responsible<br>for the detection of failures and for the restoration of the database to a state<br>that existed before the occurrence of the failure.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>V. Transaction <br>Management<br>17. Recovery System<br>673<br>© The McGraw−Hill <br>Companies, 2001<br>17.11<br>Summary<br>675<br>• The various types of storage in a computer are volatile storage, nonvolatile<br>storage, and stable storage. Data in volatile storage, such as in RAM, are lost<br>when the computer crashes. Data in nonvolatile storage, such as disk, are not<br>lost when the computer crashes, but may occasionally be lost because of fail-<br>ures such as disk crashes. Data in stable storage are never lost.<br>• Stable storage that must be accessible online is approximated with mirrored<br>disks, or other forms of RAID, which provide redundant data storage. Ofﬂine,<br>or archival, stable storage may consist of multiple tape copies of data stored<br>in a physically secure location.<br>• In case of failure, the state of the database system may no longer be consis-<br>tent; that is, it may not reﬂect a state of the world that the database is sup-<br>posed to capture. To preserve consistency, we require that each transaction be<br>atomic. It is the responsibility of the recovery scheme to ensure the atomic-<br>ity and durability property. There are basically two different approaches for<br>ensuring atomicity: log-based schemes and shadow paging.<br>• In log-based schemes, all updates are recorded on a log, which must be kept<br>in stable storage.<br>  In the deferred-modiﬁcations scheme, during the execution of a transac-<br>tion, all the write operations are deferred until the transaction partially<br>commits, at which time the system uses the information on the log asso-<br>ciated with the transaction in executing the deferred writes.<br>  In the immediate-modiﬁcations scheme, the system applies all updates<br>directly to the database. If a crash occurs, the system uses the information<br>in the log in restoring the state of the system to a previous consistent state.<br>To reduce the overhead of searching the log and redoing transactions, we can<br>use the checkpointing technique.<br>• In shadow paging, two page tables are maintained during the life of a trans-<br>action: the current page table and the shadow page table. When the transac-<br>tion starts, both page tables are identical. The shadow page table and pages<br>it points to are never changed during the duration of the transaction. When<br>the transaction partially commits, the shadow page table is discarded, and the<br>current table becomes the new page table. If the transaction aborts, the current<br>page table is simply discarded.<br>• If multiple transactions are allowed to execute concurrently, then the shadow-<br>paging technique is not applicable, but the log-based technique can be used.<br>No transaction can be allowed to update a data item that has already been<br>updated by an incomplete transaction. We can use strict two-phase locking to<br>ensure this condition.<br>• Transaction processing is based on a storage model in which main memory<br>holds a log buffer, a database buffer, and a system buffer. The system buffer<br>holds pages of system object code and local work areas of transactions.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>V. Transaction <br>Management<br>17. Recovery System<br>674<br>© The McGraw−Hill <br>Companies, 2001<br>676<br>Chapter 17<br>Recovery System<br>• Efﬁcient implementation of a recovery scheme requires that the number of<br>writes to the database and to stable storage be minimized. Log records may<br>be kept in volatile log buffer initially, but must be written to stable storage<br>when one of the following conditions occurs:<br>  Before the &lt;Ti commit&gt; log record may be output to stable storage, all<br>log records pertaining to transaction Ti must have been output to stable<br>storage.<br>  Before a block of data in main memory is output to the database (in non-<br>volatile storage), all log records pertaining to data in that block must have<br>been output to stable storage.<br>• To recover from failures that result in the loss of nonvolatile storage, we must<br>dump the entire contents of the database onto stable storage periodically—<br>say, once per day. If a failure occurs that results in the loss of physical database<br>blocks, we use the most recent dump in restoring the database to a previous<br>consistent state. Once this restoration has been accomplished, we use the log<br>to bring the database system to the most recent consistent state.<br>• Advanced recovery techniques support high-concurrency locking techniques,<br>such as those used for B+-tree concurrency control. These techniques are based<br>on logical (operation) undo, and follow the principle of repeating history.<br>When recovering from system failure, the system performs a redo pass using<br>the log, followed by an undo pass on the log to roll back incomplete transac-<br>tions.<br>• The ARIES recovery scheme is a state-of-the-art scheme that supports a num-<br>ber of features to provide greater concurrency, reduce logging overheads, and<br>minimize recovery time. It is also based on repeating of history, and allows<br>logical undo operations. The scheme ﬂushes pages on a continuous basis and<br>does not need to ﬂush all pages at the time of a checkpoint. It uses log se-<br>quence numbers (LSNs) to implement a variety of optimizations that reduce<br>the time taken for recovery.<br>• Remote backup systems provide a high degree of availability, allowing trans-<br>action processing to continue even if the primary site is destroyed by a ﬁre,<br>ﬂood, or earthquake.<br>Review Terms<br>• Recovery scheme<br>• Failure classiﬁcation<br>  Transaction failure<br>  Logical error<br>  System error<br>  System crash<br>  Data-transfer failure<br>• Fail-stop assumption<br>• Disk failure<br>• Storage types<br>  Volatile storage<br>  Nonvolatile storage<br>  Stable storage<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>V. Transaction <br>Management<br>17. Recovery System<br>675<br>© The McGraw−Hill <br>Companies, 2001<br>Exercises<br>677<br>• Blocks<br>  Physical blocks<br>  Buffer blocks<br>• Disk buffer<br>• Force-output<br>• Log-based recovery<br>• Log<br>• Log records<br>• Update log record<br>• Deferred modiﬁcation<br>• Idempotent<br>• Immediate modiﬁcation<br>• Uncommitted modiﬁcations<br>• Checkpoints<br>• Shadow paging<br>  Page table<br>  Current page table<br>  Shadow page table<br>• Garbage collection<br>• Recovery with concurrent<br>transactions<br>  Transaction rollback<br>  Fuzzy checkpoint<br>  Restart recovery<br>• Buffer management<br>• Log-record buffering<br>• Write-ahead logging (WAL)<br>• Log force<br>• Database buffering<br>• Latches<br>• Operating system and buffer<br>management<br>• Loss of nonvolatile storage<br>• Archival dump<br>• Fuzzy dump<br>• Advanced recovery technique<br>  Physical undo<br>  Logical undo<br>  Physical logging<br>  Logical logging<br>  Logical operations<br>  Transaction rollback<br>  Checkpoints<br>  Restart recovery<br>  Redo phase<br>  Undo phase<br>• Repeating history<br>• Fuzzy checkpointing<br>• ARIES<br>  Log sequence number (LSN)<br>  PageLSN<br>  Physiological redo<br>  Compensation log record<br>(CLR)<br>  DirtyPageTable<br>  Checkpoint log record<br>• High availability<br>• Remote backup systems<br>  Primary site<br>  Remote backup site<br>  Secondary site<br>• Detection of failure<br>• Transfer of control<br>• Time to recover<br>• Hot-spare conﬁguration<br>• Time to commit<br>  One-safe<br>  Two-very-safe<br>  Two-safe<br>Exercises<br>17.1 Explain the difference between the three storage types—volatile, nonvolatile,<br>and stable—in terms of I/O cost.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>V. Transaction <br>Management<br>17. Recovery System<br>676<br>© The McGraw−Hill <br>Companies, 2001<br>678<br>Chapter 17<br>Recovery System<br>17.2 Stable storage cannot be implemented.<br>a. Explain why it cannot be.<br>b. Explain how database systems deal with this problem.<br>17.3 Compare the deferred- and immediate-modiﬁcation versions of the log-based<br>recovery scheme in terms of ease of implementation and overhead cost.<br>17.4 Assume that immediate modiﬁcation is used in a system. Show, by an example,<br>how an inconsistent database state could result if log records for a transaction<br>are not output to stable storage prior to data updated by the transaction being<br>written to disk.<br>17.5 Explain the purpose of the checkpoint mechanism. How often should check-<br>points be performed? How does the frequency of checkpoints affect<br>• System performance when no failure occurs<br>• The time it takes to recover from a system crash<br>• The time it takes to recover from a disk crash<br>17.6 When the system recovers from a crash (see Section 17.6.4), it constructs an<br>undo-list and a redo-list. Explain why log records for transactions on the undo-<br>list must be processed in reverse order, while those log records for transactions<br>on the redo-list are processed in a forward direction.<br>17.7 Compare the shadow-paging recovery scheme with the log-based recovery<br>schemes in terms of ease of implementation and overhead cost.<br>17.8 Consider a database consisting of 10 consecutive disk blocks (block 1, block<br>2, . . ., block 10). Show the buffer state and a possible physical ordering of the<br>blocks after the following updates, assuming that shadow paging is used, that<br>the buffer in main memory can hold only three blocks, and that a least recently<br>used (LRU) strategy is used for buffer management.<br>read block 3<br>read block 7<br>read block 5<br>read block 3<br>read block 1<br>modify block 1<br>read block 10<br>modify block 5<br>17.9 Explain how the buffer manager may cause the database to become inconsis-<br>tent if some log records pertaining to a block are not output to stable storage<br>before the block is output to disk.<br>17.10 Explain the beneﬁts of logical logging. Give examples of one situation where<br>logical logging is preferable to physical logging and one situation where phys-<br>ical logging is preferable to logical logging.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>V. Transaction <br>Management<br>17. Recovery System<br>677<br>© The McGraw−Hill <br>Companies, 2001<br>Bibliographical Notes<br>679<br>17.11 Explain the reasons why recovery of interactive transactions is more difﬁcult<br>to deal with than is recovery of batch transactions. Is there a simple way to deal<br>with this difﬁculty? (Hint: Consider an automatic teller machine transaction in<br>which cash is withdrawn.)<br>17.12 Sometimes a transaction has to be undone after it has commited, because it was<br>erroneously executed, for example because of erroneous input by a bank teller.<br>a. Give an example to show that using the normal transaction undo mecha-<br>nism to undo such a transaction could lead to an inconsistent state.<br>b. One way to handle this situation is to bring the whole database to a state<br>prior to the commit of the erroneous transaction (called point-in-time recov-<br>ery). Transactions that committed later have their effects rolled back with<br>this scheme.<br>Suggest a modiﬁcation to the advanced recovery mechanism to imple-<br>ment point-in-time recovery.<br>c. Later non-erroneous transactions can be reexecuted logically, but cannot<br>be reexecuted using their log records. Why?<br>17.13 Logging of updates is not done explicitly in persistent programming languages.<br>Describe how page access protections provided by modern operating systems<br>can be used to create before and after images of pages that are updated. (Hint:<br>See Exercise 16.12.)<br>17.14 ARIES assumes there is space in each page for an LSN. When dealing with large<br>objects that span multiple pages, such as operating system ﬁles, an entire page<br>may be used by an object, leaving no space for the LSN. Suggest a technique to<br>handle such a situation; your technique must support physical redos but need<br>not support physiological redos.<br>17.15 Explain the difference between a system crash and a “disaster.”<br>17.16 For each of the following requirements, identify the best choice of degree of<br>durability in a remote backup system:<br>a. Data loss must be avoided but some loss of availability may be tolerated.<br>b. Transaction commit must be accomplished quickly, even at the cost of loss<br>of some committed transactions in a disaster.<br>c. A high degree of availability and durability is required, but a longer run-<br>ning time for the transaction commit protocol is acceptable.<br>Bibliographical Notes<br>Gray and Reuter [1993] is an excellent textbook source of information about recovery,<br>including interesting implementation and historical details. Bernstein et al. [1987] is<br>an early textbook source of information on concurrency control and recovery.<br>Two early papers that present initial theoretical work in the area of recovery are<br>Davies [1973] and Bjork [1973]. Chandy et al. [1975], which describes analytic models<br>for rollback and recovery strategies in database systems, is another early work in this<br>area.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>V. Transaction <br>Management<br>17. Recovery System<br>678<br>© The McGraw−Hill <br>Companies, 2001<br>680<br>Chapter 17<br>Recovery System<br>An overview of the recovery scheme of System R is presented by Gray et al.<br>[1981b]. The shadow-paging mechanism of System R is described by Lorie [1977].<br>Tutorial and survey papers on various recovery techniques for database systems in-<br>clude Gray [1978], Lindsay et al. [1980], and Verhofstad [1978]. The concepts of fuzzy<br>checkpointing and fuzzy dumps are described in Lindsay et al. [1980]. A compre-<br>hensive presentation of the principles of recovery is offered by Haerder and Reuter<br>[1983]</span><br><br><span style="background-color: #BDB2FF;" title="Chunk 86 | Start: 1720172 | End: 1740172 | Tokens: 3033">.<br>The state of the art in recovery methods is best illustrated by the ARIES recovery<br>method, described in Mohan et al. [1992] and Mohan [1990b]. Aries and its variants<br>are used in several database products, including IBM DB2 and Microsoft SQL Server.<br>Recovery in Oracle is described in Lahiri et al. [2001].<br>Specialized recovery techniques for index structures are described in Mohan and<br>Levine [1992] and Mohan [1993]; Mohan and Narang [1994] describes recovery tech-<br>niques for client–server architectures, while Mohan and Narang [1991] and Mohan<br>and Narang [1992] describe recovery techniques for parallel database architectures.<br>Remote backup for disaster recovery (loss of an entire computing facility by, for<br>example, ﬁre, ﬂood, or earthquake) is considered in King et al. [1991] and Polyzois<br>and Garcia-Molina [1994].<br>Chapter 24 lists references pertaining to long-duration transactions and related<br>recovery issues.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VI. Database System <br>Architecture<br>Introduction<br>679<br>© The McGraw−Hill <br>Companies, 2001<br>P<br>A<br>R<br>T<br>6<br>Database System Architecture<br>The architecture of a database system is greatly inﬂuenced by the underlying com-<br>puter system on which the database system runs. Database systems can be central-<br>ized, or client–server, where one server machine executes work on behalf of multi-<br>ple client machines. Database systems can also be designed to exploit parallel com-<br>puter architectures. Distributed databases span multiple geographically separated<br>machines.<br>Chapter 18 ﬁrst outlines the architectures of database systems running on server<br>systems, which are used in centralized and client–server architectures. The various<br>processes that together implement the functionality of a database are outlined here.<br>The chapter then outlines parallel computer architectures, and parallel database ar-<br>chitectures designed for different types of parallel computers. Finally, the chapter<br>outlines architectural issues in building a distributed database system.<br>Chapter 19 presents a number of issues that arise in a distributed database, and<br>describes how to deal with each issue. The issues include how to store data, how<br>to ensure atomicity of transactions that execute at multiple sites, how to perform<br>concurrency control, and how to provide high availability in the presence of failures.<br>Distributed query processing and directory systems are also described in this chapter.<br>Chapter 20 describes how various actions of a database, in particular query pro-<br>cessing, can be implemented to exploit parallel processing.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VI. Database System <br>Architecture<br>18. Database System <br>Architecture<br>680<br>© The McGraw−Hill <br>Companies, 2001<br>C<br>H<br>A<br>P<br>T<br>E<br>R<br>1<br>8<br>DatabaseSystemArchitectures<br>The architecture of a database system is greatly inﬂuenced by the underlying com-<br>puter system on which it runs, in particular by such aspects of computer architecture<br>as networking, parallelism, and distribution:<br>• Networking of computers allows some tasks to be executed on a server sys-<br>tem, and some tasks to be executed on client systems. This division of work<br>has led to client–server database systems.<br>• Parallel processing within a computer system allows database-system activi-<br>ties to be speeded up, allowing faster response to transactions, as well as more<br>transactions per second. Queries can be processed in a way that exploits the<br>parallelism offered by the underlying computer system. The need for parallel<br>query processing has led to parallel database systems.<br>• Distributing data across sites or departments in an organization allows those<br>data to reside where they are generated or most needed, but still to be acces-<br>sible from other sites and from other departments. Keeping multiple copies<br>of the database across different sites also allows large organizations to con-<br>tinue their database operations even when one site is affected by a natural<br>disaster, such as ﬂood, ﬁre, or earthquake. Distributed database systems han-<br>dle geographically or administratively distributed data spread across multiple<br>database systems.<br>We study the architecture of database systems in this chapter, starting with the<br>traditional centralized systems, and covering client–server, parallel, and distributed<br>database systems.<br>18.1<br>CentralizedandClient–ServerArchitectures<br>Centralized database systems are those that run on a single computer system and do<br>not interact with other computer systems. Such database systems span a range from<br>683<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VI. Database System <br>Architecture<br>18. Database System <br>Architecture<br>681<br>© The McGraw−Hill <br>Companies, 2001<br>684<br>Chapter 18<br>Database System Architectures<br>single-user database systems running on personal computers to high-performance<br>database systems running on high-end server systems. Client–server systems, on<br>the other hand, have functionality split between a server system, and multiple client<br>systems.<br>18.1.1<br>Centralized Systems<br>A modern, general-purpose computer system consists of one to a few CPUs and a<br>number of device controllers that are connected through a common bus that provides<br>access to shared memory (Figure 18.1). The CPUs have local cache memories that store<br>local copies of parts of the memory, to speed up access to data. Each device controller<br>is in charge of a speciﬁc type of device (for example, a disk drive, an audio device,<br>or a video display). The CPUs and the device controllers can execute concurrently,<br>competing for memory access. Cache memory reduces the contention for memory<br>access, since it reduces the number of times that the CPU needs to access the shared<br>memory.<br>We distinguish two ways in which computers are used: as single-user systems<br>and as multiuser systems. Personal computers and workstations fall into the ﬁrst cat-<br>egory. A typical single-user system is a desktop unit used by a single person, usually<br>with only one CPU and one or two hard disks, and usually only one person using the<br>Computer-System <br>Structures<br>memory controller<br>memory<br>system bus<br>disk<br>disk<br>printer<br>tape drives<br>CPU<br>disk<br>controller<br>printer<br>controller<br>tape-drive<br>controller<br>Figure 18.1<br>A centralized computer system.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VI. Database System <br>Architecture<br>18. Database System <br>Architecture<br>682<br>© The McGraw−Hill <br>Companies, 2001<br>18.1<br>Centralized and Client–Server Architectures<br>685<br>machine at a time. A typical multiuser system, on the other hand, has more disks<br>and more memory, may have multiple CPUs and has a multiuser operating system.<br>It serves a large number of users who are connected to the system via terminals.<br>Database systems designed for use by single users usually do not provide many of<br>the facilities that a multiuser database provides. In particular, they may not support<br>concurrency control, which is not required when only a single user can generate up-<br>dates. Provisions for crash-recovery in such systems are either absent or primitive–<br>for example, they may consist of simply making a backup of the database before any<br>update. Many such systems do not support SQL, and provide a simpler query lan-<br>guage, such as a variant of QBE. In contrast, database systems designed for multiuser<br>systems support the full transactional features that we have studied earlier.<br>Although general-purpose computer systems today have multiple processors, they<br>have coarse-granularity parallelism, with only a few processors (about two to four,<br>typically), all sharing the main memory. Databases running on such machines usu-<br>ally do not attempt to partition a single query among the processors; instead, they<br>run each query on a single processor, allowing multiple queries to run concurrently.<br>Thus, such systems support a higher throughput; that is, they allow a greater num-<br>ber of transactions to run per second, although individual transactions do not run<br>any faster.<br>Databases designed for single-processor machines already provide multitasking,<br>allowing multiple processes to run on the same processor in a time-shared manner,<br>giving a view to the user of multiple processes running in parallel. Thus, coarse-<br>granularity parallel machines logically appear to be identical to single-processor<br>machines, and database systems designed for time-shared machines can be easily<br>adapted to run on them.<br>In contrast, machines with ﬁne-granularity parallelism have a large number of<br>processors, and database systems running on such machines attempt to parallelize<br>single tasks (queries, for example) submitted by users. We study the architecture of<br>parallel database systems in Section 18.3.<br>18.1.2<br>Client–Server Systems<br>As personal computers became faster, more powerful, and cheaper, there was a shift<br>away from the centralized system architecture. Personal computers supplanted ter-<br>minals connected to centralized systems. Correspondingly, personal computers as-<br>sumed the user-interface functionality that used to be handled directly by the cen-<br>tralized systems. As a result, centralized systems today act as server systems that<br>satisfy requests generated by client systems. Figure 18.2 shows the general structure<br>of a client–server system.<br>Database functionality can be broadly divided into two parts—the front end and<br>the back end—as in Figure 18.3. The back end manages access structures, query<br>evaluation and optimization, concurrency control, and recovery. The front end of a<br>database system consists of tools such as forms, report writers, and graphical user-<br>interface facilities. The interface between the front end and the back end is through<br>SQL, or through an application program.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VI. Database System <br>Architecture<br>18. Database System <br>Architecture<br>683<br>© The McGraw−Hill <br>Companies, 2001<br>686<br>Chapter 18<br>Database System Architectures<br>client<br>client<br>client<br>client<br>server<br>…<br>network<br>Figure 18.2<br>General structure of a client–server system.<br>Standards such as ODBC and JDBC, which we saw in Chapter 4, were developed<br>to interface clients with servers. Any client that uses the ODBC or JDBC interfaces can<br>connect to any server that provides the interface.<br>In earlier-generation database systems, the lack of such standards necessitated that<br>the front end and the back end be provided by the same software vendor. With the<br>growth of interface standards, the front-end user interface and the back-end server<br>are often provided by different vendors. Application development tools are used to con-<br>struct user interfaces; they provide graphical tools that can be used to construct inter-<br>faces without any programming. Some of the popular application development tools<br>are PowerBuilder, Magic, and Borland Delphi; Visual Basic is also widely used for<br>application development.<br>Further, certain application programs, such as spreadsheets and statistical-analysis<br>packages, use the client–server interface directly to access data from a back-end<br>server. In effect, they provide front ends specialized for particular tasks.<br>Some transaction-processing systems provide a transactional remote procedure<br>call interface to connect clients with a server. These calls appear like ordinary pro-<br>cedure calls to the programmer, but all the remote procedure calls from a client are<br>enclosed in a single transaction at the server end. Thus, if the transaction aborts, the<br>server can undo the effects of the individual remote procedure calls.<br>SQL user-<br>interface<br>forms<br>interface<br>report<br>writer<br>graphical<br>interface<br>SQL engine<br>front-end<br>interface<br>(SQL + API)<br>back-end<br>Figure 18.3<br>Front-end and back-end functionality.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VI. Database System <br>Architecture<br>18. Database System <br>Architecture<br>684<br>© The McGraw−Hill <br>Companies, 2001<br>18.2<br>Server System Architectures<br>687<br>18.2<br>Server System Architectures<br>Server systems can be broadly categorized as transaction servers and data servers.<br>• Transaction-server systems, also called query-server systems, provide an in-<br>terface to which clients can send requests to perform an action, in response<br>to which they execute the action and send back results to the client. Usually,<br>client machines ship transactions to the server systems, where those transac-<br>tions are executed, and results are shipped back to clients that are in charge<br>of displaying the data. Requests may be speciﬁed by using SQL, or through a<br>specialized application program interface.<br>• Data-server systems allow clients to interact with the servers by making re-<br>quests to read or update data, in units such as ﬁles or pages. For example,<br>ﬁle servers provide a ﬁle-system interface where clients can create, update,<br>read, and delete ﬁles. Data servers for database systems offer much more func-<br>tionality; they support units of data—such as pages, tuples, or objects—that<br>are smaller than a ﬁle. They provide indexing facilities for data, and provide<br>transaction facilities so that the data are never left in an inconsistent state if a<br>client machine or process fails.<br>Of these, the transaction-server architecture is by far the more widely used archi-<br>tecture. We shall elaborate on the transaction-server and data-server architectures in<br>Sections 18.2.1 and 18.2.2.<br>18.2.1<br>Transaction Server Process Structure<br>A typical transaction server system today consists of multiple processes accessing<br>data in shared memory, as in Figure 18.4. The processes that form part of the database<br>system include<br>• Server processes: These are processes that receive user queries (transactions),<br>execute them, and send the results back. The queries may be submitted to the<br>server processes from a a user interface, or from a user process running em-<br>bedded SQL, or via JDBC, ODBC, or similar protocols. Some database systems<br>use a separate process for each user session, and a few use a single database<br>process for all user sessions, but with multiple threads so that multiple queries<br>can execute concurrently. (A thread is like a process, but multiple threads ex-<br>ecute as part of the same process, and all threads within a process run in the<br>same virtual memory space. Multiple threads within a process can execute<br>concurrently.) Many database systems use a hybrid architecture, with multi-<br>ple processes, each one running multiple threads.<br>• Lock manager process: This process implements lock manager functionality,<br>which includes lock grant, lock release, and deadlock detection.<br>• Database writer process: There are one or more processes that output modi-<br>ﬁed buffer blocks back to disk on a continuous basis.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VI. Database System <br>Architecture<br>18. Database System <br>Architecture<br>685<br>© The McGraw−Hill <br>Companies, 2001<br>688<br>Chapter 18<br>Database System Architectures<br>lock <br>manager<br>process<br>lock table<br>log buffer<br>shared<br>memory <br>database<br> writer<br>process<br>log writer<br>process<br>checkpoint<br>process<br>process<br>monitor<br>process<br>server<br>process<br>server<br>process<br>user<br>process<br>user<br>process<br>server<br>process<br>user<br>process<br>ODBC<br>JDBC<br>log disks<br>data disks<br>query plan cache<br>buffer pool<br>Figure 18.4<br>Shared memory and process structure.<br>• Log writer process: This process outputs log records from the log record buffer<br>to stable storage. Server processes simply add log records to the log record<br>buffer in shared memory, and if a log force is required, they request the log<br>writer process to output log records.<br>• Checkpoint process: This process performs periodic checkpoints.<br>• Process monitor process: This process monitors other processes, and if any of<br>them fails, it takes recovery actions for the process, such as aborting any trans-<br>action being executed by the failed process, and then restarting the process.<br>The shared memory contains all shared data, such as:<br>• Buffer pool<br>• Lock table<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VI. Database System <br>Architecture<br>18. Database System <br>Architecture<br>686<br>© The McGraw−Hill <br>Companies, 2001<br>18.2<br>Server System Architectures<br>689<br>• Log buffer, containing log records waiting to be output to the log on stable<br>storage<br>• Cached query plans, which can be reused if the same query is submitted again<br>All database processes can access the data in shared memory. Since multiple pro-<br>cesses may read or perform updates on data structures in shared memory, there must<br>be a mechanism to ensure that only one of them is modifying any data structure at<br>a time, and no process is reading a data structure while it is being written by others.<br>Such mutual exclusion can be implemented by means of operating system functions<br>called semaphores. Alternative implementations, with less overheads, use special<br>atomic instructions supported by the computer hardware; one type of atomic in-<br>struction tests a memory location and sets it to 1 atomically. Further implementation<br>details of mutual exclusion can be found in any standard operating system textbook.<br>The mutual exclusion mechanisms are also used to implement latches.<br>To avoid the overhead of message passing, in many database systems, server pro-<br>cesses implement locking by directly updating the lock table (which is in shared<br>memory), instead of sending lock request messages to a lock manager process. The<br>lock request procedure executes the actions that the lock manager process would<br>take on getting a lock request. The actions on lock request and release are like those<br>in Section 16.1.4, but with two signiﬁcant differences:<br>• Since multiple server processes may access shared memory, mutual exclusion<br>must be ensured on the lock table.<br>• If a lock cannot be obtained immediately because of a lock conﬂict, the lock<br>request code keeps monitoring the lock table to check when the lock has been<br>granted. The lock release code updates the lock table to note which process<br>has been granted the lock.<br>To avoid repeated checks on the lock table, operating system semaphores<br>can be used by the lock request code to wait for a lock grant notiﬁcation. The<br>lock release code must then use the semaphore mechanism to notify waiting<br>transactions that their locks have been granted.<br>Even if the system handles lock requests through shared memory, it still uses the lock<br>manager process for deadlock detection.<br>18.2.2<br>Data Servers<br>Data-server systems are used in local-area networks, where there is a high-speed<br>connection between the clients and the server, the client machines are comparable in<br>processing power to the server machine, and the tasks to be executed are computa-<br>tion intensive. In such an environment, it makes sense to ship data to client machines,<br>to perform all processing at the client machine (which may take a while), and then<br>to ship the data back to the server machine. Note that this architecture requires full<br>back-end functionality at the clients. Data-server architectures have been particularly<br>popular in object-oriented database systems.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VI. Database System <br>Architecture<br>18. Database System <br>Architecture<br>687<br>© The McGraw−Hill <br>Companies, 2001<br>690<br>Chapter 18<br>Database System Architectures<br>Interesting issues arise in such an architecture, since the time cost of communica-<br>tion between the client and the server is high compared to that of a local memory<br>reference (milliseconds, versus less than 100 nanoseconds):<br>• Page shipping versus item shipping. The unit of communication for data can<br>be of coarse granularity, such as a page, or ﬁne granularity, such as a tuple (or<br>an object, in the context of object-oriented database systems). We use the term<br>item to refer to both tuples and objects.<br>If the unit of communication is a single item, the overhead of message pass-<br>ing is high compared to the amount of data transmitted. Instead, when an item<br>is requested, it makes sense also to send back other items that are likely to be<br>used in the near future. Fetching items even before th</span><br><br><span style="background-color: #FFC6FF;" title="Chunk 87 | Start: 1740174 | End: 1760174 | Tokens: 3224">ey are requested is called<br>prefetching. Page shipping can be considered a form of prefetching if multi-<br>ple items reside on a page, since all the items in the page are shipped when a<br>process desires to access a single item in the page.<br>• Locking. Locks are usually granted by the server for the data items that it<br>ships to the client machines. A disadvantage of page shipping is that client<br>machines may be granted locks of too coarse a granularity—a lock on a page<br>implicitly locks all items contained in the page. Even if the client is not access-<br>ing some items in the page, it has implicitly acquired locks on all prefetched<br>items. Other client machines that require locks on those items may be blocked<br>unnecessarily. Techniques for lock de-escalation, have been proposed where<br>the server can request its clients to transfer back locks on prefetched items. If<br>the client machine does not need a prefetched item, it can transfer locks on the<br>item back to the server, and the locks can then be allocated to other clients.<br>• Data caching. Data that are shipped to a client on behalf of a transaction can be<br>cached at the client, even after the transaction completes, if sufﬁcient storage<br>space is available. Successive transactions at the same client may be able to<br>make use of the cached data. However, cache coherency is an issue: Even if a<br>transaction ﬁnds cached data, it must make sure that those data are up to date,<br>since they may have been updated by a different client after they were cached.<br>Thus, a message must still be exchanged with the server to check validity of<br>the data, and to acquire a lock on the data.<br>• Lock caching. If the use of data is mostly partitioned among the clients, with<br>clients rarely requesting data that are also requested by other clients, locks can<br>also be cached at the client machine. Suppose that a client ﬁnds a data item in<br>the cache, and that it also ﬁnds the lock required for an access to the data item<br>in the cache. Then, the access can proceed without any communication with<br>the server. However, the server must keep track of cached locks; if a client re-<br>quests a lock from the server, the server must call back all conﬂicting locks on<br>the data item from any other client machines that have cached the locks. The<br>task becomes more complicated when machine failures are taken into account.<br>This technique differs from lock de-escalation in that lock caching takes place<br>across transactions; otherwise, the two techniques are similar.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VI. Database System <br>Architecture<br>18. Database System <br>Architecture<br>688<br>© The McGraw−Hill <br>Companies, 2001<br>18.3<br>Parallel Systems<br>691<br>The bibliographical references provide more information about client–server data-<br>base systems.<br>18.3<br>Parallel Systems<br>Parallel systems improve processing and I/O speeds by using multiple CPUs and<br>disks in parallel. Parallel machines are becoming increasingly common, making the<br>study of parallel database systems correspondingly more important. The driving<br>force behind parallel database systems is the demands of applications that have to<br>query extremely large databases (of the order of terabytes—that is, 1012 bytes) or<br>that have to process an extremely large number of transactions per second (of the or-<br>der of thousands of transactions per second). Centralized and client–server database<br>systems are not powerful enough to handle such applications.<br>In parallel processing, many operations are performed simultaneously, as opposed<br>to serial processing, in which the computational steps are performed sequentially. A<br>coarse-grain parallel machine consists of a small number of powerful processors; a<br>massively parallel or ﬁne-grain parallel machine uses thousands of smaller proces-<br>sors. Most high-end machines today offer some degree of coarse-grain parallelism:<br>Two or four processor machines are common. Massively parallel computers can be<br>distinguished from the coarse-grain parallel machines by the much larger degree of<br>parallelism that they support. Parallel computers with hundreds of CPUs and disks<br>are available commercially.<br>There are two main measures of performance of a database system: (1) through-<br>put, the number of tasks that can be completed in a given time interval, and (2) re-<br>sponse time, the amount of time it takes to complete a single task from the time<br>it is submitted. A system that processes a large number of small transactions can<br>improve throughput by processing many transactions in parallel. A system that pro-<br>cesses large transactions can improve response time as well as throughput by per-<br>forming subtasks of each transaction in parallel.<br>18.3.1<br>Speedup and Scaleup<br>Two important issues in studying parallelism are speedup and scaleup. Running a<br>given task in less time by increasing the degree of parallelism is called speedup.<br>Handling larger tasks by increasing the degree of parallelism is called scaleup.<br>Consider a database application running on a parallel system with a certain num-<br>ber of processors and disks. Now suppose that we increase the size of the system by<br>increasing the number or processors, disks, and other components of the system. The<br>goal is to process the task in time inversely proportional to the number of processors<br>and disks allocated. Suppose that the execution time of a task on the larger machine<br>is TL, and that the execution time of the same task on the smaller machine is TS.<br>The speedup due to parallelism is deﬁned as TS/TL. The parallel system is said to<br>demonstrate linear speedup if the speedup is N when the larger system has N times<br>the resources (CPU, disk, and so on) of the smaller system. If the speedup is less than<br>N, the system is said to demonstrate sublinear speedup. Figure 18.5 illustrates linear<br>and sublinear speedup.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VI. Database System <br>Architecture<br>18. Database System <br>Architecture<br>689<br>© The McGraw−Hill <br>Companies, 2001<br>692<br>Chapter 18<br>Database System Architectures<br>linear speedup<br>sublinear speedup<br>resources<br>speed<br>Figure 18.5<br>Speedup with increasing resources.<br>Scaleup relates to the ability to process larger tasks in the same amount of time by<br>providing more resources. Let Q be a task, and let QN be a task that is N times bigger<br>than Q. Suppose that the execution time of task Q on a given machine MS is TS, and<br>the execution time of task QN on a parallel machine ML, which is N times larger than<br>MS, is TL. The scaleup is then deﬁned as TS/TL. The parallel system ML is said to<br>demonstrate linear scaleup on task Q if TL = TS. If TL &gt; TS, the system is said to<br>demonstrate sublinear scaleup. Figure 18.6 illustrates linear and sublinear scaleups<br>(where the resources increase proportional to problem size). There are two kinds of<br>scaleup that are relevant in parallel database systems, depending on how the size of<br>the task is measured:<br>• In batch scaleup, the size of the database increases, and the tasks are large jobs<br>whose runtime depends on the size of the database. An example of such a task<br>is a scan of a relation whose size is proportional to the size of the database.<br>Thus, the size of the database is the measure of the size of the problem. Batch<br>scaleup also applies in scientiﬁc applications, such as executing a query at an<br>N-times ﬁner resolution or performing an N-times longer simulation.<br>• In transaction scaleup, the rate at which transactions are submitted to the<br>database increases and the size of the database increases proportionally to<br>the transaction rate. This kind of scaleup is what is relevant in transaction-<br>processing systems where the transactions are small updates—for example, a<br>deposit or withdrawal from an account—and transaction rates grow as more<br>accounts are created. Such transaction processing is especially well adapted<br>for parallel execution, since transactions can run concurrently and indepen-<br>dently on separate processors, and each transaction takes roughly the same<br>amount of time, even if the database grows.<br>Scaleup is usually the more important metric for measuring efﬁciency of parallel<br>database systems. The goal of parallelism in database systems is usually to make sure<br>that the database system can continue to perform at an acceptable speed, even as the<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VI. Database System <br>Architecture<br>18. Database System <br>Architecture<br>690<br>© The McGraw−Hill <br>Companies, 2001<br>18.3<br>Parallel Systems<br>693<br>linear scaleup<br>sublinear scaleup<br>problem size<br>TS<br>TL<br>Figure 18.6<br>Scaleup with increasing problem size and resources.<br>size of the database and the number of transactions increases. Increasing the capac-<br>ity of the system by increasing the parallelism provides a smoother path for growth<br>for an enterprise than does replacing a centralized system by a faster machine (even<br>assuming that such a machine exists). However, we must also look at absolute per-<br>formance numbers when using scaleup measures; a machine that scales up linearly<br>may perform worse than a machine that scales less than linearly, simply because the<br>latter machine is much faster to start off with.<br>A number of factors work against efﬁcient parallel operation and can diminish<br>both speedup and scaleup.<br>• Startup costs. There is a startup cost associated with initiating a single process.<br>In a parallel operation consisting of thousands of processes, the startup time<br>may overshadow the actual processing time, affecting speedup adversely.<br>• Interference. Since processes executing in a parallel system often access shared<br>resources, a slowdown may result from the interference of each new process as<br>it competes with existing processes for commonly held resources, such as a<br>system bus, or shared disks, or even locks. Both speedup and scaleup are af-<br>fected by this phenomenon.<br>• Skew. By breaking down a single task into a number of parallel steps, we<br>reduce the size of the average step. Nonetheless, the service time for the single<br>slowest step will determine the service time for the task as a whole. It is often<br>difﬁcult to divide a task into exactly equal-sized parts, and the way that the<br>sizes are distributed is therefore skewed. For example, if a task of size 100 is<br>divided into 10 parts, and the division is skewed, there may be some tasks of<br>size less than 10 and some tasks of size more than 10; if even one task happens<br>to be of size 20, the speedup obtained by running the tasks in parallel is only<br>ﬁve, instead of ten as we would have hoped.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VI. Database System <br>Architecture<br>18. Database System <br>Architecture<br>691<br>© The McGraw−Hill <br>Companies, 2001<br>694<br>Chapter 18<br>Database System Architectures<br>18.3.2<br>Interconnection Networks<br>Parallel systems consist of a set of components (processors, memory, and disks) that<br>can communicate with each other via an interconnection network. Figure 18.7 shows<br>three commonly used types of interconnection networks:<br>• Bus. All the system components can send data on and receive data from a sin-<br>gle communication bus. This type of interconnection is shown in Figure 18.7a.<br>The bus could be an Ethernet or a parallel interconnect. Bus architectures work<br>well for small numbers of processors. However, they do not scale well with in-<br>creasing parallelism, since the bus can handle communication from only one<br>component at a time.<br>• Mesh. The components are nodes in a grid, and each component connects<br>to all its adjacent components in the grid. In a two-dimensional mesh each<br>node connects to four adjacent nodes, while in a three-dimensional mesh each<br>node connects to six adjacent nodes. Figure 18.7b shows a two-dimensional<br>mesh. Nodes that are not directly connected can communicate with one an-<br>other by routing messages via a sequence of intermediate nodes that are di-<br>rectly connected to one another. The number of communication links grows as<br>the number of components grows, and the communication capacity of a mesh<br>therefore scales better with increasing parallelism.<br>• Hypercube. The components are numbered in binary, and a component is<br>connected to another if the binary representations of their numbers differ in<br>exactly one bit. Thus, each of the n components is connected to log(n) other<br>components. Figure 18.7c shows a hypercube with 8 nodes. In a hypercube<br>interconnection, a message from a component can reach any other component<br>by going through at most log(n) links. In contrast, in a mesh architecture a<br>component may be 2(√n −1) links away from some of the other components<br>(or √n links away, if the mesh interconnection wraps around at the edges of<br>the grid). Thus communication delays in a hypercube are signiﬁcantly lower<br>than in a mesh.<br>110<br>111<br>011<br>101<br>100<br>000<br>(c) hypercube<br>(b) mesh<br>(a) bus<br>001<br>010<br>Figure 18.7<br>Interconnection networks.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VI. Database System <br>Architecture<br>18. Database System <br>Architecture<br>692<br>© The McGraw−Hill <br>Companies, 2001<br>18.3<br>Parallel Systems<br>695<br>18.3.3<br>Parallel Database Architectures<br>There are several architectural models for parallel machines. Among the most promi-<br>nent ones are those in Figure 18.8 (in the ﬁgure, M denotes memory, P denotes a<br>processor, and disks are shown as cylinders):<br>• Shared memory. All the processors share a common memory (Figure 18.8a).<br>• Shared disk. All the processors share a common set of disks (Figure 18.8b).<br>Shared-disk systems are sometimes called clusters.<br>• Shared nothing. The processors share neither a common memory nor com-<br>mon disk (Figure 18.8c).<br>• Hierarchical. This model is a hybrid of the preceding three architectures (Fig-<br>ure 18.8d).<br>In Sections 18.3.3.1 through 18.3.3.4, we elaborate on each of these models.<br>Techniques used to speed up transaction processing on data-server systems, such<br>as data and lock caching and lock de-escalation, outlined in Section 18.2.2, can also be<br>used in shared-disk parallel databases as well as in shared-nothing parallel databases.<br>In fact, they are very important for efﬁcient transaction processing in such systems.<br>P<br>P<br>M<br>P<br>P<br>P<br>M<br>M<br>M<br>P<br>P<br>P<br>P<br>P<br>P<br>P<br>P<br>P<br>P<br>P<br>P<br>P<br>P<br>P<br>(a) shared memory<br>P<br>P<br>P<br>P<br>(c) shared nothing<br>(d) hierarchical<br>P<br>M<br>P<br>P<br>P<br>P<br>(b) shared disk<br>P<br>M<br>P<br>M<br>P<br>M<br>M<br>M<br>M<br>M<br>P<br>M<br>M<br>Figure 18.8<br>Parallel database architectures.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VI. Database System <br>Architecture<br>18. Database System <br>Architecture<br>693<br>© The McGraw−Hill <br>Companies, 2001<br>696<br>Chapter 18<br>Database System Architectures<br>18.3.3.1<br>Shared Memory<br>In a shared-memory architecture, the processors and disks have access to a common<br>memory, typically via a bus or through an interconnection network. The beneﬁt of<br>shared memory is extremely efﬁcient communication between processors—data in<br>shared memory can be accessed by any processor without being moved with soft-<br>ware. A processor can send messages to other processors much faster by using mem-<br>ory writes (which usually take less than a microsecond) than by sending a message<br>through a communication mechanism. The downside of shared-memory machines is<br>that the architecture is not scalable beyond 32 or 64 processors because the bus or the<br>interconnection network becomes a bottleneck (since it is shared by all processors).<br>Adding more processors does not help after a point, since the processors will spend<br>most of their time waiting for their turn on the bus to access memory.<br>Shared-memory architectures usually have large memory caches at each proces-<br>sor, so that referencing of the shared memory is avoided whenever possible. How-<br>ever, at least some of the data will not be in the cache, and accesses will have to go<br>to the shared memory. Moreover, the caches need to be kept coherent; that is, if a<br>processor performs a write to a memory location, the data in that memory location<br>should be either updated at or removed from any processor where the data is cached.<br>Maintaining cache-coherency becomes an increasing overhead with increasing num-<br>ber of processors. Consequently, shared-memory machines are not capable of scaling<br>up beyond a point; current shared-memory machines cannot support more than 64<br>processors.<br>18.3.3.2<br>Shared Disk<br>In the shared-disk model, all processors can access all disks directly via an intercon-<br>nection network, but the processors have private memories. There are two advan-<br>tages of this architecture over a shared-memory architecture. First, since each pro-<br>cessor has its own memory, the memory bus is not a bottleneck. Second, it offers a<br>cheap way to provide a degree of fault tolerance: If a processor (or its memory) fails,<br>the other processors can take over its tasks, since the database is resident on disks<br>that are accessible from all processors. We can make the disk subsystem itself fault<br>tolerant by using a RAID architecture, as described in Chapter 11. The shared-disk<br>architecture has found acceptance in many applications.<br>The main problem with a shared-disk system is again scalability. Although the<br>memory bus is no longer a bottleneck, the interconnection to the disk subsystem is<br>now a bottleneck; it is particularly so in a situation where the database makes a large<br>number of accesses to disks. Compared to shared-memory systems, shared-disk sys-<br>tems can scale to a somewhat larger number of processors, but communication across<br>processors is slower (up to a few milliseconds in the absence of special-purpose hard-<br>ware for communication), since it has to go through a communication network.<br>DEC clusters running Rdb were one of the early commercial users of the shared-<br>disk database architecture. (Rdb is now owned by Oracle, and is called Oracle Rdb.<br>Digital Equipment Corporation (DEC) is now owned by Compaq.)<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VI. Database System <br>Architecture<br>18. Database System <br>Architecture<br>694<br>© The McGraw−Hill <br>Companies, 2001<br>18.4<br>Distributed Systems<br>697<br>18.3.3.3<br>Shared Nothing<br>In a shared-nothing system, each node of the machine consists of a processor, mem-<br>ory, and one or more disks. The processors at one node may communicate with an-<br>other processor at another node by a high-speed interconnection network. A node<br>functions as the server for the data on the disk or disks that the node owns. Since<br>local disk references are serviced by local disks at each processor, the shared-nothing<br>model overcomes the disadvantage of requiring all I/O to go through a single inter-<br>connection network; only queries, accesses to nonlocal disks, and result relations pass<br>through the network. Moreover, the interconnection networks for shared-nothing<br>systems are usually designed to be scalable, so that their transmission capacity in-<br>creases as more nodes are added. Consequently, shared-nothing architectures are<br>more scalable and can easily support a large number of processors. The main draw-<br>backs of shared-nothing systems are the costs of communication and of nonlocal disk<br>access, which are higher than in a shared-memory or shared-disk architecture since<br>sending data involves software interaction at both ends.<br>The Teradata database machine was among the earliest commercial systems to<br>use the shared-nothing database architecture. The Grace and the Gamma research<br>prototypes also used shared-nothing architectures.<br>18.3.3.4<br>Hierarchical<br>The hierarchical architecture combines the characteristics of shared-memory, shared-<br>disk, and shared-nothing architectures. At the top level, the system consists of nodes<br>connected by an interconnection network, and do not share disks or memory with<br>one another. Thus, the top level is a shared-nothing architecture. Each node of the sys-<br>tem could actually be a shared-memory system with a few processors. Alternatively,<br>each node could be a shared-disk system, and each of the systems sharing a set of<br>disks could be a shared-memory system. Thus, a system could be built as a hierarchy,<br>with shared-memory architecture with a few processo</span><br><br><span style="background-color: #FFADAD;" title="Chunk 88 | Start: 1760176 | End: 1780176 | Tokens: 3021">rs at the base, and a shared-<br>nothing architecture at the top, with possibly a shared-disk architecture in the mid-<br>dle. Figure 18.8d illustrates a hierarchical architecture with shared-memory nodes<br>connected together in a shared-nothing architecture. Commercial parallel database<br>systems today run on several of these architectures.<br>Attempts to reduce the complexity of programming such systems have yielded<br>distributed virtual-memory architectures, where logically there is a single shared<br>memory, but physically there are multiple disjoint memory systems; the virtual-<br>memory-mapping hardware, coupled with system software, allows each processor<br>to view the disjoint memories as a single virtual memory. Since access speeds differ,<br>depending on whether the page is available locally or not, such an architecture is also<br>referred to as a nonuniform memory architecture (NUMA).<br>18.4<br>Distributed Systems<br>In a distributed database system, the database is stored on several computers. The<br>computers in a distributed system communicate with one another through various<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VI. Database System <br>Architecture<br>18. Database System <br>Architecture<br>695<br>© The McGraw−Hill <br>Companies, 2001<br>698<br>Chapter 18<br>Database System Architectures<br>communication media, such as high-speed networks or telephone lines. They do not<br>share main memory or disks. The computers in a distributed system may vary in size<br>and function, ranging from workstations up to mainframe systems.<br>The computers in a distributed system are referred to by a number of different<br>names, such as sites or nodes, depending on the context in which they are mentioned.<br>We mainly use the term site, to emphasize the physical distribution of these systems.<br>The general structure of a distributed system appears in Figure 18.9.<br>The main differences between shared-nothing parallel databases and distributed<br>databases are that distributed databases are typically geographically separated, are<br>separately administered, and have a slower interconnection. Another major differ-<br>ence is that, in a distributed database system, we differentiate between local and<br>global transactions. A local transaction is one that accesses data only from sites<br>where the transaction was initiated. A global transaction, on the other hand, is one<br>that either accesses data in a site different from the one at which the transaction was<br>initiated, or accesses data in several different sites.<br>There are several reasons for building distributed database systems, including<br>sharing of data, autonomy, and availability.<br>• Sharing data. The major advantage in building a distributed database system<br>is the provision of an environment where users at one site may be able to<br>access the data residing at other sites. For instance, in a distributed banking<br>system, where each branch stores data related to that branch, it is possible for<br>a user in one branch to access data in another branch. Without this capability,<br>a user wishing to transfer funds from one branch to another would have to<br>resort to some external mechanism that would couple existing systems.<br>• Autonomy. The primary advantage of sharing data by means of data distri-<br>bution is that each site is able to retain a degree of control over data that<br>site A<br>site C<br>site B<br>communication<br>via network<br>network<br>Figure 18.9<br>A distributed system.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VI. Database System <br>Architecture<br>18. Database System <br>Architecture<br>696<br>© The McGraw−Hill <br>Companies, 2001<br>18.4<br>Distributed Systems<br>699<br>are stored locally. In a centralized system, the database administrator of the<br>central site controls the database. In a distributed system, there is a global<br>database administrator responsible for the entire system. A part of these re-<br>sponsibilities is delegated to the local database administrator for each site.<br>Depending on the design of the distributed database system, each adminis-<br>trator may have a different degree of local autonomy. The possibility of local<br>autonomy is often a major advantage of distributed databases.<br>• Availability. If one site fails in a distributed system, the remaining sites may<br>be able to continue operating. In particular, if data items are replicated in sev-<br>eral sites, a transaction needing a particular data item may ﬁnd that item in<br>any of several sites. Thus, the failure of a site does not necessarily imply the<br>shutdown of the system.<br>The failure of one site must be detected by the system, and appropriate<br>action may be needed to recover from the failure. The system must no longer<br>use the services of the failed site. Finally, when the failed site recovers or is<br>repaired, mechanisms must be available to integrate it smoothly back into the<br>system.<br>Although recovery from failure is more complex in distributed systems<br>than in centralized systems, the ability of most of the system to continue to<br>operate despite the failure of one site results in increased availability. Avail-<br>ability is crucial for database systems used for real-time applications. Loss of<br>access to data by, for example, an airline may result in the loss of potential<br>ticket buyers to competitors.<br>18.4.1<br>An Example of a Distributed Database<br>Consider a banking system consisting of four branches in four different cities. Each<br>branch has its own computer, with a database of all the accounts maintained at that<br>branch. Each such installation is thus a site. There also exists one single site that<br>maintains information about all the branches of the bank. Each branch maintains<br>(among others) a relation account(Account-schema), where<br>Account-schema = (account-number, branch-name, balance)<br>The site containing information about all the branches of the bank maintains the re-<br>lation branch(Branch-schema), where<br>Branch-schema = (branch-name, branch-city, assets)<br>There are other relations maintained at the various sites; we ignore them for the pur-<br>pose of our example.<br>To illustrate the difference between the two types of transactions—local and<br>global—at the sites, consider a transaction to add $50 to account number A-177<br>located at the Valleyview branch. If the transaction was initiated at the Valleyview<br>branch, then it is considered local; otherwise, it is considered global. A transaction<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VI. Database System <br>Architecture<br>18. Database System <br>Architecture<br>697<br>© The McGraw−Hill <br>Companies, 2001<br>700<br>Chapter 18<br>Database System Architectures<br>to transfer $50 from account A-177 to account A-305, which is located at the Hillside<br>branch, is a global transaction, since accounts in two different sites are accessed as a<br>result of its execution.<br>In an ideal distributed database system, the sites would share a common global<br>schema (although some relations may be stored only at some sites), all sites would<br>run the same distributed database-management software, and the sites would be<br>aware of each other’s existence. If a distributed database is built from scratch, it<br>would indeed be possible to achieve the above goals. However, in reality a dis-<br>tributed database has to be constructed by linking together multiple already-existing<br>database systems, each with its own schema and possibly running different database-<br>management software. Such systems are sometimes called multidatabase systems<br>or heterogeneous distributed database systems. We discuss these systems in Sec-<br>tion 19.8, where we show how to achieve a degree of global control despite the het-<br>erogeneity of the component systems.<br>18.4.2<br>Implementation Issues<br>Atomicity of transactions is an important issue in building a distributed database sys-<br>tem. If a transaction runs across two sites, unless the system designers are careful, it<br>may commit at one site and abort at another, leading to an inconsistent state. Trans-<br>action commit protocols ensure such a situation cannot arise. The two-phase commit<br>protocol (2PC) is the most widely used of these protocols.<br>The basic idea behind 2PC is for each site to execute the transaction till just before<br>commit, and then leave the commit decision to a single coordinator site; the trans-<br>action is said to be in the ready state at a site at this point. The coordinator decides<br>to commit the transaction only if the transaction reaches the ready state at every site<br>where it executed; otherwise (for example, if the transaction aborts at any site), the<br>coordinator decides to abort the transaction. Every site where the transaction exe-<br>cuted must follow the decision of the coordinator. If a site fails when a transaction is<br>in ready state, when the site recovers from failure it should be in a position to either<br>commit or abort the transaction, depending on the decision of the coordinator. The<br>2PC protocol is described in detail in Section 19.4.1.<br>Concurrency control is another issue in a distributed database. Since a transac-<br>tion may access data items at several sites, transaction managers at several sites may<br>need to coordinate to implement concurrency control. If locking is used (as is almost<br>always the case in practice), locking can be performed locally at the sites containing<br>accessed data items, but there is also a possibility of deadlock involving transactions<br>originating at multiple sites. Therefore deadlock detection needs to be carried out<br>across multiple sites. Failures are more common in distributed systems since not only<br>may computers fail, but communication links may also fail. Replication of data items,<br>which is the key to the continued functioning of distributed databases when failures<br>occur, further complicates concurrency control. Section 19.5 provides detailed cover-<br>age of concurrency control in distributed databases.<br>The standard transaction models, based on multiple actions carried out by a single<br>program unit, are often inappropriate for carrying out tasks that cross the boundaries<br>of databases that cannot or will not cooperate to implement protocols such as 2PC.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VI. Database System <br>Architecture<br>18. Database System <br>Architecture<br>698<br>© The McGraw−Hill <br>Companies, 2001<br>18.5<br>Network Types<br>701<br>Alternative approaches, based on persistent messaging for communication, are gener-<br>ally used for such tasks.<br>When the tasks to be carried out are complex, involving multiple databases and/or<br>multiple interactions with humans, coordination of the tasks and ensuring transac-<br>tion properties for the tasks become more complicated. Workﬂow management systems<br>are systems designed to help with carrying out such tasks. Section 19.4.3 describes<br>persistent messaging, while Section 24.2 describes workﬂow management systems.<br>In case an organization has to choose between a distributed architecture and a<br>centralized architecture for implementing an application, the system architect must<br>balance the advantages against the disadvantages of distribution of data. We have al-<br>ready seen the advantages of using distributed databases. The primary disadvantage<br>of distributed database systems is the added complexity required to ensure proper<br>coordination among the sites. This increased complexity takes various forms:<br>• Software-development cost. It is more difﬁcult to implement a distributed<br>database system; thus, it is more costly.<br>• Greater potential for bugs. Since the sites that constitute the distributed sys-<br>tem operate in parallel, it is harder to ensure the correctness of algorithms,<br>especially operation during failures of part of the system, and recovery from<br>failures. The potential exists for extremely subtle bugs.<br>• Increased processing overhead. The exchange of messages and the additional<br>computation required to achieve intersite coordination are a form of overhead<br>that does not arise in centralized systems.<br>There are several approaches to distributed database design, ranging from fully<br>distributed designs to ones that include a large degree of centralization. We study<br>them in Chapter 19.<br>18.5<br>Network Types<br>Distributed databases and client–server systems are built around communication<br>networks. There are basically two types of networks: local-area networks and wide-<br>area networks. The main difference between the two is the way in which they are<br>distributed geographically. In local-area networks, processors are distributed over<br>small geographical areas, such as a single building or a number of adjacent build-<br>ings. In wide-area networks, on the other hand, a number of autonomous processors<br>are distributed over a large geographical area (such as the United States or the en-<br>tire world). These differences imply major variations in the speed and reliability of<br>the communication network, and are reﬂected in the distributed operating-system<br>design.<br>18.5.1<br>Local-Area Networks<br>Local-area networks (LANs) (Figure 18.10) emerged in the early 1970s as a way<br>for computers to communicate and to share data with one another. People recog-<br>nized that, for many enterprises, numerous small computers, each with its own self-<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VI. Database System <br>Architecture<br>18. Database System <br>Architecture<br>699<br>© The McGraw−Hill <br>Companies, 2001<br>702<br>Chapter 18<br>Database System Architectures<br>workstation<br>printer<br>CPU server<br>processors<br>Computer-System Structures<br>PC<br>workstation<br>file server<br>processors<br>gateway<br>Figure 18.10<br>Local-area network.<br>contained applications, are more economical than a single large system. Because each<br>small computer is likely to need access to a full complement of peripheral devices<br>(such as disks and printers), and because some form of data sharing is likely to oc-<br>cur in a single enterprise, it was a natural step to connect these small systems into a<br>network.<br>LANs are generally used in an ofﬁce environment. All the sites in such systems<br>are close to one another, so the communication links tend to have a higher speed and<br>lower error rate than do their counterparts in wide-area networks. The most common<br>links in a local-area network are twisted pair, coaxial cable, ﬁber optics, and, increas-<br>ingly, wireless connections. Communication speeds range from a few megabits per<br>second (for wireless local-area networks), to 1 gigabit per second for Gigabit Ether-<br>net. Standard Ethernet runs at 10 megabits per second, while Fast Ethernet run at 100<br>megabits per second.<br>A storage-area network (SAN) is a special type of high-speed local-area network<br>designed to connect large banks of storage devices (disks) to computers that use the<br>data. Thus storage-area networks help build large-scale shared-disk systems. The moti-<br>vation for using storage-area networks to connect multiple computers to large banks<br>of storage devices is essentially the same as that for shared-disk databases, namely<br>• Scalability by adding more computers<br>• High availability, since data is still accessible even if a computer fails<br>RAID organizations are used in the storage devices to ensure high availability of the<br>data, permitting processing to continue even if individual disks fail. Storage area<br>networks are usually built with redundancy, such as multiple paths between nodes,<br>so if a component such as a link or a connection to the network fails, the network<br>continues to function.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VI. Database System <br>Architecture<br>18. Database System <br>Architecture<br>700<br>© The McGraw−Hill <br>Companies, 2001<br>18.6<br>Summary<br>703<br>18.5.2<br>Wide-Area Networks<br>Wide-area networks (WANs) emerged in the late 1960s, mainly as an academic re-<br>search project to provide efﬁcient communication among sites, allowing hardware<br>and software to be shared conveniently and economically by a wide community of<br>users. Systems that allowed remote terminals to be connected to a central computer<br>via telephone lines were developed in the early 1960s, but they were not true WANs.<br>The ﬁrst WAN to be designed and developed was the Arpanet. Work on the Arpanet<br>began in 1968. The Arpanet has grown from a four-site experimental network to a<br>worldwide network of networks, the Internet, comprising hundreds of millions of<br>computer systems. Typical links on the Internet are ﬁber-optic lines and, sometimes,<br>satellite channels. Data rates for wide-area links typically range from a few megabits<br>per second to hundreds of gigabits per second. The last link, to end user sites, is of-<br>ten based on digital subscriber loop (DSL) technology supporting a few megabits per<br>second), or cable modem (supporting 10 megabits per second), or dial-up modem<br>connections over phone lines (supporting up to 56 kilobits per second).<br>WANs can be classiﬁed into two types:<br>• In discontinuous connection WANs, such as those based on wireless connec-<br>tions, hosts are connected to the network only part of the time.<br>• In continuous connection WANs, such as the wired Internet, hosts are con-<br>nected to the network at all times.<br>Networks that are not continuously connected typically do not allow transactions<br>across sites, but may keep local copies of remote data, and refresh the copies peri-<br>odically (every night, for instance). For applications where consistency is not critical,<br>such as sharing of documents, groupware systems such as Lotus Notes allow up-<br>dates of remote data to be made locally, and the updates are then propagated back<br>to the remote site periodically. There is a potential for conﬂicting updates at differ-<br>ent sites, conﬂicts that have to be detected and resolved. A mechanism for detecting<br>conﬂicting updates is described later, in Section 23.5.4; the resolution mechanism for<br>conﬂicting updates is, however, application dependent.<br>18.6<br>Summary<br>• Centralized database systems run entirely on a single computer. With the<br>growth of personal computers and local-area networking, the database front-<br>end functionality has moved increasingly to clients, with server systems pro-<br>viding the back-end functionality. Client–server interface protocols have<br>helped the growth of client–server database systems.<br>• Servers can be either transaction servers or data servers, although the use<br>of transaction servers greatly exceeds the use of data servers for providing<br>database services.<br>  Transaction servers have multiple processes, possibly running on multiple<br>processors. So that these processes have access to common data, such as<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VI. Database System <br>Architecture<br>18. Database System <br>Architecture<br>701<br>© The McGraw−Hill <br>Companies, 2001<br>704<br>Chapter 18<br>Database System Architectures<br>the database buffer, systems store such data in shared memory. In addition<br>to processes that handle queries, there are system processes that carry out<br>tasks such as lock and log management and checkpointing.<br>  Data server systems supply raw data to clients. Such systems strive to<br>minimize communication between clients and servers by caching data<br>and locks at the clients. Parallel database systems use similar optimiza-<br>tions.<br>• Parallel database systems consist of multiple processors and multiple disks<br>connected by a fast interconnection network. Speedup measures how much<br>we can increase processing speed by increasing parallelism, for a single trans-<br>action. Scaleup measures how well we can handle an increased number of<br>transactions by increasing parallelism. Interference, skew, and start–up costs<br>act as barriers to getting ideal speedup and scaleup.<br>• Parallel database architectures include the shared-memory, shared-disk,<br>shared-nothing, and hierarchical architectures. These architectures have dif-<br>ferent tradeoffs of scalability versus communication speed.<br>• A distributed database is a collection of partially independent databases that<br>(ideally) share a common schema, and coordinate processing of transactions<br>that access nonlocal data. The processors communicate with one another thro-<br>ugh a communication network that handles routing and connection strategies.<br>• Principally, there ar</span><br><br><span style="background-color: #FFD6A5;" title="Chunk 89 | Start: 1780178 | End: 1800178 | Tokens: 3132">e two types of communication networks: local-area net-<br>works and wide-area networks. Local-area networks connect nodes that are<br>distributed over small geographical areas, such as a single building or a few<br>adjacent buildings. Wide-area networks connect nodes spread over a large<br>geographical area. The Internet is the most extensively used wide-area net-<br>work today.<br>Storage-area networks are a special type of local-area network designed<br>to provide fast interconnection between large banks of storage devices and<br>multiple computers.<br>Review Terms<br>• Centralized systems<br>• Server systems<br>• Coarse-granularity parallelism<br>• Fine-granularity parallelism<br>• Database process structure<br>• Mutual exclusion<br>• Thread<br>• Server processes<br>  Lock manager process<br>  Database writer process<br>  Log writer process<br>  Checkpoint process<br>  Process monitor process<br>• Client–server systems<br>• Transaction-server<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VI. Database System <br>Architecture<br>18. Database System <br>Architecture<br>702<br>© The McGraw−Hill <br>Companies, 2001<br>Exercises<br>705<br>• Query-server<br>• Data server<br>  Prefetching<br>  De-escalation<br>  Data caching<br>  Cache coherency<br>  Lock caching<br>  Call back<br>• Parallel systems<br>• Throughput<br>• Response time<br>• Speedup<br>  Linear speedup<br>  Sublinear speedup<br>• Scaleup<br>  Linear scaleup<br>  Sublinear scaleup<br>  Batch scaleup<br>  Transaction scaleup<br>• Startup costs<br>• Interference<br>• Skew<br>• Interconnection networks<br>  Bus<br>  Mesh<br>  Hypercube<br>• Parallel database architectures<br>  Shared memory<br>  Shared disk (clusters)<br>  Shared nothing<br>  Hierarchical<br>• Fault tolerance<br>• Distributed virtual-memory<br>• Nonuniform memory architecture<br>(NUMA)<br>• Distributed systems<br>• Distributed database<br>  Sites (nodes)<br>  Local transaction<br>  Global transaction<br>  Local autonomy<br>• Multidatabase systems<br>• Network types<br>  Local-area networks (LAN)<br>  Wide-area networks (WAN)<br>  Storage-area network (SAN)<br>Exercises<br>18.1 Why is it relatively easy to port a database from a single processor machine to<br>a multiprocessor machine if individual queries need not be parallelized?<br>18.2 Transaction server architectures are popular for client-server relational data-<br>bases, where transactions are short. On the other hand, data server architec-<br>tures are popular for client-server object-oriented database systems, where<br>transactions are expected to be relatively long. Give two reasons why data<br>servers may be popular for object-oriented databases but not for relational<br>databases.<br>18.3 Instead of storing shared structures in shared memory, an alternative architec-<br>ture would be to store them in the local memory of a special process, and access<br>the shared data by interprocess communication with the process. What would<br>be the drawback of such an architecture?<br>18.4 In typical client–server systems the server machine is much more powerful<br>than the clients; that is, its processor is faster, it may have multiple proces-<br>sors, and it has more memory and disk capacity. Consider instead a scenario<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VI. Database System <br>Architecture<br>18. Database System <br>Architecture<br>703<br>© The McGraw−Hill <br>Companies, 2001<br>706<br>Chapter 18<br>Database System Architectures<br>where client and server machines have exactly the same power. Would it make<br>sense to build a client–server system in such a scenario? Why? Which scenario<br>would be better suited to a data-server architecture?<br>18.5 Consider an object-oriented database system based on a client-server architec-<br>ture, with the server acting as a data server.<br>a. What is the effect of the speed of the interconnection between the client<br>and the server on the choice between object and page shipping?<br>b. If page shipping is used, the cache of data at the client can be organized<br>either as an object cache or a page cache. The page cache stores data in units<br>of a page, while the object cache stores data in units of objects. Assume<br>objects are smaller than a page. Describe one beneﬁt of an object cache<br>over a page cache.<br>18.6 What is lock de-escalation, and under what conditions is it required? Why is it<br>not required if the unit of data shipping is an item?<br>18.7 Suppose you were in charge of the database operations of a company whose<br>main job is to process transactions. Suppose the company is growing rapidly<br>each year, and has outgrown its current computer system. When you are choos-<br>ing a new parallel computer, what measure is most relevant—speedup, batch<br>scaleup, or transaction scaleup? Why?<br>18.8 Suppose a transaction is written in C with embedded SQL, and about 80 percent<br>of the time is spent in the SQL code, with the remaining 20 percent spent in C<br>code. How much speedup can one hope to attain if parallelism is used only for<br>the SQL code? Explain.<br>18.9 What are the factors that can work against linear scaleup in a transaction pro-<br>cessing system? Which of the factors are likely to be the most important in<br>each of the following architectures: shared memory, shared disk, and shared<br>nothing?<br>18.10 Consider a bank that has a collection of sites, each running a database system.<br>Suppose the only way the databases interact is by electronic transfer of money<br>between one another. Would such a system qualify as a distributed database?<br>Why?<br>18.11 Consider a network based on dial-up phone lines, where sites communicate<br>periodically, such as every night. Such networks are often conﬁgured with a<br>server site and multiple client sites. The client sites connect only to the server,<br>and exchange data with other clients by storing data at the server and retriev-<br>ing data stored at the server by other clients. What is the advantage of such an<br>architecture over one where a site can exchange data with another site only by<br>ﬁrst dialing it up?<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VI. Database System <br>Architecture<br>18. Database System <br>Architecture<br>704<br>© The McGraw−Hill <br>Companies, 2001<br>Bibliographical Notes<br>707<br>Bibliographical Notes<br>Patterson and Hennessy [1995] and Stone [1993] are textbooks that provide a good<br>introduction to the area of computer architecture.<br>Gray and Reuter [1993] provides a textbook description of transaction processing,<br>including the architecture of client–server and distributed systems. Geiger [1995] and<br>Signore et al. [1995] describe the ODBC standard for client–server connectivity. North<br>[1995] describes the use of a variety of tools for client–server database access.<br>Carey et al. [1991] and Franklin et al. [1993] describe data-caching techniques for<br>client–server database systems. Biliris and Orenstein [1994] survey object storage<br>management systems, including client–server related issues. Franklin et al. [1992]<br>and Mohan and Narang [1994] describe recovery techniques for client-server sys-<br>tems.<br>DeWitt and Gray [1992] survey parallel database systems, including their archi-<br>tecture and performance measures. A survey of parallel computer architectures is<br>presented by Duncan [1990]. Dubois and Thakkar [1992] is a collection of papers on<br>scalable shared-memory architectures.<br>Ozsu and Valduriez [1999], Bell and Grimson [1992] and Ceri and Pelagatti [1984]<br>provide textbook coverage of distributed database systems. Further references per-<br>taining to parallel and distributed database systems appear in the bibliographical<br>notes of Chapters 20 and 19, respectively.<br>Comer and Droms [1999] and Thomas [1996] describe the computer networking<br>and the Internet. Tanenbaum [1996] and Halsall [1992] provide general overviews of<br>computer networks. Discussions concerning ATM networks and switches are offered<br>by de Prycker [1993].<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VI. Database System <br>Architecture<br>19. Distributed Databases<br>705<br>© The McGraw−Hill <br>Companies, 2001<br>C<br>H<br>A<br>P<br>T<br>E<br>R<br>1<br>9<br>Distributed Databases<br>Unlike parallel systems, in which the processors are tightly coupled and constitute<br>a single database system, a distributed database system consists of loosely coupled<br>sites that share no physical components. Furthermore, the database systems that run<br>on each site may have a substantial degree of mutual independence. We discussed<br>the basic structure of distributed systems in Chapter 18.<br>Each site may participate in the execution of transactions that access data at one<br>site, or several sites. The main difference between centralized and distributed data-<br>base systems is that, in the former, the data reside in one single location, whereas in<br>the latter, the data reside in several locations. This distribution of data is the cause of<br>many difﬁculties in transaction processing and query processing. In this chapter, we<br>address these difﬁculties.<br>We start by classifying distributed databases as homogeneous or heterogeneous,<br>in Section 19.1. We then address the question of how to store data in a distributed<br>database in Section 19.2. In Section 19.3, we outline a model for transaction processing<br>in a distributed database. In Section 19.4, we describe how to implement atomic trans-<br>actions in a distributed database by using special commit protocols. In Section 19.5,<br>we describe concurrency control in distributed databases. In Section 19.6, we outline<br>how to provide high availability in a distributed database by exploiting replication,<br>so the system can continue processing transactions even when there is a failure. We<br>address query processing in distributed databases in Section 19.7. In Section 19.8, we<br>outline issues in handling heterogeneous databases. In Section 19.9, we describe di-<br>rectory systems, which can be viewed as a specialized form of distributed databases.<br>19.1<br>HomogeneousandHeterogeneousDatabases<br>In a homogeneous distributed database, all sites have identical database manage-<br>ment system software, are aware of one another, and agree to cooperate in processing<br>users’ requests. In such a system, local sites surrender a portion of their autonomy<br>709<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VI. Database System <br>Architecture<br>19. Distributed Databases<br>706<br>© The McGraw−Hill <br>Companies, 2001<br>710<br>Chapter 19<br>Distributed Databases<br>in terms of their right to change schemas or database management system software.<br>That software must also cooperate with other sites in exchanging information about<br>transactions, to make transaction processing possible across multiple sites.<br>In contrast, in a heterogeneous distributed database, different sites may use dif-<br>ferent schemas, and different database management system software. The sites may<br>not be aware of one another, and they may provide only limited facilities for cooper-<br>ation in transaction processing. The differences in schemas are often a major problem<br>for query processing, while the divergence in software becomes a hindrance for pro-<br>cessing transactions that access multiple sites.<br>In this chapter, we concentrate on homogeneous distributed databases. However,<br>in Section 19.8 we brieﬂy discuss query processing issues in heterogeneous distributed<br>database systems. Transaction processing issues in such systems are covered later, in<br>Section 24.6.<br>19.2<br>Distributed Data Storage<br>Consider a relation r that is to be stored in the database. There are two approaches to<br>storing this relation in the distributed database:<br>• Replication. The system maintains several identical replicas (copies) of the<br>relation, and stores each replica at a different site. The alternative to replication<br>is to store only one copy of relation r.<br>• Fragmentation. The system partitions the relation into several fragments, and<br>stores each fragment at a different site.<br>Fragmentation and replication can be combined: A relation can be partitioned into<br>several fragments and there may be several replicas of each fragment. In the follow-<br>ing subsections, we elaborate on each of these techniques.<br>19.2.1<br>Data Replication<br>If relation r is replicated, a copy of relation r is stored in two or more sites. In the most<br>extreme case, we have full replication, in which a copy is stored in every site in the<br>system.<br>There are a number of advantages and disadvantages to replication.<br>• Availability. If one of the sites containing relation r fails, then the relation r<br>can be found in another site. Thus, the system can continue to process queries<br>involving r, despite the failure of one site.<br>• Increased parallelism. In the case where the majority of accesses to the rela-<br>tion r result in only the reading of the relation, then several sites can process<br>queries involving r in parallel. The more replicas of r there are, the greater the<br>chance that the needed data will be found in the site where the transaction<br>is executing. Hence, data replication minimizes movement of data between<br>sites.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VI. Database System <br>Architecture<br>19. Distributed Databases<br>707<br>© The McGraw−Hill <br>Companies, 2001<br>19.2<br>Distributed Data Storage<br>711<br>• Increased overhead on update. The system must ensure that all replicas of a<br>relation r are consistent; otherwise, erroneous computations may result. Thus,<br>whenever r is updated, the update must be propagated to all sites containing<br>replicas. The result is increased overhead. For example, in a banking system,<br>where account information is replicated in various sites, it is necessary to en-<br>sure that the balance in a particular account agrees in all sites.<br>In general, replication enhances the performance of read operations and increases<br>the availability of data to read-only transactions. However, update transactions incur<br>greater overhead. Controlling concurrent updates by several transactions to repli-<br>cated data is more complex than in centralized systems, which we saw in Chapter<br>16. We can simplify the management of replicas of relation r by choosing one of them<br>as the primary copy of r. For example, in a banking system, an account can be as-<br>sociated with the site in which the account has been opened. Similarly, in an airline-<br>reservation system, a ﬂight can be associated with the site at which the ﬂight origi-<br>nates. We shall examine the primary copy scheme and other options for distributed<br>concurrency control in Section 19.5.<br>19.2.2<br>Data Fragmentation<br>If relation r is fragmented, r is divided into a number of fragments r1, r2, . . . , rn. These<br>fragments contain sufﬁcient information to allow reconstruction of the original re-<br>lation r. There are two different schemes for fragmenting a relation: horizontal frag-<br>mentation and vertical fragmentation. Horizontal fragmentation splits the relation by<br>assigning each tuple of r to one or more fragments. Vertical fragmentation splits the<br>relation by decomposing the scheme R of relation r.<br>We shall illustrate these approaches by fragmenting the relation account, with the<br>schema<br>Account-schema = (account-number, branch-name, balance)<br>In horizontal fragmentation, a relation r is partitioned into a number of subsets,<br>r1, r2, . . . , rn. Each tuple of relation r must belong to at least one of the fragments, so<br>that the original relation can be reconstructed, if needed.<br>As an illustration, the account relation can be divided into several different frag-<br>ments, each of which consists of tuples of accounts belonging to a particular branch.<br>If the banking system has only two branches—Hillside and Valleyview—then there<br>are two different fragments:<br>account1 = σbranch-name = “Hillside” (account)<br>account2 = σbranch-name = “Valleyview” (account)<br>Horizontal fragmentation is usually used to keep tuples at the sites where they are<br>used the most, to minimize data transfer.<br>In general, a horizontal fragment can be deﬁned as a selection on the global relation<br>r. That is, we use a predicate Pi to construct fragment ri:<br>ri = σPi (r)<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VI. Database System <br>Architecture<br>19. Distributed Databases<br>708<br>© The McGraw−Hill <br>Companies, 2001<br>712<br>Chapter 19<br>Distributed Databases<br>We reconstruct the relation r by taking the union of all fragments; that is,<br>r = r1 ∪r2 ∪· · · ∪rn<br>In our example, the fragments are disjoint. By changing the selection predicates<br>used to construct the fragments, we can have a particular tuple of r appear in more<br>than one of the ri.<br>In its simplest form, vertical fragmentation is the same as decomposition (see<br>Chapter 7). Vertical fragmentation of r(R) involves the deﬁnition of several subsets<br>of attributes R1, R2, . . . , Rn of the schema R so that<br>R = R1 ∪R2 ∪· · · ∪Rn<br>Each fragment ri of r is deﬁned by<br>ri = ΠRi (r)<br>The fragmentation should be done in such a way that we can reconstruct relation r<br>from the fragments by taking the natural join<br>r = r1<br> r2<br> r3<br> · · ·<br> rn<br>One way of ensuring that the relation r can be reconstructed is to include the<br>primary-key attributes of R in each of the Ri. More generally, any superkey can be<br>used. It is often convenient to add a special attribute, called a tuple-id, to the schema<br>R. The tuple-id value of a tuple is a unique value that distinguishes the tuple from all<br>other tuples. The tuple-id attribute thus serves as a candidate key for the augmented<br>schema, and is included in each of the Ris. The physical or logical address for a tuple<br>can be used as a tuple-id, since each tuple has a unique address.<br>To illustrate vertical fragmentation, consider a university database with a relation<br>employee-info that stores, for each employee, employee-id, name, designation, and salary.<br>For privacy reasons, this relation may be fragmented into a relation employee-private-<br>info containing employee-id and salary, and another relation employee-public-info con-<br>taining attributes employee-id, name, and designation. These may be stored at different<br>sites, again for security reasons.<br>The two types of fragmentation can be applied to a single schema; for instance, the<br>fragments obtained by horizontally fragmenting a relation can be further partitioned<br>vertically. Fragments can also be replicated. In general, a fragment can be replicated,<br>replicas of fragments can be fragmented further, and so on.<br>19.2.3<br>Transparency<br>The user of a distributed database system should not be required to know either<br>where the data are physically located or how the data can be accessed at the speciﬁc<br>local site. This characteristic, called data transparency, can take several forms:<br>• Fragmentation transparency. Users are not required to know how a relation<br>has been fragmented.<br>• Replication transparency. Users view each data object as logically unique.<br>The distributed system may replicate an object to increase either system per-<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VI. Database System <br>Architecture<br>19. Distributed Databases<br>709<br>© The McGraw−Hill <br>Companies, 2001<br>19.3<br>Distributed Transactions<br>713<br>formance or data availability. Users do not have to be concerned with what<br>data objects have been replicated, or where replicas have been placed.<br>• Location transparency. Users are not required to know the physical location<br>of the data. The distributed database system should be able to ﬁnd any data<br>as long as the data identiﬁer is supplied by the user transaction.<br>Data items—such as relations, fragments, and replicas — must have unique names.<br>This property is easy to ensure in a centralized database. In a distributed database,<br>however, we must take care to ensure that two sites do not use the same name for<br>distinct data items.<br>One solution to this problem is to require all names to be registered in a central<br>name server. The name server helps to ensure that the same name does not get used<br>for different data items. We can also use the name server to locate a data item, given<br>the name of the item. This approach, however, suffers from two major disadvantages.<br>First, the name server may become a performance bottleneck when data items are<br>located by their names, resulting in poor performance. Second, if the name server<br>crashes, it may not be possible for any site in the distributed system to continue<br>to run.<br>A </span><br><br><span style="background-color: #FDFFB6;" title="Chunk 90 | Start: 1800180 | End: 1820180 | Tokens: 3319">more widely used alternative approach requires that each site preﬁx its own<br>site identiﬁer to any name that it generates. This approach ensures that no two sites<br>generate the same name (since each site has a unique identiﬁer). Furthermore, no<br>central control is required. This solution, however, fails to achieve location trans-<br>parency, since site identiﬁers are attached to names. Thus, the account relation might<br>be referred to as site17.account, or account@site17, rather than as simply account. Many<br>database systems use the internet address of a site to identify it.<br>To overcome this problem, the database system can create a set of alternative<br>names or aliases for data items. A user may thus refer to data items by simple<br>names that are translated by the system to complete names. The mapping of aliases<br>to the real names can be stored at each site. With aliases, the user can be unaware of<br>the physical location of a data item. Furthermore, the user will be unaffected if the<br>database administrator decides to move a data item from one site to another.<br>Users should not have to refer to a speciﬁc replica of a data item. Instead, the<br>system should determine which replica to reference on a read request, and should<br>update all replicas on a write request. We can ensure that it does so by maintaining a<br>catalog table, which the system uses to determine all replicas for the data item.<br>19.3<br>Distributed Transactions<br>Access to the various data items in a distributed system is usually accomplished<br>through transactions, which must preserve the ACID properties (Section 15.1). There<br>are two types of transaction that we need to consider. The local transactions are those<br>that access and update data in only one local database; the global transactions are<br>those that access and update data in several local databases. Ensuring the ACID prop-<br>erties of the local transactions can be done as described in Chapters 15, 16, and 17.<br>However, for global transactions, this task is much more complicated, since several<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VI. Database System <br>Architecture<br>19. Distributed Databases<br>710<br>© The McGraw−Hill <br>Companies, 2001<br>714<br>Chapter 19<br>Distributed Databases<br>sites may be participating in execution. The failure of one of these sites, or the failure<br>of a communication link connecting these sites, may result in erroneous computa-<br>tions.<br>In this section we study the system structure of a distributed database, and its<br>possible failure modes. On the basis of the model presented in this section, in Sec-<br>tion 19.4 we study protocols for ensuring atomic commit of global transactions, and<br>in Section 19.5 we study protocols for concurrency control in distributed databases.<br>In Section 19.6 we study how a distributed database can continue functioning even<br>in the presence of various types of failure.<br>19.3.1<br>System Structure<br>Each site has its own local transaction manager, whose function is to ensure the ACID<br>properties of those transactions that execute at that site. The various transaction man-<br>agers cooperate to execute global transactions. To understand how such a manager<br>can be implemented, consider an abstract model of a transaction system, in which<br>each site contains two subsystems:<br>• The transaction manager manages the execution of those transactions (or sub-<br>transactions) that access data stored in a local site. Note that each such trans-<br>action may be either a local transaction (that is, a transaction that executes at<br>only that site) or part of a global transaction (that is, a transaction that executes<br>at several sites).<br>• The transaction coordinator coordinates the execution of the various transac-<br>tions (both local and global) initiated at that site.<br>The overall system architecture appears in Figure 19.1.<br>TM1<br>TMn<br>computer 1<br>computer n<br>TC1<br>TCn<br>transaction<br>coordinator<br>transaction<br>manager<br>Figure 19.1<br>System architecture.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VI. Database System <br>Architecture<br>19. Distributed Databases<br>711<br>© The McGraw−Hill <br>Companies, 2001<br>19.3<br>Distributed Transactions<br>715<br>The structure of a transaction manager is similar in many respects to the structure<br>of a centralized system. Each transaction manager is responsible for<br>• Maintaining a log for recovery purposes<br>• Participating in an appropriate concurrency-control scheme to coordinate the<br>concurrent execution of the transactions executing at that site<br>As we shall see, we need to modify both the recovery and concurrency schemes to<br>accommodate the distribution of transactions.<br>The transaction coordinator subsystem is not needed in the centralized environ-<br>ment, since a transaction accesses data at only a single site. A transaction coordinator,<br>as its name implies, is responsible for coordinating the execution of all the transac-<br>tions initiated at that site. For each such transaction, the coordinator is responsible<br>for<br>• Starting the execution of the transaction<br>• Breaking the transaction into a number of subtransactions and distributing<br>these subtransactions to the appropriate sites for execution<br>• Coordinating the termination of the transaction, which may result in the trans-<br>action being committed at all sites or aborted at all sites<br>19.3.2<br>System Failure Modes<br>A distributed system may suffer from the same types of failure that a centralized<br>system does (for example, software errors, hardware errors, or disk crashes). There<br>are, however, additional types of failure with which we need to deal in a distributed<br>environment. The basic failure types are<br>• Failure of a site<br>• Loss of messages<br>• Failure of a communication link<br>• Network partition<br>The loss or corruption of messages is always a possibility in a distributed sys-<br>tem. The system uses transmission-control protocols, such as TCP/IP, to handle such<br>errors. Information about such protocols may be found in standard textbooks on net-<br>working (see the bibliographical notes).<br>However, if two sites A and B are not directly connected, messages from one to<br>the other must be routed through a sequence of communication links. If a communi-<br>cation link fails, messages that would have been transmitted across the link must be<br>rerouted. In some cases, it is possible to ﬁnd another route through the network, so<br>that the messages are able to reach their destination. In other cases, a failure may re-<br>sult in there being no connection between some pairs of sites. A system is partitioned<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VI. Database System <br>Architecture<br>19. Distributed Databases<br>712<br>© The McGraw−Hill <br>Companies, 2001<br>716<br>Chapter 19<br>Distributed Databases<br>if it has been split into two (or more) subsystems, called partitions, that lack any con-<br>nection between them. Note that, under this deﬁnition, a subsystem may consist of a<br>single node.<br>19.4<br>Commit Protocols<br>If we are to ensure atomicity, all the sites in which a transaction T executed must<br>agree on the ﬁnal outcome of the execution. T must either commit at all sites, or it<br>must abort at all sites. To ensure this property, the transaction coordinator of T must<br>execute a commit protocol.<br>Among the simplest and most widely used commit protocols is the two-phase<br>commit protocol (2PC), which is described in Section 19.4.1. An alternative is the<br>three-phase commit protocol (3PC), which avoids certain disadvantages of the 2PC<br>protocol but adds to complexity and overhead. Section 19.4.2 brieﬂy outlines the 3PC<br>protocol.<br>19.4.1<br>Two-Phase Commit<br>We ﬁrst describe how the two-phase commit protocol (2PC) operates during normal<br>operation, then describe how it handles failures and ﬁnally how it carries out recov-<br>ery and concurrency control.<br>Consider a transaction T initiated at site Si, where the transaction coordinator<br>is Ci.<br>19.4.1.1<br>The Commit Protocol<br>When T completes its execution—that is, when all the sites at which T has executed<br>inform Ci that T has completed—Ci starts the 2PC protocol.<br>• Phase 1. Ci adds the record &lt;prepare T&gt; to the log, and forces the log onto sta-<br>ble storage. It then sends a prepare T message to all sites at which T executed.<br>On receiving such a message, the transaction manager at that site determines<br>whether it is willing to commit its portion of T. If the answer is no, it adds a<br>record &lt;no T&gt; to the log, and then responds by sending an abort T message<br>to Ci. If the answer is yes, it adds a record &lt;ready T&gt; to the log, and forces<br>the log (with all the log records corresponding to T) onto stable storage. The<br>transaction manager then replies with a ready T message to Ci.<br>• Phase 2. When Ci receives responses to the prepare T message from all the<br>sites, or when a prespeciﬁed interval of time has elapsed since the prepare<br>T message was sent out, Ci can determine whether the transaction T can be<br>committed or aborted. Transaction T can be committed if Ci received a ready<br>T message from all the participating sites. Otherwise, transaction T must be<br>aborted. Depending on the verdict, either a record &lt;commit T&gt; or a record<br>&lt;abort T&gt; is added to the log and the log is forced onto stable storage. At<br>this point, the fate of the transaction has been sealed. Following this point, the<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VI. Database System <br>Architecture<br>19. Distributed Databases<br>713<br>© The McGraw−Hill <br>Companies, 2001<br>19.4<br>Commit Protocols<br>717<br>coordinator sends either a commit T or an abort T message to all participating<br>sites. When a site receives that message, it records the message in the log.<br>A site at which T executed can unconditionally abort T at any time before it sends<br>the message ready T to the coordinator. Once the message is sent, the transaction is<br>said to be in the ready state at the site. The ready T message is, in effect, a promise<br>by a site to follow the coordinator’s order to commit T or to abort T. To make such a<br>promise, the needed information must ﬁrst be stored in stable storage. Otherwise, if<br>the site crashes after sending ready T, it may be unable to make good on its promise.<br>Further, locks acquired by the transaction must continue to be held till the transaction<br>completes.<br>Since unanimity is required to commit a transaction, the fate of T is sealed as soon<br>as at least one site responds abort T. Since the coordinator site Si is one of the sites at<br>which T executed, the coordinator can decide unilaterally to abort T. The ﬁnal verdict<br>regarding T is determined at the time that the coordinator writes that verdict (commit<br>or abort) to the log and forces that verdict to stable storage. In some implementations<br>of the 2PC protocol, a site sends an acknowledge T message to the coordinator at the<br>end of the second phase of the protocol. When the coordinator receives the acknowl-<br>edge T message from all the sites, it adds the record &lt;complete T&gt; to the log.<br>19.4.1.2<br>Handling of Failures<br>The 2PC protocol responds in differenct ways to various types of failures:<br>• Failure of a participating site. If the coordinator Ci detects that a site has<br>failed, it takes these actions: If the site fails before responding with a ready<br>T message to Ci, the coordinator assumes that it responded with an abort T<br>message. If the site fails after the coordinator has received the ready T message<br>from the site, the coordinator executes the rest of the commit protocol in the<br>normal fashion, ignoring the failure of the site.<br>When a participating site Sk recovers from a failure, it must examine its log<br>to determine the fate of those transactions that were in the midst of execution<br>when the failure occurred. Let T be one such transaction. We consider each of<br>the possible cases:<br>  The log contains a &lt;commit T&gt; record. In this case, the site executes<br>redo(T).<br>  The log contains an &lt;abort T&gt; record. In this case, the site executes undo(T).<br>  The log contains a &lt;ready T&gt; record. In this case, the site must consult Ci<br>to determine the fate of T. If Ci is up, it notiﬁes Sk regarding whether T<br>committed or aborted. In the former case, it executes redo(T); in the latter<br>case, it executes undo(T). If Ci is down, Sk must try to ﬁnd the fate of T<br>from other sites. It does so by sending a querystatus T message to all the<br>sites in the system. On receiving such a message, a site must consult its<br>log to determine whether T has executed there, and if T has, whether T<br>committed or aborted. It then notiﬁes Sk about this outcome. If no site has<br>the appropriate information (that is, whether T committed or aborted),<br>then Sk can neither abort nor commit T. The decision concerning T is<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VI. Database System <br>Architecture<br>19. Distributed Databases<br>714<br>© The McGraw−Hill <br>Companies, 2001<br>718<br>Chapter 19<br>Distributed Databases<br>postponed until Sk can obtain the needed information. Thus, Sk must pe-<br>riodically resend the querystatus message to the other sites. It continues<br>to do so until a site that contains the needed information recovers. Note<br>that the site at which Ci resides always has the needed information.<br>  The log contains no control records (abort, commit, ready) concerning T.<br>Thus, we know that Sk failed before responding to the prepare T message<br>from Ci. Since the failure of Sk precludes the sending of such a response,<br>by our algorithm Ci must abort T. Hence, Sk must execute undo(T).<br>• Failure of the coordinator. If the coordinator fails in the midst of the execu-<br>tion of the commit protocol for transaction T, then the participating sites must<br>decide the fate of T. We shall see that, in certain cases, the participating sites<br>cannot decide whether to commit or abort T, and therefore these sites must<br>wait for the recovery of the failed coordinator.<br>  If an active site contains a &lt;commit T&gt; record in its log, then T must be<br>committed.<br>  If an active site contains an &lt;abort T&gt; record in its log, then T must be<br>aborted.<br>  If some active site does not contain a &lt;ready T&gt; record in its log, then<br>the failed coordinator Ci cannot have decided to commit T, because a site<br>that does not have a &lt;ready T&gt; record in its log cannot have sent a ready<br>T message to Ci. However, the coordinator may have decided to abort T,<br>but not to commit T. Rather than wait for Ci to recover, it is preferable to<br>abort T.<br>  If none of the preceding cases holds, then all active sites must have a<br>&lt;ready T&gt; record in their logs, but no additional control records (such<br>as &lt;abort T&gt; or &lt;commit T&gt;). Since the coordinator has failed, it is im-<br>possible to determine whether a decision has been made, and if one has,<br>what that decision is, until the coordinator recovers. Thus, the active sites<br>must wait for Ci to recover. Since the fate of T remains in doubt, T may<br>continue to hold system resources. For example, if locking is used, T may<br>hold locks on data at active sites. Such a situation is undesirable, because<br>it may be hours or days before Ci is again active. During this time, other<br>transactions may be forced to wait for T. As a result, data items may be<br>unavailable not only on the failed site (Ci), but on active sites as well. This<br>situation is called the blocking problem, because T is blocked pending the<br>recovery of site Ci.<br>• Network partition. When a network partitions, two possibilities exist:<br>1. The coordinator and all its participants remain in one partition. In this<br>case, the failure has no effect on the commit protocol.<br>2. The coordinator and its participants belong to several partitions. From the<br>viewpoint of the sites in one of the partitions, it appears that the sites in<br>other partitions have failed. Sites that are not in the partition containing<br>the coordinator simply execute the protocol to deal with failure of the<br>coordinator. The coordinator and the sites that are in the same partition as<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VI. Database System <br>Architecture<br>19. Distributed Databases<br>715<br>© The McGraw−Hill <br>Companies, 2001<br>19.4<br>Commit Protocols<br>719<br>the coordinator follow the usual commit protocol, assuming that the sites<br>in the other partitions have failed.<br>Thus, the major disadvantage of the 2PC protocol is that coordinator failure may re-<br>sult in blocking, where a decision either to commit or to abort T may have to be<br>postponed until Ci recovers.<br>19.4.1.3<br>Recovery and Concurrency Control<br>When a failed site restarts, we can perform recovery by using, for example, the re-<br>covery algorithm described in Section 17.9. To deal with distributed commit proto-<br>cols (such as 2PC and 3PC), the recovery procedure must treat in-doubt transactions<br>specially; in-doubt transactions are transactions for which a &lt;ready T&gt; log record is<br>found, but neither a &lt;commit T&gt; log record nor an &lt;abort T&gt; log record is found.<br>The recovering site must determine the commit–abort status of such transactions by<br>contacting other sites, as described in Section 19.4.1.2.<br>If recovery is done as just described, however, normal transaction processing at<br>the site cannot begin until all in-doubt transactions have been committed or rolled<br>back. Finding the status of in-doubt transactions can be slow, since multiple sites<br>may have to be contacted. Further, if the coordinator has failed, and no other site has<br>information about the commit–abort status of an incomplete transaction, recovery<br>potentially could become blocked if 2PC is used. As a result, the site performing<br>restart recovery may remain unusable for a long period.<br>To circumvent this problem, recovery algorithms typically provide support for<br>noting lock information in the log. (We are assuming here that locking is used for<br>concurrency control.) Instead of writing a &lt;ready T&gt; log record, the algorithm writes<br>a &lt;ready T, L&gt; log record, where L is a list of all write locks held by the transaction<br>T when the log record is written. At recovery time, after performing local recovery<br>actions, for every in-doubt transaction T, all the write locks noted in the &lt;ready T,<br>L&gt; log record (read from the log) are reacquired.<br>After lock reacquisition is complete for all in-doubt transactions, transaction pro-<br>cessing can start at the site, even before the commit–abort status of the in-doubt trans-<br>actions is determined. The commit or rollback of in-doubt transactions proceeds con-<br>currently with the execution of new transactions. Thus, site recovery is faster, and<br>never gets blocked. Note that new transactions that have a lock conﬂict with any<br>write locks held by in-doubt transactions will be unable to make progress until the<br>conﬂicting in-doubt transactions have been committed or rolled back.<br>19.4.2<br>Three-Phase Commit<br>The three-phase commit (3PC) protocol is an extension of the two-phase commit pro-<br>tocol that avoids the blocking problem under certain assumptions. In particular, it is<br>assumed that no network partition occurs, and not more than k sites fail, where k is<br>some predetermined number. Under these assumptions, the protocol avoids blocking<br>by introducing an extra third phase where multiple sites are involved in the decision<br>to commit. Instead of directly noting the commit decision in its persistent storage, the<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VI. Database System <br>Architecture<br>19. Distributed Databases<br>716<br>© The McGraw−Hill <br>Companies, 2001<br>720<br>Chapter 19<br>Distributed Databases<br>coordinator ﬁrst ensures that at least k other sites know that it intended to commit<br>the transaction. If the coordinator fails, the remaining sites ﬁrst select a new coor-<br>dinator. This new coordinator checks the status of the protocol from the remaining<br>sites; if the coordinator had decided to commit, at least one of the other k sites that it<br>informed will be up and will ensure that the commit decision is respected. The new<br>coordinator restarts the third phase of the protocol if some site knew that the old co-<br>ordinator intended to commit the transaction. Otherwise the new coordinator aborts<br>the transaction.<br>While the 3PC protocol has the desirable property </span><br><br><span style="background-color: #CAFFBF;" title="Chunk 91 | Start: 1820182 | End: 1840182 | Tokens: 3247">of not blocking unless k sites<br>fail, it has the drawback that a partitioning of the network will appear to be the same<br>as more than k sites failing, which would lead to blocking. The protocol also has to<br>be carefully implemented to ensure that network partitioning (or more than k sites<br>failing) does not result in inconsistencies, where a transaction is committed in one<br>partition, and aborted in another. Because of its overhead, the 3PC protocol is not<br>widely used. See the bibliographical notes for references giving more details of the<br>3PC protocol.<br>19.4.3<br>Alternative Models of Transaction Processing<br>For many applications, the blocking problem of two-phase commit is not acceptable.<br>The problem here is the notion of a single transaction that works across multiple sites.<br>In this section we describe how to use persistent messaging to avoid the problem of<br>distributed commit, and then brieﬂy outline the larger issue of workﬂows; workﬂows<br>are considered in more detail in Section 24.2.<br>To understand persistent messaging consider how one might transfer funds be-<br>tween two different banks, each with its own computer. One approach is to have a<br>transaction span the two sites, and use two-phase commit to ensure atomicity. How-<br>ever, the transaction may have to update the total bank balance, and blocking could<br>have a serious impact on all other transactions at each bank, since almost all transac-<br>tions at the bank would update the total bank balance.<br>In contrast, consider how fund transfer by a bank check occurs. The bank ﬁrst<br>deducts the amount of the check from the available balance and prints out a check.<br>The check is then physically transferred to the other bank where it is deposited. After<br>verifying the check, the bank increases the local balance by the amount of the check.<br>The check constitutes a message sent between the two banks. So that funds are not<br>lost or incorrectly increased, the check must not be lost, and must not be duplicated<br>and deposited more than once. When the bank computers are connected by a net-<br>work, persistent messages provide the same service as the check (but much faster, of<br>course).<br>Persistent messages are messages that are guaranteed to be delivered to the re-<br>cipient exactly once (neither less nor more), regardless of failures, if the transaction<br>sending the message commits, and are guaranteed to not be delivered if the transac-<br>tion aborts. Database recovery techniques are used to implement persistent messag-<br>ing on top of the normal network channels, as we will see shortly. In contrast, regular<br>messages may be lost or may even be delivered multiple times in some situations.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VI. Database System <br>Architecture<br>19. Distributed Databases<br>717<br>© The McGraw−Hill <br>Companies, 2001<br>19.4<br>Commit Protocols<br>721<br>Error handling is more complicated with persistent messaging than with two-<br>phase commit. For instance, if the account where the check is to be deposited has<br>been closed, the check must be sent back to the originating account and credited<br>back there. Both sites must therefore be provided with error handling code, along<br>with code to handle the persistent messages. In contrast, with two-phase commit,<br>the error would be detected by the transaction, which would then never deduct the<br>amount in the ﬁrst place.<br>The types of exception conditions that may arise depend on the application, so<br>it is not possible for the database system to handle exceptions automatically. The<br>application programs that send and receive persistent messages must include code<br>to handle exception conditions and bring the system back to a consistent state. For<br>instance, it is not acceptable to just lose the money being transfered if the receiving<br>account has been closed; the money must be credited back to the originating account,<br>and if that is not possible for some reason, humans must be alerted to resolve the<br>situation manually.<br>There are many applications where the beneﬁt of eliminating blocking is well<br>worth the extra effort to implement systems that use persistent messages. In fact, few<br>organizations would agree to support two-phase commit for transactions originating<br>outside the organization, since failures could result in blocking of access to local data.<br>Persistent messaging therefore plays an important role in carrying out transactions<br>that cross organizational boundaries.<br>Workﬂows provide a general model of transaction processing involving multiple<br>sites and possibly human processing of certain steps. For instance, when a bank re-<br>ceives a loan application, there are many steps it must take, including contacting ex-<br>ternal credit-checking agencies, before approving or rejecting a loan application. The<br>steps, together, form a workﬂow. We study workﬂows in more detail in Section 24.2.<br>We also note that persistent messaging forms the underlying basis for workﬂows in<br>a distributed environment.<br>We now consider the implementation of persistent messaging. Persistent messag-<br>ing can be implemented on top of an unreliable messaging infrastructure, which may<br>lose messages or deliver them multiple times, by these protocols:<br>• Sending site protocol: When a transaction wishes to send a persistent mes-<br>sage, it writes a record containing the message in a special relation messages-<br>to-send, instead of directly sending out the message. The message is also given<br>a unique message identiﬁer.<br>A message delivery process monitors the relation, and when a new message is<br>found, it sends the message to its destination. The usual database concurrency<br>control mechanisms ensure that the system process reads the message only<br>after the transaction that wrote the message commits; if the transaction aborts,<br>the usual recovery mechanism would delete the message from the relation.<br>The message delivery process deletes a message from the relation only af-<br>ter it receives an acknowledgment from the destination site. If it receives no<br>acknowledgement from the destination site, after some time it sends the mes-<br>sage again. It repeats this until an acknowledgment is received. In case of per-<br>manent failures, the system will decide, after some period of time, that the<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VI. Database System <br>Architecture<br>19. Distributed Databases<br>718<br>© The McGraw−Hill <br>Companies, 2001<br>722<br>Chapter 19<br>Distributed Databases<br>message is undeliverable. Exception handling code provided by the applica-<br>tion is then invoked to deal with the failure.<br>Writing the message to a relation and processing it only after the transaction<br>commits ensures that the message will be delivered if and only if the transac-<br>tion commits. Repeatedly sending it guarantees it will be delivered even if<br>there are (temporary) system or network failures.<br>• Receiving site protocol: When a site receives a persistent message, it runs a<br>transaction that adds the message to a special received-messages relation, pro-<br>vided it is not already present in the relation (the unique message identiﬁer<br>detects duplicates). After the transaction commits, or if the message was al-<br>ready present in the relation, the receiving site sends an acknowledgment back<br>to the sending site.<br>Note that sending the acknowledgment before the transaction commits is<br>not safe, since a system failure may then result in loss of the message. Check-<br>ing whether the message has been received earlier is essential to avoid multi-<br>ple deliveries of the message.<br>In many messaging systems, it is possible for messages to get delayed ar-<br>bitrarily, although such delays are very unlikely. Therefore, to be safe, the<br>message must never be deleted from the received-messages relation. Deleting<br>it could result in a duplicate delivery not being detected. But as a result,<br>the received-messages relation may grow indeﬁnitely. To deal with this prob-<br>lem, each message is given a timestamp, and if the timestamp of a received<br>message is older than some cutoff, the message is discarded. All messages<br>recorded in the received-messages relation that are older than the cutoff can be<br>deleted.<br>19.5<br>ConcurrencyControlinDistributedDatabases<br>We show here how some of the concurrency-control schemes discussed in Chapter 16<br>can be modiﬁed so that they can be used in a distributed environment. We assume<br>that each site participates in the execution of a commit protocol to ensure global trans-<br>action atomicity.<br>The protocols we describe in this section require updates to be done on all replicas<br>of a data item. If any site containing a replica of a data item has failed, updates to the<br>data item cannot be processed. In Section 19.6 we describe protocols that can continue<br>transaction processing even if some sites or links have failed, thereby providing high<br>availability.<br>19.5.1<br>Locking Protocols<br>The various locking protocols described in Chapter 16 can be used in a distributed<br>environment. The only change that needs to be incorporated is in the way the lock<br>manager deals with replicated data. We present several possible schemes that are<br>applicable to an environment where data can be replicated in several sites. As in<br>Chapter 16, we shall assume the existence of the shared and exclusive lock modes.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VI. Database System <br>Architecture<br>19. Distributed Databases<br>719<br>© The McGraw−Hill <br>Companies, 2001<br>19.5<br>Concurrency Control in Distributed Databases<br>723<br>19.5.1.1<br>Single Lock-Manager Approach<br>In the single lock-manager approach, the system maintains a single lock manager<br>that resides in a single chosen site—say Si. All lock and unlock requests are made at<br>site Si. When a transaction needs to lock a data item, it sends a lock request to Si.<br>The lock manager determines whether the lock can be granted immediately. If the<br>lock can be granted, the lock manager sends a message to that effect to the site at<br>which the lock request was initiated. Otherwise, the request is delayed until it can<br>be granted, at which time a message is sent to the site at which the lock request was<br>initiated. The transaction can read the data item from any one of the sites at which a<br>replica of the data item resides. In the case of a write, all the sites where a replica of<br>the data item resides must be involved in the writing.<br>The scheme has these advantages:<br>• Simple implementation. This scheme requires two messages for handling<br>lock requests, and one message for handling unlock requests.<br>• Simple deadlock handling. Since all lock and unlock requests are made at one<br>site, the deadlock-handling algorithms discussed in Chapter 16 can be applied<br>directly to this environment.<br>The disadvantages of the scheme are:<br>• Bottleneck. The site Si becomes a bottleneck, since all requests must be pro-<br>cessed there.<br>• Vulnerability. If the site Si fails, the concurrency controller is lost. Either pro-<br>cessing must stop, or a recovery scheme must be used so that a backup site<br>can take over lock management from Si, as described in Section 19.6.5.<br>19.5.1.2<br>Distributed Lock Manager<br>A compromise between the advantages and disadvantages can be achieved through<br>the distributed lock-manager approach, in which the lock-manager function is dis-<br>tributed over several sites.<br>Each site maintains a local lock manager whose function is to administer the lock<br>and unlock requests for those data items that are stored in that site. When a trans-<br>action wishes to lock data item Q, which is not replicated and resides at site Si, a<br>message is sent to the lock manager at site Si requesting a lock (in a particular lock<br>mode). If data item Q is locked in an incompatible mode, then the request is delayed<br>until it can be granted. Once it has determined that the lock request can be granted,<br>the lock manager sends a message back to the initiator indicating that it has granted<br>the lock request.<br>There are several alternative ways of dealing with replication of data items, which<br>we study in Sections 19.5.1.3 to 19.5.1.6.<br>The distributed lock manager scheme has the advantage of simple implementa-<br>tion, and reduces the degree to which the coordinator is a bottleneck. It has a reason-<br>ably low overhead, requiring two message transfers for handling lock requests, and<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VI. Database System <br>Architecture<br>19. Distributed Databases<br>720<br>© The McGraw−Hill <br>Companies, 2001<br>724<br>Chapter 19<br>Distributed Databases<br>one message transfer for handling unlock requests. However, deadlock handling is<br>more complex, since the lock and unlock requests are no longer made at a single site:<br>There may be intersite deadlocks even when there is no deadlock within a single site.<br>The deadlock-handling algorithms discussed in Chapter 16 must be modiﬁed, as we<br>shall discuss in Section 19.5.4, to detect global deadlocks.<br>19.5.1.3<br>Primary Copy<br>When a system uses data replication, we can choose one of the replicas as the primary<br>copy. Thus, for each data item Q, the primary copy of Q must reside in precisely one<br>site, which we call the primary site of Q.<br>When a transaction needs to lock a data item Q, it requests a lock at the primary<br>site of Q. As before, the response to the request is delayed until it can be granted.<br>Thus, the primary copy enables concurrency control for replicated data to be han-<br>dled like that for unreplicated data. This similarity allows for a simple implementa-<br>tion. However, if the primary site of Q fails, Q is inaccessible, even though other sites<br>containing a replica may be accessible.<br>19.5.1.4<br>Majority Protocol<br>The majority protocol works this way: If data item Q is replicated in n different sites,<br>then a lock-request message must be sent to more than one-half of the n sites in which<br>Q is stored. Each lock manager determines whether the lock can be granted immedi-<br>ately (as far as it is concerned). As before, the response is delayed until the request can<br>be granted. The transaction does not operate on Q until it has successfully obtained<br>a lock on a majority of the replicas of Q.<br>This scheme deals with replicated data in a decentralized manner, thus avoiding<br>the drawbacks of central control. However, it suffers from these disadvantages:<br>• Implementation. The majority protocol is more complicated to implement<br>than are the previous schemes. It requires 2(n/2 + 1) messages for handling<br>lock requests, and (n/2 + 1) messages for handling unlock requests.<br>• Deadlock handling. In addition to the problem of global deadlocks due to<br>the use of a distributed lock-manager approach, it is possible for a deadlock<br>to occur even if only one data item is being locked. As an illustration, consider<br>a system with four sites and full replication. Suppose that transactions T1 and<br>T2 wish to lock data item Q in exclusive mode. Transaction T1 may succeed<br>in locking Q at sites S1 and S3, while transaction T2 may succeed in locking<br>Q at sites S2 and S4. Each then must wait to acquire the third lock; hence, a<br>deadlock has occurred. Luckily, we can avoid such deadlocks with relative<br>ease, by requiring all sites to request locks on the replicas of a data item in the<br>same predetermined order.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VI. Database System <br>Architecture<br>19. Distributed Databases<br>721<br>© The McGraw−Hill <br>Companies, 2001<br>19.5<br>Concurrency Control in Distributed Databases<br>725<br>19.5.1.5<br>Biased Protocol<br>The biased protocol is another approach to handling replication. The difference from<br>the majority protocol is that requests for shared locks are given more favorable treat-<br>ment than requests for exclusive locks.<br>• Shared locks. When a transaction needs to lock data item Q, it simply requests<br>a lock on Q from the lock manager at one site that contains a replica of Q.<br>• Exclusive locks. When a transaction needs to lock data item Q, it requests a<br>lock on Q from the lock manager at all sites that contain a replica of Q.<br>As before, the response to the request is delayed until it can be granted.<br>The biased scheme has the advantage of imposing less overhead on read oper-<br>ations than does the majority protocol. This savings is especially signiﬁcant in com-<br>mon cases in which the frequency of read is much greater than the frequency of write.<br>However, the additional overhead on writes is a disadvantage. Furthermore, the bi-<br>ased protocol shares the majority protocol’s disadvantage of complexity in handling<br>deadlock.<br>19.5.1.6<br>Quorum Consensus Protocol<br>The quorum consensus protocol is a generalization of the majority protocol. The<br>quorum consensus protocol assigns each site a nonnegative weight. It assigns read<br>and write operations on an item x two integers, called read quorum Qr and write<br>quorum Qw, that must satisfy the following condition, where S is the total weight of<br>all sites at which x resides:<br>Qr + Qw &gt; S and 2 ∗Qw &gt; S<br>To execute a read operation, enough replicas must be read that their total weight<br>is ≥Qr. To execute a write operation, enough replicas must be written so that their<br>total weight is ≥Qw.<br>The beneﬁt of the quorum consensus approach is that it can permit the cost of ei-<br>ther reads or writes to be selectively reduced by appropriately deﬁning the read and<br>write quorums. For instance, with a small read quorum, reads need to read fewer<br>replicas, but the write quorum will be higher, hence writes can succeed only if corre-<br>spondingly more replicas are available. Also, if higher weights are given to some sites<br>(for example, those less likely to fail), fewer sites need to be accessed for acquiring<br>locks.<br>In fact, by setting weights and quorums appropriately, the quorum consensus pro-<br>tocol can simulate the majority protocol and the biased protocols.<br>19.5.2<br>Timestamping<br>The principal idea behind the timestamping scheme in Section 16.2 is that each trans-<br>action is given a unique timestamp that the system uses in deciding the serialization<br>order. Our ﬁrst task, then, in generalizing the centralized scheme to a distributed<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VI. Database System <br>Architecture<br>19. Distributed Databases<br>722<br>© The McGraw−Hill <br>Companies, 2001<br>726<br>Chapter 19<br>Distributed Databases<br>site<br>identifier<br>global unique<br>identifier<br>local unique<br>timestamp<br>Figure 19.2<br>Generation of unique timestamps.<br>scheme is to develop a scheme for generating unique timestamps. Then, the various<br>protocols can operate directly to the nonreplicated environment.<br>There are two primary methods for generating unique timestamps, one central-<br>ized and one distributed. In the centralized scheme, a single site distributes the time-<br>stamps. The site can use a logical counter or its own local clock for this purpose.<br>In the distributed scheme, each site generates a unique local timestamp by using<br>either a logical counter or the local clock. We obtain the unique global timestamp by<br>concatenating the unique local timestamp with the site identiﬁer, which also must be<br>unique (Figure 19.2). The order of concatenation is important! We use the site iden-<br>tiﬁer in the least signiﬁcant position to ensure that the global timestamps generated<br>in one site are not always greater than those generated in another site. Compare this<br>technique for generating unique timestamps with the one that we presented in Sec-<br>tion 19.2.3 for generating unique names.<br>We may still have a problem if one site generates local timestamps at a rate faster<br>than that of the other sites. In such a case, the fast site’s logical counter will be larger<br>than that of other sites. Therefore, all timestamps generated by the fast site will be<br>larger than those generated by other sites. What we need is a mechanism to ensure<br>that local timestamps are generated fairly across the system. We deﬁne within each<br>site Si a logical clock (LCi), which generates the unique local timestamp. The logical<br>clock can be implemented as a counter that is incremented after a new local time-<br>stamp is generated. To ensure that the various logical clocks are synchronized</span><br><br><span style="background-color: #9BF6FF;" title="Chunk 92 | Start: 1840184 | End: 1860184 | Tokens: 3270">, we<br>require that a site Si advance its logical clock whenever a transaction Ti with time-<br>stamp &lt;x,y&gt; visits that site and x is greater than the current value of LCi. In this case,<br>site Si advances its logical clock to the value x + 1.<br>If the system clock is used to generate timestamps, then timestamps will be as-<br>signed fairly, provided that no site has a system clock that runs fast or slow. Since<br>clocks may not be perfectly accurate, a technique similar to that for logical clocks<br>must be used to ensure that no clock gets far ahead of or behind another clock.<br>19.5.3<br>Replication with Weak Degrees of Consistency<br>Many commercial databases today support replication, which can take one of several<br>forms. With master–slave replication, the database allows updates at a primary site,<br>and automatically propagates updates to replicas at other sites. Transactions may<br>read the replicas at other sites, but are not permitted to update them.<br>An important feature of such replication is that transactions do not obtain locks at<br>remote sites. To ensure that transactions running at the replica sites see a consistent<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VI. Database System <br>Architecture<br>19. Distributed Databases<br>723<br>© The McGraw−Hill <br>Companies, 2001<br>19.5<br>Concurrency Control in Distributed Databases<br>727<br>(but perhaps outdated) view of the database, the replica should reﬂect a transaction-<br>consistent snapshot of the data at the primary; that is, the replica should reﬂect all<br>updates of transactions up to some transaction in the serialization order, and should<br>not reﬂect any updates of later transactions in the serialization order.<br>The database may be conﬁgured to propagate updates immediately after they oc-<br>cur at the primary, or to propagate updates only periodically.<br>Master–slave replication is particularly useful for distributing information, for in-<br>stance from a central ofﬁce to branch ofﬁces of an organization. Another use for this<br>form of replication is in creating a copy of the database to run large queries, so that<br>queries do not interfere with transactions. Updates should be propagated periodi-<br>cally—every night, for example—so that update propagation does not interfere with<br>query processing.<br>The Oracle database system supports a create snapshot statement, which can cre-<br>ate a transaction-consistent snapshot copy of a relation, or set of relations, at a remote<br>site. It also supports snapshot refresh, which can be done either by recomputing the<br>snapshot or by incrementally updating it. Oracle supports automatic refresh, either<br>continuously or at periodic intervals.<br>With multimaster replication (also called update-anywhere replication) updates<br>are permitted at any replica of a data item, and are automatically propagated to<br>all replicas. This model is the basic model used to manage replicas in distributed<br>databases. Transactions update the local copy and the system updates other replicas<br>transparently.<br>One way of updating replicas is to apply immediate update with two-phase com-<br>mit, using one of the distributed concurrency-control techniques we have seen. Many<br>database systems use the biased protocol, where writes have to lock and update all<br>replicas and reads lock and read any one replica, as their currency-control technique.<br>Many database systems provide an alternative form of updating: They update at<br>one site, with lazy propagation of updates to other sites, instead of immediately<br>applying updates to all replicas as part of the transaction performing the update.<br>Schemes based on lazy propagation allow transaction processing (including updates)<br>to proceed even if a site is disconnected from the network, thus improving availabil-<br>ity, but, unfortunately, do so at the cost of consistency. One of two approaches is<br>usually followed when lazy propagation is used:<br>• Updates at replicas are translated into updates at a primary site, which are<br>then propagated lazily to all replicas.<br>This approach ensures that updates to an item are ordered serially, although<br>serializability problems can occur, since transactions may read an old value of<br>some other data item and use it to perform an update.<br>• Updates are performed at any replica and propagated to all other replicas.<br>This approach can cause even more problems, since the same data item<br>may be updated concurrently at multiple sites.<br>Some conﬂicts due to the lack of distributed concurrency control can be detected<br>when updates are propagated to other sites (we shall see how in Section 23.5.4),<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VI. Database System <br>Architecture<br>19. Distributed Databases<br>724<br>© The McGraw−Hill <br>Companies, 2001<br>728<br>Chapter 19<br>Distributed Databases<br>but resolving the conﬂict involves rolling back committed transactions, and dura-<br>bility of committed transactions is therefore not guaranteed. Further, human inter-<br>vention may be required to deal with conﬂicts. The above schemes should therefore<br>be avoided or used with care.<br>19.5.4<br>Deadlock Handling<br>The deadlock-prevention and deadlock-detection algorithms in Chapter 16 can be<br>used in a distributed system, provided that modiﬁcations are made. For example,<br>we can use the tree protocol by deﬁning a global tree among the system data items.<br>Similarly, the timestamp-ordering approach could be directly applied to a distributed<br>environment, as we saw in Section 19.5.2.<br>Deadlock prevention may result in unnecessary waiting and rollback. Further-<br>more, certain deadlock-prevention techniques may require more sites to be involved<br>in the execution of a transaction than would otherwise be the case.<br>If we allow deadlocks to occur and rely on deadlock detection, the main problem<br>in a distributed system is deciding how to maintain the wait-for graph. Common<br>techniques for dealing with this issue require that each site keep a local wait-for<br>graph. The nodes of the graph correspond to all the transactions (local as well as<br>nonlocal) that are currently either holding or requesting any of the items local to that<br>site. For example, Figure 19.3 depicts a system consisting of two sites, each maintain-<br>ing its local wait-for graph. Note that transactions T2 and T3 appear in both graphs,<br>indicating that the transactions have requested items at both sites.<br>These local wait-for graphs are constructed in the usual manner for local transac-<br>tions and data items. When a transaction Ti on site S1 needs a resource in site S2, it<br>sends a request message to site S2. If the resource is held by transaction Tj, the system<br>inserts an edge Ti →Tj in the local wait-for graph of site S2.<br>Clearly, if any local wait-for graph has a cycle, deadlock has occurred. On the<br>other hand, the fact that there are no cycles in any of the local wait-for graphs does<br>not mean that there are no deadlocks. To illustrate this problem, we consider the<br>local wait-for graphs of Figure 19.3. Each wait-for graph is acyclic; nevertheless, a<br>deadlock exists in the system because the union of the local wait-for graphs contains<br>a cycle. This graph appears in Figure 19.4.<br>T2<br>T4<br>T1<br>T2<br>T5<br>T3<br>T3<br>site S1<br>site S2<br>Figure 19.3<br>Local wait-for graphs.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VI. Database System <br>Architecture<br>19. Distributed Databases<br>725<br>© The McGraw−Hill <br>Companies, 2001<br>19.5<br>Concurrency Control in Distributed Databases<br>729<br>T1<br>T4<br>T5<br>T2<br>T3<br>Figure 19.4<br>Global wait-for graph for Figure 19.3.<br>In the centralized deadlock detection approach, the system constructs and main-<br>tains a global wait-for graph (the union of all the local graphs) in a single site: the<br>deadlock-detection coordinator. Since there is communication delay in the system,<br>we must distinguish between two types of wait-for graphs. The real graph describes<br>the real but unknown state of the system at any instance in time, as would be seen<br>by an omniscient observer. The constructed graph is an approximation generated by<br>the controller during the execution of the controller’s algorithm. Obviously, the con-<br>troller must generate the constructed graph in such a way that, whenever the detec-<br>tion algorithm is invoked, the reported results are correct. Correct means in this case<br>that, if a deadlock exists, it is reported promptly, and if the system reports a deadlock,<br>it is indeed in a deadlock state.<br>The global wait-for graph can be reconstructed or updated under these conditions:<br>• Whenever a new edge is inserted in or removed from one of the local wait-for<br>graphs.<br>• Periodically, when a number of changes have occurred in a local wait-for<br>graph.<br>• Whenever the coordinator needs to invoke the cycle-detection algorithm.<br>When the coordinator invokes the deadlock-detection algorithm, it searches its<br>global graph. If it ﬁnds a cycle, it selects a victim to be rolled back. The coordinator<br>must notify all the sites that a particular transaction has been selected as victim. The<br>sites, in turn, roll back the victim transaction.<br>This scheme may produce unnecessary rollbacks if:<br>• False cycles exist in the global wait-for graph. As an illustration, consider a<br>snapshot of the system represented by the local wait-for graphs of Figure 19.5.<br>Suppose that T2 releases the resource that it is holding in site S1, resulting in<br>the deletion of the edge T1 →T2 in S1. Transaction T2 then requests a resource<br>held by T3 at site S2, resulting in the addition of the edge T2 →T3 in S2. If the<br>insert T2 →T3 message from S2 arrives before the remove T1 →T2 message<br>from S1, the coordinator may discover the false cycle T1 →T2 →T3 after the<br>insert (but before the remove). Deadlock recovery may be initiated, although<br>no deadlock has occurred.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VI. Database System <br>Architecture<br>19. Distributed Databases<br>726<br>© The McGraw−Hill <br>Companies, 2001<br>730<br>Chapter 19<br>Distributed Databases<br>T1<br>T2<br>T2<br>T1<br>T3<br>S2<br>T1<br>T3<br>coordinator<br>S1<br>Figure 19.5<br>False cycles in the global wait-for graph.<br>Note that the false-cycle situation could not occur under two-phase locking.<br>The likelihood of false cycles is usually sufﬁciently low that they do not cause<br>a serious performance problem.<br>• A deadlock has indeed occurred and a victim has been picked, while one of the<br>transactions was aborted for reasons unrelated to the deadlock. For example,<br>suppose that site S1 in Figure 19.3 decides to abort T2. At the same time, the<br>coordinator has discovered a cycle, and has picked T3 as a victim. Both T2 and<br>T3 are now rolled back, although only T2 needed to be rolled back.<br>Deadlock detection can be done in a distributed manner, with several sites taking<br>on parts of the task, instead of being done at a single site, However, such algorithms<br>are more complicated and more expensive. See the bibliographical notes for refer-<br>ences to such algorithms.<br>19.6<br>Availability<br>One of the goals in using distributed databases is high availability; that is, the data-<br>base must function almost all the time. In particular, since failures are more likely<br>in large distributed systems, a distributed database must continue functioning even<br>when there are various types of failures. The ability to continue functioning even<br>during failures is referred to as robustness.<br>For a distributed system to be robust, it must detect failures, reconﬁgure the system<br>so that computation may continue, and recover when a processor or a link is repaired.<br>The different types of failures are handled in different ways. For example, message<br>loss is handled by retransmission. Repeated retransmission of a message across a link,<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VI. Database System <br>Architecture<br>19. Distributed Databases<br>727<br>© The McGraw−Hill <br>Companies, 2001<br>19.6<br>Availability<br>731<br>without receipt of an acknowledgment, is usually a symptom of a link failure. The<br>network usually attempts to ﬁnd an alternative route for the message. Failure to ﬁnd<br>such a route is usually a symptom of network partition.<br>It is generally not possible, however, to differentiate clearly between site failure<br>and network partition. The system can usually detect that a failure has occurred, but<br>it may not be able to identify the type of failure. For example, suppose that site S1<br>is not able to communicate with S2. It could be that S2 has failed. However, another<br>possibility is that the link between S1 and S2 has failed, resulting in network parti-<br>tion. The problem is partly addressed by using multiple links between sites, so that<br>even if one link fails the sites will remain connected. However, multiple link failure<br>can still occur, so there are situations where we cannot be sure whether a site failure<br>or network partition has occurred.<br>Suppose that site S1 has discovered that a failure has occurred. It must then ini-<br>tiate a procedure that will allow the system to reconﬁgure, and to continue with the<br>normal mode of operation.<br>• If transactions were active at a failed/inaccessible site at the time of the failure,<br>these transactions should be aborted. It is desirable to abort such transactions<br>promptly, since they may hold locks on data at sites that are still active; wait-<br>ing for the failed/inaccessible site to become accessible again may impede<br>other transactions at sites that are operational.<br>However, in some cases, when data objects are replicated it may be possible<br>to proceed with reads and updates even though some replicas are inaccessible.<br>In this case, when a failed site recovers, if it had replicas of any data object, it<br>must obtain the current values of these data objects, and must ensure that it<br>receives all future updates. We address this issue in Section 19.6.1.<br>• If replicated data are stored at a failed/inaccessible site, the catalog should be<br>updated so that queries do not reference the copy at the failed site. When a<br>site rejoins, care must be taken to ensure that data at the site is consistent, as<br>we will see in Section 19.6.3.<br>• If a failed site is a central server for some subsystem, an election must be held<br>to determine the new server (see Section 19.6.5). Examples of central servers<br>include a name server, a concurrency coordinator, or a global deadlock detec-<br>tor.<br>Since it is, in general, not possible to distinguish between network link failures and<br>site failures, any reconﬁguration scheme must be designed to work correctly in case<br>of a partitioning of the network. In particular, these situations must be avoided:<br>• Two or more central servers are elected in distinct partitions.<br>• More than one partition updates a replicated data item.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VI. Database System <br>Architecture<br>19. Distributed Databases<br>728<br>© The McGraw−Hill <br>Companies, 2001<br>732<br>Chapter 19<br>Distributed Databases<br>19.6.1<br>Majority-Based Approach<br>The majority-based approach to distributed concurrency control in Section 19.5.1.4<br>can be modiﬁed to work in spite of failures. In this approach, each data object stores<br>with it a version number to detect when it was last written to. Whenever a transaction<br>writes an object it also updates the version number in this way:<br>• If data object a is replicated in n different sites, then a lock-request message<br>must be sent to more than one-half of the n sites in which a is stored. The<br>transaction does not operate on a until it has successfully obtained a lock on a<br>majority of the replicas of a.<br>• Read operations look at all replicas on which a lock has been obtained, and<br>read the value from the replica that has the highest version number. (Option-<br>ally, they may also write this value back to replicas with lower version num-<br>bers.) Writes read all the replicas just like reads to ﬁnd the highest version<br>number (this step would normally have been performed earlier in the trans-<br>action by a read, and the result can be reused). The new version number is<br>one more than the highest version number. The write operation writes all the<br>replicas on which it has obtained locks, and sets the version number at all the<br>replicas to the new version number.<br>Failures during a transaction (whether network partitions or site failures) can be tol-<br>erated as long as (1) the sites available at commit contain a majority of replicas of all<br>the objects written to and (2) during reads, a majority of replicas are read to ﬁnd the<br>version numbers. If these requirements are violated, the transaction must be aborted.<br>As long as the requirements are satisﬁed, the two-phase commit protocol can be used,<br>as usual, on the sites that are available.<br>In this scheme, reintegration is trivial; nothing needs to be done. This is because<br>writes would have updated a majority of the replicas, while reads will read a majority<br>of the replicas and ﬁnd at least one replica that has the latest version.<br>The version numbering technique used with the majority protocol can also be used<br>to make the quorum consensus protocol work in the presence of failures. We leave the<br>(straightforward) details to the reader. However, the danger of failures preventing the<br>system from processing transactions increases if some sites are given higher weights.<br>19.6.2<br>Read One, Write All Available Approach<br>As a special case of quorum consensus, we can employ the biased protocol by giving<br>unit weights to all sites, setting the read quorum to 1, and setting the write quorum to<br>n (all sites). In this special case, there is no need to use version numbers; however, if<br>even a single site containing a data item fails, no write to the item can proceed, since<br>the write quorum will not be available. This protocol is called the read one, write all<br>protocol since all replicas must be written.<br>To allow work to proceed in the event of failures, we would like to be able to use a<br>read one, write all available protocol. In this approach, a read operation proceeds as<br>in the read one, write all scheme; any available replica can be read, and a read lock is<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VI. Database System <br>Architecture<br>19. Distributed Databases<br>729<br>© The McGraw−Hill <br>Companies, 2001<br>19.6<br>Availability<br>733<br>obtained at that replica. A write operation is shipped to all replicas; and write locks<br>are acquired on all the replicas. If a site is down, the transaction manager proceeds<br>without waiting for the site to recover.<br>While this approach appears very attractive, there are several complications. In<br>particular, temporary communication failure may cause a site to appear to be un-<br>available, resulting in a write not being performed, but when the link is restored,<br>the site is not aware that it has to perform some reintegration actions to catch up on<br>writes it has lost. Further, if the network partitions, each partition may proceed to<br>update the same data item, believing that sites in the other partitions are all dead.<br>The read one, write all available scheme can be used if there is never any network<br>partitioning, but it can result in inconsistencies in the event of network partitions.<br>19.6.3<br>Site Reintegration<br>Reintegration of a repaired site or link into the system requires care. When a failed<br>site recovers, it must initiate a procedure to update its system tables to reﬂect changes<br>made while it was down. If the site had replicas of any data items, it must obtain<br>the current values of these data items and ensure that it receives all future updates.<br>Reintegration of a site is more complicated than it may seem to be at ﬁrst glance,<br>since there may be updates to the data items processed during the time that the site<br>is recovering.<br>An easy solution is to halt the entire system temporarily while the failed site rejoins<br>it. In most applications, however, such a temporary halt is unacceptably disruptive.<br>Techniques have been developed to allow failed sites to reintegrate while concurrent<br>updates to data items proceed concurrently. Before a read or write lock is granted<br>on any data item, the site must ensure that it has caught up on all updates to the<br>data item. If a fai</span><br><br><span style="background-color: #A0C4FF;" title="Chunk 93 | Start: 1860186 | End: 1880186 | Tokens: 3250">led link recovers, two or more partitions can be rejoined. Since a<br>partitioning of the network limits the allowable operations by some or all sites, all<br>sites should be informed promptly of the recovery of the link. See the bibliographical<br>notes for more information on recovery in distributed systems.<br>19.6.4<br>Comparison with Remote Backup<br>Remote backup systems, which we studied in Section 17.10, and replication in dis-<br>tributed databases are two alternative approaches to providing high availability. The<br>main difference between the two schemes is that with remote backup systems, ac-<br>tions such as concurrency control and recovery are performed at a single site, and<br>only data and log records are replicated at the other site. In particular, remote backup<br>systems help avoid two-phase commit, and its resultant overheads. Also, transac-<br>tions need to contact only one site (the primary site), and thus avoid the overhead<br>of running transaction code at multiple sites. Thus remote backup systems offer a<br>lower-cost approach to high availability than replication.<br>On the other hand, replication can provide greater availability by having multiple<br>replicas available, and using the majority protocol.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VI. Database System <br>Architecture<br>19. Distributed Databases<br>730<br>© The McGraw−Hill <br>Companies, 2001<br>734<br>Chapter 19<br>Distributed Databases<br>19.6.5<br>Coordinator Selection<br>Several of the algorithms that we have presented require the use of a coordinator. If<br>the coordinator fails because of a failure of the site at which it resides, the system can<br>continue execution only by restarting a new coordinator on another site. One way to<br>continue execution is by maintaining a backup to the coordinator, which is ready to<br>assume responsibility if the coordinator fails.<br>A backup coordinator is a site that, in addition to other tasks, maintains enough<br>information locally to allow it to assume the role of coordinator with minimal disrup-<br>tion to the distributed system. All messages directed to the coordinator are received<br>by both the coordinator and its backup. The backup coordinator executes the same<br>algorithms and maintains the same internal state information (such as, for a concur-<br>rency coordinator, the lock table) as does the actual coordinator. The only difference<br>in function between the coordinator and its backup is that the backup does not take<br>any action that affects other sites. Such actions are left to the actual coordinator.<br>In the event that the backup coordinator detects the failure of the actual coordi-<br>nator, it assumes the role of coordinator. Since the backup has all the information<br>available to it that the failed coordinator had, processing can continue without inter-<br>ruption.<br>The prime advantage to the backup approach is the ability to continue processing<br>immediately. If a backup were not ready to assume the coordinator’s responsibility,<br>a newly appointed coordinator would have to seek information from all sites in the<br>system so that it could execute the coordination tasks. Frequently, the only source<br>of some of the requisite information is the failed coordinator. In this case, it may be<br>necessary to abort several (or all) active transactions, and to restart them under the<br>control of the new coordinator.<br>Thus, the backup-coordinator approach avoids a substantial amount of delay while<br>the distributed system recovers from a coordinator failure. The disadvantage is the<br>overhead of duplicate execution of the coordinator’s tasks. Furthermore, a coordina-<br>tor and its backup need to communicate regularly to ensure that their activities are<br>synchronized.<br>In short, the backup-coordinator approach incurs overhead during normal pro-<br>cessing to allow fast recovery from a coordinator failure.<br>In the absence of a designated backup coordinator, or in order to handle multiple<br>failures, a new coordinator may be chosen dynamically by sites that are live. Elec-<br>tion algorithms enable the sites to choose the site for the new coordinator in a decen-<br>tralized manner. Election algorithms require that a unique identiﬁcation number be<br>associated with each active site in the system.<br>The bully algorithm for election works as follows. To keep the notation and the<br>discussion simple, assume that the identiﬁcation number of site Si is i and that the<br>chosen coordinator will always be the active site with the largest identiﬁcation num-<br>ber. Hence, when a coordinator fails, the algorithm must elect the active site that has<br>the largest identiﬁcation number. The algorithm must send this number to each active<br>site in the system. In addition, the algorithm must provide a mechanism by which a<br>site recovering from a crash can identify the current coordinator. Suppose that site Si<br>sends a request that is not answered by the coordinator within a prespeciﬁed time<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VI. Database System <br>Architecture<br>19. Distributed Databases<br>731<br>© The McGraw−Hill <br>Companies, 2001<br>19.7<br>Distributed Query Processing<br>735<br>interval T. In this situation, it is assumed that the coordinator has failed, and Si tries<br>to elect itself as the site for the new coordinator.<br>Site Si sends an election message to every site that has a higher identiﬁcation num-<br>ber. Site Si then waits, for a time interval T, for an answer from any one of these sites.<br>If it receives no response within time T, it assumes that all sites with numbers greater<br>than i have failed, and it elects itself as the site for the new coordinator and sends a<br>message to inform all active sites with identiﬁcation numbers lower than i that it is<br>the site at which the new coordinator resides.<br>If Si does receive an answer, it begins a time interval T ′, to receive a message<br>informing it that a site with a higher identiﬁcation number has been elected. (Some<br>other site is electing itself coordinator, and should report the results within time T ′.)<br>If Si receives no message within T ′, then it assumes the site with a higher number<br>has failed, and site Si restarts the algorithm.<br>After a failed site recovers, it immediately begins execution of the same algorithm.<br>If there are no active sites with higher numbers, the recovered site forces all sites with<br>lower numbers to let it become the coordinator site, even if there is a currently active<br>coordinator with a lower number. It is for this reason that the algorithm is termed the<br>bully algorithm.<br>19.7<br>Distributed Query Processing<br>In Chapter 14, we saw that there are a variety of methods for computing the answer<br>to a query. We examined several techniques for choosing a strategy for processing a<br>query that minimize the amount of time that it takes to compute the answer. For cen-<br>tralized systems, the primary criterion for measuring the cost of a particular strategy<br>is the number of disk accesses. In a distributed system, we must take into account<br>several other matters, including<br>• The cost of data transmission over the network<br>• The potential gain in performance from having several sites process parts of<br>the query in parallel<br>The relative cost of data transfer over the network and data transfer to and from disk<br>varies widely depending on the type of network and on the speed of the disks. Thus,<br>in general, we cannot focus solely on disk costs or on network costs. Rather, we must<br>ﬁnd a good tradeoff between the two.<br>19.7.1<br>Query Transformation<br>Consider an extremely simple query: “Find all the tuples in the account relation.” Al-<br>though the query is simple — indeed, trivial—processing it is not trivial, since the<br>account relation may be fragmented, replicated, or both, as we saw in Section 19.2.<br>If the account relation is replicated, we have a choice of replica to make. If no repli-<br>cas are fragmented, we choose the replica for which the transmission cost is lowest.<br>However, if a replica is fragmented, the choice is not so easy to make, since we need<br>to compute several joins or unions to reconstruct the account relation. In this case,<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VI. Database System <br>Architecture<br>19. Distributed Databases<br>732<br>© The McGraw−Hill <br>Companies, 2001<br>736<br>Chapter 19<br>Distributed Databases<br>the number of strategies for our simple example may be large. Query optimization<br>by exhaustive enumeration of all alternative strategies may not be practical in such<br>situations.<br>Fragmentation transparency implies that a user may write a query such as<br>σbranch-name = “Hillside” (account)<br>Since account is deﬁned as<br>account1 ∪account2<br>the expression that results from the name translation scheme is<br>σbranch-name = “Hillside” (account1 ∪account2)<br>Using the query-optimization techniques of Chapter 13, we can simplify the preced-<br>ing expression automatically. The result is the expression<br>σbranch-name = “Hillside” (account1) ∪σbranch-name = “Hillside” (account2)<br>which includes two subexpressions. The ﬁrst involves only account1, and thus can<br>be evaluated at the Hillside site. The second involves only account2, and thus can be<br>evaluated at the Valleyview site.<br>There is a further optimization that can be made in evaluating<br>σbranch-name = “Hillside” (account1)<br>Since account1 has only tuples pertaining to the Hillside branch, we can eliminate the<br>selection operation. In evaluating<br>σbranch-name = “Hillside” (account2)<br>we can apply the deﬁnition of the account2 fragment to obtain<br>σbranch-name = “Hillside” (σbranch-name = “Valleyview” (account))<br>This expression is the empty set, regardless of the contents of the account relation.<br>Thus, our ﬁnal strategy is for the Hillside site to return account1 as the result of<br>the query.<br>19.7.2<br>Simple Join Processing<br>As we saw in Chapter 13, a major decision in the selection of a query-processing strat-<br>egy is choosing a join strategy. Consider the following relational-algebra expression:<br>account<br> depositor<br> branch<br>Assume that the three relations are neither replicated nor fragmented, and that ac-<br>count is stored at site S1, depositor at S2, and branch at S3. Let SI denote the site<br>at which the query was issued. The system needs to produce the result at site SI.<br>Among the possible strategies for processing this query are these:<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VI. Database System <br>Architecture<br>19. Distributed Databases<br>733<br>© The McGraw−Hill <br>Companies, 2001<br>19.7<br>Distributed Query Processing<br>737<br>• Ship copies of all three relations to site SI. Using the techniques of Chapter 13,<br>choose a strategy for processing the entire query locally at site SI.<br>• Ship a copy of the account relation to site S2, and compute temp1 = account<br><br>depositor at S2. Ship temp1 from S2 to S3, and compute temp2 = temp1<br> branch<br>at S3. Ship the result temp2 to SI.<br>• Devise strategies similar to the previous one, with the roles of S1, S2, S3 ex-<br>changed.<br>No one strategy is always the best one. Among the factors that must be considered<br>are the volume of data being shipped, the cost of transmitting a block of data be-<br>tween a pair of sites, and the relative speed of processing at each site. Consider the<br>ﬁrst two strategies listed. If we ship all three relations to SI, and indices exist on<br>these relations, we may need to re-create these indices at SI. This re-creation of in-<br>dices entails extra processing overhead and extra disk accesses. However, the second<br>strategy has the disadvantage that a potentially large relation (customer<br> account)<br>must be shipped from S2 to S3. This relation repeats the address data for a customer<br>once for each account that the customer has. Thus, the second strategy may result in<br>extra network transmission compared to the ﬁrst strategy.<br>19.7.3<br>Semijoin Strategy<br>Suppose that we wish to evaluate the expression r1<br> r2, where r1 and r2 are stored<br>at sites S1 and S2, respectively. Let the schemas of r1 and r2 be R1 and R2. Suppose<br>that we wish to obtain the result at S1. If there are many tuples of r2 that do not<br>join with any tuple of r1, then shipping r2 to S1 entails shipping tuples that fail to<br>contribute to the result. We want to remove such tuples before shipping data to S1,<br>particularly if network costs are high.<br>A possible strategy to accomplish all this is:<br>1. Compute temp1 ←ΠR1 ∩R2 (r1) at S1.<br>2. Ship temp1 from S1 to S2.<br>3. Compute temp2 ←r2<br> temp1 at S2.<br>4. Ship temp2 from S2 to S1.<br>5. Compute r1<br> temp2 at S1. The resulting relation is the same as r1<br> r2.<br>Before considering the efﬁciency of this strategy, let us verify that the strategy com-<br>putes the correct answer. In step 3, temp2 has the result of r2<br> ΠR1 ∩R2 (r1). In step<br>5, we compute<br>r1<br> r2<br> ΠR1 ∩R2 (r1)<br>Since join is associative and commutative, we can rewrite this expression as<br>(r1<br> ΠR1 ∩R2 (r1))<br> r2<br>Since r1<br> Π(R1 ∩R2) (r1) = r1, the expression is, indeed, equal to r1<br> r2, the<br>expression we are trying to evaluate.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VI. Database System <br>Architecture<br>19. Distributed Databases<br>734<br>© The McGraw−Hill <br>Companies, 2001<br>738<br>Chapter 19<br>Distributed Databases<br>This strategy is particularly advantageous when relatively few tuples of r2 con-<br>tribute to the join. This situation is likely to occur if r1 is the result of a relational-<br>algebra expression involving selection. In such a case, temp2 may have signiﬁcantly<br>fewer tuples than r2. The cost savings of the strategy result from having to ship only<br>temp2, rather than all of r2, to S1. Additional cost is incurred in shipping temp1 to S2.<br>If a sufﬁciently small fraction of tuples in r2 contribute to the join, the overhead of<br>shipping temp1 will be dominated by the savings of shipping only a fraction of the<br>tuples in r2.<br>This strategy is called a semijoin strategy, after the semijoin operator of the rela-<br>tional algebra, denoted<br>n. The semijoin of r1 with r2, denoted r1<br>n r2, is<br>ΠR1(r1<br> r2)<br>Thus, r1<br>n r2 selects those tuples of r1 that contributed to r1<br> r2. In step 3, temp2<br>= r2<br>n r1.<br>For joins of several relations, this strategy can be extended to a series of semijoin<br>steps. A substantial body of theory has been developed regarding the use of semijoins<br>for query optimization. Some of this theory is referenced in the bibliographical notes.<br>19.7.4<br>Join Strategies that Exploit Parallelism<br>Consider a join of four relations:<br>r1<br> r2<br> r3<br> r4<br>where relation ri is stored at site Si. Assume that the result must be presented at site<br>S1. There are many possible strategies for parallel evaluation. (We study the issue<br>of parallel processing of queries in detail in Chapter 20.) In one such strategy, r1 is<br>shipped to S2, and r1<br> r2 computed at S2. At the same time, r3 is shipped to S4,<br>and r3<br> r4 computed at S4. Site S2 can ship tuples of (r1<br> r2) to S1 as they are<br>produced, rather than wait for the entire join to be computed. Similarly, S4 can ship<br>tuples of (r3<br> r4) to S1. Once tuples of (r1<br> r2) and (r3<br> r4) arrive at S1, the<br>computation of (r1<br> r2)<br> (r3<br> r4) can begin, with the pipelined join technique<br>of Section 13.7.2.2. Thus, computation of the ﬁnal join result at S1 can be done<br>in parallel with the computation of (r1<br> r2) at S2, and with the computation of<br>(r3<br> r4) at S4.<br>19.8<br>Heterogeneous Distributed Databases<br>Many new database applications require data from a variety of preexisting databases<br>located in a heterogeneous collection of hardware and software environments. Ma-<br>nipulation of information located in a heterogeneous distributed database requires<br>an additional software layer on top of existing database systems. This software layer<br>is called a multidatabase system. The local database systems may employ different<br>logical models and data-deﬁnition and data-manipulation languages, and may dif-<br>fer in their concurrency-control and transaction-management mechanisms. A multi-<br>database system creates the illusion of logical database integration without requiring<br>physical database integration.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VI. Database System <br>Architecture<br>19. Distributed Databases<br>735<br>© The McGraw−Hill <br>Companies, 2001<br>19.8<br>Heterogeneous Distributed Databases<br>739<br>Full integration of heterogeneous systems into a homogeneous distributed data-<br>base is often difﬁcult or impossible:<br>• Technical difﬁculties. The investment in application programs based on ex-<br>isting database systems may be huge, and the cost of converting these appli-<br>cations may be prohibitive.<br>• Organizational difﬁculties. Even if integration is technically possible, it may<br>not be politically possible, because the existing database systems belong to dif-<br>ferent corporations or organizations. In such cases, it is important for a multi-<br>database system to allow the local database systems to retain a high degree of<br>autonomy over the local database and transactions running against that data.<br>For these reasons, multidatabase systems offer signiﬁcant advantages that out-<br>weigh their overhead. In this section, we provide an overview of the challenges faced<br>in constructing a multidatabase environment from the standpoint of data deﬁnition<br>and query processing. Section 24.6 provides an overview of transaction management<br>issues in multidatabases.<br>19.8.1<br>Uniﬁed View of Data<br>Each local database management system may use a different data model. For in-<br>stance, some may employ the relational model, whereas others may employ older<br>data models, such as the network model (see Appendix A) or the hierarchical model<br>(see Appendix B).<br>Since the multidatabase system is supposed to provide the illusion of a single,<br>integrated database system, a common data model must be used. A commonly used<br>choice is the relational model, with SQL as the common query language. Indeed, there<br>are several systems available today that allow SQL queries to a nonrelational database<br>management system.<br>Another difﬁculty is the provision of a common conceptual schema. Each local sys-<br>tem provides its own conceptual schema. The multidatabase system must integrate<br>these separate schemas into one common schema. Schema integration is a compli-<br>cated task, mainly because of the semantic heterogeneity.<br>Schema integration is not simply straightforward translation between data-deﬁni-<br>tion languages. The same attribute names may appear in different local databases but<br>with different meanings. The data types used in one system may not be supported by<br>other systems, and translation between types may not be simple. Even for identical<br>data types, problems may arise from the physical representation of data: One system<br>may use ASCII, another EBCDIC; ﬂoating-point representations may differ; integers<br>may be represented in big-endian or little-endian form. At the semantic level, an inte-<br>ger value for length may be inches in one system and millimeters in another, thus<br>creating an awkward situation in which equality of integers is only an approximate<br>notion (as is always the case for ﬂoating-point numbers). The same name may ap-<br>pear in different languages in different systems. For example, a system based in the<br>United States may refer to the city “Cologne,” whereas one in Germany refers to it as<br>“K¨oln.”<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VI. Database System <br>Architecture<br>19. Distributed Databases<br>736<br>© The McGraw−Hill <br>Companies, 2001<br>740<br>Chapter 19<br>Distributed Databases<br>All these seemingly minor distinctions must be properly recorded in the com-<br>mon global conceptual schema. Translation functions must be provided. Indices must<br>be annotated for system-dependent behavior (for example, the sort order of nonal-<br>phanumeric characters is not the same in ASCII as in EBCDIC). As we noted earlier,<br>the alternative of converting each database to a common format may not be feasible<br>without obsoleting existing application programs.<br>19.8.2<br>Query Processing<br>Query processing in a heterogeneous database can be complicated. Some of the issues<br>are:<br>• Given a query on a global schema, the query may have to be translated into<br>quer</span><br><br><span style="background-color: #BDB2FF;" title="Chunk 94 | Start: 1880188 | End: 1900188 | Tokens: 3150">ies on local schemas at each of the sites where the query has to be exe-<br>cuted. The query results have to be translated back into the global schema.<br>The task is simpliﬁed by writing wrappers for each data source, which pro-<br>vide a view of the local data in the global schema. Wrappers also translate<br>queries on the global schema into queries on the local schema, and translate<br>results back into the global schema. Wrappers may be provided by individual<br>sites, or may be written separately as part of the multidatabase system.<br>Wrappers can even be used to provide a relational view of nonrelational<br>data sources, such as Web pages (possibly with forms interfaces), ﬂat ﬁles,<br>hierarchical and network databases, and directory systems.<br>• Some data sources may provide only limited query capabilities; for instance,<br>they may support selections, but not joins. They may even restrict the form<br>of selections, allowing selections only on certain ﬁelds; Web data sources with<br>form interfaces are an example of such data sources. Queries may therefore<br>have to be broken up, to be partly performed at the data source and partly at<br>the site issuing the query.<br>• In general, more than one site may need to be accessed to answer a given<br>query. Answers retrieved from the sites may have to be processed to remove<br>duplicates. Suppose one site contains account tuples satisfying the selection<br>balance &lt; 100, while another contains account tuples satisfying balance &gt; 50.<br>A query on the entire account relation would require access to both sites and<br>removal of duplicate answers resulting from tuples with balance between 50<br>and 100, which are replicated at both sites.<br>• Global query optimization in a heterogeneous database is difﬁcult, since the<br>query execution system may not know what the costs are of alternative query<br>plans at different sites. The usual solution is to rely on only local-level opti-<br>mization, and just use heuristics at the global level.<br>Mediator systems are systems that integrate multiple heterogeneous data sources,<br>providing an integrated global view of the data and providing query facilities on<br>the global view. Unlike full-ﬂedged multidatabase systems, mediator systems do not<br>bother about transaction processing. (The terms mediator and multidatabase are of-<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VI. Database System <br>Architecture<br>19. Distributed Databases<br>737<br>© The McGraw−Hill <br>Companies, 2001<br>19.9<br>Directory Systems<br>741<br>ten used in an interchangeable fashion, and systems that are called mediators may<br>support limited forms of transactions.) The term virtual database is used to refer<br>to multidatabase/mediator systems, since they provide the appearance of a single<br>database with a global schema, although data exist on multiple sites in local schemas.<br>19.9<br>Directory Systems<br>Consider an organization that wishes to make data about its employees available to<br>a variety of people in the organization; example of the kinds of data would include<br>name, designation, employee-id, address, email address, phone number, fax num-<br>ber, and so on. In the precomputerization days, organizations would create physical<br>directories of employees and distribute them across the organization. Even today,<br>telephone companies create physical directories of customers.<br>In general, a directory is a listing of information about some class of objects such as<br>persons. Directories can be used to ﬁnd information about a speciﬁc object, or in the<br>reverse direction to ﬁnd objects that meet a certain requirement. In the world of phys-<br>ical telephone directories, directories that satisfy lookups in the forward direction are<br>called white pages, while directories that satisfy lookups in the reverse direction are<br>called yellow pages.<br>In today’s networked world, the need for directories is still present and, if any-<br>thing, even more important. However, directories today need to be available over a<br>computer network, rather than in a physical (paper) form.<br>19.9.1<br>Directory Access Protocols<br>Directory information can be made available through Web interfaces, as many orga-<br>nizations, and phone companies in particular do. Such interfaces are good for hu-<br>mans. However, programs too, need to access directory information. Directories can<br>be used for storing other types of information, much like ﬁle system directories. For<br>instance, Web browsers can store personal bookmarks and other browser settings in<br>a directory system. A user can thus access the same settings from multiple locations,<br>such as at home and at work, without having to share a ﬁle system.<br>Several directory access protocols have been developed to provide a standardized<br>way of accessing data in a directory. The most widely used among them today is the<br>Lightweight Directory Access Protocol (LDAP).<br>Obviously all the types of data in our examples can be stored without much trou-<br>ble in a database system, and accessed through protocols such as JDBC or ODBC. The<br>question then is, why come up with a specialized protocol for accessing directory<br>information? There are at least two answers to the question.<br>• First, directory access protocols are simpliﬁed protocols that cater to a lim-<br>ited type of access to data. They evolved in parallel with the database access<br>protocols.<br>• Second, and more important, directory systems provide a simple mechanism<br>to name objects in a hierarchical fashion, similar to ﬁle system directory names,<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VI. Database System <br>Architecture<br>19. Distributed Databases<br>738<br>© The McGraw−Hill <br>Companies, 2001<br>742<br>Chapter 19<br>Distributed Databases<br>which can be used in a distributed directory system to specify what informa-<br>tion is stored in each of the directory servers. For example, a particular direc-<br>tory server may store information for Bell Laboratories employees in Murray<br>Hill, while another may store information for Bell Laboratories employees in<br>Bangalore, giving both sites autonomy in controlling their local data. The di-<br>rectory access protocol can be used to obtain data from both directories, across<br>a network. More importantly, the directory system can be set up to automati-<br>cally forward queries made at one site to the other site, without user interven-<br>tion.<br>For these reasons, several organizations have directory systems to make organiza-<br>tional information available online. As may be expected, several directory implemen-<br>tations ﬁnd it beneﬁcial to use relational databases to store data, instead of creating<br>special-purpose storage systems.<br>19.9.2<br>LDAP: Lightweight Directory Access Protocol<br>In general a directory system is implemented as one or more servers, which service<br>multiple clients. Clients use the application programmer interface deﬁned by direc-<br>tory system to communicate with the directory servers. Directory access protocols<br>also deﬁne a data model and access control.<br>The X.500 directory access protocol, deﬁned by the International Organization for<br>Standardization (ISO), is a standard for accessing directory information. However,<br>the protocol is rather complex, and is not widely used. The Lightweight Directory<br>Access Protocol (LDAP) provides many of the X.500 features, but with less complex-<br>ity, and is widely used. In the rest of this section, we shall outline the data model and<br>access protocol details of LDAP.<br>19.9.2.1<br>LDAP Data Model<br>In LDAP directories store entries, which are similar to objects. Each entry must have a<br>distinguished name (DN), which uniquely identiﬁes the entry. A DN is in turn made<br>up of a sequence of relative distinguished names (RDNs). For example, an entry may<br>have the following distinguished name.<br>cn=Silberschatz, ou=Bell Labs, o=Lucent, c=USA<br>As you can see, the distinguished name in this example is a combination of a name<br>and (organizational) address, starting with a person’s name, then giving the orga-<br>nizational unit (ou), the organization (o), and country (c). The order of the compo-<br>nents of a distinguished name reﬂects the normal postal address order, rather than<br>the reverse order used in specifying path names for ﬁles. The set of RDNs for a DN is<br>deﬁned by the schema of the directory system.<br>Entries can also have attributes. LDAP provides binary, string, and time types, and<br>additionally the types tel for telephone numbers, and PostalAddress for addresses<br>(lines separated by a “$” character). Unlike those in the relational model, attributes<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VI. Database System <br>Architecture<br>19. Distributed Databases<br>739<br>© The McGraw−Hill <br>Companies, 2001<br>19.9<br>Directory Systems<br>743<br>are multivalued by default, so it is possible to store multiple telephone numbers or<br>addresses for an entry.<br>LDAP allows the deﬁnition of object classes with attribute names and types. In-<br>heritance can be used in deﬁning object classes. Moreover, entries can be speciﬁed to<br>be of one or more object classes. It is not necessary that there be a single most-speciﬁc<br>object class to which an entry belongs.<br>Entries are organized into a directory information tree (DIT), according to their<br>distinguished names. Entries at the leaf level of the tree usually represent speciﬁc<br>objects. Entries that are internal nodes represent objects such as organizational units,<br>organizations, or countries. The children of a node have a DN containing all the RDNs<br>of the parent, and one or more additional RDNs. For instance, an internal node may<br>have a DN c=USA, and all entries below it have the value USA for the RDN c.<br>The entire distinguished name need not be stored in an entry; The system can<br>generate the distinguished name of an entry by traversing up the DIT from the entry,<br>collecting the RDN=value components to create the full distinguished name.<br>Entries may have more than one distinguished name—for example, an entry for a<br>person in more than one organization. To deal with such cases, the leaf level of a DIT<br>can be an alias, which points to an entry in another branch of the tree.<br>19.9.2.2<br>Data Manipulation<br>Unlike SQL, LDAP does not deﬁne either a data-deﬁnition language or a data manip-<br>ulation language. However, LDAP deﬁnes a network protocol for carrying out data<br>deﬁnition and manipulation. Users of LDAP can either use an application program-<br>ming interface, or use tools provided by various vendors to perform data deﬁnition<br>and manipulation. LDAP also deﬁnes a ﬁle format called LDAP Data Interchange<br>Format (LDIF) that can be used for storing and exchanging information.<br>The querying mechanism in LDAP is very simple, consisting of just selections and<br>projections, without any join. A query must specify the following:<br>• A base—that is, a node within a DIT—by giving its distinguished name (the<br>path from the root to the node).<br>• A search condition, which can be a Boolean combination of conditions on in-<br>dividual attributes. Equality, matching by wild-card characters, and approxi-<br>mate equality (the exact deﬁnition of approximate equality is system depen-<br>dent) are supported.<br>• A scope, which can be just the base, the base and its children, or the entire<br>subtree beneath the base.<br>• Attributes to return.<br>• Limits on number of results and resource consumption.<br>The query can also specify whether to automatically dereference aliases; if alias deref-<br>erences are turned off, alias entries can be returned as answers.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VI. Database System <br>Architecture<br>19. Distributed Databases<br>740<br>© The McGraw−Hill <br>Companies, 2001<br>744<br>Chapter 19<br>Distributed Databases<br>One way of querying an LDAP data source is by using LDAP URLs. Examples of<br>LDAP URLs are:<br>ldap:://aura.research.bell-labs.com/o=Lucent,c=USA<br>ldap:://aura.research.bell-labs.com/o=Lucent,c=USA??sub?cn=Korth<br>The ﬁrst URL returns all attributes of all entries at the server with organization being<br>Lucent, and country being USA. The second URL executes a search query (selection)<br>cn=Korth on the subtree of the node with distinguished name o=Lucent, c=USA. The<br>question marks in the URL separate different ﬁelds. The ﬁrst ﬁeld is the distinguished<br>name, here o=Lucent,c=USA. The second ﬁeld, the list of attributes to return, is left<br>empty, meaning return all attributes. The third attribute, sub, indicates that the entire<br>subtree is to be searched. The last parameter is the search condition.<br>A second way of querying an LDAP directory is by using an application program-<br>ming interface. Figure 19.6 shows a piece of C code used to connect to an LDAP server<br>and run a query against the server. The code ﬁrst opens a connection to an LDAP<br>server by ldap open and ldap bind. It then executes a query by ldap search s. The<br>arguments to ldap search s are the LDAP connection handle, the DN of the base from<br>which the search should be done, the scope of the search, the search condition, the<br>list of attributes to be returned, and an attribute called attrsonly, which, if set to 1,<br>would result in only the schema of the result being returned, without any actual tu-<br>ples. The last argument is an output argument that returns the result of the search as<br>an LDAPMessage structure.<br>The ﬁrst for loop iterates over and prints each entry in the result. Note that an<br>entry may have multiple attributes, and the second for loop prints each attribute.<br>Since attributes in LDAP may be multivalued, the third for loop prints each value of<br>an attribute. The calls ldap msgfree and ldap value free free memory that is allocated<br>by the LDAP libraries. Figure 19.6 does not show code for handling error conditions.<br>The LDAP API also contains functions to create, update, and delete entries, as well<br>as other operations on the DIT. Each function call behaves like a separate transaction;<br>LDAP does not support atomicity of multiple updates.<br>19.9.2.3<br>Distributed Directory Trees<br>Information about an organization may be split into multiple DITs, each of which<br>stores information about some entries. The sufﬁx of a DIT is a sequence of RDN=value<br>pairs that identify what information the DIT stores; the pairs are concatenated to the<br>rest of the distinguished name generated by traversing from the entry to the root.<br>For instance, the sufﬁx of a DIT may be o=Lucent, c=USA, while another may have<br>the sufﬁx o=Lucent, c=India. The DITs may be organizationally and geographically<br>separated.<br>A node in a DIT may contain a referral to another node in another DIT; for in-<br>stance, the organizational unit Bell Labs under o=Lucent, c=USA may have its own<br>DIT, in which case the DIT for o=Lucent, c=USA would have a node ou=Bell Labs<br>representing a referral to the DIT for Bell Labs.<br>Referrals are the key component that help organize a distributed collection of di-<br>rectories into an integrated system. When a server gets a query on a DIT, it may<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VI. Database System <br>Architecture<br>19. Distributed Databases<br>741<br>© The McGraw−Hill <br>Companies, 2001<br>19.9<br>Directory Systems<br>745<br>#include &lt;stdio.h&gt;<br>#include &lt;ldap.h&gt;<br>main() {<br>LDAP *ld;<br>LDAPMessage *res, *entry;<br>char *dn, *attr, *attrList[] = {“telephoneNumber”, NULL};<br>BerElement *ptr;<br>int vals, i;<br>ld = ldap open(“aura.research.bell-labs.com”, LDAP PORT);<br>ldap simple bind(ld, “avi”, “avi-passwd”) ;<br>ldap search s(ld, “o=Lucent, c=USA”, LDAP SCOPE SUBTREE, “cn=Korth”,<br>attrList, /*attrsonly*/ 0, &amp;res);<br>printf(“found %d entries”, ldap count entries(ld, res));<br>for (entry=ldap ﬁrst entry(ld, res); entry != NULL;<br>entry = ldap next entry(ld, entry)<br>{<br>dn = ldap get dn(ld, entry);<br>printf(“dn: %s”, dn);<br>ldap memfree(dn);<br>for (attr = ldap ﬁrst attribute(ld, entry, &amp;ptr);<br>attr ! NULL;<br>attr = ldap next attribute(ld, entry, ptr))<br>{<br>printf(“%s: ”, attr);<br>vals = ldap get values(ld, entry, attr);<br>for (i=0; vals[i] != NULL; i++)<br>printf(“%s, ”, vals[i]);<br>ldap value free(vals);<br>}<br>}<br>ldap msgfree(res);<br>ldap unbind(ld);<br>}<br>Figure 19.6<br>Example of LDAP code in C.<br>return a referral to the client, which then issues a query on the referenced DIT. Ac-<br>cess to the referenced DIT is transparent, proceeding without the user’s knowledge.<br>Alternatively, the server itself may issue the query to the referred DIT and return the<br>results along with locally computed results.<br>The hierarchical naming mechanism used by LDAP helps break up control of in-<br>formation across parts of an organization. The referral facility then helps integrate all<br>the directories in an organization into a single virtual directory.<br>Although it is not an LDAP requirement, organizations often choose to break up<br>information either by geography (for instance, an organization may maintain a direc-<br>tory for each site where the organization has a large presence) or by organizational<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VI. Database System <br>Architecture<br>19. Distributed Databases<br>742<br>© The McGraw−Hill <br>Companies, 2001<br>746<br>Chapter 19<br>Distributed Databases<br>structure (for instance, each organizational unit, such as department, maintains its<br>own directory).<br>Many LDAP implementations support master–slave and multimaster replication<br>of DITs, although replication is not part of the current LDAP version 3 standard. Work<br>on standardizing replication in LDAP is in progress.<br>19.10<br>Summary<br>• A distributed database system consists of a collection of sites, each of which<br>maintains a local database system. Each site is able to process local transac-<br>tions: those transactions that access data in only that single site. In addition, a<br>site may participate in the execution of global transactions; those transactions<br>that access data in several sites. The execution of global transactions requires<br>communication among the sites.<br>• Distributed databases may be homogeneous, where all sites have a common<br>schema and database system code, or heterogeneous, where the schemas and<br>system codes may differ.<br>• There are several issues involved in storing a relation in the distributed data-<br>base, including replication and fragmentation. It is essential that the system<br>minimize the degree to which a user needs to be aware of how a relation is<br>stored.<br>• A distributed system may suffer from the same types of failure that can afﬂict<br>a centralized system. There are, however, additional failures with which we<br>need to deal in a distributed environment, including the failure of a site, the<br>failure of a link, loss of a message, and network partition. Each of these prob-<br>lems needs to be considered in the design of a distributed recovery scheme.<br>• To ensure atomicity, all the sites in which a transaction T executed must agree<br>on the ﬁnal outcome of the execution. T either commits at all sites or aborts at<br>all sites. To ensure this property, the transaction coordinator of T must execute<br>a commit protocol. The most widely used commit protocol is the two-phase<br>commit protocol.<br>• The two-phase commit protocol may lead to blocking, the situation in which<br>the fate of a transaction cannot be determined until a failed site (the coordi-<br>nator) recovers. We can use the three-phase commit protocol to reduce the<br>probability of blocking.<br>• Persistent messaging provides an alternative model for handling distributed<br>transactions. The model breaks a single transaction into parts that are exe-<br>cuted at different databases. Persistent messages (which are guaranteed to be<br>delivered exactly once, regardless of failures), are sent to remote sites to re-<br>quest actions to be taken there. While persistent messaging avoids the block-<br>ing problem, application developers have to write code to handle various<br>types of failures.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VI. Database System <br>Architecture<br>19. Distributed Databases<br>743<br>© The McGraw−Hill <br>Companies, 2001<br>19.10<br>Summary<br>747<br>• The various concurrency-control schemes used in a centralized system can be<br>modiﬁed for use in a distributed environment.<br>  In the case of locking protocols, the only change that needs to be incor-<br>porated is in the way that the lo</span><br><br><span style="background-color: #FFC6FF;" title="Chunk 95 | Start: 1900190 | End: 1920190 | Tokens: 3058">ck manager is implemented. There are<br>a variety of different approaches here. One or more central coordinators<br>may be used. If, instead, a distributed lock-manager approach is taken,<br>replicated data must be treated specially.<br>  Protocols for handling replicated data include the primary-copy, majority,<br>biased, and quorum-consensus protocols. These have different tradeoffs<br>in terms of cost and ability to work in the presence of failures.<br>  In the case of timestamping and validation schemes, the only needed<br>change is to develop a mechanism for generating unique global time-<br>stamps.<br>  Many database systems support lazy replication, where updates are prop-<br>agated to replicas outside the scope of the transaction that performed the<br>update. Such facilities must be used with great care, since they may result<br>in nonserializable executions.<br>• Deadlock detection in a distributed lock-manager environment requires co-<br>operation between multiple sites, since there may be global deadlocks even<br>when there are no local deadlocks.<br>• To provide high availability, a distributed database must detect failures, recon-<br>ﬁgure itself so that computation may continue, and recover when a processor<br>or a link is repaired. The task is greatly complicated by the fact that it is hard<br>to distinguish between network partitions or site failures.<br>The majority protocol can be extended by using version numbers to per-<br>mit transaction processing to proceed even in the presence of failures. While<br>the protocol has a signiﬁcant overhead, it works regardless of the type of fail-<br>ure. Less-expensive protocols are available to deal with site failures, but they<br>assume network partitioning does not occur.<br>• Some of the distributed algorithms require the use of a coordinator. To provide<br>high availability, the system must maintain a backup copy that is ready to as-<br>sume responsibility if the coordinator fails. Another approach is to choose the<br>new coordinator after the coordinator has failed. The algorithms that deter-<br>mine which site should act as a coordinator are called election algorithms.<br>• Queries on a distributed database may need to access multiple sites. Several<br>optimization techniques are available to choose which sites need to be ac-<br>cessed. Based on fragmentation and replication, the techniques can use semi-<br>join techniques to reduce data transfer.<br>• Heterogeneous distributed databases allow sites to have their own schemas<br>and database system code. A multidatabase system provides an environment<br>in which new database applications can access data from a variety of pre-<br>existing databases located in various heterogeneous hardware and software<br>environments. The local database systems may employ different logical mod-<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VI. Database System <br>Architecture<br>19. Distributed Databases<br>744<br>© The McGraw−Hill <br>Companies, 2001<br>748<br>Chapter 19<br>Distributed Databases<br>els and data-deﬁnition and data-manipulation languages, and may differ in<br>their concurrency-control and transaction-management mechanisms. A mul-<br>tidatabase system creates the illusion of logical database integration, without<br>requiring physical database integration.<br>• Directory systems can be viewed as a specialized form of database, where<br>information is organized in a hierarchical fashion similar to the way ﬁles are<br>organized in a ﬁle system. Directories are accessed by standardized directory<br>access protocols such as LDAP.<br>Directories can be distributed across multiple sites to provide autonomy to<br>individual sites. Directories can contain referrals to other directories, which<br>help build an integrated view whereby a query is sent to a single directory,<br>and it is transparently executed at all relevant directories.<br>Review Terms<br>• Homogeneous distributed<br>database<br>• Heterogeneous distributed<br>database<br>• Data replication<br>• Primary copy<br>• Data fragmentation<br>  Horizontal fragmentation<br>  Vertical fragmentation<br>• Data transparency<br>  Fragmentation transparency<br>  Replication transparency<br>  Location transparency<br>• Name server<br>• Aliases<br>• Distributed transactions<br>  Local transactions<br>  Global transactions<br>• Transaction manager<br>• Transaction coordinator<br>• System failure modes<br>• Network partition<br>• Commit protocols<br>• Two-phase commit protocol (2PC)<br>  Ready state<br>  In-doubt transactions<br>  Blocking problem<br>• Three-phase commit protocol<br>(3PC)<br>• Persistent messaging<br>• Concurrency control<br>• Single lock-manager<br>• Distributed lock-manager<br>• Protocols for replicas<br>  Primary copy<br>  Majority protocol<br>  Biased protocol<br>  Quorum consensus protocol<br>• Timestamping<br>• Master–slave replication<br>• Multimaster (update-anywhere)<br>replication<br>• Transaction-consistent snapshot<br>• Lazy propagation<br>• Deadlock handling<br>  Local wait-for graph<br>  Global wait-for graph<br>  False cycles<br>• Availability<br>• Robustness<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VI. Database System <br>Architecture<br>19. Distributed Databases<br>745<br>© The McGraw−Hill <br>Companies, 2001<br>Exercises<br>749<br>  Majority based approach<br>  Read one, write all<br>  Read one, write all available<br>  Site reintegration<br>• Coordinator selection<br>• Backup coordinator<br>• Election algorithms<br>• Bully algorithm<br>• Distributed query processing<br>• Semijoin strategy<br>• Multidatabase system<br>• Autonomy<br>• Mediators<br>• Virtual database<br>• Directory systems<br>• LDAP: Lightweight directory<br>access protocol<br>  Distinguished name (DN)<br>  Relative distinguished names<br>RDNs<br>  Directory information<br>tree (DIT)<br>• Distributed directory trees<br>• DIT sufﬁx<br>• Referral<br>Exercises<br>19.1 Discuss the relative advantages of centralized and distributed databases.<br>19.2 Explain how the following differ: fragmentation transparency, replication<br>transparency, and location transparency.<br>19.3 How might a distributed database designed for a local-area network differ<br>from one designed for a wide-area network?<br>19.4 When is it useful to have replication or fragmentation of data? Explain your<br>answer.<br>19.5 Explain the notions of transparency and autonomy. Why are these notions de-<br>sirable from a human-factors standpoint?<br>19.6 To build a highly available distributed system, you must know what kinds of<br>failures can occur.<br>a. List possible types of failure in a distributed system.<br>b. Which items in your list from part a are also applicable to a centralized<br>system?<br>19.7 Consider a failure that occurs during 2PC for a transaction. For each possible<br>failure that you listed in Exercise 19.6a, explain how 2PC ensures transaction<br>atomicity despite the failure.<br>19.8 Consider a distributed system with two sites, A and B. Can site A distinguish<br>among the following?<br>• B goes down.<br>• The link between A and B goes down.<br>• B is extremely overloaded and response time is 100 times longer than nor-<br>mal.<br>What implications does your answer have for recovery in distributed systems?<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VI. Database System <br>Architecture<br>19. Distributed Databases<br>746<br>© The McGraw−Hill <br>Companies, 2001<br>750<br>Chapter 19<br>Distributed Databases<br>19.9 The persistent messaging scheme described in this chapter depends on times-<br>tamps combined with discarding of received messages if they are too old. Sug-<br>gest an alternative scheme based on sequence numbers instead of timestamps.<br>19.10 Give an example where the read one, write all available approach leads to an<br>erroneous state.<br>19.11 If we apply a distributed version of the multiple-granularity protocol of Chap-<br>ter 16 to a distributed database, the site responsible for the root of the DAG may<br>become a bottleneck. Suppose we modify that protocol as follows:<br>• Only intention-mode locks are allowed on the root.<br>• All transactions are given all possible intention-mode locks on the root<br>automatically.<br>Show that these modiﬁcations alleviate this problem without allowing any<br>nonserializable schedules.<br>19.12 Explain the difference between data replication in a distributed system and the<br>maintenance of a remote backup site.<br>19.13 Give an example where lazy replication can lead to an inconsistent database<br>state even when updates get an exclusive lock on the primary (master) copy.<br>19.14 Study and summarize the facilities that the database system you are using pro-<br>vides for dealing with inconsistent states that can be reached with lazy propa-<br>gation of updates.<br>19.15 Discuss the advantages and disadvantages of the two methods that we pre-<br>sented in Section 19.5.2 for generating globally unique timestamps.<br>19.16 Consider the following deadlock-detection algorithm. When transaction Ti, at<br>site S1, requests a resource from Tj, at site S3, a request message with time-<br>stamp n is sent. The edge (Ti, Tj, n) is inserted in the local wait-for of S1. The<br>edge (Ti, Tj, n) is inserted in the local wait-for graph of S3 only if Tj has re-<br>ceived the request message and cannot immediately grant the requested re-<br>source. A request from Ti to Tj in the same site is handled in the usual manner;<br>no timestamps are associated with the edge (Ti, Tj). A central coordinator in-<br>vokes the detection algorithm by sending an initiating message to each site in<br>the system.<br>On receiving this message, a site sends its local wait-for graph to the coordi-<br>nator. Note that such a graph contains all the local information that the site has<br>about the state of the real graph. The wait-for graph reﬂects an instantaneous<br>state of the site, but it is not synchronized with respect to any other site.<br>When the controller has received a reply from each site, it constructs a graph<br>as follows:<br>• The graph contains a vertex for every transaction in the system.<br>• The graph has an edge (Ti, Tj) if and only if<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VI. Database System <br>Architecture<br>19. Distributed Databases<br>747<br>© The McGraw−Hill <br>Companies, 2001<br>Exercises<br>751<br>  There is an edge (Ti, Tj) in one of the wait-for graphs.<br>  An edge (Ti, Tj, n) (for some n) appears in more than one wait-for<br>graph.<br>Show that, if there is a cycle in the constructed graph, then the system is in a<br>deadlock state, and that, if there is no cycle in the constructed graph, then the<br>system was not in a deadlock state when the execution of the algorithm began.<br>19.17 Consider a relation that is fragmented horizontally by plant-number:<br>employee (name, address, salary, plant-number)<br>Assume that each fragment has two replicas: one stored at the New York site<br>and one stored locally at the plant site. Describe a good processing strategy for<br>the following queries entered at the San Jose site.<br>a. Find all employees at the Boca plant.<br>b. Find the average salary of all employees.<br>c. Find the highest-paid employee at each of the following sites: Toronto, Ed-<br>monton, Vancouver, Montreal.<br>d. Find the lowest-paid employee in the company.<br>19.18 Consider the relations<br>employee (name, address, salary, plant-number)<br>machine (machine-number, type, plant-number)<br>Assume that the employee relation is fragmented horizontally by plant-number,<br>and that each fragment is stored locally at its corresponding plant site. Assume<br>that the machine relation is stored in its entirety at the Armonk site. Describe a<br>good strategy for processing each of the following queries.<br>a. Find all employees at the plant that contains machine number 1130.<br>b. Find all employees at plants that contain machines whose type is “milling<br>machine.”<br>c. Find all machines at the Almaden plant.<br>d. Find employee<br> machine.<br>19.19 For each of the strategies of Exercise 19.18, state how your choice of a strategy<br>depends on:<br>a. The site at which the query was entered<br>b. The site at which the result is desired<br>19.20 Compute r<br>n s for the relations of Figure 19.7.<br>19.21 Is ri<br>n rj necessarily equal to rj<br>n ri? Under what conditions does ri<br>n<br>rj = rj<br>n ri hold?<br>19.22 Given that the LDAP functionality can be implemented on top of a database<br>system, what is the need for the LDAP standard?<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VI. Database System <br>Architecture<br>19. Distributed Databases<br>748<br>© The McGraw−Hill <br>Companies, 2001<br>752<br>Chapter 19<br>Distributed Databases<br>r<br>A<br>B<br>C<br>s<br>C<br>D<br>E<br>1<br>2<br>3<br>3<br>4<br>5<br>4<br>5<br>6<br>3<br>6<br>8<br>1<br>2<br>4<br>2<br>3<br>2<br>5<br>3<br>2<br>1<br>4<br>1<br>8<br>9<br>7<br>1<br>2<br>3<br>Figure 19.7<br>Relations for Exercise 19.20.<br>19.23 Describe how LDAP can be used to provide multiple hierarchical views of data,<br>without replicating the base level data.<br>Bibliographical Notes<br>Textbook discussions of distributed databases are offered by Ozsu and Valduriez<br>[1999] and Ceri and Pelagatti [1984]. Computer networks are discussed in Tanen-<br>baum [1996] and Halsall [1992]. Rothnie et al. [1977] was an early survey on dis-<br>tributed database systems. Breitbart et al. [1999b] presents an overview of distributed<br>databases.<br>The implementation of the transaction concept in a distributed database are pre-<br>sented by Gray [1981], Traiger et al. [1982], Spector and Schwarz [1983], and Eppinger<br>et al. [1991]. The 2PC protocol was developed by Lampson and Sturgis [1976] and<br>Gray [1978]. The three-phase commit protocol is from Skeen [1981]. Mohan and Lind-<br>say [1983] discuss two modiﬁed versions of 2PC, called presume commit and presume<br>abort, that reduce the overhead of 2PC by deﬁning default assumptions regarding the<br>fate of transactions.<br>The bully algorithm in Section 19.6.5 is from Garcia-Molina [1982]. Distributed<br>clock synchronization is discussed in Lamport [1978]. Distributed concurrency con-<br>trol is covered by Rosenkrantz et al. [1978], Bernstein et al. [1978], Bernstein et al.<br>[1980b], Menasce et al. [1980], Bernstein and Goodman [1980], Bernstein and Good-<br>man [1981a], Bernstein and Goodman [1982], and Garcia-Molina and Wiederhold<br>[1982].<br>The transaction manager of R* is described in Mohan et al. [1986]. Concurrency<br>control for replicated data that is based on the concept of voting is presented by Gif-<br>ford [1979] and Thomas [1979]. Validation techniques for distributed concurrency-<br>control schemes are described by Schlageter [1981], Ceri and Owicki [1983], and<br>Bassiouni [1988]. Discussions of semantic-based transaction-management techniques<br>are offered by Garcia-Molina [1983], Kumar and Stonebraker [1988] and Badrinath<br>and Ramamritham [1992].<br>Attar et al. [1984] discusses the use of transactions in distributed recovery in data-<br>base systems with replicated data. A survey of techniques for recovery in distributed<br>database systems is presented by Kohler [1981].<br>Recently, the problem of concurrent updates to replicated data has re-emerged<br>as an important research issue in the context of data warehouses. Problems in this<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VI. Database System <br>Architecture<br>19. Distributed Databases<br>749<br>© The McGraw−Hill <br>Companies, 2001<br>Bibliographical Notes<br>753<br>environment are discussed in Gray et al. [1996]. Anderson et al. [1998] discusses is-<br>sues concerning lazy replication and consistency. Breitbart et al. [1999a] describe lazy<br>update protocols for handling replication. The user manuals of various database sys-<br>tems provide details of how they handle replication and consistency.<br>Persistent messaging in Oracle is described in Gawlick [1998] while Huang and<br>Garcia-Molina [2001] addresses exactly-once semantics in a replicated messaging sys-<br>tem.<br>Distributed deadlock-detection algorithms are presented by Rosenkrantz et al.<br>[1978], Menasce and Muntz [1979], Gligor and Shattuck [1980], Chandy and Misra<br>[1982], Chandy et al. [1983], and Obermarck [1982]. Knapp [1987] surveys the dis-<br>tributed deadlock-detection literature, Exercise 19.16 is from Stuart et al. [1984].<br>Distributed query processing is discussed in Wong [1977], Epstein et al. [1978],<br>Hevner and Yao [1979], Epstein and Stonebraker [1980], Apers et al. [1983], Ceri and<br>Pelagatti [1983], and Wong [1983]. Selinger and Adiba [1980] and Daniels et al. [1982]<br>discuss the approach to distributed query processing taken by R* (a distributed ver-<br>sion of System R). Mackert and Lohman [1986] provides a performance evaluation<br>of query-processing algorithms in R*. The performance results also serve to validate<br>the cost model used in the R* query optimizer. Theoretical results concerning semi-<br>joins are presented by Bernstein and Chiu [1981], Chiu and Ho [1980], Bernstein and<br>Goodman [1981b], and Kambayashi et al. [1982].<br>Dynamic query optimization in multidatabases is addressed by Ozcan et al. [1997].<br>Adali et al. [1996] and Papakonstantinou et al. [1996] describe query optimization<br>issues in mediator systems.<br>Weltman and Dahbura [2000] and Howes et al. [1999] provide textbook coverage<br>of LDAP. Kapitskaia et al. [2000] describes issues in caching LDAP directory data.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VI. Database System <br>Architecture<br>20. Parallel Databases<br>750<br>© The McGraw−Hill <br>Companies, 2001<br>C<br>H<br>A<br>P<br>T<br>E<br>R<br>2<br>0<br>Parallel Databases<br>In this chapter, we discuss fundamental algorithms for parallel database systems that<br>are based on the relational data model. In particular, we focus on the placement of<br>data on multiple disks and the parallel evaluation of relational operations, both of<br>which have been instrumental in the success of parallel databases.<br>20.1<br>Introduction<br>Fifteen years ago, parallel database systems had been nearly written off, even by<br>some of their staunchest advocates. Today, they are successfully marketed by practi-<br>cally every database system vendor. Several trends fueled this transition:<br>• The transaction requirements of organizations have grown with increasing<br>use of computers. Moreover, the growth of the World Wide Web has created<br>many sites with millions of viewers, and the increasing amounts of data col-<br>lected from these viewers has produced extremely large databases at many<br>companies.<br>• Organizations are using these increasingly large volumes of data—such as<br>data about what items people buy, what Web links users clicked on, or when<br>people make telephone calls—to plan their activities and pricing. Queries<br>used for such purposes are called decision-support queries, and the data re-<br>quirements for such queries may run into terabytes. Single-processor systems<br>are not capable of handling such large volumes of data at the required rates.<br>• The set-oriented nature of database queries naturally lends itself to paral-<br>lelization. A number of commercial and research systems have demonstrated<br>the power and scalability of parallel query processing.<br>• As microprocessors have become cheap, parallel machines have become com-<br>mon and relatively inexpensive.<br>755<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VI. Database System <br>Architecture<br>20. Parallel Databases<br>751<br>© The McGraw−Hill <br>Companies, 2001<br>756<br>Chapter 20<br>Parallel Databases<br>As we discussed in Chapter 18, parallelism is used to provide speedup, where<br>queries are executed faster because more resources, such as processors and disks, are<br>provided. Parallelism is also used to provide scaleup, where increasing workloads<br>are handled without increased response time, via an increase in the degree of paral-<br>lelism.<br>We outlined in Chapter 18 the different architectures for parallel database systems:<br>shared-memory, shared-disk, shared-nothing, and hierarchical architectures. Brieﬂy,<br>in shared-memory architectures, all processors share a common memory and disks;<br>in shared-disk architectures, processors have independent memories, but share disks;<br>in shared-nothing architectures, processors share neither memory nor disks; and hi-<br>erarchical architectures have nodes that share neither memory nor disks with each<br>other, but internally each node has a shared-memory or a shared-disk architecture.<br>20.2<br>I/O Parallelism<br>In it simplest form, I/O parallelism refers to reducing the time required to retrieve<br>relations from disk by partitioning the relations on multiple disks. The most common<br>form of data partitioning in a parallel database environment is horizontal partitioning.<br>In horizontal partitioning, the tuples of a relation are divided (or declustered) among<br>ma</span><br><br><span style="background-color: #FFADAD;" title="Chunk 96 | Start: 1920192 | End: 1940192 | Tokens: 3255">ny disks, so that each tuple resides on one disk. Several partitioning strategies<br>have been proposed.<br>20.2.1<br>Partitioning Techniques<br>We present three basic data-partitioning strategies. Assume that there are n disks,<br>D0, D1, . . . , Dn−1, across which the data are to be partitioned.<br>• Round-robin. This strategy scans the relation in any order and sends the ith<br>tuple to disk number Di mod n. The round-robin scheme ensures an even dis-<br>tribution of tuples across disks; that is, each disk has approximately the same<br>number of tuples as the others.<br>• Hash partitioning. This declustering strategy designates one or more attrib-<br>utes from the given relation’s schema as the partitioning attributes. A hash<br>function is chosen whose range is {0, 1, . . . , n −1}. Each tuple of the original<br>relation is hashed on the partitioning attributes. If the hash function returns i,<br>then the tuple is placed on disk Di.<br>• Range partitioning. This strategy distributes contiguous attribute-value<br>ranges to each disk. It chooses a partitioning attribute, A, as a partitioning<br>vector. The relation is partitioned as follows. Let [v0, v1, . . . , vn−2] denote the<br>partitioning vector, such that, if i &lt; j, then vi &lt; vj. Consider a tuple t such<br>that t[A] = x. If x &lt; v0, then t goes on disk D0. If x ≥vn−2, then t goes on disk<br>Dn−1. If vi ≤x &lt; vi+1, then t goes on disk Di+1.<br>For example, range partitioning with three disks numbered 0, 1, and 2 may<br>assign tuples with values less than 5 to disk 0, values between 5 and 40 to disk<br>1, and values greater than 40 to disk 2.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VI. Database System <br>Architecture<br>20. Parallel Databases<br>752<br>© The McGraw−Hill <br>Companies, 2001<br>20.2<br>I/O Parallelism<br>757<br>20.2.2<br>Comparison of Partitioning Techniques<br>Once a relation has been partitioned among several disks, we can retrieve it in paral-<br>lel, using all the disks. Similarly, when a relation is being partitioned, it can be written<br>to multiple disks in parallel. Thus, the transfer rates for reading or writing an entire<br>relation are much faster with I/O parallelism than without it. However, reading an<br>entire relation, or scanning a relation, is only one kind of access to data. Access to data<br>can be classiﬁed as follows:<br>1. Scanning the entire relation<br>2. Locating a tuple associatively (for example, employee-name = “Campbell”);<br>these queries, called point queries, seek tuples that have a speciﬁed value<br>for a speciﬁc attribute<br>3. Locating all tuples for which the value of a given attribute lies within a spec-<br>iﬁed range (for example, 10000 &lt; salary &lt; 20000); these queries are called<br>range queries.<br>The different partitioning techniques support these types of access at different levels<br>of efﬁciency:<br>• Round-robin. The scheme is ideally suited for applications that wish to read<br>the entire relation sequentially for each query. With this scheme, both point<br>queries and range queries are complicated to process, since each of the n disks<br>must be used for the search.<br>• Hash partitioning. This scheme is best suited for point queries based on the<br>partitioning attribute. For example, if a relation is partitioned on the telephone-<br>number attribute, then we can answer the query “Find the record of the em-<br>ployee with telephone-number = 555-3333” by applying the partitioning hash<br>function to 555-3333 and then searching that disk. Directing a query to a sin-<br>gle disk saves the startup cost of initiating a query on multiple disks, and<br>leaves the other disks free to process other queries.<br>Hash partitioning is also useful for sequential scans of the entire relation.<br>If the hash function is a good randomizing function, and the partitioning at-<br>tributes form a key of the relation, then the number of tuples in each of the<br>disks is approximately the same, without much variance. Hence, the time<br>taken to scan the relation is approximately 1/n of the time required to scan<br>the relation in a single disk system.<br>The scheme, however, is not well suited for point queries on nonparti-<br>tioning attributes. Hash-based partitioning is also not well suited for answer-<br>ing range queries, since, typically, hash functions do not preserve proximity<br>within a range. Therefore, all the disks need to be scanned for range queries<br>to be answered.<br>• Range partitioning. This scheme is well suited for point and range queries on<br>the partitioning attribute. For point queries, we can consult the partitioning<br>vector to locate the disk where the tuple resides. For range queries, we consult<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VI. Database System <br>Architecture<br>20. Parallel Databases<br>753<br>© The McGraw−Hill <br>Companies, 2001<br>758<br>Chapter 20<br>Parallel Databases<br>the partitioning vector to ﬁnd the range of disks on which the tuples may<br>reside. In both cases, the search narrows to exactly those disks that might have<br>any tuples of interest.<br>An advantage of this feature is that, if there are only a few tuples in the<br>queried range, then the query is typically sent to one disk, as opposed to<br>all the disks. Since other disks can be used to answer other queries, range<br>partitioning results in higher throughput while maintaining good response<br>time. On the other hand, if there are many tuples in the queried range (as<br>there are when the queried range is a larger fraction of the domain of the re-<br>lation), many tuples have to be retrieved from a few disks, resulting in an I/O<br>bottleneck (hot spot) at those disks. In this example of execution skew, all pro-<br>cessing occurs in one—or only a few—partitions. In contrast, hash partition-<br>ing and round-robin partitioning would engage all the disks for such queries,<br>giving a faster response time for approximately the same throughput.<br>The type of partitioning also affects other relational operations, such as joins, as<br>we shall see in Section 20.5. Thus, the choice of partitioning technique also depends<br>on the operations that need to be executed. In general, hash partitioning or range<br>partitioning are preferred to round-robin partitioning.<br>In a system with many disks, the number of disks across which to partition a rela-<br>tion can be chosen in this way: If a relation contains only a few tuples that will ﬁt into<br>a single disk block, then it is better to assign the relation to a single disk. Large rela-<br>tions are preferably partitioned across all the available disks. If a relation consists of<br>m disk blocks and there are n disks available in the system, then the relation should<br>be allocated min(m, n) disks.<br>20.2.3<br>Handling of Skew<br>When a relation is partitioned (by a technique other than round-robin), there may be<br>a skew in the distribution of tuples, with a high percentage of tuples placed in some<br>partitions and fewer tuples in other partitions. The ways that skew may appear are<br>classiﬁed as:<br>• Attribute-value skew<br>• Partition skew<br>Attribute-value skew refers to the fact that some values appear in the partitioning<br>attributes of many tuples. All the tuples with the same value for the partitioning<br>attribute end up in the same partition, resulting in skew. Partition skew refers to<br>the fact that there may be load imbalance in the partitioning, even when there is no<br>attribute skew.<br>Attribute-value skew can result in skewed partitioning regardless of whether range<br>partitioning or hash partitioning is used. If the partition vector is not chosen carefully,<br>range partitioning may result in partition skew. Partition skew is less likely with hash<br>partitioning, if a good hash function is chosen.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VI. Database System <br>Architecture<br>20. Parallel Databases<br>754<br>© The McGraw−Hill <br>Companies, 2001<br>20.2<br>I/O Parallelism<br>759<br>As Section 18.3.1 noted, even a small skew can result in a signiﬁcant decrease in<br>performance. Skew becomes an increasing problem with a higher degree of paral-<br>lelism. For example, if a relation of 1000 tuples is divided into 10 parts, and the di-<br>vision is skewed, then there may be some partitions of size less than 100 and some<br>partitions of size more than 100; if even one partition happens to be of size 200, the<br>speedup that we would obtain by accessing the partitions in parallel is only 5, instead<br>of the 10 for which we would have hoped. If the same relation has to be partitioned<br>into 100 parts, a partition will have 10 tuples on an average. If even one partition has<br>40 tuples (which is possible, given the large number of partitions) the speedup that<br>we would obtain by accessing them in parallel would be 25, rather than 100. Thus,<br>we see that the loss of speedup due to skew increases with parallelism.<br>A balanced range-partitioning vector can be constructed by sorting: The relation<br>is ﬁrst sorted on the partitioning attributes. The relation is then scanned in sorted<br>order. After every 1/n of the relation has been read, the value of the partitioning<br>attribute of the next tuple is added to the partition vector. Here, n denotes the number<br>of partitions to be constructed. In case there are many tuples with the same value<br>for the partitioning attribute, the technique can still result in some skew. The main<br>disadvantage of this method is the extra I/O overhead incurred in doing the initial<br>sort.<br>The I/O overhead for constructing balanced range-partition vectors can be re-<br>duced by constructing and storing a frequency table, or histogram, of the attribute<br>values for each attribute of each relation. Figure 20.1 shows an example of a his-<br>togram for an integer-valued attribute that takes values in the range 1 to 25. A his-<br>togram takes up only a little space, so histograms on several different attributes<br>can be stored in the system catalog. It is straightforward to construct a balanced<br>range-partitioning function given a histogram on the partitioning attributes. If the<br>histogram is not stored, it can be computed approximately by sampling the relation,<br>using only tuples from a randomly chosen subset of the disk blocks of the relation.<br>value<br>frequency<br>50<br>40<br>30<br>20<br>10<br>1–5<br>6–10<br>11–15 16–20 21–25 <br>Figure 20.1<br>Example of histogram.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VI. Database System <br>Architecture<br>20. Parallel Databases<br>755<br>© The McGraw−Hill <br>Companies, 2001<br>760<br>Chapter 20<br>Parallel Databases<br>Another approach to minimizing the effect of skew, particularly with range par-<br>titioning, is to use virtual processors. In the virtual processor approach, we pretend<br>there are several times as many virtual processors as the number of real processors.<br>Any of the partitioning techniques and query evaluation techniques that we study<br>later in this chapter can be used, but they map tuples and work to virtual processors<br>instead of to real processors. Virtual processors, in turn, are mapped to real proces-<br>sors, usually by round-robin partitioning.<br>The idea is that even if one range had many more tuples than the others because<br>of skew, these tuples would get split across multiple virtual processor ranges. Round<br>robin allocation of virtual processors to real processors would distribute the extra<br>work among multiple real processors, so that one processor does not have to bear all<br>the burden.<br>20.3<br>Interquery Parallelism<br>In interquery parallelism, different queries or transactions execute in parallel with<br>one another. Transaction throughput can be increased by this form of parallelism.<br>However, the response times of individual transactions are no faster than they would<br>be if the transactions were run in isolation. Thus, the primary use of interquery par-<br>allelism is to scaleup a transaction-processing system to support a larger number of<br>transactions per second.<br>Interquery parallelism is the easiest form of parallelism to support in a database<br>system—particularly in a shared-memory parallel system. Database systems designed<br>for single-processor systems can be used with few or no changes on a shared-memory<br>parallel architecture, since even sequential database systems support concurrent pro-<br>cessing. Transactions that would have operated in a time-shared concurrent manner<br>on a sequential machine operate in parallel in the shared-memory parallel architec-<br>ture.<br>Supporting interquery parallelism is more complicated in a shared-disk or shared-<br>nothing architecture. Processors have to perform some tasks, such as locking and<br>logging, in a coordinated fashion, and that requires that they pass messages to each<br>other. A parallel database system must also ensure that two processors do not update<br>the same data independently at the same time. Further, when a processor accesses<br>or updates data, the database system must ensure that the processor has the latest<br>version of the data in its buffer pool. The problem of ensuring that the version is the<br>latest is known as the cache-coherency problem.<br>Various protocols are available to guarantee cache coherency; often, cache-coheren-<br>cy protocols are integrated with concurrency-control protocols so that their overhead<br>is reduced. One such protocol for a shared-disk system is this:<br>1. Before any read or write access to a page, a transaction locks the page in shared<br>or exclusive mode, as appropriate. Immediately after the transaction obtains<br>either a shared or exclusive lock on a page, it also reads the most recent copy<br>of the page from the shared disk.<br>2. Before a transaction releases an exclusive lock on a page, it ﬂushes the page to<br>the shared disk; then, it releases the lock.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VI. Database System <br>Architecture<br>20. Parallel Databases<br>756<br>© The McGraw−Hill <br>Companies, 2001<br>20.4<br>Intraquery Parallelism<br>761<br>This protocol ensures that, when a transaction sets a shared or exclusive lock on a<br>page, it gets the correct copy of the page.<br>More complex protocols avoid the repeated reading and writing to disk required<br>by the preceding protocol. Such protocols do not write pages to disk when exclusive<br>locks are released. When a shared or exclusive lock is obtained, if the most recent<br>version of a page is in the buffer pool of some processor, the page is obtained from<br>there. The protocols have to be designed to handle concurrent requests. The shared-<br>disk protocols can be extended to shared-nothing architectures by this scheme: Each<br>page has a home processor Pi, and is stored on disk Di. When other processors want<br>to read or write the page, they send requests to the home processor Pi of the page,<br>since they cannot directly communicate with the disk. The other actions are the same<br>as in the shared-disk protocols.<br>The Oracle 8 and Oracle Rdb systems are examples of shared-disk parallel database<br>systems that support interquery parallelism.<br>20.4<br>Intraquery Parallelism<br>Intraquery parallelism refers to the execution of a single query in parallel on multi-<br>ple processors and disks. Using intraquery parallelism is important for speeding up<br>long-running queries. Interquery parallelism does not help in this task, since each<br>query is run sequentially.<br>To illustrate the parallel evaluation of a query, consider a query that requires a<br>relation to be sorted. Suppose that the relation has been partitioned across multiple<br>disks by range partitioning on some attribute, and the sort is requested on the parti-<br>tioning attribute. The sort operation can be implemented by sorting each partition in<br>parallel, then concatenating the sorted partitions to get the ﬁnal sorted relation.<br>Thus, we can parallelize a query by parallelizing individual operations. There is<br>another source of parallelism in evaluating a query: The operator tree for a query can<br>contain multiple operations. We can parallelize the evaluation of the operator tree by<br>evaluating in parallel some of the operations that do not depend on one another. Fur-<br>ther, as Chapter 13 mentions, we may be able to pipeline the output of one operation<br>to another operation. The two operations can be executed in parallel on separate pro-<br>cessors, one generating output that is consumed by the other, even as it is generated.<br>In summary, the execution of a single query can be parallelized in two ways:<br>• Intraoperation parallelism. We can speed up processing of a query by paral-<br>lelizing the execution of each individual operation, such as sort, select, project,<br>and join. We consider intraoperation parallelism in Section 20.5.<br>• Interoperation parallelism. We can speed up processing of a query by execut-<br>ing in parallel the different operations in a query expression. We consider this<br>form of parallelism in Section 20.6.<br>The two forms of parallelism are complementary, and can be used simultaneously<br>on a query. Since the number of operations in a typical query is small, compared to<br>the number of tuples processed by each operation, the ﬁrst form of parallelism can<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VI. Database System <br>Architecture<br>20. Parallel Databases<br>757<br>© The McGraw−Hill <br>Companies, 2001<br>762<br>Chapter 20<br>Parallel Databases<br>scale better with increasing parallelism. However, with the relatively small number of<br>processors in typical parallel systems today, both forms of parallelism are important.<br>In the following discussion of parallelization of queries, we assume that the queries<br>are read only. The choice of algorithms for parallelizing query evaluation depends<br>on the machine architecture. Rather than presenting algorithms for each architecture<br>separately, we use a shared-nothing architecture model in our description. Thus, we<br>explicitly describe when data have to be transferred from one processor to another.<br>We can simulate this model easily by using the other architectures, since transfer<br>of data can be done via shared memory in a shared-memory architecture, and via<br>shared disks in a shared-disk architecture. Hence, algorithms for shared-nothing ar-<br>chitectures can be used on the other architectures too. We mention occasionally how<br>the algorithms can be further optimized for shared-memory or shared-disk systems.<br>To simplify the presentation of the algorithms, assume that there are n processors,<br>P0, P1, . . . , Pn−1, and n disks D0, D1, . . . , Dn−1, where disk Di is associated with pro-<br>cessor Pi. A real system may have multiple disks per processor. It is not hard to<br>extend the algorithms to allow multiple disks per processor: We simply allow Di to<br>be a set of disks. However, for simplicity, we assume here that Di is a single disk.<br>20.5<br>Intraoperation Parallelism<br>Since relational operations work on relations containing large sets of tuples, we can<br>parallelize the operations by executing them in parallel on different subsets of the re-<br>lations. Since the number of tuples in a relation can be large, the degree of parallelism<br>is potentially enormous. Thus, intraoperation parallelism is natural in a database sys-<br>tem. We shall study parallel versions of some common relational operations in Sec-<br>tions 20.5.1 through 20.5.3.<br>20.5.1<br>Parallel Sort<br>Suppose that we wish to sort a relation that resides on n disks D0, D1, . . . , Dn−1. If the<br>relation has been range partitioned on the attributes on which it is to be sorted, then,<br>as noted in Section 20.2.2, we can sort each partition separately, and can concatenate<br>the results to get the full sorted relation. Since the tuples are partitioned on n disks,<br>the time required for reading the entire relation is reduced by the parallel access.<br>If the relation has been partitioned in any other way, we can sort it in one of two<br>ways:<br>1. We can range partition it on the sort attributes, and then sort each partition<br>separately.<br>2. We can use a parallel version of the external sort–merge algorithm.<br>20.5.1.1<br>Range-Partitioning Sort<br>Range-partitioning sort works in two steps: ﬁrst range partitioning the relation, then<br>sorting each partition separately. When we sort by range partitioning the relation,<br>it is not necessary to range-partition the relation on the same set of processors or<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VI. Database Sys</span><br><br><span style="background-color: #FFD6A5;" title="Chunk 97 | Start: 1940194 | End: 1960194 | Tokens: 3400">tem <br>Architecture<br>20. Parallel Databases<br>758<br>© The McGraw−Hill <br>Companies, 2001<br>20.5<br>Intraoperation Parallelism<br>763<br>disks as those on which that relation is stored. Suppose that we choose processors<br>P0, P1, . . . , Pm, where m &lt; n to sort the relation. There are two steps involved in this<br>operation:<br>1. Redistribute the tuples in the relation, using a range-partition strategy, so that<br>all tuples that lie within the ith range are sent to processor Pi, which stores<br>the relation temporarily on disk Di.<br>To implement range partitioning, in parallel every processor reads the tu-<br>ples from its disk and sends the tuples to their destination processor. Each<br>processor P0, P1, . . . , Pm also receives tuples belonging to its partition, and<br>stores them locally. This step requires disk I/O and communication overhead.<br>2. Each of the processors sorts its partition of the relation locally, without inter-<br>action with the other processors. Each processor executes the same operation<br>—namely, sorting—on a different data set. (Execution of the same operation<br>in parallel on different sets of data is called data parallelism.)<br>The ﬁnal merge operation is trivial, because the range partitioning in the<br>ﬁrst phase ensures that, for 1 ≤i &lt; j ≤m, the key values in processor Pi are<br>all less than the key values in Pj.<br>We must do range partitioning with a good range-partition vector, so that each<br>partition will have approximately the same number of tuples. Virtual processor par-<br>titioning can also be used to reduce skew.<br>20.5.1.2<br>Parallel External Sort–Merge<br>Parallel external sort–merge is an alternative to range partitioning. Suppose that a<br>relation has already been partitioned among disks D0, D1, . . . , Dn−1 (it does not mat-<br>ter how the relation has been partitioned). Parallel external sort–merge then works<br>this way:<br>1. Each processor Pi locally sorts the data on disk Di.<br>2. The system then merges the sorted runs on each processor to get the ﬁnal<br>sorted output.<br>The merging of the sorted runs in step 2 can be parallelized by this sequence of<br>actions:<br>1. The system range-partitions the sorted partitions at each processor Pi (all by<br>the same partition vector) across the processors P0, P1, . . . , Pm−1. It sends the<br>tuples in sorted order, so that each processor receives the tuples in sorted<br>streams.<br>2. Each processor Pi performs a merge on the streams as they are received, to get<br>a single sorted run.<br>3. The system concatenates the sorted runs on processors P0, P1, . . . , Pm−1 to get<br>the ﬁnal result.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VI. Database System <br>Architecture<br>20. Parallel Databases<br>759<br>© The McGraw−Hill <br>Companies, 2001<br>764<br>Chapter 20<br>Parallel Databases<br>As described, this sequence of actions results in an interesting form of execution<br>skew, since at ﬁrst every processor sends all blocks of partition 0 to P0, then every<br>processor sends all blocks of partition 1 to P1, and so on. Thus, while sending hap-<br>pens in parallel, receiving tuples becomes sequential: ﬁrst only P0 receives tuples,<br>then only P1 receives tuples, and so on. To avoid this problem, each processor repeat-<br>edly sends a block of data to each partition. In other words, each processor sends the<br>ﬁrst block of every partition, then sends the second block of every partition, and so<br>on. As a result, all processors receive data in parallel.<br>Some machines, such as the Teradata DBC series machines, use specialized hard-<br>ware to perform merging. The Y-net interconnection network in the Teradata DBC<br>machines can merge output from multiple processors to give a single sorted output.<br>20.5.2<br>Parallel Join<br>The join operation requires that the system test pairs of tuples to see whether they<br>satisfy the join condition; if they do, the system adds the pair to the join output.<br>Parallel join algorithms attempt to split the pairs to be tested over several processors.<br>Each processor then computes part of the join locally. Then, the system collects the<br>results from each processor to produce the ﬁnal result.<br>20.5.2.1<br>Partitioned Join<br>For certain kinds of joins, such as equi-joins and natural joins, it is possible to partition<br>the two input relations across the processors, and to compute the join locally at each<br>processor. Suppose that we are using n processors, and that the relations to be joined<br>are r and s. Partitioned join then works this way: The system partitions the relations<br>r and s each into n partitions, denoted r0, r1, . . . , rn−1 and s0, s1, . . . , sn−1. The system<br>sends partitions ri and si to processor Pi, where their join is computed locally.<br>The partitioned join technique works correctly only if the join is an equi-join (for<br>example, r<br>r.A=s.B s) and if we partition r and s by the same partitioning function<br>on their join attributes. The idea of partitioning is exactly the same as that behind the<br>partitioning step of hash–join. In a partitioned join, however, there are two different<br>ways of partitioning r and s:<br>• Range partitioning on the join attributes<br>• Hash partitioning on the join attributes<br>In either case, the same partitioning function must be used for both relations. For<br>range partitioning, the same partition vector must be used for both relations. For<br>hash partitioning, the same hash function must be used on both relations. Figure 20.2<br>depicts the partitioning in a partitioned parallel join.<br>Once the relations are partitioned, we can use any join technique locally at each<br>processor Pi to compute the join of ri and si. For example, hash–join, merge–join, or<br>nested-loop join could be used. Thus, we can use partitioning to parallelize any join<br>technique.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VI. Database System <br>Architecture<br>20. Parallel Databases<br>760<br>© The McGraw−Hill <br>Companies, 2001<br>20.5<br>Intraoperation Parallelism<br>765<br>P0<br>r0<br>P1<br>r1<br>s<br>r<br>P2<br>r2<br>P3<br>r3<br>s0<br>s1<br>s2<br>s3<br>...<br>...<br>...<br>.<br>.<br>.<br>.<br>.<br>.<br>.<br>.<br>Figure 20.2<br>Partitioned parallel join.<br>If one or both of the relations r and s are already partitioned on the join attributes<br>(by either hash partitioning or range partitioning), the work needed for partitioning<br>is reduced greatly. If the relations are not partitioned, or are partitioned on attributes<br>other than the join attributes, then the tuples need to be repartitioned. Each processor<br>Pi reads in the tuples on disk Di, computes for each tuple t the partition j to which t<br>belongs, and sends tuple t to processor Pj. Processor Pj stores the tuples on disk Dj.<br>We can optimize the join algorithm used locally at each processor to reduce I/O by<br>buffering some of the tuples to memory, instead of writing them to disk. We describe<br>such optimizations in Section 20.5.2.3.<br>Skew presents a special problem when range partitioning is used, since a partition<br>vector that splits one relation of the join into equal-sized partitions may split the other<br>relations into partitions of widely varying size. The partition vector should be such<br>that | ri | + | si | (that is, the sum of the sizes of ri and si) is roughly equal over all<br>the i = 0, 1, . . . , n −1. With a good hash function, hash partitioning is likely to have<br>a smaller skew, except when there are many tuples with the same values for the join<br>attributes.<br>20.5.2.2<br>Fragment-and-Replicate Join<br>Partitioning is not applicable to all types of joins. For instance, if the join condition<br>is an inequality, such as r<br>r.a&lt;s.b s, it is possible that all tuples in r join with some<br>tuple in s (and vice versa). Thus, there may be no easy way of partitioning r and s so<br>that tuples in partition ri join with only tuples in partition si.<br>We can parallelize such joins by using a technique called fragment and replicate. We<br>ﬁrst consider a special case of fragment and replicate—asymmetric fragment-and-<br>replicate join—which works as follows.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VI. Database System <br>Architecture<br>20. Parallel Databases<br>761<br>© The McGraw−Hill <br>Companies, 2001<br>766<br>Chapter 20<br>Parallel Databases<br>1. The system partitions one of the relations—say, r. Any partitioning technique<br>can be used on r, including round-robin partitioning.<br>2. The system replicates the other relation, s, across all the processors.<br>3. Processor Pi then locally computes the join of ri with all of s, using any join<br>technique.<br>The asymmetric fragment-and-replicate scheme appears in Figure 20.3a. If r is al-<br>ready stored by partitioning, there is no need to partition it further in step 1. All that<br>is required is to replicate s across all processors.<br>The general case of fragment and replicate join appears in Figure 20.3b; it works<br>this way: The system partitions relation r into n partitions, r0, r1, . . . , rn−1, and parti-<br>tions s into m partitions, s0, s1, . . . , sm−1. As before, any partitioning technique may<br>be used on r and on s. The values of m and n do not need to be equal, but they<br>must be chosen so that there are at least m ∗n processors. Asymmetric fragment and<br>replicate is simply a special case of general fragment and replicate, where m = 1.<br>Fragment and replicate reduces the sizes of the relations at each processor, compared<br>to asymmetric fragment and replicate.<br>r0<br>P0,0<br>s0<br>s1<br>s2<br>s<br>s3<br>sm–1<br>r1<br>r<br>r2<br>r3<br>rn–1<br>Pn–1, m–1 <br>...<br>P0<br>r0<br>P1<br>r1<br>r<br>s<br>P2<br>r2<br>P3<br>r3<br>...<br>...<br>P1,0<br>P2,0<br>P0,1<br>P1,1<br>P2,1<br>P0,2<br>P1,2<br>P0,3<br>. . .<br>.<br>.<br>.<br>.<br>.<br>.<br>.<br>.<br>.<br>.<br>(a) Asymmetric<br>fragment and replicate<br>(b) Fragment and replicate<br>Figure 20.3<br>Fragment-and-replicate schemes.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VI. Database System <br>Architecture<br>20. Parallel Databases<br>762<br>© The McGraw−Hill <br>Companies, 2001<br>20.5<br>Intraoperation Parallelism<br>767<br>Let the processors be P0,0, P0,1, . . . , P0,m−1, P1,0, . . . , Pn−1,m−1. Processor Pi,j com-<br>putes the join of ri with sj. Each processor must get the tuples in the partitions it<br>works on. To do so, the system replicates ri to processors Pi,0, Pi,1, . . . , Pi,m−1 (which<br>form a row in Figure 20.3b), and replicates si to processors P0,i, P1,i, . . . , Pn−1,i (which<br>form a column in Figure 20.3b). Any join technique can be used at each processor Pi,j.<br>Fragment and replicate works with any join condition, since every tuple in r can<br>be tested with every tuple in s. Thus, it can be used where partitioning cannot be.<br>Fragment and replicate usually has a higher cost than partitioning when both re-<br>lations are of roughly the same size, since at least one of the relations has to be repli-<br>cated. However, if one of the relations—say, s—is small, it may be cheaper to repli-<br>cate s across all processors, rather than to repartition r and s on the join attributes. In<br>such a case, asymmetric fragment and replicate is preferable, even though partition-<br>ing could be used.<br>20.5.2.3<br>Partitioned Parallel Hash–Join<br>The partitioned hash–join of Section 13.5.5 can be parallelized. Suppose that we have<br>n processors, P0, P1, . . . , Pn−1, and two relations r and s, such that the relations r and<br>s are partitioned across multiple disks. Recall from Section 13.5.5 that the smaller<br>relation is chosen as the build relation. If the size of s is less than that of r, the parallel<br>hash–join algorithm proceeds this way:<br>1. Choose a hash function—say, h1—that takes the join attribute value of each<br>tuple in r and s and maps the tuple to one of the n processors. Let ri denote the<br>tuples of relation r that are mapped to processor Pi; similarly, let si denote the<br>tuples of relation s that are mapped to processor Pi. Each processor Pi reads<br>the tuples of s that are on its disk Di, and sends each tuple to the appropriate<br>processor on the basis of hash function h1.<br>2. As the destination processor Pi receives the tuples of si, it further partitions<br>them by another hash function, h2, which the processor uses to compute the<br>hash–join locally. The partitioning at this stage is exactly the same as in the<br>partitioning phase of the sequential hash–join algorithm. Each processor Pi<br>executes this step independently from the other processors.<br>3. Once the tuples of s have been distributed, the system redistributes the larger<br>relation r across the m processors by the hash function h1, in the same way<br>as before. As it receives each tuple, the destination processor repartitions it<br>by the function h2, just as the probe relation is partitioned in the sequential<br>hash–join algorithm.<br>4. Each processor Pi executes the build and probe phases of the hash–join algo-<br>rithm on the local partitions ri and si of r and s to produce a partition of the<br>ﬁnal result of the hash–join.<br>The hash–join at each processor is independent of that at other processors, and<br>receiving the tuples of ri and si is similar to reading them from disk. Therefore, any<br>of the optimizations of the hash–join described in Chapter 13 can be applied as well<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VI. Database System <br>Architecture<br>20. Parallel Databases<br>763<br>© The McGraw−Hill <br>Companies, 2001<br>768<br>Chapter 20<br>Parallel Databases<br>to the parallel case. In particular, we can use the hybrid hash–join algorithm to cache<br>some of the incoming tuples in memory, and thus avoid the costs of writing them<br>and of reading them back in.<br>20.5.2.4<br>Parallel Nested-Loop Join<br>To illustrate the use of fragment-and-replicate–based parallelization, consider the<br>case where the relation s is much smaller than relation r. Suppose that relation r is<br>stored by partitioning; the attribute on which it is partitioned does not matter. Sup-<br>pose too that there is an index on a join attribute of relation r at each of the partitions<br>of relation r.<br>We use asymmetric fragment and replicate, with relation s being replicated and<br>with the existing partitioning of relation r. Each processor Pj where a partition of<br>relation s is stored reads the tuples of relation s stored in Dj, and replicates the tuples<br>to every other processor Pi. At the end of this phase, relation s is replicated at all sites<br>that store tuples of relation r.<br>Now, each processor Pi performs an indexed nested-loop join of relation s with<br>the ith partition of relation r. We can overlap the indexed nested-loop join with the<br>distribution of tuples of relation s, to reduce the costs of writing the tuples of relation<br>s to disk, and of reading them back. However, the replication of relation s must be<br>synchronized with the join so that there is enough space in the in-memory buffers<br>at each processor Pi to hold the tuples of relation s that have been received but that<br>have not yet been used in the join.<br>20.5.3<br>Other Relational Operations<br>The evaluation of other relational operations also can be parallelized:<br>• Selection. Let the selection be σθ(r). Consider ﬁrst the case where θ is of the<br>form ai = v, where ai is an attribute and v is a value. If the relation r is par-<br>titioned on ai, the selection proceeds at a single processor. If θ is of the form<br>l ≤ai ≤u—that is, θ is a range selection—and the relation has been range-<br>partitioned on ai, then the selection proceeds at each processor whose parti-<br>tion overlaps with the speciﬁed range of values. In all other cases, the selection<br>proceeds in parallel at all the processors.<br>• Duplicate elimination. Duplicates can be eliminated by sorting; either of the<br>parallel sort techniques can be used, optimized to eliminate duplicates as soon<br>as they appear during sorting. We can also parallelize duplicate elimination by<br>partitioning the tuples (by either range or hash partitioning) and eliminating<br>duplicates locally at each processor.<br>• Projection. Projection without duplicate elimination can be performed as tu-<br>ples are read in from disk in parallel. If duplicates are to be eliminated, either<br>of the techniques just described can be used.<br>• Aggregation. Consider an aggregation operation. We can parallelize the op-<br>eration by partitioning the relation on the grouping attributes, and then com-<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VI. Database System <br>Architecture<br>20. Parallel Databases<br>764<br>© The McGraw−Hill <br>Companies, 2001<br>20.5<br>Intraoperation Parallelism<br>769<br>puting the aggregate values locally at each processor. Either hash partitioning<br>or range partitioning can be used. If the relation is already partitioned on the<br>grouping attributes, the ﬁrst step can be skipped.<br>We can reduce the cost of transferring tuples during partitioning by partly<br>computing aggregate values before partitioning, at least for the commonly<br>used aggregate functions. Consider an aggregation operation on a relation r,<br>using the sum aggregate function on attribute B, with grouping on attribute<br>A. The system can perform the operation at each processor Pi on those r tuples<br>stored on disk Di. This computation results in tuples with partial sums at each<br>processor; there is one tuple at Pi for each value for attribute A present in r<br>tuples stored on Di. The system partitions the result of the local aggregation<br>on the grouping attribute A, and performs the aggregation again (on tuples<br>with the partial sums) at each processor Pi to get the ﬁnal result.<br>As a result of this optimization, fewer tuples need to be sent to other pro-<br>cessors during partitioning. This idea can be extended easily to the min and<br>max aggregate functions. Extensions to the count and avg aggregate functions<br>are left for you to do in Exercise 20.8.<br>The parallelization of other operations is covered in several of the the exercises.<br>20.5.4<br>Cost of Parallel Evaluation of Operations<br>We achieve parallelism by partitioning the I/O among multiple disks, and partition-<br>ing the CPU work among multiple processors. If such a split is achieved without any<br>overhead, and if there is no skew in the splitting of work, a parallel operation using n<br>processors will take 1/n times as long as the same operation on a single processor. We<br>already know how to estimate the cost of an operation such as a join or a selection.<br>The time cost of parallel processing would then be 1/n of the time cost of sequential<br>processing of the operation.<br>We must also account for the following costs:<br>• Startup costs for initiating the operation at multiple processors<br>• Skew in the distribution of work among the processors, with some processors<br>getting a larger number of tuples than others<br>• Contention for resources—such as memory, disk, and the communication<br>network—resulting in delays<br>• Cost of assembling the ﬁnal result by transmitting partial results from each<br>processor<br>The time taken by a parallel operation can be estimated as<br>Tpart + Tasm + max(T0, T1, . . . , Tn−1)<br>where Tpart is the time for partitioning the relations, Tasm is the time for assembling<br>the results and Ti the time taken for the operation at processor Pi. Assuming that the<br>tuples are distributed without any skew, the number of tuples sent to each processor<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VI. Database System <br>Architecture<br>20. Parallel Databases<br>765<br>© The McGraw−Hill <br>Companies, 2001<br>770<br>Chapter 20<br>Parallel Databases<br>can be estimated as 1/n of the total number of tuples. Ignoring contention, the cost<br>Ti of the operations at each processor Pi can then be estimated by the techniques in<br>Chapter 13.<br>The preceding estimate will be an optimistic estimate, since skew is common. Even<br>though breaking down a single query into a number of parallel steps reduces the size<br>of the average step, it is the time for processing the single slowest step that deter-<br>mines the time taken for processing the query as a whole. A partitioned parallel eval-<br>uation, for instance, is only as fast as the slowest of the parallel executions. Thus, any<br>skew in the distribution of the work across processors greatly affects performance.<br>The problem of skew in partitioning is closely related to the problem of partition<br>overﬂow in sequential hash–joins (Chapter 13). We can use overﬂow resolution and<br>avoidance techniques developed for hash–joins to handle skew when hash partition-<br>ing is used. We can use balanced range partitioning and virtual processor partitioning<br>to minim</span><br><br><span style="background-color: #FDFFB6;" title="Chunk 98 | Start: 1960196 | End: 1980196 | Tokens: 3127">ize skew due to range partitioning, as in Section 20.2.3.<br>20.6<br>Interoperation Parallelism<br>There are two forms of interoperation parallelism: pipelined parallelism, and inde-<br>pendent parallelism.<br>20.6.1<br>Pipelined Parallelism<br>As discussed in Chapter 13, pipelining forms an important source of economy of<br>computation for database query processing. Recall that, in pipelining, the output tu-<br>ples of one operation, A, are consumed by a second operation, B, even before the ﬁrst<br>operation has produced the entire set of tuples in its output. The major advantage of<br>pipelined execution in a sequential evaluation is that we can carry out a sequence of<br>such operations without writing any of the intermediate results to disk.<br>Parallel systems use pipelining primarily for the same reason that sequential sys-<br>tems do. However, pipelines are a source of parallelism as well, in the same way that<br>instruction pipelines are a source of parallelism in hardware design. It is possible to<br>run operations A and B simultaneously on different processors, so that B consumes<br>tuples in parallel with A producing them. This form of parallelism is called pipelined<br>parallelism.<br>Consider a join of four relations:<br>r1<br> r2<br> r3<br> r4<br>We can set up a pipeline that allows the three joins to be computed in parallel. Sup-<br>pose processor P1 is assigned the computation of temp1<br>←<br>r1<br><br>r2, and P2 is<br>assigned the computation of r3<br><br>temp1. As P1 computes tuples in r1<br><br>r2, it<br>makes these tuples available to processor P2. Thus, P2 has available to it some of the<br>tuples in r1<br> r2 before P1 has ﬁnished its computation. P2 can use those tuples<br>that are available to begin computation of temp1<br> r3, even before r1<br> r2 is fully<br>computed by P1. Likewise, as P2 computes tuples in (r1<br> r2)<br> r3, it makes these<br>tuples available to P3, which computes the join of these tuples with r4.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VI. Database System <br>Architecture<br>20. Parallel Databases<br>766<br>© The McGraw−Hill <br>Companies, 2001<br>20.6<br>Interoperation Parallelism<br>771<br>Pipelined parallelism is useful with a small number of processors, but does not<br>scale up well. First, pipeline chains generally do not attain sufﬁcient length to pro-<br>vide a high degree of parallelism. Second, it is not possible to pipeline relational<br>operators that do not produce output until all inputs have been accessed, such as the<br>set-difference operation. Third, only marginal speedup is obtained for the frequent<br>cases in which one operator’s execution cost is much higher than are those of the<br>others.<br>All things considered, when the degree of parallelism is high, pipelining is a less<br>important source of parallelism than partitioning. The real reason for using pipelin-<br>ing is that pipelined executions can avoid writing intermediate results to disk.<br>20.6.2<br>Independent Parallelism<br>Operations in a query expression that do not depend on one another can be executed<br>in parallel. This form of parallelism is called independent parallelism.<br>Consider the join r1<br> r2<br> r3<br> r4. Clearly, we can compute temp1 ←r1<br> r2<br>in parallel with temp2<br>←r3<br> r4. When these two computations complete, we<br>compute<br>temp1<br> temp2<br>To obtain further parallelism, we can pipeline the tuples in temp1 and temp2 into<br>the computation of temp1<br> temp2, which is itself carried out by a pipelined join<br>(Section 13.7.2.2).<br>Like pipelined parallelism, independent parallelism does not provide a high de-<br>gree of parallelism, and is less useful in a highly parallel system, although it is useful<br>with a lower degree of parallelism.<br>20.6.3<br>Query Optimization<br>Query optimizers account in large measure for the success of relational technology.<br>Recall that a query optimizer takes a query and ﬁnds the cheapest execution plan<br>among the many possible execution plans that give the same answer.<br>Query optimizers for parallel query evaluation are more complicated than query<br>optimizers for sequential query evaluation. First, the cost models are more compli-<br>cated, since partitioning costs have to be accounted for, and issues such as skew and<br>resource contention must be taken into account. More important is the issue of how<br>to parallelize a query. Suppose that we have somehow chosen an expression (from<br>among those equivalent to the query) to be used for evaluating the query. The ex-<br>pression can be represented by an operator tree, as in Section 13.1.<br>To evaluate an operator tree in a parallel system, we must make the following<br>decisions:<br>• How to parallelize each operation, and how many processors to use for it<br>• What operations to pipeline across different processors, what operations to ex-<br>ecute independently in parallel, and what operations to execute sequentially,<br>one after the other<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VI. Database System <br>Architecture<br>20. Parallel Databases<br>767<br>© The McGraw−Hill <br>Companies, 2001<br>772<br>Chapter 20<br>Parallel Databases<br>These decisions constitute the task of scheduling the execution tree.<br>Determining the resources of each kind—such as processors, disks, and memory<br>—that should be allocated to each operation in the tree is another aspect of the opti-<br>mization problem. For instance, it may appear wise to use the maximum amount of<br>parallelism available, but it is a good idea not to execute certain operations in paral-<br>lel. Operations whose computational requirements are signiﬁcantly smaller than the<br>communication overhead should be clustered with one of their neighbors. Otherwise,<br>the advantage of parallelism is negated by the overhead of communication.<br>One concern is that long pipelines do not lend themselves to good resource utiliza-<br>tion. Unless the operations are coarse grained, the ﬁnal operation of the pipeline may<br>wait for a long time to get inputs, while holding precious resources, such as memory.<br>Hence, long pipelines should be avoided.<br>The number of parallel evaluation plans from which to choose is much larger than<br>the number of sequential evaluation plans. Optimizing parallel queries by consid-<br>ering all alternatives is therefore much more expensive than optimizing sequential<br>queries. Hence, we usually adopt heuristic approaches to reduce the number of par-<br>allel execution plans that we have to consider. We describe two popular heuristics<br>here.<br>The ﬁrst heuristic is to consider only evaluation plans that parallelize every oper-<br>ation across all processors, and that do not use any pipelining. This approach is used<br>in the Teradata DBC series machines. Finding the best such execution plan is like do-<br>ing query optimization in a sequential system. The main differences lie in how the<br>partitioning is performed and what cost-estimation formula is used.<br>The second heuristic is to choose the most efﬁcient sequential evaluation plan,<br>and then to parallelize the operations in that evaluation plan. The Volcano parallel<br>database popularized a model of parallelization called the exchange-operator model.<br>This model uses existing implementations of operations, operating on local copies of<br>data, coupled with an exchange operation that moves data around between different<br>processors. Exchange operators can be introduced into an evaluation plan to trans-<br>form it into a parallel evaluation plan.<br>Yet another dimension of optimization is the design of physical-storage organi-<br>zation to speed up queries. The optimal physical organization differs for different<br>queries. The database administrator must choose a physical organization that ap-<br>pears to be good for the expected mix of database queries. Thus, the area of parallel<br>query optimization is complex, and it is still an area of active research.<br>20.7<br>Design of Parallel Systems<br>So far this chapter has concentrated on parallelization of data storage and of query<br>processing. Since large-scale parallel database systems are used primarily for storing<br>large volumes of data, and for processing decision-support queries on those data,<br>these topics are the most important in a parallel database system. Parallel loading<br>of data from external sources is an important requirement, if we are to handle large<br>volumes of incoming data.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VI. Database System <br>Architecture<br>20. Parallel Databases<br>768<br>© The McGraw−Hill <br>Companies, 2001<br>20.8<br>Summary<br>773<br>A large parallel database system must also address these availability issues:<br>• Resilience to failure of some processors or disks<br>• Online reorganization of data and schema changes<br>We consider these issues here.<br>With a large number of processors and disks, the probability that at least one pro-<br>cessor or disk will malfunction is signiﬁcantly greater than in a single-processor sys-<br>tem with one disk. A poorly designed parallel system will stop functioning if any<br>component (processor or disk) fails. Assuming that the probability of failure of a sin-<br>gle processor or disk is small, the probability of failure of the system goes up linearly<br>with the number of processors and disks. If a single processor or disk would fail once<br>every 5 years, a system with 100 processors would have a failure every 18 days.<br>Therefore, large-scale parallel database systems, such as Compaq Himalaya,<br>Teradata, and Informix XPS (now a division of IBM), are designed to operate even<br>if a processor or disk fails. Data are replicated across at least two processors. If a pro-<br>cessor fails, the data that it stored can still be accessed from the other processors. The<br>system keeps track of failed processors and distributes the work among functioning<br>processors. Requests for data stored at the failed site are automatically routed to the<br>backup sites that store a replica of the data. If all the data of a processor A are repli-<br>cated at a single processor B, B will have to handle all the requests to A as well as<br>those to itself, and that will result in B becoming a bottleneck. Therefore, the replicas<br>of the data of a processor are partitioned across multiple other processors.<br>When we are dealing with large volumes of data (ranging in the terabytes), simple<br>operations, such as creating indices, and changes to schema, such as adding a column<br>to a relation, can take a long time — perhaps hours or even days. Therefore, it is<br>unacceptable for the database system to be unavailable while such operations are in<br>progress. Many parallel database systems, such as the Compaq Himalaya systems,<br>allow such operations to be performed online, that is, while the system is executing<br>other transactions.<br>Consider, for instance, online index construction. A system that supports this fea-<br>ture allows insertions, deletions, and updates on a relation even as an index is being<br>built on the relation. The index-building operation therefore cannot lock the entire<br>relation in shared mode, as it would have done otherwise. Instead, the process keeps<br>track of updates that occur while it is active, and incorporates the changes into the<br>index being constructed.<br>20.8<br>Summary<br>• Parallel databases have gained signiﬁcant commercial acceptance in the past<br>15 years.<br>• In I/O parallelism, relations are partitioned among available disks so that<br>they can be retrieved faster. Three commonly used partitioning techniques<br>are round-robin partitioning, hash partitioning, and range partitioning.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VI. Database System <br>Architecture<br>20. Parallel Databases<br>769<br>© The McGraw−Hill <br>Companies, 2001<br>774<br>Chapter 20<br>Parallel Databases<br>• Skew is a major problem, especially with increasing degrees of parallelism.<br>Balanced partitioning vectors, using histograms, and virtual processor parti-<br>tioning are among the techniques used to reduce skew.<br>• In interquery parallelism, we run different queries concurrently to increase<br>throughput.<br>• Intraquery parallelism attempts to reduce the cost of running a query. There<br>are two types of intraquery parallelism: intraoperation parallelism and inter-<br>operation parallelism.<br>• We use intraoperation parallelism to execute relational operations, such as<br>sorts and joins, in parallel. Intraoperation parallelism is natural for relational<br>operations, since they are set oriented.<br>• There are two basic approaches to parallelizing a binary operation such as a<br>join.<br>  In partitioned parallelism, the relations are split into several parts, and<br>tuples in ri are joined with only tuples from si. Partitioned parallelism<br>can only be used for natural and equi-joins.<br>  In fragment and replicate, both relations are partitioned and each parti-<br>tion is replicated. In asymmetric fragment-and-replicate, one of the rela-<br>tions is replicated while the other is partitioned. Unlike partitioned par-<br>allelism, fragment and replicate and asymmetric fragment-and-replicate<br>can be used with any join condition.<br>Both parallelization techniques can work in conjunction with any join tech-<br>nique.<br>• In independent parallelism, different operations that do not depend on one<br>another are executed in parallel.<br>• In pipelined parallelism, processors send the results of one operation to an-<br>other operation as those results are computed, without waiting for the entire<br>operation to ﬁnish.<br>• Query optimization in parallel databases is signiﬁcantly more complex than<br>query optimization in sequential databases.<br>Review Terms<br>• Decision-support queries<br>• I/O parallelism<br>• Horizontal partitioning<br>• Partitioning techniques<br>  Round-robin<br>  Hash partitioning<br>  Range partitioning<br>• Partitioning attribute<br>• Partitioning vector<br>• Point query<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VI. Database System <br>Architecture<br>20. Parallel Databases<br>770<br>© The McGraw−Hill <br>Companies, 2001<br>Exercises<br>775<br>• Range query<br>• Skew<br>  Execution skew<br>  Attribute-value skew<br>  Partition skew<br>• Handling of skew<br>  Balanced range-partitioning<br>vector<br>  Histogram<br>  Virtual processors<br>• Interquery parallelism<br>• Cache coherency<br>• Intraquery parallelism<br>  Intraoperation parallelism<br>  Interoperation parallelism<br>• Parallel sort<br>  Range-partitioning sort<br>  Parallel external sort–merge<br>• Data parallelism<br>• Parallel join<br>  Partitioned join<br>  Fragment-and-replicate join<br>  Asymmetric fragment-and-<br>replicate join<br>  Partitioned parallel hash–join<br>  Parallel nested-loop join<br>• Parallel selection<br>• Parallel duplicate elimination<br>• Parallel projection<br>• Parallel aggregation<br>• Cost of parallel evaluation<br>• Interoperation parallelism<br>  Pipelined parallelism<br>  Independent parallelism<br>• Query optimization<br>• Scheduling<br>• Exchange-operator model<br>• Design of parallel systems<br>• Online index construction<br>Exercises<br>20.1 For each of the three partitioning techniques, namely round-robin, hash par-<br>titioning, and range partitioning, give an example of a query for which that<br>partitioning technique would provide the fastest response.<br>20.2 In a range selection on a range-partitioned attribute, it is possible that only<br>one disk may need to be accessed. Describe the beneﬁts and drawbacks of this<br>property.<br>20.3 What factors could result in skew when a relation is partitioned on one of its<br>attributes by:<br>a. Hash partitioning<br>b. Range partitioning<br>In each case, what can be done to reduce the skew?<br>20.4 What form of parallelism (interquery, interoperation, or intraoperation) is likely<br>to be the most important for each of the following tasks.<br>a. Increasing the throughput of a system with many small queries<br>b. Increasing the throughput of a system with a few large queries, when the<br>number of disks and processors is large<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VI. Database System <br>Architecture<br>20. Parallel Databases<br>771<br>© The McGraw−Hill <br>Companies, 2001<br>776<br>Chapter 20<br>Parallel Databases<br>20.5 With pipelined parallelism, it is often a good idea to perform several operations<br>in a pipeline on a single processor, even when many processors are available.<br>a. Explain why.<br>b. Would the arguments you advanced in part a hold if the machine has a<br>shared-memory architecture? Explain why or why not.<br>c. Would the arguments in part a hold with independent parallelism? (That<br>is, are there cases where, even if the operations are not pipelined and there<br>are many processors available, it is still a good idea to perform several<br>operations on the same processor?)<br>20.6 Give an example of a join that is not a simple equi-join for which partitioned<br>parallelism can be used. What attributes should be used for partitioning?<br>20.7 Consider join processing using symmetric fragment and replicate with range<br>partitioning. How can you optimize the evaluation if the join condition is of<br>the form | r.A −s.B |<br>≤k, where k is a small constant. Here, | x | denotes the<br>absolute value of x. A join with such a join condition is called a band join.<br>20.8 Describe a good way to parallelize each of the following.<br>a. The difference operation<br>b. Aggregation by the count operation<br>c. Aggregation by the count distinct operation<br>d. Aggregation by the avg operation<br>e. Left outer join, if the join condition involves only equality<br>f. Left outer join, if the join condition involves comparisons other than equal-<br>ity<br>g. Full outer join, if the join condition involves comparisons other than equal-<br>ity<br>20.9 Recall that histograms are used for constructing load-balanced range parti-<br>tions.<br>a. Suppose you have a histogram where values are between 1 and 100, and<br>are partitioned into 10 ranges, 1–10, 11–20, . . ., 91–100, with frequencies<br>15, 5, 20, 10, 10, 5, 5, 20, 5, and 5, respectively. Give a load-balanced range<br>partitioning function to divide the values into 5 partitions.<br>b. Write an algorithm for computing a balanced range partition with p parti-<br>tions, given a histogram of frequency distributions containing n ranges.<br>20.10 Describe the beneﬁts and drawbacks of pipelined parallelism.<br>20.11 Some parallel database systems store an extra copy of each data item on disks<br>attached to a different processor, to avoid loss of data if one of the processors<br>fails.<br>a. Why is it a good idea to partition the copies of the data items of a processor<br>across multiple processors?<br>b. What are the beneﬁts and drawbacks of using RAID storage instead of stor-<br>ing an extra copy of each data item?<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VI. Database System <br>Architecture<br>20. Parallel Databases<br>772<br>© The McGraw−Hill <br>Companies, 2001<br>Bibliographical Notes<br>777<br>Bibliographical Notes<br>Relational database systems began appearing in the marketplace in 1983; now, they<br>dominate it. By the late 1970s and early 1980s, as the relational model gained reason-<br>ably sound footing, people recognized that relational operators are highly paralleliz-<br>able and have good dataﬂow properties. A commercial system, Teradata, and several<br>research projects, such as GRACE (Kitsuregawa et al. [1983], Fushimi et al. [1986]),<br>GAMMA (DeWitt et al. [1986], DeWitt [1990]), and Bubba (Boral et al. [1990]) were<br>launched in quick succession. Researchers used these parallel database systems to in-<br>vestigate the practicality of parallel execution of relational operators. Subsequently,<br>in the late 1980s and the 1990s, several more companies—such as Tandem, Oracle,<br>Sybase, Informix, and Red-Brick (now a part of Informix, which is itself now a part of<br>IBM)—entered the parallel database market. Research projects in the academic world<br>include XPRS (Stonebraker et al. [1989]) and Volcano (Graefe [1990]).<br>Locking in parallel databases is discussed in Joshi [1991], Mohan and Narang<br>[1991], and Mohan and Narang [1992]. Cache-coherency protocols for parallel data-<br>base systems are discussed by Dias et al. [1989], Mohan and Narang [1991], Mohan<br>and Narang [1992], and Rahm [1993]. Carey et al. [1991] discusses caching issues in a<br>client–server system. Parallelism and recovery in database systems are discussed by<br>Bayer et al. [1980].<br>Graefe [1993] presents an excellent survey of query processing, including paral-<br>lel processing of queries. Parallel sorting is discussed in DeWitt et al. [1992]. Parallel<br>join algo</span><br><br><span style="background-color: #CAFFBF;" title="Chunk 99 | Start: 1980198 | End: 2000198 | Tokens: 3079">rithms are described by Nakayama et al. [1984], Kitsuregawa et al. [1983],<br>Richardson et al. [1987], Schneider and DeWitt [1989], Kitsuregawa and Ogawa [1990],<br>Lin et al. [1994], and Wilschut et al. [1995], among other works. Parallel join algo-<br>rithms for shared-memory architectures are described by Tsukuda et al. [1992], Desh-<br>pande and Larson [1992], and Shatdal and Naughton [1993].<br>Skew handling in parallel joins is described by Walton et al. [1991], Wolf [1991],<br>and DeWitt et al. [1992]. Sampling techniques for parallel databases are described<br>by Seshadri and Naughton [1992] and Ganguly et al. [1996]. The exchange-operator<br>model was advocated by Graefe [1990] and Graefe [1993].<br>Parallel query-optimization techniques are described by H. Lu and Tan [1991],<br>Hong and Stonebraker [1991], Ganguly et al. [1992], Lanzelotte et al. [1993], Hasan<br>and Motwani [1995], and Jhingran et al. [1997].<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VII. Other Topics<br>Introduction<br>773<br>© The McGraw−Hill <br>Companies, 2001<br>P<br>A<br>R<br>T<br>7<br>Other Topics<br>Chapter 21 covers a number of issues in building and maintaining applications and<br>administering database systems. The chapter ﬁrst outlines how to implement user in-<br>terfaces, in particular Web-based interfaces. Other issues such as performance tuning<br>(to improve application speed), standards issues in electronic commerce, and how to<br>handle legacy systems are also covered in this chapter.<br>Chapter 22 describes a number of recent advances in querying and information<br>retrieval. It ﬁrst covers SQL extensions to support new types of queries, in particular<br>queries typically posed by data analysts. It next covers data warehousing, whereby<br>data generated by different parts of an organization are gathered centrally. The chap-<br>ter then outlines data mining, which aims at ﬁnding patterns of various complex<br>forms in large volumes of data. Finally, the chapter describes information retrieval,<br>which deals with techniques for querying collections of text documents, such as Web<br>pages, to ﬁnd documents of interest.<br>Chapter 22 describes data types, such as temporal data, spatial data, and multime-<br>dia data, and the issues in storing such data in databases. Applications such as mobile<br>computing and its connections with databases, are also described in this chapter.<br>Finally, Chapter 23 describes several advanced transaction-processing techniques,<br>including transaction-processing monitors, transactional workﬂows, long-duration<br>transactions, and multidatabase transactions.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VII. Other Topics<br>21. Application <br>Development and <br>Administration<br>774<br>© The McGraw−Hill <br>Companies, 2001<br>C<br>H<br>A<br>P<br>T<br>E<br>R<br>2<br>1<br>Application Development<br>and Administration<br>Practically all use of databases occurs from within application programs. Correspon-<br>dingly, almost all user interaction with databases is indirect, via application pro-<br>grams. Not surprisingly, therefore, database systems have long supported tools such<br>as form and GUI builders, which help in rapid development of applications that in-<br>terface with users. In recent years, the Web has become the most widely used user<br>interface to databases.<br>Once an application has been built, it is often found to run slower than the design-<br>ers wanted, or to handle fewer transactions per second than they required. Applica-<br>tions can be made to run signiﬁcantly faster by performance tuning, which consists of<br>ﬁnding and eliminating bottlenecks and adding appropriate hardware such as mem-<br>ory or disks. Benchmarks help to characterize the performance of database systems.<br>Standards are very important for application development, especially in the age of<br>the internet, since applications need to communicate with each other to perform use-<br>ful tasks. A variety of standards have been proposed that affect database application<br>development.<br>Electronic commerce is becoming an integral part of how we purchase goods and<br>services and databases play an important role in that domain.<br>Legacy systems are systems based on older-generation technology. They are often<br>at the core of organizations, and run mission-critical applications. We outline issues<br>in interfacing with legacy systems, and how they can be replaced by other systems.<br>21.1<br>Web Interfaces to Databases<br>The World Wide Web (Web, for short), is a distributed information system based on<br>hypertext. Web interfaces to databases have become very important. After outlining<br>several reasons for interfacing databases with the Web (Section 21.1.1), we provide<br>an overview of Web technology (Section 21.1.2) and then study Web servers (Sec-<br>tion 21.1.3) and outline some state-of-the art techniques for building Web interfaces<br>781<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VII. Other Topics<br>21. Application <br>Development and <br>Administration<br>775<br>© The McGraw−Hill <br>Companies, 2001<br>782<br>Chapter 21<br>Application Development and Administration<br>to databases, using servlets and server-side scripting languages (Sections 21.1.4 and<br>21.1.5). We describe techniques for improving performance in Section 21.1.6.<br>21.1.1<br>Motivation<br>The Web has become important as a front end to databases for several reasons: Web<br>browsers provide a universal front end to information supplied by back ends located<br>anywhere in the world. The front end can run on any computer system, and there is<br>no need for a user to download any special-purpose software to access information.<br>Further, today, almost everyone who can afford it has access to the Web.<br>With the growth of information services and electronic commerce on the Web,<br>databases used for information services, decision support, and transaction process-<br>ing must be linked with the Web. The HTML forms interface is convenient for trans-<br>action processing. The user can ﬁll in details in an order form, then click a submit<br>button to send a message to the server. The server executes an application program<br>corresponding to the order form, and this action in turn executes transactions on a<br>database at the server site. The server formats the results of the transaction and sends<br>them back to the user.<br>Another reason for interfacing databases to the Web is that presenting only static<br>(ﬁxed) documents on a Web site has some limitations, even when the user is not<br>doing any querying or transaction processing:<br>• Fixed Web documents do not allow the display to be tailored to the user. For<br>instance, a newspaper may want to tailor its display on a per-user basis, to<br>give prominence to news articles that are likely to be of interest to the user.<br>• When the company data are updated, the Web documents become obsolete<br>if they are not updated simultaneously. The problem becomes more acute if<br>multiple Web documents replicate important data, and all must be updated.<br>We can ﬁx these problems by generating Web documents dynamically from a database.<br>When a document is requested, a program gets executed at the server site, which in<br>turn runs queries on a database, and generates the requested document on the basis<br>of the query results. Whenever relevant data in the database are updated, the gener-<br>ated documents will automatically become up-to-date. The generated document can<br>also be tailored to the user on the basis of user information stored in the database.<br>Web interfaces provide attractive beneﬁts even for database applications that are<br>used only with a single organization. The HyperText Markup Language (HTML)<br>standard allows text to be neatly formatted, with important information highlighted.<br>Hyperlinks, which are links to other documents, can be associated with regions of<br>the displayed data. Clicking on a hyperlink fetches and displays the linked docu-<br>ment. Hyperlinks are very useful for browsing data, permitting users to get more<br>details of parts of the data as desired.<br>Finally, browsers today can fetch programs along with HTML documents, and run<br>the programs on the browser, in safe mode—that is, without damaging data on the<br>user’s computer. Programs can be written in client-side scripting languages, such as<br>Javascript, or can be “applets” written in the Java language. These programs permit<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VII. Other Topics<br>21. Application <br>Development and <br>Administration<br>776<br>© The McGraw−Hill <br>Companies, 2001<br>21.1<br>Web Interfaces to Databases<br>783<br>the construction of sophisticated user interfaces, beyond what is possible with just<br>HTML, interfaces that can be used without downloading and installing any software.<br>Thus, Web interfaces are powerful and visually attractive, and are likely to eclipse<br>special-purpose interfaces for all except a small class of users.<br>21.1.2<br>Web Fundamentals<br>Here we review some of the fundamental technology behind the World Wide Web,<br>for readers who are not familiar with it.<br>21.1.2.1<br>Uniform Resource Locators<br>A uniform resource locator (URL) is a globally unique name for each document that<br>can be accessed on the Web. An example of a URL is<br>http://www.bell-labs.com/topic/book/db-book<br>The ﬁrst part of the URL indicates how the document is to be accessed: “http” indi-<br>cates that the document is to be accessed by the HyperText Transfer Protocol, which is<br>a protocol for transferring HTML documents. The second part gives the unique name<br>of a machine that has a Web server. The rest of the URL is the path name of the ﬁle on<br>the machine, or other unique identiﬁer of the document within the machine.<br>Much data on the Web is dynamically generated. A URL can contain the identiﬁer<br>of a program located on the Web server machine, as well as arguments to be given to<br>the program. An example of such a URL is<br>http://www.google.com/search?q=silberschatz<br>which says that the program search on the server www.google.com should be exe-<br>cuted with the argument q=silberschatz. The program executes, using the given ar-<br>guments, and returns an HTML document, which is then sent to the front end.<br>21.1.2.2<br>HyperText Markup Language<br>Figure 21.1 is an example of the source of an HTML document. Figure 21.2 shows the<br>displayed image that this document creates.<br>The ﬁgures show how HTML can display a table and a simple form that allows<br>users to select the type (account or loan) from a menu and to input a number in a text<br>box. HTML also supports several other input types. Clicking on the submit button<br>causes the program BankQuery (speciﬁed in the form action ﬁeld) to be executed<br>with the user-provided values for the arguments type and number (speciﬁed in the<br>select and input ﬁelds). The program generates an HTML document, which is then<br>sent back and displayed to the user; we will see how to construct such programs in<br>Sections 21.1.3, 21.1.4, and 21.1.5.<br>HTML supports stylesheets, which can alter the default deﬁnitions of how an HTML<br>formatting construct is displayed, as well as other display attributes such as back-<br>ground color of the page. The cascading stylesheet (css) standard allows the same<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VII. Other Topics<br>21. Application <br>Development and <br>Administration<br>777<br>© The McGraw−Hill <br>Companies, 2001<br>784<br>Chapter 21<br>Application Development and Administration<br>&lt;html&gt;<br>&lt;body&gt;<br>&lt;table BORDER COLS=3&gt;<br>&lt;tr&gt; &lt;td&gt;A-101&lt;/td&gt; &lt;td&gt;Downtown&lt;/td&gt; &lt;td&gt;500&lt;/td&gt; &lt;/tr&gt;<br>&lt;tr&gt; &lt;td&gt;A-102&lt;/td&gt; &lt;td&gt;Perryridge&lt;/td&gt; &lt;td&gt;400&lt;/td&gt; &lt;/tr&gt;<br>&lt;tr&gt; &lt;td&gt;A-201&lt;/td&gt; &lt;td&gt;Brighton<br>&lt;/td&gt; &lt;td&gt;900&lt;/td&gt; &lt;/tr&gt;<br>&lt;/table&gt;<br>&lt;center&gt; The &lt;i&gt;account&lt;/i&gt; relation &lt;/center&gt;<br>&lt;form action=“BankQuery” method=get&gt;<br>Select account/loan and enter number &lt;br&gt;<br>&lt;select name=”type”&gt;<br>&lt;option value=“account” selected&gt;Account<br>&lt;option value=“loan”&gt; Loan<br>&lt;/select&gt;<br>&lt;input type=text size=5 name=“number”&gt;<br>&lt;input type=submit value=“submit”&gt;<br>&lt;/body&gt;<br>&lt;/html&gt;<br>Figure 21.1<br>An HTML source text.<br>stylesheet to be used for multiple HTML documents, giving a uniform look to all<br>the pages on a Web site.<br>21.1.2.3<br>Client-Side Scripting and Applets<br>Embedding of program code in documents allows Web pages to be active, carry-<br>ing out activities such as animation by executing programs at the local site, rather<br>than just presenting passive text and graphics. The primary use of such programs<br>is ﬂexible interaction with the user, beyond the limited interaction power provided<br>by HTML and HTML forms. Further, executing programs at the client site speeds up<br>A–101<br>A–102<br>A–201<br>Downtown<br>Downtown<br>Perryridge<br>Perryridge<br>Brighton<br>Brighton<br>The         <br>The         relation<br>relation<br>Select account/loan and enter number<br>500<br>400<br>900<br>Account<br>submit<br>account<br>account<br>Figure 21.2<br>Display of HTML source from Figure 21.1.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VII. Other Topics<br>21. Application <br>Development and <br>Administration<br>778<br>© The McGraw−Hill <br>Companies, 2001<br>21.1<br>Web Interfaces to Databases<br>785<br>interaction greatly, compared to every interaction being sent to a server site for pro-<br>cessing.<br>A danger in supporting such programs is that, if the design of the system is done<br>carelessly, program code embedded in a Web page (or equivalently, in an e-mail mes-<br>sage) can perform malicious actions on the user’s computer. The malicious actions<br>could range from reading private information, to deleting or modifying information<br>on the computer, up to taking control of the computer and propagating the code<br>to other computers (through e-mail, for example). A number of e-mail viruses have<br>spread widely in recent years in this way.<br>The Java language became very popular because it provides a safe mode for ex-<br>ecuting programs on user’s computers. Java code can be compiled into platform-<br>independent “byte-code” that can be executed on any browser that supports Java.<br>Unlike local programs, Java programs (applets) downloaded as part of a Web page<br>have no authority to perform any actions that could be destructive. They are permit-<br>ted to display data on the screen, or to make a network connection to the server from<br>which the Web page was downloaded, in order to fetch more information. However,<br>they are not permitted to access local ﬁles, to execute any system programs, or to<br>make network connections to any other computers.<br>While Java is a full-ﬂedged programming language, there are simpler languages,<br>called scripting languages, that can enrich user interaction, while providing the same<br>protection as Java. These languages provide constructs that can be embedded with<br>an HTML document. Client-side scripting languages are languages designed to be<br>executed on the client’s Web browser. Of these, the Javascript language is by far the<br>most widely used. There are also special-purpose scripting languages for special-<br>ized tasks such as animation (for example, Macromedia Flash and Shockwave), and<br>three-dimensional modeling (Virtual Reality Markup Language (VRML)). Scripting<br>languages can also be used on the server side, as we shall see.<br>21.1.3<br>Web Servers and Sessions<br>A Web server is a program running on the server machine, which accepts requests<br>from a Web browser and sends back results in the form of HTML documents. The<br>browser and Web server communicate by a protocol called the HyperText Trans-<br>fer Protocol (HTTP). HTTP provides powerful features, beyond the simple transfer<br>of documents. The most important feature is the ability to execute programs, with<br>arguments supplied by the user, and deliver the results back as an HTML document.<br>As a result, a Web server can easily act as an intermediary to provide access to<br>a variety of information services. A new service can be created by creating and in-<br>stalling an application program that provides the service. The common gateway in-<br>terface (CGI) standard deﬁnes how the Web server communicates with application<br>programs. The application program typically communicates with a database server,<br>through ODBC, JDBC, or other protocols, in order to get or store data.<br>Figure 21.3 shows a Web service using a three-tier architecture, with a Web server,<br>an application server, and a database server. Using multiple levels of servers in-<br>creases system overhead; the CGI interface starts a new process to service each re-<br>quest, which results in even greater overhead.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VII. Other Topics<br>21. Application <br>Development and <br>Administration<br>779<br>© The McGraw−Hill <br>Companies, 2001<br>786<br>Chapter 21<br>Application Development and Administration<br>HTTP<br>browser<br>server<br>application server<br>data<br>database server<br>network<br>web server<br>network<br>Figure 21.3<br>Three-tier Web architecture.<br>Most Web services today therefore use a two-tier Web architecture, where the ap-<br>plication program runs within the Web server, as in Figure 21.4. We study systems<br>based on the two-tier architecture in more detail in subsequent sections.<br>Be aware that there is no continuous connection between the client and the server.<br>In contrast, when a user logs on to a computer, or connects to an ODBC or JDBC<br>server, a session is created, and session information is retained at the server and the<br>client until the session is terminated—information such as whether the user was<br>authenticated using a password and what session options the user set. The reason<br>that HTTP is connectionless is that most computers have limits on the number of<br>simultaneous connections they can accommodate, and if a large number of sites on<br>the Web open connections, this limit would be exceeded, denying service to further<br>users. With a connectionless service, the connection is broken as soon as a request is<br>satisﬁed, leaving connections available for other requests.<br>Most information services need session information. For instance, services typ-<br>ically restrict access to information, and therefore need to authenticate users. Au-<br>thentication should be done once per session, and further interactions in the session<br>should not require reauthentication.<br>To create the view of such sessions, extra information has to be stored at the client,<br>and returned with each request in a session, for a server to identify that a request is<br>part of a user session. Extra information about the session also has to be maintained<br>at the server.<br>This extra information is maintained in the form of a cookie at the client; a cookie<br>is simply a small piece of text containing identifying information. The server sends a<br>cookie to the client after authentication, and also keeps a copy locally. Cookies sent<br>to different clients contain different identifying text. The browser sends the cookie<br>automatically on further document requests from the same server. By comparing the<br>cookie with locally stored cookies at the server, the server can identify the request as<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VII. Other Topics<br>21. Application <br>Development and <br>Administration<br>780<br>© The McGraw−Hill <br>Companies, 2001<br>21.1<br>Web Interfaces to Databases<br>787<br>HTTP<br>browser<br>server<br>data<br>database server<br>web server and<br>application server<br>network<br>Figure 21.4<br>Two-tier Web architecture.<br>part of an ongoing session. Cookies can also be used for storing user preferences and<br>using them when the server replies to a request. Cookies can be stored permanently<br>at the browser; they identify the user on subsequent visits to the same site, without<br>any identiﬁcation information being typed in.<br>21.1.4<br>Servlets<br>In a two-tier Web architecture, the application runs as part of the Web server itself.<br>One way of implementing such an architecture is to load Java programs with the<br>Web server. The Java servlet speciﬁcation deﬁnes an application programming inter-<br>face for communication between the Web server and the application program. The<br>word servlet also refers to a Java program that implements the servlet interface. The<br>program is loaded into the Web server when the server starts up or when the server<br>receives a Web request for executing the servlet application. Figure 21.5 is an example<br>of servlet code to implement the form in Figure 21.1.<br>The servlet is called BankQueryServlet, while the form s</span><br><br><span style="background-color: #9BF6FF;" title="Chunk 100 | Start: 2000200 | End: 2020200 | Tokens: 3271">peciﬁes that action=“Bank-<br>Query”. The Web server must be told that this servlet is to be used to handle requests<br>for BankQuery.<br>The example will give you an idea of how servlets are used. For details needed to<br>build your own servlet application, you can consult a book on servlets or read the<br>online documentation on servlets that is part of the Java documentation from Sun.<br>See the bibliographical notes for references to these sources.<br>The form speciﬁes that the HTTP get mechanism is used for transmitting parame-<br>ters (post is the other widely used mechanism). So the doGet() method of the servlet,<br>which is deﬁned in the code, gets invoked. Each request results in a new thread<br>within which the call is executed, so multiple requests can be handled in parallel.<br>Any values from the form menus and input ﬁelds on the Web page, as well as<br>cookies, pass through an object of the HttpServletRequest class that is created for the<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VII. Other Topics<br>21. Application <br>Development and <br>Administration<br>781<br>© The McGraw−Hill <br>Companies, 2001<br>788<br>Chapter 21<br>Application Development and Administration<br>public class BankQueryServlet extends HttpServlet {<br>public void doGet(HttpServletRequest request, HttpServletResponse result)<br>throws ServletException, IOException<br>{<br>String type = request.getParameter(“type”);<br>String number = request.getParameter(“number”);<br>... code to ﬁnd the loan amount/account balance ...<br>... using JDBC to communicate with the database ..<br>... we assume the value is stored in the variable balance<br>result.setContentType(“text/html”);<br>PrintWriter out = result.getWriter();<br>out.println(“&lt;HEAD&gt;&lt;TITLE&gt; Query Result&lt;/TITLE&gt;&lt;/HEAD&gt;”);<br>out.println(“&lt;BODY&gt;”);<br>out.println(“Balance on ” + type + number + “ = ” + balance);<br>out.println(“&lt;/BODY&gt;”);<br>out.close();<br>}<br>}<br>Figure 21.5<br>Example of servlet code.<br>request, and the reply to the request passes through an object of the class HttpServlet-<br>Response.1<br>The doGet() method code in the example extracts values of the parameter’s type<br>and number by using request.getParameter(), and uses these to run a query against<br>a database. The code used to access the database is not shown; refer to Section 4.13.2<br>for details of how to use JDBC to access a database. The system returns the results of<br>the query to the requester by printing them out in HTML format to the HttpServlet-<br>Response result.<br>The servlet API provides a convenient method of creating sessions. Invoking the<br>method getSession(true) of the class HttpServletRequest creates a new object of type<br>HttpSession if this is the ﬁrst request from that client; the argument true says that a<br>session must be created if the request is a new request. The method returns an exist-<br>ing object if it had been created already for that browser session. Internally, cookies<br>are used to recognize that a request is from the same browser session as an earlier<br>request. The servlet code can store and look up (attribute-name, value) pairs in the<br>HttpSession object, to maintain state across multiple requests. For instance, the ﬁrst<br>request in a session may ask for a user-id and password, and store the user-id in the<br>session object. On subsequent requests from the browser session, the user-id will be<br>found in the session object.<br>Displaying a set of results from a query is a common task for many database appli-<br>cations. It is possible to build a generic function that will take any JDBC ResulSet as<br>argument, and display the tuples in the ResulSet appropriately. JDBC metadata calls<br>1.<br>The servlet interface can also support non-HTTP requests, although our examples only use HTTP.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VII. Other Topics<br>21. Application <br>Development and <br>Administration<br>782<br>© The McGraw−Hill <br>Companies, 2001<br>21.1<br>Web Interfaces to Databases<br>789<br>can be used to ﬁnd information such as the number of columns, and the name and<br>types of the columns, in the query result; this information is then used to print the<br>query result.<br>21.1.5<br>Server-Side Scripting<br>Writing even a simple Web application in a programming language such as Java or C<br>is a rather time-consuming task that requires many lines of code and programmers<br>familiar with the intricacies of the language. An alternative approach, that of server-<br>side scripting, provides a much easier method for creating many applications. Script-<br>ing languages provide constructs that can be embedded within HTML documents. In<br>server-side scripting, before delivering a Web page, the server executes the scripts<br>embedded within the HTML contents of the page. Each piece of script, when exe-<br>cuted, can generate text that is added to the page (or may even delete content from<br>the page). The source code of the scripts is removed from the page, so the client may<br>not even be aware that the page orignally had any code in it. The executed script may<br>contain SQL code that is executed against a database.<br>Several scripting languages have appeared in recent years. These include Server-<br>Side Javascript from Netscape, JScript from Microsoft, JavaServer Pages (JSP) from<br>Sun, the PHP Hypertext Preprocessor (PHP), ColdFusion’s ColdFusion Markup Lan-<br>guage (CFML) and Zope’s DTML. In fact, it is even possible to embed code written<br>in older scripting languages such as VBScript, Perl, and Python into HTML pages.<br>For instance, Microsoft’s Active Server Pages (ASP) supports embedded VBScript<br>and JScript. Other approaches have extended report-writer software, originally de-<br>veloped for generating printable reports, to generate HTML reports. These also sup-<br>port HTML forms for getting parameter values that are used in the queries embedded<br>in the reports.<br>Clearly, there are many options from which to choose. They all support similar<br>features, but differ in the style of programming and the ease with which simple ap-<br>plications can be created.<br>21.1.6<br>Improving Performance<br>Web sites may be accessed by millions or billions of people from across the globe, at<br>rates of thousands of requests per second, or even more, for the most popular sites.<br>Ensuring that requests are served with low response times is a major challenge for<br>Web site developers.<br>Caching techniques of various types are used to exploit commonalities between<br>transactions. For instance, suppose the application code for servicing each request<br>needs to contact a database through JDBC. Creating a new JDBC connection may take<br>several milliseconds, so opening a new connection for each request is not a good idea<br>if very high transaction rates are to be supported. Many applications create a pool of<br>open JDBC connections, and each request uses one of the connections from the pool.<br>Many requests may result in exactly the same query being executed on the data-<br>base. The cost of communication with the database can be greatly reduced by caching<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VII. Other Topics<br>21. Application <br>Development and <br>Administration<br>783<br>© The McGraw−Hill <br>Companies, 2001<br>790<br>Chapter 21<br>Application Development and Administration<br>the results of earlier queries, and reusing them, so long as the query result has not<br>changed at the database. Some Web servers support such query result caching.<br>Costs can be further reduced by caching the ﬁnal Web page that is sent in response<br>to a request. If a new request comes with exactly the same parameters as a previous<br>request, if the resultant Web page is in the cache it can be reused, avoiding the cost of<br>recomputing the page.<br>Cached query results and cached Web pages are forms of materialized views. If the<br>underlying database data changes, they can be discarded, or can be recomputed, or<br>even incrementally updated, as in materialized view maintenance (Section 14.5). For<br>example, the IBM Web server that was used in the 2000 Olympics can keep track of<br>what data a cached Web page depends on and recompute the page if the data change.<br>21.2<br>Performance Tuning<br>Tuning the performance of a system involves adjusting various parameters and de-<br>sign choices to improve its performance for a speciﬁc application. Various aspects of<br>a database-system design—ranging from high-level aspects such as the schema and<br>transaction design, to database parameters such as buffer sizes, down to hardware<br>issues such as number of disks—affect the performance of an application. Each of<br>these aspects can be adjusted so that performance is improved.<br>21.2.1<br>Location of Bottlenecks<br>The performance of most systems (at least before they are tuned) is usually limited<br>primarily by the performance of one or a few components, called bottlenecks. For<br>instance, a program may spend 80 percent of its time in a small loop deep in the code,<br>and the remaining 20 percent of the time on the rest of the code; the small loop then is<br>a bottleneck. Improving the performance of a component that is not a bottleneck does<br>little to improve the overall speed of the system; in the example, improving the speed<br>of the rest of the code cannot lead to more than a 20 percent improvement overall,<br>whereas improving the speed of the bottleneck loop could result in an improvement<br>of nearly 80 percent overall, in the best case.<br>Hence, when tuning a system, we must ﬁrst try to discover what are the bottle-<br>necks, and then to eliminate the bottlenecks by improving the performance of the<br>components causing them. When one bottleneck is removed, it may turn out that an-<br>other component becomes the bottleneck. In a well-balanced system, no single com-<br>ponent is the bottleneck. If the system contains bottlenecks, components that are not<br>part of the bottleneck are underutilized, and could perhaps have been replaced by<br>cheaper components with lower performance.<br>For simple programs, the time spent in each region of the code determines the<br>overall execution time. However, database systems are much more complex, and can<br>be modeled as queueing systems. A transaction requests various services from the<br>database system, starting from entry into a server process, disk reads during exe-<br>cution, CPU cycles, and locks for concurrency control. Each of these services has a<br>queue associated with it, and small transactions may spend most of their time wait-<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VII. Other Topics<br>21. Application <br>Development and <br>Administration<br>784<br>© The McGraw−Hill <br>Companies, 2001<br>21.2<br>Performance Tuning<br>791<br>ing in queues—especially in disk I/O queues—instead of executing code. Figure 21.6<br>illustrates some of the queues in a database system.<br>As a result of the numerous queues in the database, bottlenecks in a database sys-<br>tem typically show up in the form of long queues for a particular service, or, equiva-<br>lently, in high utilizations for a particular service. If requests are spaced exactly uni-<br>formly, and the time to service a request is less than or equal to the time before the<br>next request arrives, then each request will ﬁnd the resource idle and can therefore<br>start execution immediately without waiting. Unfortunately, the arrival of requests<br>in a database system is never so uniform, and is instead random.<br>If a resource, such as a disk, has a low utilization, then, when a request is made,<br>the resource is likely to be idle, in which case the waiting time for the request will be<br>0. Assuming uniformly randomly distributed arrivals, the length of the queue (and<br>correspondingly the waiting time) go up exponentially with utilization; as utilization<br>approaches 100 percent, the queue length increases sharply, resulting in excessively<br>long waiting times. The utilization of a resource should be kept low enough that<br>queue length is short. As a rule of the thumb, utilizations of around 70 percent are<br>considered to be good, and utilizations above 90 percent are considered excessive,<br>since they will result in signiﬁcant delays. To learn more about the theory of queueing<br>systems, generally referred to as queueing theory, you can consult the references<br>cited in the bibliographical notes.<br>concurrency control <br>manager<br>disk manager<br>CPU manager<br>transaction<br>manager<br>transaction<br>monitor<br>transaction<br>source<br>buffer<br>manager<br>lock<br>grant<br>lock<br>request<br>page<br>reply<br>page<br>request<br>page<br>reply<br>page<br>request<br>…<br>…<br>Figure 21.6<br>Queues in a database system.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VII. Other Topics<br>21. Application <br>Development and <br>Administration<br>785<br>© The McGraw−Hill <br>Companies, 2001<br>792<br>Chapter 21<br>Application Development and Administration<br>21.2.2<br>Tunable Parameters<br>Database administrators can tune a database system at three levels. The lowest level<br>is at the hardware level. Options for tuning systems at this level include adding disks<br>or using a RAID system if disk I/O is a bottleneck, adding more memory if the disk<br>buffer size is a bottleneck, or moving to a faster processor if CPU use is a bottleneck.<br>The second level consists of the database-system parameters, such as buffer size<br>and checkpointing intervals. The exact set of database-system parameters that can be<br>tuned depends on the speciﬁc database system. Most database-system manuals pro-<br>vide information on what database-system parameters can be adjusted, and how you<br>should choose values for the parameters. Well-designed database systems perform<br>as much tuning as possible automatically, freeing the user or database administrator<br>from the burden. For instance, in many database systems the buffer size is ﬁxed but<br>tunable. If the system automatically adjusts the buffer size by observing indicators<br>such as page-fault rates, then the user will not have to worry about tuning the buffer<br>size.<br>The third level is the highest level. It includes the schema and transactions. The<br>administrator can tune the design of the schema, the indices that are created, and<br>the transactions that are executed, to improve performance. Tuning at this level is<br>comparatively system independent.<br>The three levels of tuning interact with one another; we must consider them to-<br>gether when tuning a system. For example, tuning at a higher level may result in the<br>hardware bottleneck changing from the disk system to the CPU, or vice versa.<br>21.2.3<br>Tuning of Hardware<br>Even in a well-designed transaction processing system, each transaction usually has<br>to do at least a few I/O operations, if the data required by the transaction is on disk.<br>An important factor in tuning a transaction processing system is to make sure that<br>the disk subsystem can handle the rate at which I/O operations are required. For in-<br>stance, disks today have an access time of about 10 milliseconds, and transfer times<br>of 20 MB per second, which gives about 100 random access I/O operations of 1 KB<br>each. If each transaction requires just 2 I/O operations, a single disk would support at<br>most 50 transactions per second. The only way to support more transactions per sec-<br>ond is to increase the number of disks. If the system needs to support n transactions<br>per second, each performing 2 I/O operations, data must be striped (or otherwise<br>partitioned) across n/50 disks (ignoring skew).<br>Notice here that the limiting factor is not the capacity of the disk, but the speed<br>at which random data can be accessed (limited in turn by the speed at which the<br>disk arm can move). The number of I/O operations per transaction can be reduced<br>by storing more data in memory. If all data are in memory, there will be no disk I/O<br>except for writes. Keeping frequently used data in memory reduces the number of<br>disk I/Os, and is worth the extra cost of memory. Keeping very infrequently used<br>data in memory would be a waste, since memory is much more expensive than disk.<br>The question is, for a given amount of money available for spending on disks or<br>memory, what is the best way to spend the money to achieve maximum number of<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VII. Other Topics<br>21. Application <br>Development and <br>Administration<br>786<br>© The McGraw−Hill <br>Companies, 2001<br>21.2<br>Performance Tuning<br>793<br>transactions per second. A reduction of 1 I/O per second saves (price per disk drive) /<br>(access per second per disk). Thus, if a particular page is accessed n times per second,<br>the saving due to keeping it in memory is n times the above value. Storing a page in<br>memory costs (price per MB of memory) / (pages per MB of memory). Thus, the<br>break-even point is<br>n ∗<br>price per disk drive<br>access per second per disk = price per MB of memory<br>pages per MB of memory<br>We can rearrange the equation, and substitute current values for each of the above<br>parameters to get a value for n; if a page is accessed more frequently than this, it is<br>worth buying enough memory to store it. Current disk technology and memory and<br>disk prices give a value of n around 1/300 times per second (or equivalently, once in<br>5 minutes) for pages that are randomly accessed.<br>This reasoning is captured by the rule of thumb called the 5-minute rule: If a page<br>is used more frequently than once in 5 minutes, it should be cached in memory. In<br>other words, it is worth buying enough memory to cache all pages that are accessed<br>at least once in 5 minutes on an average. For data that are accessed less frequently,<br>buy enough disks to support the rate of I/O required for the data.<br>The formula for ﬁnding the break-even point depends on factors, such as the costs<br>of disks and memory, that have changed by factors of 100 or 1000 over the past<br>decade. However, it is interesting to note that the ratios of the changes have been<br>such that the break-even point has remained at roughly 5 minutes; the 5-minute rule<br>has not changed to say, a 1-hour rule or a 1-second rule!<br>For data that are sequentially accessed, signiﬁcantly more pages can be read per<br>second. Assuming 1 MB of data is read at a time, we get the 1-minute rule, which<br>says that sequentially accessed data should be cached in memory if they are used at<br>least once in 1 minute.<br>The rules of thumb take only number of I/O operations into account, and do not<br>consider factors such as response time. Some applications need to keep even infre-<br>quently used data in memory, to support response times that are less than or compa-<br>rable to disk access time.<br>Another aspect of tuning is in whether to use RAID 1 or RAID 5. The answer de-<br>pends on how frequently the data are updated, since RAID 5 is much slower than<br>RAID 1 on random writes: RAID 5 requires 2 reads and 2 writes to execute a single ran-<br>dom write request. If an application performs r random reads and w random writes<br>per second to support a particular throughput, a RAID 5 implementation would re-<br>quire r + 4w I/O operations per second whereas a RAID 1 implementation would<br>require r + w I/O operations per second. We can then calculate the number of disks<br>required to support the required I/O operations per second by dividing the result of<br>the calculation by 100 I/O operations per second (for current generation disks). For<br>many applications, r and w are large enough that the (r + w)/100 disks can easily<br>hold two copies of all the data. For such applications, if RAID 1 is used, the required<br>number of disks is actually less than the required number of disks if RAID 5 is used!<br>Thus RAID 5 is useful only when the data storage requirements are very large, but<br>the I/O rates and data transfer requirements are small, that is, for very large and very<br>“cold” data.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VII. Other Topics<br>21. Application <br>Development and <br>Administration<br>787<br>© The McGraw−Hill <br>Companies, 2001<br>794<br>Chapter 21<br>Application Development and Administration<br>21.2.4<br>Tuning of the Schema<br>Within the constraints of the chosen normal form, it is possible to partition relations<br>vertically. For example, consider the account relation, with the schema<br>account (account-number, branch-name, balance)<br>for which account-number is a key. Within the constraints of the normal forms (BCNF<br>and third normal forms), we can partition the account</span><br><br><span style="background-color: #A0C4FF;" title="Chunk 101 | Start: 2020202 | End: 2040202 | Tokens: 3196"> relation into two relations:<br>account-branch (account-number, branch-name)<br>account-balance (account-number, balance)<br>The two representations are logically equivalent, since account-number is a key, but<br>they have different performance characteristics.<br>If most accesses to account information look at only the account-number and bal-<br>ance, then they can be run against the account-balance relation, and access is likely to be<br>somewhat faster, since the branch-name attribute is not fetched. For the same reason,<br>more tuples of account-balance will ﬁt in the buffer than corresponding tuples of ac-<br>count, again leading to faster performance. This effect would be particularly marked<br>if the branch-name attribute were large. Hence, a schema consisting of account-branch<br>and account-balance would be preferable to a schema consisting of the account relation<br>in this case.<br>On the other hand, if most accesses to account information require both balance and<br>branch-name, using the account relation would be preferable, since the cost of the join<br>of account-balance and account-branch would be avoided. Also, the storage overhead<br>would be lower, since there would be only one relation, and the attribute account-<br>number would not be replicated.<br>Another trick to improve performance is to store a denormalized relation, such<br>as a join of account and depositor, where the information about branch-names and<br>balances is repeated for every account holder. More effort has to be expended to<br>make sure the relation is consistent whenever an update is carried out. However,<br>a query that fetches the names of the customers and the associated balances will<br>be speeded up, since the join of account and depositor will have been precomputed. If<br>such a query is executed frequently, and has to be performed as efﬁciently as possible,<br>the denormalized relation could be beneﬁcial.<br>Materialized views can provide the beneﬁts that denormalized relations provide,<br>at the cost of some extra storage; we describe performance tuning of materialized<br>views in Section 21.2.6. A major advantage to materialized views over denormal-<br>ized relations is that maintaining consistency of redundant data becomes the job of<br>the database system, not the programmer. Thus, materialized views are preferable,<br>whenever they are supported by the database system.<br>Another approach to speed up the computation of the join without materializing<br>it, is to cluster records that would match in the join on the same disk page. We saw<br>such clustered ﬁle organizations in Section 11.7.2.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VII. Other Topics<br>21. Application <br>Development and <br>Administration<br>788<br>© The McGraw−Hill <br>Companies, 2001<br>21.2<br>Performance Tuning<br>795<br>21.2.5<br>Tuning of Indices<br>We can tune the indices in a system to improve performance. If queries are the bottle-<br>neck, we can often speed them up by creating appropriate indices on relations. If<br>updates are the bottleneck, there may be too many indices, which have to be updated<br>when the relations are updated. Removing indices may speed up certain updates.<br>The choice of the type of index also is important. Some database systems support<br>different kinds of indices, such as hash indices and B-tree indices. If range queries<br>are common, B-tree indices are preferable to hash indices. Whether to make an index<br>a clustered index is another tunable parameter. Only one index on a relation can<br>be made clustered, by storing the relation sorted on the index attributes. Generally,<br>the index that beneﬁts the most number of queries and updates should be made<br>clustered.<br>To help identify what indices to create, and which index (if any) on each relation<br>should be clustered, some database systems provide tuning wizards. These tools use<br>the past history of queries and updates (called the workload) to estimate the effects<br>of various indices on the execution time of the queries and updates in the workload.<br>Recommendations on what indices to create are based on these estimates.<br>21.2.6<br>Using Materialized Views<br>Maintaining materialized views can greatly speed up certain types of queries, in par-<br>ticular aggregate queries. Recall the example from Section 14.5 where the total loan<br>amount at each branch (obtained by summing the loan amounts of all loans at the<br>branch) is required frequently. As we saw in that section, creating a materialized view<br>storing the total loan amount for each branch can greatly speed up such queries.<br>Materialized views should be used with care, however, since there is not only a<br>space overhead for storing them but, more important, there is also a time overhead<br>for maintaining materialized views. In the case of immediate view maintenance, if<br>the updates of a transaction affect the materialized view, the materialized view must<br>be updated as part of the same transaction. The transaction may therefore run slower.<br>In the case of deferred view maintenance, the materialized view is updated later;<br>until it is updated, the materialized view may be inconsistent with the database rela-<br>tions. For instance, the materialized view may be brought up-to-date when a query<br>uses the view, or periodically. Using deferred maintenance reduces the burden on<br>update transactions.<br>An important question is, how does one select which materialized views to main-<br>tain? The system administrator can make the selection manually by examining the<br>types of queries in the workload, and ﬁnding out which queries need to run faster<br>and which updates/queries may be executed slower. From the examination, the sys-<br>tem administrator may choose an appropriate set of materialized views. For instance,<br>the administrator may ﬁnd that a certain aggregate is used frequently, and choose to<br>materialize it, or may ﬁnd that a particular join is computed frequently, and choose<br>to materialize it.<br>However, manual choice is tedious for even moderately large sets of query types,<br>and making a good choice may be difﬁcult, since it requires understanding the costs<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VII. Other Topics<br>21. Application <br>Development and <br>Administration<br>789<br>© The McGraw−Hill <br>Companies, 2001<br>796<br>Chapter 21<br>Application Development and Administration<br>of different alternatives; only the query optimizer can estimate the costs with reason-<br>able accuracy, without actually executing the query. Thus a good set of views may<br>only be found by trial and error—that is, by materializing one or more views, run-<br>ning the workload, and measuring the time taken to run the queries in the workload.<br>The administrator repeats the process until a set of views is found that gives accept-<br>able performance.<br>A better alternative is to provide support for selecting materialized views within<br>the database system itself, integrated with the query optimizer. Some database sys-<br>tems, such as Microsoft SQL Server 7.5 and the RedBrick Data Warehouse from In-<br>formix, provide tools to help the database administrator with index and materialized<br>view selection. These tools examine the workload (the history of queries and updates)<br>and suggest indices and views to be materialized. The user may specify the impor-<br>tance of speeding up different queries, which the administrator takes into account<br>when selecting views to materialize.<br>Microsoft’s materialized view selection tool also permits the user to ask “what<br>if” questions, whereby the user can pick a view, and the optimizer then estimates the<br>effect of materializing the view on the total cost of the workload and on the individual<br>costs of different query/update types in the workload.<br>In fact, even automated selection techniques are implemented in a similar manner<br>internally: Different alternatives are tried, and for each the query optimizer estimates<br>the costs and beneﬁts of materializing it.<br>Greedy heuristics for materialized view selection operate roughly this way: They<br>estimate the beneﬁts of materializing different views, and choose the view that gives<br>either the maximum beneﬁt or the maximum beneﬁt per unit space (that is, bene-<br>ﬁt divided by the space required to store the view). Once the heuristic has selected<br>a view, the beneﬁts of other views may have changed, so the heuristic recomputes<br>these, and chooses the next best view for materialization. The process continues until<br>either the available disk space for storing materialized views is exhausted, or the cost<br>of view maintenance increases above acceptable limits.<br>21.2.7<br>Tuning of Transactions<br>In this section, we study two approaches for improving transaction performance:<br>• Improve set orientation<br>• Reduce lock contention<br>In the past, optimizers on many database systems were not particularly good, so how<br>a query was written would have a big inﬂuence on how it was executed, and there-<br>fore on the performance. Today’s advanced optimizers can transform even badly<br>written queries and execute them efﬁciently, so the need for tuning individual queries<br>is less important than it used to be. However, complex queries containing nested sub-<br>queries are not optimized very well by many optimizers. Most systems provide a<br>mechanism to ﬁnd out the exact execution plan for a query; this information can be<br>used to rewrite the query in a form that the optimizer can deal with better.<br>In embedded SQL, if a query is executed frequently with different values for a<br>parameter, it may help to combine the calls into a more set-oriented query that is<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VII. Other Topics<br>21. Application <br>Development and <br>Administration<br>790<br>© The McGraw−Hill <br>Companies, 2001<br>21.2<br>Performance Tuning<br>797<br>executed only once. The costs of communication of SQL queries can be high in client<br>–server systems, so combining the embedded SQL calls is particularly helpful in such<br>systems.<br>For example, consider a program that steps through each department speciﬁed in<br>a list, invoking an embedded SQL query to ﬁnd the total expenses of the department<br>by using the group by construct on a relation expenses(date, employee, department,<br>amount). If the expenses relation does not have a clustered index on department, each<br>such query will result in a scan of the relation. Instead, we can use a single SQL query<br>to ﬁnd total expenses of all departments; the query can be evaluated with a single<br>scan. The relevant departments can then be looked up in this (much smaller) tempo-<br>rary relation containing the aggregate. Even if there is an index that permits efﬁcient<br>access to tuples of a given department, using multiple SQL queries can have a high<br>communication overhead in a client–server system. Communication cost can be re-<br>duced by using a single SQL query, fetching its results to the client side, and then<br>stepping through the results to ﬁnd the required tuples.<br>Another technique used widely in client–server systems to reduce the cost of com-<br>munication and SQL compilation is to use stored procedures, where queries are stored<br>at the server in the form of procedures, which may be precompiled. Clients can in-<br>voke these stored procedures, rather than communicate entire queries.<br>Concurrent execution of different types of transactions can sometimes lead to poor<br>performance because of contention on locks. Consider, for example, a banking data-<br>base. During the day, numerous small update transactions are executed almost con-<br>tinuously. Suppose that a large query that computes statistics on branches is run at<br>the same time. If the query performs a scan on a relation, it may block out all updates<br>on the relation while it runs, and that can have a disastrous effect on the performance<br>of the system.<br>Some database systems—Oracle, for example—permit multiversion concurrency<br>control, whereby queries are executed on a snapshot of the data, and updates can<br>go on concurrently. This feature should be used if available. If it is not available,<br>an alternative option is to execute large queries at times when updates are few or<br>nonexistent. For databases supporting Web sites, there may be no such quiet period<br>for updates.<br>Another alternative is to use weaker levels of consistency, whereby evaluation of<br>the query has a minimal impact on concurrent updates, but the query results are not<br>guaranteed to be consistent. The application semantics determine whether approxi-<br>mate (inconsistent) answers are acceptable.<br>Long update transactions can cause performance problems with system logs, and<br>can increase the time taken to recover from system crashes. If a transaction performs<br>many updates, the system log may become full even before the transaction com-<br>pletes, in which case the transaction will have to be rolled back. If an update transac-<br>tion runs for a long time (even with few updates), it may block deletion of old parts<br>of the log, if the logging system is not well designed. Again, this blocking could lead<br>to the log getting ﬁlled up.<br>To avoid such problems, many database systems impose strict limits on the num-<br>ber of updates that a single transaction can carry out. Even if the system does not<br>impose such limits, it is often helpful to break up a large update transaction into a set<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VII. Other Topics<br>21. Application <br>Development and <br>Administration<br>791<br>© The McGraw−Hill <br>Companies, 2001<br>798<br>Chapter 21<br>Application Development and Administration<br>of smaller update transactions where possible. For example, a transaction that gives<br>a raise to every employee in a large corporation could be split up into a series of<br>small transactions, each of which updates a small range of employee-ids. Such trans-<br>actions are called minibatch transactions. However, minibatch transactions must be<br>used with care. First, if there are concurrent updates on the set of employees, the<br>result of the set of smaller transactions may not be equivalent to that of the single<br>large transaction. Second, if there is a failure, the salaries of some of the employees<br>would have been increased by committed transactions, but salaries of other employ-<br>ees would not. To avoid this problem, as soon as the system recovers from failure, we<br>must execute the transactions remaining in the batch.<br>21.2.8<br>Performance Simulation<br>To test the performance of a database system even before it is installed, we can cre-<br>ate a performance-simulation model of the database system. Each service shown in<br>Figure 21.6, such as the CPU, each disk, the buffer, and the concurrency control, is<br>modeled in the simulation. Instead of modeling details of a service, the simulation<br>model may capture only some aspects of each service, such as the service time—that<br>is, the time taken to ﬁnish processing a request once processing has begun. Thus, the<br>simulation can model a disk access from just the average disk access time.<br>Since requests for a service generally have to wait their turn, each service has an<br>associated queue in the simulation model. A transaction consists of a series of re-<br>quests. The requests are queued up as they arrive, and are serviced according to the<br>policy for that service, such as ﬁrst come, ﬁrst served. The models for services such as<br>CPU and the disks conceptually operate in parallel, to account for the fact that these<br>subsystems operate in parallel in a real system.<br>Once the simulation model for transaction processing is built, the system admin-<br>istrator can run a number of experiments on it. The administrator can use experi-<br>ments with simulated transactions arriving at different rates to ﬁnd how the system<br>would behave under various load conditions. The administrator could run other ex-<br>periments that vary the service times for each service to ﬁnd out how sensitive the<br>performance is to each of them. System parameters, too, can be varied, so that per-<br>formance tuning can be done on the simulation model.<br>21.3<br>Performance Benchmarks<br>As database servers become more standardized, the differentiating factor among the<br>products of different vendors is those products’ performance. Performance bench-<br>marks are suites of tasks that are used to quantify the performance of software sys-<br>tems.<br>21.3.1<br>Suites of Tasks<br>Since most software systems, such as databases, are complex, there is a good deal of<br>variation in their implementation by different vendors. As a result, there is a signiﬁ-<br>cant amount of variation in their performance on different tasks. One system may be<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VII. Other Topics<br>21. Application <br>Development and <br>Administration<br>792<br>© The McGraw−Hill <br>Companies, 2001<br>21.3<br>Performance Benchmarks<br>799<br>the most efﬁcient on a particular task; another may be the most efﬁcient on a differ-<br>ent task. Hence, a single task is usually insufﬁcient to quantify the performance of the<br>system. Instead, the performance of a system is measured by suites of standardized<br>tasks, called performance benchmarks.<br>Combining the performance numbers from multiple tasks must be done with care.<br>Suppose that we have two tasks, T1 and T2, and that we measure the throughput of a<br>system as the number of transactions of each type that run in a given amount of time<br>—say, 1 second. Suppose that system A runs T1 at 99 transactions per second, and<br>that T2 runs at 1 transaction per second. Similarly, let system B run both T1 and T2 at<br>50 transactions per second. Suppose also that a workload has an equal mixture of the<br>two types of transactions.<br>If we took the average of the two pairs of numbers (that is, 99 and 1, versus 50<br>and 50), it might appear that the two systems have equal performance. However, it<br>is wrong to take the averages in this fashion—if we ran 50 transactions of each type,<br>system A would take about 50.5 seconds to ﬁnish, whereas system B would ﬁnish in<br>just 2 seconds!<br>The example shows that a simple measure of performance is misleading if there<br>is more than one type of transaction. The right way to average out the numbers is to<br>take the time to completion for the workload, rather than the average throughput for<br>each transaction type. We can then compute system performance accurately in trans-<br>actions per second for a speciﬁed workload. Thus, system A takes 50.5/100, which is<br>0.505 seconds per transaction, whereas system B takes 0.02 seconds per transaction,<br>on average. In terms of throughput, system A runs at an average of 1.98 transac-<br>tions per second, whereas system B runs at 50 transactions per second. Assuming<br>that transactions of all the types are equally likely, the correct way to average out<br>the throughputs on different transaction types is to take the harmonic mean of the<br>throughputs. The harmonic mean of n throughputs t1, . . . , tn is deﬁned as<br>n<br>1<br>t1 + 1<br>t2 + · · · + 1<br>tn<br>For our example, the harmonic mean for the throughputs in system A is 1.98. For<br>system B, it is 50. Thus, system B is approximately 25 times faster than system A on<br>a workload consisting of an equal mixture of the two example types of transactions.<br>21.3.2<br>Database-Application Classes<br>Online transaction processing (OLTP) and decision support (including online ana-<br>lytical processing (OLAP)) are two broad classes of applications handled by database<br>systems. These two classes of tasks have different requirements. High concurrency<br>and clever techniques to speed up commit processing are required for supporting<br>a high rate of update transactions. On the other hand, good query-evaluation algo-<br>rithms and query optimization are required for decision support. The architecture of<br>some database systems has been tuned to transaction processing; that of others, such<br>as the Teradata DBC series of parallel database systems, has been tuned to decision<br>support. Other vendors try to strike a balance between the two tasks.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VII. Other Topics<br>21. Application <br>Development and <br>Administration<br>793<br>© The McGraw−Hill <br>Companies, 2001<br>800<br>Chapter 21<br>Application Development and Administration<br>Applications usually ha</span><br><br><span style="background-color: #BDB2FF;" title="Chunk 102 | Start: 2040204 | End: 2060204 | Tokens: 3096">ve a mixture of transaction-processing and decision- sup-<br>port requirements. Hence, which database system is best for an application depends<br>on what mix of the two requirements the application has.<br>Suppose that we have throughput numbers for the two classes of applications<br>separately, and the application at hand has a mix of transactions in the two classes.<br>We must be careful even about taking the harmonic mean of the throughput num-<br>bers, because of interference between the transactions. For example, a long-running<br>decision-support transaction may acquire a number of locks, which may prevent all<br>progress of update transactions. The harmonic mean of throughputs should be used<br>only if the transactions do not interfere with one another.<br>21.3.3<br>The TPC Benchmarks<br>The Transaction Processing Performance Council (TPC), has deﬁned a series of<br>benchmark standards for database systems.<br>The TPC benchmarks are deﬁned in great detail. They deﬁne the set of relations<br>and the sizes of the tuples. They deﬁne the number of tuples in the relations not as a<br>ﬁxed number, but rather as a multiple of the number of claimed transactions per sec-<br>ond, to reﬂect that a larger rate of transaction execution is likely to be correlated with<br>a larger number of accounts. The performance metric is throughput, expressed as<br>transactions per second (TPS). When its performance is measured, the system must<br>provide a response time within certain bounds, so that a high throughput cannot be<br>obtained at the cost of very long response times. Further, for business applications,<br>cost is of great importance. Hence, the TPC benchmark also measures performance<br>in terms of price per TPS. A large system may have a high number of transactions<br>per second, but may be expensive (that is, have a high price per TPS). Moreover, a<br>company cannot claim TPC benchmark numbers for its systems without an external<br>audit that ensures that the system faithfully follows the deﬁnition of the benchmark,<br>including full support for the ACID properties of transactions.<br>The ﬁrst in the series was the TPC-A benchmark, which was deﬁned in 1989. This<br>benchmark simulates a typical bank application by a single type of transaction that<br>models cash withdrawal and deposit at a bank teller. The transaction updates sev-<br>eral relations—such as the bank balance, the teller’s balance, and the customer’s<br>balance—and adds a record to an audit trail relation. The benchmark also incorpo-<br>rates communication with terminals, to model the end-to-end performance of the<br>system realistically. The TPC-B benchmark was designed to test the core performance<br>of the database system, along with the operating system on which the system runs.<br>It removes the parts of the TPC-A benchmark that deal with users, communication,<br>and terminals, to focus on the back-end database server. Neither TPC-A nor TPC-B is<br>widely used today.<br>The TPC-C benchmark was designed to model a more complex system than the<br>TPC-A benchmark. The TPC-C benchmark concentrates on the main activities in an<br>order-entry environment, such as entering and delivering orders, recording payments,<br>checking status of orders, and monitoring levels of stock. The TPC-C benchmark is<br>still widely used for transaction processing.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VII. Other Topics<br>21. Application <br>Development and <br>Administration<br>794<br>© The McGraw−Hill <br>Companies, 2001<br>21.3<br>Performance Benchmarks<br>801<br>The TPC-D benchmark was designed to test the performance of database systems<br>on decision-support queries. Decision-support systems are becoming increasingly<br>important today. The TPC-A, TPC-B, and TPC-C benchmarks measure performance<br>on transaction-processing workloads, and should not be used as a measure of per-<br>formance on decision-support queries. The D in TPC-D stands for decision support.<br>The TPC-D benchmark schema models a sales/distribution application, with parts,<br>suppliers, customers, and orders, along with some auxiliary information. The sizes<br>of the relations are deﬁned as a ratio, and database size is the total size of all the rela-<br>tions, expressed in gigabytes. TPC-D at scale factor 1 represents the TPC-D benchmark<br>on a 1-gigabyte database, while scale factor 10 represents a 10-gigabyte database. The<br>benchmark workload consists of a set of 17 SQL queries modeling common tasks ex-<br>ecuted on decision-support systems. Some of the queries make use of complex SQL<br>features, such as aggregation and nested queries.<br>The benchmark’s users soon realized that the various TPC-D queries could be<br>signiﬁcantly speeded up by using materialized views and other redundant informa-<br>tion. There are applications, such as periodic reporting tasks, where the queries are<br>known in advance and materialized view can be carefully selected to speed up the<br>queries. It is necessary, however, to account for the overhead of maintaining materal-<br>ized views.<br>The TPC-R benchmark (where R stands for reporting) is a reﬁnement of the TPC-D<br>benchmark. The schema is the same, but there are 22 queries, of which 16 are from<br>TPC-D. In addition, there are two updates, a set of inserts and a set of deletes. The<br>database running the benchmark is permitted to use materialized views and other<br>redundant information.<br>In contrast the TPC-H benchmark (where H represents ad hoc) uses the same<br>schema and workload as TPC-R but prohibits materialized views and other redun-<br>dant information, and permits indices only on primary and foreign keys. This bench-<br>mark models ad hoc querying where the queries are not known beforehand, so it is<br>not possible to create appropriate materialized views ahead of time.<br>Both TPC-H and TPC-R measure performance in this way: The power test runs<br>the queries and updates one at a time sequentially, and 3600 seconds divided by<br>geometric mean of the execution times of the queries (in seconds) gives a measure<br>of queries per hour. The throughput test runs multiple streams in parallel, with each<br>stream executing all 22 queries. There is also a parallel update stream. Here the total<br>time for the entire run is used to compute the number of queries per hour.<br>The composite query per hour metric, which is the overall metric, is then ob-<br>tained as the square root of the the product of the power and throughput metrics. A<br>composite price/performance metric is deﬁned by dividing the system price by the<br>composite metric.<br>The TPC-W Web commerce benchmark is an end-to-end benchmark that models<br>Web sites having static content (primarily images) and dynamic content generated<br>from a database. Caching of dynamic content is speciﬁcally permitted, since it is very<br>useful for speeding up Web sites. The benchmark models an electronic bookstore,<br>and like other TPC benchmarks, provides for different scale factors. The primary per-<br>formance metrics are Web interactions per second (WIPS) and price per WIPS.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VII. Other Topics<br>21. Application <br>Development and <br>Administration<br>795<br>© The McGraw−Hill <br>Companies, 2001<br>802<br>Chapter 21<br>Application Development and Administration<br>21.3.4<br>The OODB Benchmarks<br>The nature of applications in an object-oriented database, OODB, is different from<br>that of typical transaction-processing applications. Therefore, a different set of bench-<br>marks has been proposed for OODBs. The Object Operations benchmark, version 1,<br>popularly known as the OO1 benchmark, was an early proposal. The OO7 bench-<br>mark follows a philosophy different from that of the TPC benchmarks. The TPC bench-<br>marks provide one or two numbers (in terms of average transactions per second, and<br>transactions per second per dollar); the OO7 benchmark provides a set of numbers,<br>containing a separate benchmark number for each of several different kinds of oper-<br>ations. The reason for this approach is that it is not yet clear what is the typical OODB<br>transaction. It is clear that such a transaction will carry out certain operations, such<br>as traversing a set of connected objects or retrieving all objects in a class, but it is not<br>clear exactly what mix of these operations will be used. Hence, the benchmark pro-<br>vides separate numbers for each class of operations; the numbers can be combined<br>in an appropriate way, depending on the speciﬁc application.<br>21.4<br>Standardization<br>Standards deﬁne the interface of a software system; for example, standards deﬁne the<br>syntax and semantics of a programming language, or the functions in an application-<br>program interface, or even a data model (such as the object-oriented-database stan-<br>dards). Today, database systems are complex, and are often made up of multiple in-<br>dependently created parts that need to interact. For example, client programs may<br>be created independently of back-end systems, but the two must be able to interact<br>with each other. A company that has multiple heterogeneous database systems may<br>need to exchange data between the databases. Given such a scenario, standards play<br>an important role.<br>Formal standards are those developed by a standards organization or by industry<br>groups, through a public process. Dominant products sometimes become de facto<br>standards, in that they become generally accepted as standards without any formal<br>process of recognition. Some formal standards, like many aspects of the SQL-92 and<br>SQL:1999 standards, are anticipatory standards that lead the marketplace; they deﬁne<br>features that vendors then implement in products. In other cases, the standards, or<br>parts of the standards, are reactionary standards, in that they attempt to standardize<br>features that some vendors have already implemented, and that may even have be-<br>come de facto standards. SQL-89 was in many ways reactionary, since it standardized<br>features, such as integrity checking, that were already present in the IBM SAA SQL<br>standard and in other databases.<br>Formal standards committees are typically composed of representatives of the<br>vendors, and members from user groups and standards organizations such as the<br>International Organization for Standardization (ISO) or the American National Stan-<br>dards Institute (ANSI), or professional bodies, such as the Institute of Electrical and<br>Electronics Engineers (IEEE). Formal standards committees meet periodically, and<br>members present proposals for features to be added to or modiﬁed in the standard.<br>After a (usually extended) period of discussion, modiﬁcations to the proposal, and<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VII. Other Topics<br>21. Application <br>Development and <br>Administration<br>796<br>© The McGraw−Hill <br>Companies, 2001<br>21.4<br>Standardization<br>803<br>public review, members vote on whether to accept or reject a feature. Some time after<br>a standard has been deﬁned and implemented, its shortcomings become clear, and<br>new requirements become apparent. The process of updating the standard then be-<br>gins, and a new version of the standard is usually released after a few years. This<br>cycle usually repeats every few years, until eventually (perhaps many years later)<br>the standard becomes technologically irrelevant, or loses its user base.<br>The DBTG CODASYL standard for network databases, formulated by the Database<br>Task Group, was one of the early formal standards for databases. IBM database prod-<br>ucts used to establish de facto standards, since IBM commanded much of the database<br>market. With the growth of relational databases came a number of new entrants in<br>the database business; hence, the need for formal standards arose. In recent years,<br>Microsoft has created a number of speciﬁcations that also have become de facto<br>standards. A notable example is ODBC, which is now used in non-Microsoft envi-<br>ronments. JDBC, whose speciﬁcation was created by Sun Microsystems, is another<br>widely used de facto standard.<br>This section give a very high level overview of different standards, concentrating<br>on the goals of the standard. The bibliographical notes at the end of the chapter pro-<br>vide references to detailed descriptions of the standards mentioned in this section.<br>21.4.1<br>SQL Standards<br>Since SQL is the most widely used query language, much work has been done on<br>standardizing it. ANSI and ISO, with the various database vendors, have played a<br>leading role in this work. The SQL-86 standard was the initial version. The IBM Sys-<br>tems Application Architecture (SAA) standard for SQL was released in 1987. As peo-<br>ple identiﬁed the need for more features, updated versions of the formal SQL stan-<br>dard were developed, called SQL-89 and SQL-92.<br>The latest version of the SQL standard, called SQL:1999, adds a variety of features<br>to SQL. We have seen many of these features in earlier chapters, and will see a few in<br>later chapters. The standard is broken into several parts:<br>• SQL/Framework (Part 1) provides an overview of the standard.<br>• SQL/Foundation (Part 2) deﬁnes the basics of the standard: types, schemas, ta-<br>bles, views, query and update statements, expressions, security model, predi-<br>cates, assignment rules, transaction management and so on.<br>• SQL/CLI (Call Level Interface) (Part 3) deﬁnes application program interfaces<br>to SQL.<br>• SQL/PSM (Persistent Stored Modules) (Part 4) deﬁnes extensions to SQL to<br>make it procedural.<br>• SQL/Bindings (Part 5) deﬁnes standards for embedded SQL for different em-<br>bedding languages.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VII. Other Topics<br>21. Application <br>Development and <br>Administration<br>797<br>© The McGraw−Hill <br>Companies, 2001<br>804<br>Chapter 21<br>Application Development and Administration<br>The SQL:1999 OLAP features (Section 22.2.3) have been speciﬁed as an amendment<br>to the earlier version of the SQL:1999 standard. There are several other parts under<br>development, including<br>• Part 7: SQL/Temporal deals with standards for temporal data.<br>• Part 9: SQL/MED (Management of External Data) deﬁnes standards for in-<br>terfacing an SQL system to external sources. By writing wrappers, system de-<br>signers can treat external data sources, such as ﬁles or data in nonrelational<br>databases, as if they were “foreign” tables.<br>• Part 10: SQL/OLB (Object Language Bindings) deﬁnes standards for embed-<br>ding SQL in Java.<br>The missing numbers (Parts 6 and 8) cover features such as distributed transaction<br>processing and multimedia data, for which there is as yet no agreement on the stan-<br>dards. The multimedia standards propose to cover storage and retrieval of text data,<br>spatial data, and still images.<br>21.4.2<br>Database Connectivity Standards<br>The ODBC standard is a widely used standard for communication between client<br>applications and database systems. ODBC is based on the SQL Call-Level Interface<br>(CLI) standards developed by the X/Open industry consortium and the SQL Access<br>Group, but has several extensions. The ODBC API deﬁnes a CLI, an SQL syntax deﬁ-<br>nition, and rules about permissible sequences of CLI calls. The standard also deﬁnes<br>conformance levels for the CLI and the SQL syntax. For example, the core level of the<br>CLI has commands to connect to a database, to prepare and execute SQL statements,<br>to get back results or status values and to manage transactions. The next level of con-<br>formance (level 1) requires support for catalog information retrieval and some other<br>features over and above the core-level CLI; level 2 requires further features, such as<br>ability to send and retrieve arrays of parameter values and to retrieve more detailed<br>catalog information.<br>ODBC allows a client to connect simultaneously to multiple data sources and to<br>switch among them, but transactions on each are independent; ODBC does not sup-<br>port two-phase commit.<br>A distributed system provides a more general environment than a client–server<br>system. The X/Open consortium has also developed the X/Open XA standards for<br>interoperation of databases. These standards deﬁne transaction-management prim-<br>itives (such as transaction begin, commit, abort, and prepare-to-commit) that com-<br>pliant databases should provide; a transaction manager can invoke these primitives<br>to implement distributed transactions by two-phase commit. The XA standards are<br>independent of the data model and of the speciﬁc interfaces between clients and<br>databases to exchange data. Thus, we can use the XA protocols to implement a dis-<br>tributed transaction system in which a single transaction can access relational as well<br>as object-oriented databases, yet the transaction manager ensures global consistency<br>via two-phase commit.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VII. Other Topics<br>21. Application <br>Development and <br>Administration<br>798<br>© The McGraw−Hill <br>Companies, 2001<br>21.4<br>Standardization<br>805<br>There are many data sources that are not relational databases, and in fact may not<br>be databases at all. Examples are ﬂat ﬁles and email stores. Microsoft’s OLE-DB is<br>a C++ API with goals similar to ODBC, but for nondatabase data sources that may<br>provide only limited querying and update facilities. Just like ODBC, OLE-DB provides<br>constructs for connecting to a data source, starting a session, executing commands,<br>and getting back results in the form of a rowset, which is a set of result rows.<br>However, OLE-DB differes from ODBC in several ways. To support data sources<br>with limited feature support, features in OLE-DB are divided into a number of inter-<br>faces, and a data source may implement only a subset of the interfaces. An OLE-DB<br>program can negotiate with a data source to ﬁnd what interfaces are supported. In<br>ODBC commands are always in SQL. In OLE-DB, commands may be in any language<br>supported by the data source; while some sources may support SQL, or a limited<br>subset of SQL, other sources may provide only simple capabilities such as accessing<br>data in a ﬂat ﬁle, without any query capability. Another major difference of OLE-DB<br>from ODBC is that a rowset is an object that can be shared by multiple applications<br>through shared memory. A rowset object can be updated by one application, and<br>other applications sharing that object would get notiﬁed about the change.<br>The Active Data Objects (ADO) API, also created by Microsoft, provides an easy-<br>to-use interface to the OLE-DB functionality, which can be called from scripting lan-<br>guages, such as VBScript and JScript.<br>21.4.3<br>Object Database Standards<br>Standards in the area of object-oriented databases have so far been driven primarily<br>by OODB vendors. The Object Database Management Group (ODMG) is a group formed<br>by OODB vendors to standardize the data model and language interfaces to OODBs.<br>The C++ language interface speciﬁed by ODMG was discussed in Chapter 8. The<br>ODMG has also speciﬁed a Java interface and a Smalltalk interface.<br>The Object Management Group (OMG) is a consortium of companies, formed with<br>the objective of developing a standard architecture for distributed software applica-<br>tions based on the object-oriented model. OMG brought out the Object Management<br>Architecture (OMA) reference model. The Object Request Broker (ORB) is a component<br>of the OMA architecture that provides message dispatch to distributed objects trans-<br>parently, so the physical location of the object is not important. The Common Object<br>Request Broker Architecture (CORBA) provides a detailed speciﬁcation of the ORB,<br>and includes an Interface Description Language (IDL), which is used to deﬁne the<br>data types used for data interchange. The IDL helps to support data conversion when<br>data are shipped between systems with different data representations.<br>21.4.4<br>XML-Based Standards<br>A wide variety of standards based on XML (see Chapter 10) have been deﬁned for<br>a wide variety of applications. Many of these standards are related to e-commerce.<br>They include standards promulgated by nonproﬁt consortia and corporate-backed<br>efforts to create defacto standards. RosettaNet, which falls into the former category,<br>uses XML-based standards to facilitate supply-chain management in the computer<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VII. Other Topics<br>21. </span><br><br><span style="background-color: #FFC6FF;" title="Chunk 103 | Start: 2060206 | End: 2080206 | Tokens: 3144">Application <br>Development and <br>Administration<br>799<br>© The McGraw−Hill <br>Companies, 2001<br>806<br>Chapter 21<br>Application Development and Administration<br>and information technology industries. Companies such as Commerce One provide<br>Web-based procurement systems, supply-chain management, and electonic market-<br>places (including online auctions). BizTalk is a framework of XML schemas and guide-<br>lines, backed by Microsoft. These and other frameworks deﬁne catalogs, service de-<br>scriptions, invoices, purchase orders, order status requests, shipping bills, and related<br>items.<br>Participants in electronic marketplaces may store data in a variety of database sys-<br>tems. These systems may use different data models, data formats, and data types.<br>Furthermore, there may be semantic differences (metric versus English measure, dis-<br>tinct monetary currencies, and so forth) in the data. Standards for electronic market-<br>places include methods for wrapping each of these heterogeneous systems with an<br>XML schema. These XML wrappers form the basis of a uniﬁed view of data across all<br>of the participants in the marketplace.<br>Simple Object Access Protocol (SOAP) is a remote procedure call standard that uses<br>XML to encode data (both parameters and results), and uses HTTP as the transport<br>protocol; that is, a procedure call becomes an HTTP request. SOAP is backed by the<br>World Wide Web Consortium (W3C) and is gaining wide acceptance in industry<br>(including IBM and Microsoft). SOAP can be used in a variety of applications. For<br>instance, in business-to-business e-commerce, applications running at one site can<br>access data from other sites through SOAP. Microsoft has deﬁned standards for ac-<br>cessing OLAP and mining data with SOAP. (OLAP and data mining are covered in<br>Chapter 22.)<br>The W3C standard query language for XML is called XQuery. As of early 2001 the<br>standard was in working draft stage, and should be ﬁnalized by the end of the year.<br>Earlier XML query languages include Quilt (on which XQuery is based), XML-QL,<br>and XQL.<br>21.5<br>E-Commerce∗∗<br>E-commerce refers to the process of carrying out various activities related to com-<br>merce, through electronic means, primarily through the internet. The types of activi-<br>ties include:<br>• Presale activities, needed to inform the potential buyer about the product or<br>service being sold.<br>• The sale process, which includes negotiations on price and quality of service,<br>and other contractual matters.<br>• The marketplace: When there are multiple sellers and buyers for a product,<br>a marketplace, such as a stock exchange, helps in negotiating the price to be<br>paid for the product. Auctions are used when there is a single seller and mul-<br>tiple buyers, and reverse auctions are used when there is a single buyer and<br>multiple sellers.<br>• Payment for the sale.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VII. Other Topics<br>21. Application <br>Development and <br>Administration<br>800<br>© The McGraw−Hill <br>Companies, 2001<br>21.5<br>E-Commerce∗∗<br>807<br>• Activities related to delivery of the product or service. Some products and<br>services can be delivered over the internet; for others the internet is used only<br>for providing shipping information and for tracking shipments of products.<br>• Customer support and postsale service.<br>Databases are used extensively to support these activities. For some of the activi-<br>ties the use of databases is straightforward, but there are interesting application de-<br>velopment issues for the other activities.<br>21.5.1<br>E-Catalogs<br>Any e-commerce site provides users with a catalog of the products and services that<br>the site supplies. The services provided by an e-catalog may vary considerably.<br>At the minimum, an e-catalog must provide browsing and search facilities to help<br>customers ﬁnd the product they are looking for. To help with browsing, products<br>should be organized into an intuitive hierarchy, so a few clicks on hyperlinks can<br>lead a customer to the products they are interested in. Keywords provided by the<br>customer (for example, “digital camera” or “computer”) should speed up the process<br>of ﬁnding required products. E-catalogs should also provide a means for customers<br>to easily compare alternatives from which to choose among competing products.<br>E-catalogs can be customized for the customer. For instance, a retailer may have<br>an agreement with a large company to supply some products at a discount. An em-<br>ployee of the company, viewing the catalog to purchase products for the company,<br>should see prices as per the negotiated discount, instead of the regular prices. Be-<br>cause of legal restrictions on sales of some types of items, customers who are under-<br>age, or from certain states or countries, should not be shown items that cannot be<br>legally sold to them. Catalogs can also be personalized to individual users, on the<br>basis of past buying history. For instance, frequent customers may be offered special<br>discounts on some items.<br>Supporting such customization requires customer information as well as special<br>pricing/discount information and sales restriction information to be stored in a data-<br>base. There are also challenges in supporting very high transaction rates, which are<br>often tackled by caching of query results or generated Web pages.<br>21.5.2<br>Marketplaces<br>When there are multiple sellers or multiple buyers (or both) for a product, a market-<br>place helps in negotiating the price to be paid for the product. There are several dif-<br>ferent types of marketplaces:<br>• In a reverse auction system a buyer states requirements, and sellers bid for<br>supplying the item. The supplier quoting the lowest price wins. In a closed<br>bidding system, the bids are not made public, whereas in an open bidding<br>system the bids are made public.<br>• In an auction there are multiple buyers and a single seller. For simplicity, as-<br>sume that there is only one instance of each item being sold. Buyers bid for<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VII. Other Topics<br>21. Application <br>Development and <br>Administration<br>801<br>© The McGraw−Hill <br>Companies, 2001<br>808<br>Chapter 21<br>Application Development and Administration<br>the items being sold, and the highest bidder for an item gets to buy the item<br>at the bid price.<br>When there are multiple copies of an item, things become more compli-<br>cated: Suppose there are four items, and one bidder may want three copies<br>for $10 each, while another wants two copies for $13 each. It is not possible<br>to satisfy both bids. If the items will be of no value if they are not sold (for<br>instance, airline seats, which must be sold before the plane leaves), the seller<br>simply picks a set of bids that maximizes the income. Otherwise the decision<br>is more complicated.<br>• In an exchange, such as a stock exchange, there are multiple sellers and mul-<br>tiple buyers. Buyers can specify the maximum price they are willing to pay,<br>while sellers specify the minimum price they want. There is usually a market<br>maker who matches buy and sell bids, deciding on the price for each trade (for<br>instance, at the price of the sell bid).<br>There are other more complex types of marketplaces.<br>Among the database issues in handling marketplaces are these:<br>• Bidders need to be authenticated before they are allowed to bid.<br>• Bids (buy or sell) need to be recorded securely in a database. Bids need to be<br>communicated quickly to other people involved in the marketplace (such as<br>all the buyers or all the sellers), who may be numerous.<br>• Delays in broadcasting bids can lead to ﬁnancial losses to some participants.<br>• The volumes of trades may be extremely large at times of stock market volatil-<br>ity, or toward the end of auctions. Thus, very high performance databases with<br>large degrees of parallelism are used for such systems.<br>21.5.3<br>Order Settlement<br>After items have been selected (perhaps through an electronic catalog), and the price<br>determined (perhaps by an electronic marketplace), the order has to be settled. Set-<br>tlement involves payment for goods and the delivery of the goods.<br>A simple but unsecure way of paying electronically is to send a credit card number.<br>There are two major problems. First, credit card fraud is possible. When a buyer pays<br>for physical goods, companies can ensure that the address for delivery matches the<br>card holder’s address, so no one else can receive the goods, but for goods delivered<br>electronically no such check is possible. Second, the seller has to be trusted to bill only<br>for the agreed-on item and to not pass on the card number to unauthorized people<br>who may misuse it.<br>Several protocols are available for secure payments that avoid both the problems<br>listed above. In addition, they provide for better privacy, whereby the seller may not<br>be given any unnecessary details about the buyer, and the credit card company is not<br>provided any unnecessary information about the items purchased. All information<br>transmitted must be encrypted so that anyone intercepting the data on the network<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VII. Other Topics<br>21. Application <br>Development and <br>Administration<br>802<br>© The McGraw−Hill <br>Companies, 2001<br>21.6<br>Legacy Systems<br>809<br>cannot ﬁnd out the contents. Public/private key encryption is widely used for this<br>task.<br>The protocols must also prevent person-in-the-middle attacks, where someone<br>can impersonate the bank or credit-card company, or even the seller, or buyer, and<br>steal secret information. Impersonation can be perpetrated by passing off a fake key<br>as someone else’s public key (the bank’s or credit-card company’s, or the merchant’s<br>or the buyer’s). Impersonation is prevented by a system of digital certiﬁcates, where-<br>by public keys are signed by a certiﬁcation agency, whose public key is well known<br>(or which in turn has its public key certiﬁed by another certiﬁcation agency and so<br>on up to a key that is well known). From the well-known public key, the system can<br>authenticate the other keys by checking the certiﬁcates in reverse sequence.<br>The Secure Electronic Transaction (SET) protocol is one such secure payment pro-<br>tocol. The protocol requires several rounds of communication between the buyer,<br>seller, and the bank, in order to guarantee safety of the transaction.<br>There are also systems that provide for greater anonymity, similar to that pro-<br>vided by physical cash. The DigiCash payment system is one such system. When<br>a payment is made in such a system, it is not possible to identify the purchaser. In<br>contrast, identifying purchasers is very easy with credit cards, and even in the case<br>of SET, it is possible to identify the purchaser with the cooperation of the credit card<br>company or bank.<br>21.6<br>Legacy Systems<br>Legacy systems are older-generation systems that are incompatible with current-<br>generation standards and systems. Such systems may still contain valuable data, and<br>may support critical applications. The legacy systems of today are typically those<br>built with technologies such as databases that use the network or hierarchical data<br>models, or use Cobol and ﬁle systems without a database.<br>Porting legacy applications to a more modern environment is often costly in terms<br>of both time and money, since they are often very large, consisting of millions of lines<br>of code developed by teams of programmers, over several decades.<br>Thus, it is important to support these older-generation, or legacy, systems, and<br>to facilitate their interoperation with newer systems. One approach used to interop-<br>erate between relational databases and legacy databases is to build a layer, called a<br>wrapper, on top of the legacy systems that can make the legacy system appear to be<br>a relational database. The wrapper may provide support for ODBC or other intercon-<br>nection standards such as OLE-DB, which can be used to query and update the legacy<br>system. The wrapper is responsible for converting relational queries and updates into<br>queries and updates on the legacy system.<br>When an organization decides to replace a legacy system by a new system, it must<br>follow a process called reverse engineering, which consists of going over the code<br>of the legacy system to come up with schema designs in the required data model<br>(such as an E-R model or an object-oriented data model). Reverse engineering also<br>examines the code to ﬁnd out what procedures and processes were implemented, in<br>order to get a high-level model of the system. Reverse engineering is needed because<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VII. Other Topics<br>21. Application <br>Development and <br>Administration<br>803<br>© The McGraw−Hill <br>Companies, 2001<br>810<br>Chapter 21<br>Application Development and Administration<br>legacy systems usually do not have high-level documentation of their schema and<br>overall system design. When coming up with the design of a new system, the design<br>is reviewed, so that it can be improved rather than just reimplemented as is. Exten-<br>sive coding is required to support all the functionality (such as user interface and<br>reporting systems) that were provided by the legacy system. The overall process is<br>called re-engineering.<br>When a new system has been built and tested, the system must be populated<br>with data from the legacy system, and all further activities must be carried out on<br>the new system. However, abruptly transitioning to a new system, which is called<br>the big-bang approach, carries several risks. First, users may not be familiar with the<br>interfaces of the new system. Second there may be bugs or performance problems<br>in the new system that were not discovered when it was tested. Such problems may<br>lead to great losses for companies, since their ability to carry out critical transactions<br>such as sales and purchases may be severely affected. In some extreme cases the new<br>system has even been abandoned, and the legacy system reused, after an attempted<br>switchover failed.<br>An alternative approach, called the chicken-little approach, incrementally replaces<br>the functionality of the legacy system. For example, the new user interfaces may be<br>used with the old system in the back end, or vice versa. Another option is to use<br>the new system only for some functionality that can be decoupled from the legacy<br>system. In either case, the legacy and new systems coexist for some time. There is<br>therefore a need for developing and using wrappers on the legacy system to provide<br>required functionality to interoperate with the new system. This approach, therefore<br>has a higher development cost associated with it.<br>21.7<br>Summary<br>• The Web browser has emerged as the most widely used user interface to<br>databases. HTML provides the ability to deﬁne interfaces that combine hyper-<br>links with forms facilities. Web browsers communicate with Web servers by<br>the HTTP protocol. Web servers can pass on requests to application programs,<br>and return the results to the browser.<br>• There are several client-side scripting languages—Javascript is the most widely<br>used—that provide richer user interaction at the browser end.<br>• Web servers execute application programs to implement desired functionality.<br>Servlets are a widely used mechanism to write application programs that run<br>as part of the Web server process, in order to reduce overheads. There are also<br>many server-side scripting languages that are interpreted by the Web server<br>and provide application program functionality as part of the Web server.<br>• Tuning of the database-system parameters, as well as the higher-level database<br>design—such as the schema, indices, and transactions—is important for good<br>performance. Tuning is best done by identifying bottlenecks and eliminating<br>them.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VII. Other Topics<br>21. Application <br>Development and <br>Administration<br>804<br>© The McGraw−Hill <br>Companies, 2001<br>21.7<br>Summary<br>811<br>• Performance benchmarks play an important role in comparisons of database<br>systems, especially as systems become more standards compliant. The TPC<br>benchmark suites are widely used, and the different TPC benchmarks are use-<br>ful for comparing the performance of databases under different workloads.<br>• Standards are important because of the complexity of database systems and<br>their need for interoperation. Formal standards exist for SQL. Defacto stan-<br>dards, such as ODBC and JDBC, and standards adopted by industry groups,<br>such as CORBA, have played an important role in the growth of client–server<br>database systems. Standards for object-oriented databases, such as ODMG, are<br>being developed by industry groups.<br>• E-commerce systems are fast becoming a core part of how commerce is per-<br>formed. There are several database issues in e-commerce systems. Catalog<br>management, especially personalization of the catalog, is done with databases.<br>Electronic marketplaces help in pricing of products through auctions, reverse<br>auctions, or exchanges. High-performance database systems are needed to<br>handle such trading. Orders are settled by electronic payment systems, which<br>also need high-performance database systems to handle very high transaction<br>rates.<br>• Legacy systems are systems based on older-generation technologies such as<br>nonrelational databases or even directly on ﬁle systems. Interfacing legacy<br>systems with new-generation systems is often important when they run<br>mission-critical systems. Migrating from legacy systems to new-generation<br>systems must be done carefully to avoid disruptions, which can be very ex-<br>pensive.<br>Review Terms<br>• Web interfaces to databases<br>• HyperText Markup Language<br>(HTML)<br>• Hyperlinks<br>• Uniform resource locator (URL)<br>• Client-side scripting<br>• Applets<br>• Client-side scripting language<br>• Web servers<br>• Session<br>• HyperText Transfer Protocol<br>(HTTP)<br>• Common Gateway Interface<br>(CGI)<br>• Connectionless<br>• Cookie<br>• Servlets<br>• Server-side scripting<br>• Performance tuning<br>• Bottlenecks<br>• Queueing systems<br>• Tunable parameters<br>• Tuning of hardware<br>• Five-minute rule<br>• One-minute rule<br>• Tuning of the schema<br>• Tuning of indices<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VII. Other Topics<br>21. Application <br>Development and <br>Administration<br>805<br>© The McGraw−Hill <br>Companies, 2001<br>812<br>Chapter 21<br>Application Development and Administration<br>• Materialized views<br>• Immediate view maintenance<br>• Deferred view maintenance<br>• Tuning of transactions<br>• Improving set orientedness<br>• Minibatch transactions<br>• Performance simulation<br>• Performance benchmarks<br>• Service time<br>• Time to completion<br>• Database-application classes<br>• The TPC benchmarks<br>  TPC-A<br>  TPC-B<br>  TPC-C<br>  TPC-D<br>  TPC-R<br>  TPC-H<br>  TPC-W<br>• Web interactions per second<br>• OODB benchmarks<br>  OO1<br>  OO7<br>• Standardization<br>  Formal standards<br>  De facto standards<br>  Anticipatory standards<br>  Reactionary standards<br>• Database connectivity standards<br>  ODBC<br>  OLE-DB<br>  X/Open XA standards<br>• Object database standards<br>  ODMG<br>  CORBA<br>• XML-based standards<br>• E-commerce<br>• E-catalogs<br>• Marketplaces<br>  Auctions<br>  Reverse-auctions<br>  Exchange<br>• Order settlement<br>• Digital certiﬁcates<br>• Legacy systems<br>• Reverse engineering<br>• Re-engineering<br>Exercises<br>21.1 What is the main reason why servlets give better performance than programs<br>that use the common gateway interface (CGI), even though Java programs gen-<br>erally run slower than C or C++ programs.<br>21.2 List some beneﬁts and drawbacks of connectionless protocols over protocols<br>that maintain connections.<br>21.3 List three ways in which caching can be used to speed up Web server perfor-<br>mance.<br>21.4<br>a. What are the three broad levels at which a database system can be tuned<br>to improve performance?<br>b. Give two examples of how tuning can be done, for each of the levels.<br>21.5 What is the motivation for splitting a long transaction into a series of small<br>ones? What problems could arise as a result, and how can these problems be<br>averted?<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VII. Other Topics<br>21. Application <br>Development and <br>Administration<br>806<br>© The McGraw−Hill <br>Companies, 2001<br>Exercises<br>813<br>21.6 Suppose a system runs three types of transactions. T</span><br><br><span style="background-color: #FFADAD;" title="Chunk 104 | Start: 2080208 | End: 2100208 | Tokens: 3130">ransactions of type A run<br>at the rate of 50 per second, transactions of type B run at 100 per second, and<br>transactions of type C run at 200 per second. Suppose the mix of transactions<br>has 25 percent of type A, 25 percent of type B, and 50 percent of type C.<br>a. What is the average transaction throughput of the system, assuming there<br>is no interference between the transactions.<br>b. What factors may result in interference between the transactions of differ-<br>ent types, leading to the calculated throughput being incorrect?<br>21.7 Suppose the price of memory falls by half, and the speed of disk access (num-<br>ber of accesses per second) doubles, while all other factors remain the same.<br>What would be the effect of this change on the 5 minute and 1 minute rule?<br>21.8 List some of the features of the TPC benchmarks that help make them realistic<br>and dependable measures.<br>21.9 Why was the TPC-D benchmark replaced by the TPC-H and TPC-R benchmarks?<br>21.10 List some beneﬁts and drawbacks of an anticipatory standard compared to a<br>reactionary standard.<br>21.11 Suppose someone impersonates a company and gets a certiﬁcate from a certiﬁ-<br>cate issuing authority. What is the effect on things (such as purchase orders or<br>programs) certiﬁed by the impersonated company, and on things certiﬁed by<br>other companies?<br>Project Suggestions<br>Each of the following is a large project, which can be a semester-long project done by<br>a group of students. The difﬁculty of the project can be adjusted easily by adding or<br>deleting features.<br>Project 21.1 Consider the E-R schema of Exercise 2.7 (Chapter 2), which represents<br>information about teams in a league. Design and implement a Web-based sys-<br>tem to enter, update, and view the data.<br>Project 21.2 Design and implement a shopping cart system that lets shoppers collect<br>items into a shopping cart (you can decide what information is to be supplied<br>for each item) and purchased together. You can extend and use the E-R schema<br>of Exercise 2.12 of Chapter 2. You should check for availability of the item and<br>deal with nonavailable items as you feel appropriate.<br>Project 21.3 Design and implement a Web-based system to record student registra-<br>tion and grade information for courses at a university.<br>Project 21.4 Design and implement a system that permits recording of course perfor-<br>mance information—speciﬁcally, the marks given to each student in each as-<br>signment or exam of a course, and computation of a (weighted) sum of marks<br>to get the total course marks. The number of assignments/exams should not<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VII. Other Topics<br>21. Application <br>Development and <br>Administration<br>807<br>© The McGraw−Hill <br>Companies, 2001<br>814<br>Chapter 21<br>Application Development and Administration<br>be predeﬁned; that is, more assignments/exams can be added at any time. The<br>system should also support grading, permitting cutoffs to be speciﬁed for var-<br>ious grades.<br>You may also wish to integrate it with the student registration system of<br>Project 21.3 (perhaps being implemented by another project team).<br>Project 21.5 Design and implement a Web-based system for booking classrooms at<br>your university. Periodic booking (ﬁxed days/times each week for a whole<br>semester) must be supported. Cancellation of speciﬁc lectures in a periodic<br>booking should also be supported.<br>You may also wish to integrate it with the student registration system of<br>Project 21.3 (perhaps being implemented by another project team) so that class-<br>rooms can be booked for courses, and cancellations of a lecture or extra lectures<br>can be noted at a single interface, and will be reﬂected in the classroom booking<br>and communicated to students via e-mail.<br>Project 21.6 Design and implement a system for managing online multiple-choice<br>tests. You should support distributed contribution of questions (by teaching<br>assistants, for example), editing of questions by whoever is in charge of the<br>course, and creation of tests from the available set of questions. You should<br>also be able to administer tests online, either at a ﬁxed time for all students, or<br>at any time but with a time limit from start to ﬁnish (support one or both), and<br>give students feedback on their scores at the end of the allotted time.<br>Project 21.7 Design and implement a system for managing e-mail customer service.<br>Incoming mail goes to a common pool. There is a set of customer service agents<br>who reply to e-mail. If the e-mail is part of an ongoing series of replies (tracked<br>using the in-reply-to ﬁeld of e-mail) the mail should preferably be replied to<br>by the same agent who replied earlier. The system should track all incoming<br>mail and replies, so an agent can see the history of questions from a customer<br>before replying to an email.<br>Project 21.8 Design and implement a simple electronic marketplace where items can<br>be listed for sale or for purchase under various categories (which should form<br>a hierarchy). You may also wish to support alerting services, whereby a user<br>can register interest in items in a particular category, perhaps with other con-<br>straints as well, without publicly advertising his/her interest, and is notiﬁed<br>when such an item is listed for sale.<br>Project 21.9 Design and implement a Web-based newsgroup system. Users should<br>be able to subscribe to newsgroups, and browse articles in newsgroups. The<br>system tracks which articles were read by a user, so they are not displayed<br>again. Also provide search against old articles.<br>You may also wish to provide a rating service for articles, so that articles<br>with high rating are highlighted permitting the busy reader to skip low-rated<br>articles.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VII. Other Topics<br>21. Application <br>Development and <br>Administration<br>808<br>© The McGraw−Hill <br>Companies, 2001<br>Bibliographical Notes<br>815<br>Project 21.10 Design and implement a Web-based system for managing a sports “lad-<br>der.” Many people register, and may be given some initial rankings (perhaps<br>based on past performance). Anyone can challenge anyone else to a match, and<br>the rankings are adjusted according to the result.<br>One simple system for adjusting rankings just moves the winner ahead of<br>the loser in the rank order, in case the winner was behind earlier. You can try<br>to invent more complicated rank adjustment systems.<br>Project 21.11 Design and implement a publications listing service. The service should<br>permit entering of information about publications, such as title, authors, year,<br>where the publication appeared, pages, and so forth. Authors should be a sep-<br>arate entity with attributes such as name, institution, department, e-mail, ad-<br>dress, and home page.<br>Your application should support multiple views on the same data. For in-<br>stance, you should provide all publications by a given author (sorted by year,<br>for example), or all publications by authors from a given institution or depart-<br>ment. You should also support search by keywords, on the overall database as<br>well as within each of the views.<br>Bibliographical Notes<br>Information about servlets, including tutorials, standard speciﬁcations, and software,<br>is available on java.sun.com/products/servlet. Information about JSP is available at<br>java.sun.com/products/jsp.<br>An early proposal for a database-system benchmark (the Wisconsin benchmark)<br>was made by Bitton et al. [1983]. The TPC-A,-B, and -C benchmarks are described<br>in Gray [1991]. An online version of all the TPC benchmarks descriptions, as well<br>as benchmark results, is available on the World Wide Web at the URL www.tpc.org;<br>the site also contains up-to-date information about new benchmark proposals. Poess<br>and Floyd [2000] gives an overview of the TPC-H, TPC-R, and TPC-W benchmarks.<br>The OO1 benchmark for OODBs is described in Cattell and Skeen [1992]; the OO7<br>benchmark is described in Carey et al. [1993].<br>Kleinrock [1975] and Kleinrock [1976] is a popular two-volume textbook on queue-<br>ing theory.<br>Shasha [1992] provides a good overview of database tuning. O’Neil and O’Neil<br>[2000] provides a very good textbook coverage of performance measurement and<br>tuning. The ﬁve minute and one minute rules are described in Gray and Putzolu<br>[1987] and Gray and Graefe [1997]. Brown et al. [1994] describes an approach to<br>automated tuning. Index selection and materialized view selection are addressed<br>by Ross et al. [1996], Labio et al. [1997], Gupta [1997], Chaudhuri and Narasayya<br>[1997], Agrawal et al. [2000] and Mistry et al. [2001].<br>The American National Standard SQL-86 is described in ANSI [1986]. The IBM<br>Systems Application Architecture deﬁnition of SQL is speciﬁed by IBM [1987]. The<br>standards for SQL-89 and SQL-92 are available as ANSI [1989] and ANSI [1992] re-<br>spectively. For references on the SQL:1999 standard, see the bibliographical notes of<br>Chapter 9.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VII. Other Topics<br>21. Application <br>Development and <br>Administration<br>809<br>© The McGraw−Hill <br>Companies, 2001<br>816<br>Chapter 21<br>Application Development and Administration<br>The X/Open SQL call-level interface is deﬁned in X/Open [1993]; the ODBC API is<br>described in Microsoft [1997] and Sanders [1998]. The X/Open XA interface is deﬁned<br>in X/Open [1991]. More information about ODBC, OLE-DB, and ADO can be found<br>on the Web site www.microsoft.com/data, and in a number of books on the subject<br>that can be found through www.amazon.com. The ODMG 3.0 standard is deﬁned in<br>Cattell [2000]. ACM Sigmod Record, which is published quarterly, has a regular section<br>on standards in databases, including benchmark standards.<br>A wealth of information on XML based standards is available online. You can use<br>a Web search engine such as Google to search for more detailed and up-to-date infor-<br>mation about the XML and other standards.<br>Loeb [1998] provides a detailed description of secure electronic transactions. Busi-<br>ness process reengineering is covered by Cook [1996]. Kirchmer [1999] describes ap-<br>plication implementation using standard software such as Enterprise Resource Plan-<br>ning (ERP) packages. Umar [1997] covers reengineering and issues in dealing with<br>legacy systems.<br>Tools<br>There are many Web development tools that support database connectivity through<br>servlets, JSP, Javascript, or other mechanisms. We list a few of the better-known ones<br>here: Java SDK from Sun (java.sun.com), Apache’s Tomcat (jakarta.apache.org) and<br>Web server (apache.org), IBM WebSphere (www.software.ibm.com), Microsoft’s ASP<br>tools (www.microsoft.com), Allaire’s Coldfusion and JRun products (www.allaire.com),<br>Caucho’s Resin (www.caucho.com), and Zope (www.zope.org). A few of these, such<br>as Apache, are free for any use, some are free for noncommercial use or for per-<br>sonal use, while others need to be paid for. See the respective Web sites for more<br>information.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VII. Other Topics<br>22. Advanced Querying and <br>Information Retrieval<br>810<br>© The McGraw−Hill <br>Companies, 2001<br>C<br>H<br>A<br>P<br>T<br>E<br>R<br>2<br>2<br>Advanced Querying and<br>Information Retrieval<br>Businesses have begun to exploit the burgeoning data online to make better decisions<br>about their activities, such as what items to stock and how best to target customers<br>to increase sales. Many of their queries are rather complicated, however, and certain<br>types of information cannot be extracted even by using SQL.<br>Several techniques and tools are available to help with decision support. Several<br>tools for data analysis allow analysts to view data in different ways. Other analy-<br>sis tools precompute summaries of very large amounts of data, in order to give fast<br>responses to queries. The SQL:1999 standard now contains additional constructs to<br>support data analysis. Another approach to getting knowledge from data is to use<br>data mining, which aims at detecting various types of patterns in large volumes of<br>data. Data mining supplements various types of statistical techniques with similar<br>goals.<br>Textual data, too, has grown explosively. Textual data is unstructured, unlike the<br>rigidly structured data in relational databases. Querying of unstructured textual data<br>is referred to as information retrieval. Information retrieval systems have much in com-<br>mon with database systems—in particular, the storage and retrieval of data on sec-<br>ondary storage. However, the emphasis in the ﬁeld of information systems is differ-<br>ent from that in database systems, concentrating on issues such as querying based on<br>keywords; the relevance of documents to the query; and the analysis, classiﬁcation,<br>and indexing of documents.<br>This chapter covers decision support, including online analytical processing and<br>data mining and information retrieval.<br>22.1<br>Decision-Support Systems<br>Database applications can be broadly classiﬁed into transaction processing and de-<br>cision support, as we have seen earlier in Section 21.3.2. Transaction-processing sys-<br>tems are widely used today, and companies have accumulated a vast amount of in-<br>formation generated by these systems.<br>817<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VII. Other Topics<br>22. Advanced Querying and <br>Information Retrieval<br>811<br>© The McGraw−Hill <br>Companies, 2001<br>818<br>Chapter 22<br>Advanced Querying and Information Retrieval<br>For example, company databases often contain enormous quantities of informa-<br>tion about customers and transactions. The size of the information storage required<br>may range up to hundreds of gigabytes, or even terabytes, for large retail chains.<br>Transaction information for a retailer may include the name or identiﬁer (such as<br>credit-card number) of the customer, the items purchased, the price paid, and the<br>dates on which the purchases were made. Information about the items purchased<br>may include the name of the item, the manufacturer, the model number, the color,<br>and the size. Customer information may include credit history, annual income, resi-<br>dence, age, and even educational background.<br>Such large databases can be treasure troves of information for making business<br>decisions, such as what items to stock and what discounts to offer. For instance, a<br>retail company may notice a sudden spurt in purchases of ﬂannel shirts in the Paciﬁc<br>Northwest, may realize that there is a trend, and may start stocking a larger number<br>of such shirts in shops in that area. As another example, a car company may ﬁnd, on<br>querying its database, that most of its small sports cars are bought by young women<br>whose annual incomes are above $50,000. The company may then target its market-<br>ing to attract more such women to buy its small sports cars, and may avoid wasting<br>money trying to attract other categories of people to buy those cars. In both cases, the<br>company has identiﬁed patterns in customer behavior, and has used the patterns to<br>make business decisions.<br>The storage and retrieval of data for decision support raises several issues:<br>• Although many decision support queries can be written in SQL, others either<br>cannot be expressed in SQL or cannot be expressed easily in SQL. Several SQL<br>extensions have therefore been proposed to make data analysis easier. The<br>area of online analytical processing (OLAP) deals with tools and techniques for<br>data analysis that can give nearly instantaneous answers to queries request-<br>ing summarized data, even though the database may be extremely large. In<br>Section 22.2, we study SQL extensions for data analysis, and techniques for<br>online analytical processing.<br>• Database query languages are not suited to the performance of detailed sta-<br>tistical analyses of data. There are several packages, such as SAS and S++, that<br>help in statistical analysis. Such packages have been interfaced with databases,<br>to allow large volumes of data to be stored in the database and retrieved efﬁ-<br>ciently for analysis. The ﬁeld of statistical analysis is a large discipline on its<br>own; see the references in the bibliographical notes for more information.<br>• Knowledge-discovery techniques attempt to discover automatically statisti-<br>cal rules and patterns from data. The ﬁeld of data mining combines knowledge<br>discovery techniques invented by artiﬁcial intelligence researchers and statis-<br>tical analysts, with efﬁcient implementation techniques that enable them to be<br>used on extremely large databases. Section 22.3 discusses data mining.<br>• Large companies have diverse sources of data that they need to use for making<br>business decisions. The sources may store the data under different schemas.<br>For performance reasons (as well as for reasons of organization control), the<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VII. Other Topics<br>22. Advanced Querying and <br>Information Retrieval<br>812<br>© The McGraw−Hill <br>Companies, 2001<br>22.2<br>Data Analysis and OLAP<br>819<br>data sources usually will not permit other parts of the company to retrieve<br>data on demand.<br>To execute queries efﬁciently on such diverse data, companies have built<br>data warehouses. Data warehouses gather data from multiple sources under a<br>uniﬁed schema, at a single site. Thus, they provide the user a single uniform<br>interface to data. We study issues in building and maintaining a data ware-<br>house in Section 22.4.<br>The area of decision support can be broadly viewed as covering all the above<br>areas, although some people use the term in a narrower sense that excludes statistical<br>analysis and data mining.<br>22.2<br>Data Analysis and OLAP<br>Although complex statistical analysis is best left to statistics packages, databases<br>should support simple, commonly used, forms of data analysis. Since the data stored<br>in databases are usually large in volume, they need to be summarized in some fash-<br>ion if we are to derive information that humans can use.<br>OLAP tools support interactive analysis of summary information. Several SQL ex-<br>tensions have been developed to support OLAP tools. There are many commonly<br>used tasks that cannot be done using the basic SQL aggregation and grouping facili-<br>ties. Examples include ﬁnding percentiles, or cumulative distributions, or aggregates<br>over sliding windows on sequentially ordered data. A number of extensions of SQL<br>have been recently proposed to support such tasks, and implemented in products<br>such as Oracle and IBM DB2.<br>22.2.1<br>Online Analytical Processing<br>Statistical analysis often requires grouping on multiple attributes. Consider an ap-<br>plication where a shop wants to ﬁnd out what kinds of clothes are popular. Let us<br>suppose that clothes are characterized by their item-name, color, and size, and that<br>we have a relation sales with the schema sales(item-name, color, size, number). Suppose<br>that item-name can take on the values (skirt, dress, shirt, pant), color can take on the<br>values (dark, pastel, white), and size can take on values (small, medium, large).<br>Given a relation used for data analysis, we can identify some of its attributes as<br>measure attributes, since they measure some value, and can be aggregated upon. For<br>instance, the attribute number of the sales relation is a measure attribute, since it mea-<br>sures the number of units sold. Some (or all) of the other attributes of the relation<br>are identiﬁed as dimension attributes, since they deﬁne the dimensions on which<br>measure attributes, and summaries of measure attributes, are viewed. In the sales re-<br>lation, item-name, color, and size are dimension attributes. (A more realistic version of<br>the sales relation would have additional dimensions, such as time and sales location,<br>and additional measures such as monetary value of the sale.)<br>Data that can be modeled as dimension attributes and measure attributes are called<br>multidimensional data.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VII. Other Topics<br>22. Advanced Querying and <br>Information Retrieval<br>813<br>© The McGraw−Hill <br>Companies, 2001<br>820<br>Chapter 22<br>Advanced Querying and Information Retrieval<br>size:<br>all<br>item-name<br>color<br>dark<br>pastel<br>white<br>Total<br>skirt<br>8<br>35<br>10<br>53<br>dress<br>20<br>10<br>5<br>35<br>shirt<br>14<br>7<br>28<br>49<br>pant<br>20<br>2<br>5<br>27<br>Total<br>62<br>54<br>48<br>164<br>Figure 22.</span><br><br><span style="background-color: #FFD6A5;" title="Chunk 105 | Start: 2100210 | End: 2120210 | Tokens: 3304">1<br>Cross tabulation of sales by item-name and color.<br>To analyze the multidimensional data, a manager may want to see data laid out<br>as shown in the table in Figure 22.1. The table shows total numbers for different<br>combinations of item-name and color. The value of size is speciﬁed to be all, indicating<br>that the displayed values are a summary across all values of size.<br>The table in Figure 22.1 is an example of a cross-tabulation (or cross-tab, for short),<br>also referred to as a pivot-table. In general, a cross-tab is a table where values for one<br>attribute (say A) form the row headers, values for another attribute (say B) form the<br>column headers, and the values in an individual cell are derived as follows. Each cell<br>can be identiﬁed by (ai, bj), where ai is a value for A and bj a value for B. If there<br>is at most one tuple with any (ai, bj) value, the value in the cell is derived from that<br>single tuple (if any); for instance, it could be the value of one or more other attributes<br>of the tuple. If there can be multiple tuples with an (ai, bj) value, the value in the cell<br>must be derived by aggregation on the tuples with that value. In our example, the<br>aggregation used is the sum of the values for attribute number. In our example, the<br>cross-tab also has an extra column and an extra row storing the totals of the cells in<br>the row/column. Most cross-tabs have such summary rows and columns.<br>A cross-tab is different from relational tables usually stored in databases, since the<br>number of columns in the cross-tab depends on the actual data. A change in the data<br>values may result in adding more columns, which is not desirable for data storage.<br>However, a cross-tab view is desirable for display to users. It is straightforward to<br>represent a cross-tab without summary values in a relational form with a ﬁxed num-<br>ber of columns. A cross-tab with summary rows/columns can be represented by in-<br>troducing a special value all to represent subtotals, as in Figure 22.2. The SQL:1999<br>standard actually uses the null value in place of all, but to avoid confusion with<br>regular null values, we shall continue to use all.<br>Consider the tuples (skirt, all, 53) and (dress, all, 35). We have obtained these tu-<br>ples by eliminating individual tuples with different values for color, and by replacing<br>the value of number by an aggregate—namely, sum. The value all can be thought of<br>as representing the set of all values for an attribute. Tuples with the value all only for<br>the color dimension can be obtained by an SQL query performing a group by on the<br>column item-name. Similarly, a group by on color can be used to get the tuples with<br>the value all for item-name, and a group by with no attributes (which can simply be<br>omitted in SQL) can be used to get the tuple with value all for item-name and color.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VII. Other Topics<br>22. Advanced Querying and <br>Information Retrieval<br>814<br>© The McGraw−Hill <br>Companies, 2001<br>22.2<br>Data Analysis and OLAP<br>821<br>item-name<br>color<br>number<br>skirt<br>dark<br>8<br>skirt<br>pastel<br>35<br>skirt<br>white<br>10<br>skirt<br>all<br>53<br>dress<br>dark<br>20<br>dress<br>pastel<br>10<br>dress<br>white<br>5<br>dress<br>all<br>35<br>shirt<br>dark<br>14<br>shirt<br>pastel<br>7<br>shirt<br>white<br>28<br>shirt<br>all<br>49<br>pant<br>dark<br>20<br>pant<br>pastel<br>2<br>pant<br>white<br>5<br>pant<br>all<br>27<br>all<br>dark<br>62<br>all<br>pastel<br>54<br>all<br>white<br>48<br>all<br>all<br>164<br>Figure 22.2<br>Relational representation of the data in Figure 22.1.<br>The generalization of a cross-tab, which is 2-dimensional, to n dimensions can be<br>visualized as an n-dimensional cube, called the data cube. Figure 22.3 shows a data<br>cube on the sales relation. The data cube has three dimensions, namely item-name,<br>color, and size, and the measure attribute is number. Each cell is identiﬁed by values<br>for these three dimensions. Each cell in the data cube contains a value, just as in a<br>cross-tab. In Figure 22.3, the value contained in a cell is shown on one of the faces of<br>the cell; other faces of the cell are shown blank if they are visible.<br>The value for a dimension may be all, in which case the cell contains a summary<br>over all values of that dimension, as in the case of cross-tabs. The number of different<br>ways in which the tuples can be grouped for aggregation can be large. In fact, for a<br>table with n dimensions, aggregation can be performed with grouping on each of the<br>2n subsets of the n dimensions.1<br>An online analytical processing or OLAP system is an interactive system that per-<br>mits an analyst to view different summaries of multidimensional data. The word<br>online indicates that the an analyst must be able to request new summaries and get<br>responses online, within a few seconds, and should not be forced to wait for a long<br>time to see the result of a query.<br>With an OLAP system, a data analyst can look at different cross-tabs on the same<br>data by interactively selecting the attributes in the cross-tab. Each cross-tab is a<br>1.<br>Grouping on the set of all n dimensions is useful only if the table may have duplicates.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VII. Other Topics<br>22. Advanced Querying and <br>Information Retrieval<br>815<br>© The McGraw−Hill <br>Companies, 2001<br>822<br>Chapter 22<br>Advanced Querying and Information Retrieval<br>8<br>35<br>10<br>53<br>20<br>10<br>8<br>35<br>14<br>7<br>28<br>49<br>20<br>2<br>5<br>27<br>62<br>2<br>8<br>5<br>7<br>22<br>4<br>7<br>6<br>12<br>29<br>2<br>5<br>3<br>1<br>11<br>54<br>48<br>164<br>34<br>21<br>77<br>4<br>9<br>42<br>16<br>18<br>45<br>dark<br>pastel<br>white<br>skirt dress<br>shirts<br>item name<br>pant<br>all<br>all<br>small<br>large<br>medium<br>all<br>color<br>size<br>Figure 22.3<br>Three-dimensional data cube.<br>two-dimensional view on a multidimensional data cube. For instance the analyst may<br>select a cross-tab on item-name and size, or a cross-tab on color and size. The operation<br>of changing the dimensions used in a cross-tab is called pivoting.<br>An OLAP system provides other functionality as well. For instance, the analyst<br>may wish to see a cross-tab on item-name and color for a ﬁxed value of size, for ex-<br>ample, large, instead of the sum across all sizes. Such an operation is referred to as<br>slicing, since it can be thought of as viewing a slice of the data cube. The operation is<br>sometimes called dicing, particularly when values for multiple dimensions are ﬁxed.<br>When a cross-tab is used to view a multidimensional cube, the values of dimension<br>attributes that are not part of the cross-tab are shown above the cross-tab. The value of<br>such an attribute can be all, as shown in Figure 22.1, indicating that data in the cross-<br>tab are a summary over all values for the attribute. Slicing/dicing simply consists of<br>selecting speciﬁc values for these attributes, which are then displayed on top of the<br>cross-tab.<br>OLAP systems permit users to view data at any desired level of granularity. The<br>operation of moving from ﬁner-granularity data to a coarser granularity (by means<br>of aggregation) is called a rollup. In our example, starting from the data cube on the<br>sales table, we got our example cross-tab by rolling up on the attribute size. The op-<br>posite operation—that of moving from coarser-granularity data to ﬁner-granularity<br>data—is called a drill down. Clearly, ﬁner-granularity data cannot be generated from<br>coarse-granularity data; they must be generated either from the original data, or from<br>even ﬁner-granularity summary data.<br>Analysts may wish to view a dimension at different levels of detail. For instance,<br>an attribute of type datetime contains a date and a time of day. Using time precise to<br>a second (or less) may not be meaningful: An analyst who is interested in rough time<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VII. Other Topics<br>22. Advanced Querying and <br>Information Retrieval<br>816<br>© The McGraw−Hill <br>Companies, 2001<br>22.2<br>Data Analysis and OLAP<br>823<br>Hour of day<br>Date<br>DateTime<br>Day of week<br>Month<br>Quarter<br>Year<br>State<br>Country<br>Region<br>City<br>a) Time Hierarchy<br>b) Location Hierarchy<br>Figure 22.4<br>Hierarchies on dimensions.<br>of day may look at only the hour value. An analyst who is interested in sales by day<br>of the week may map the date to a day-of-the-week and look only at that. Another<br>analyst may be interested in aggregates over a month, or a quarter, or for an entire<br>year.<br>The different levels of detail for an attribute can be organized into a hierarchy.<br>Figure 22.4(a) shows a hierarchy on the datetime attribute. As another example, Fig-<br>ure 22.4(b) shows a hierarchy on location, with the city being at the bottom of the<br>hierarchy, state above it, country at the next level, and region being the top level. In<br>our earlier example, clothes can be grouped by category (for instance, menswear or<br>womenswear); category would then lie above item-name in our hierarchy on clothes.<br>At the level of actual values, skirts and dresses would fall under the womenswear<br>category and pants and shirts under the menswear category.<br>An analyst may be interested in viewing sales of clothes divided as menswear and<br>womenswear, and not interested in individual values. After viewing the aggregates<br>at the level of womenswear and menswear, an analyst may drill down the hierarchy<br>to look at individual values. An analyst looking at the detailed level may drill up the<br>hierarchy, and look at coarser-level aggregates. Both levels can be displayed on the<br>same cross-tab, as in Figure 22.5.<br>22.2.2<br>OLAP Implementation<br>The earliest OLAP systems used multidimensional arrays in memory to store data<br>cubes, and are referred to as multidimensional OLAP (MOLAP) systems. Later, OLAP<br>facilities were integrated into relational systems, with data stored in a relational data-<br>base. Such systems are referred to as relational OLAP (ROLAP) systems. Hybrid<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VII. Other Topics<br>22. Advanced Querying and <br>Information Retrieval<br>817<br>© The McGraw−Hill <br>Companies, 2001<br>824<br>Chapter 22<br>Advanced Querying and Information Retrieval<br>category<br>dark<br>item-name<br>womenswear<br>menswear<br>total<br>skirt<br>dress<br>subtotal<br>skirt<br>dress<br>subtotal<br>8<br>20<br>28<br>14<br>20<br>34<br>62<br>pastel<br>8<br>20<br>28<br>14<br>20<br>34<br>62<br>white<br>total<br>10<br>5<br>15<br>28<br>5<br>33<br>48<br>53<br>35<br>49<br>27<br>88<br>76<br>164<br>Figure 22.5<br>Cross tabulation of sales with hierarchy on item-name.<br>systems, which store some summaries in memory and store the base data and other<br>summaries in a relational database, are called hybrid OLAP (HOLAP) systems.<br>Many OLAP systems are implemented as client–server systems. The server con-<br>tains the relational database as well as any MOLAP data cubes. Client systems obtain<br>views of the data by communicating with the server.<br>A na¨ıve way of computing the entire data cube (all groupings) on a relation is to<br>use any standard algorithm for computing aggregate operations, one grouping at a<br>time. The na¨ıve algorithm would require a large number of scans of the relation. A<br>simple optimization is to compute an aggregation on, say, (item-name, color) from an<br>aggregation (item-name, color, size), instead of from the original relation. For the stan-<br>dard SQL aggregate functions, we can compute an aggregate with grouping on a set<br>of attributes A from an aggregate with grouping on a set of attributes B if A ⊆B; you<br>can do so as an exercise (see Exercise 22.1), but note that to compute avg, we addi-<br>tionally need the count value. (For some non-standard aggregate functions, such as<br>median, aggregates cannot be computed as above; the optimization described here<br>do not apply to such “non-decomposable” aggregate functions.) The amount of data<br>read drops signiﬁcantly by computing an aggregate from another aggregate, instead<br>of from the original relation. Further improvements are possible; for instance, multi-<br>ple groupings can be computed on a single scan of the data. See the bibliographical<br>notes for references to algorithms for efﬁciently computing data cubes.<br>Early OLAP implementations precomputed and stored entire data cubes, that is,<br>groupings on all subsets of the dimension attributes. Precomputation allows OLAP<br>queries to be answered within a few seconds, even on datasets that may contain<br>millions of tuples adding up to gigabytes of data. However, there are 2n groupings<br>with n dimension attributes; hierarchies on attributes increase the number further.<br>As a result, the entire data cube is often larger than the original relation that formed<br>the data cube and in many cases it is not feasible to store the entire data cube.<br>Instead of precomputing and storing all possible groupings, it makes sense to pre-<br>compute and store some of the groupings, and to compute others on demand. In-<br>stead of computing queries from the original relation, which may take a very long<br>time, we can compute them from other precomputed queries. For instance, suppose<br>a query requires summaries by (item-name, color), which has not been precomputed.<br>The query result can be computed from summaries by (item-name, color, size), if that<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VII. Other Topics<br>22. Advanced Querying and <br>Information Retrieval<br>818<br>© The McGraw−Hill <br>Companies, 2001<br>22.2<br>Data Analysis and OLAP<br>825<br>has been precomputed. See the bibliographical notes for references on how to select<br>a good set of groupings for precomputation, given limits on the storage available for<br>precomputed results.<br>The data in a data cube cannot be generated by a single SQL query using the basic<br>group by constructs, since aggregates are computed for several different groupings<br>of the dimension attributes. Section 22.2.3 discusses SQL extensions to support OLAP<br>functionality.<br>22.2.3<br>Extended Aggregation<br>The SQL-92 aggregation functionality is limited, so several extensions were imple-<br>mented by different databases. The SQL:1999 standard, however, deﬁnes a rich set of<br>aggregate functions, which we outline in this section and in the next two sections. The<br>Oracle and IBM DB2 databases support most of these features, and other databases<br>will no doubt support these features in the near future.<br>The new aggregate functions on single attributes are standard deviation and vari-<br>ance (stddev and variance). Standard deviation is the square root of variance.2 Some<br>database systems support other aggregate functions such as median and mode. Some<br>database systems even allow users to add new aggregate functions.<br>SQL:1999 also supports a new class of binary aggregate functions, which can com-<br>pute statistical results on pairs of attributes; they include correlations, covariances,<br>and regression curves, which give a line approximating the relation between the val-<br>ues of the pair of attributes. Deﬁnitions of these functions may be found in any stan-<br>dard textbook on statistics, such as those referenced in the bibliographical notes.<br>SQL:1999 also supports generalizations of the group by construct, using the cube<br>and rollup constructs. A representative use of the cube construct is:<br>select item-name, color, size, sum(number)<br>from sales<br>group by cube(item-name, color, size)<br>This query computes the union of eight different groupings of the sales relation:<br>{ (item-name, color, size), (item-name, color), (item-name, size),<br>(color, size), (item-name), (color), (size), () }<br>where () denotes an empty group by list.<br>For each grouping, the result contains the null value for attributes not present in<br>the grouping. For instance, the table in Figure 22.2, with occurrences of all replaced<br>by null, can be computed by the query<br>select item-name, color, sum(number)<br>from sales<br>group by cube(item-name, color)<br>2.<br>The SQL:1999 standard actually supports two types of variance, called population variance and sam-<br>ple variance, and correspondingly two types of standard deviation. The deﬁnitions of the two types differ<br>slightly; see a statistics textbook for details.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VII. Other Topics<br>22. Advanced Querying and <br>Information Retrieval<br>819<br>© The McGraw−Hill <br>Companies, 2001<br>826<br>Chapter 22<br>Advanced Querying and Information Retrieval<br>A representative rollup construct is<br>select item-name, color, size, sum(number)<br>from sales<br>group by rollup(item-name, color, size)<br>Here, only four groupings are generated:<br>{ (item-name, color, size), (item-name, color), (item-name), () }<br>Rollup can be used to generate aggregates at multiple levels of a hierarchy on a<br>column. For instance, suppose we have a table itemcategory(item-name, category) giv-<br>ing the category of each item. Then the query<br>select category, item-name, sum(number)<br>from sales, category<br>where sales.item-name = itemcategory.item-name<br>group by rollup(category, item-name)<br>would give a hierarchical summary by item-name and by category.<br>Multiple rollups and cubes can be used in a single group by clause. For instance,<br>the following query<br>select item-name, color, size, sum(number)<br>from sales<br>group by rollup(item-name), rollup(color, size)<br>generates the groupings<br>{ (item-name, color, size), (item-name, color), (item-name),<br>(color, size), (color), () }<br>To understand why, note that rollup(item-name) generates two groupings, {(item-<br>name), ()}, and rollup(color, size) generates three groupings, {(color, size), (color), () }.<br>The cross product of the two gives us the six groupings shown.<br>As we mentioned in Section 22.2.1, SQL:1999 uses the value null to indicate the<br>usual sense of null as well as all. This dual use of null can cause ambiguity if the<br>attributes used in a rollup or cube clause contain null values. The function grouping<br>can be applied on an attribute; it returns 1 if the value is a null value representing all,<br>and returns 0 in all other cases. Consider the following query:<br>select item-name, color, size, sum(number),<br>grouping(item-name) as item-name-ﬂag,<br>grouping(color) as color-ﬂag,<br>grouping(size) as size-ﬂag<br>from sales<br>group by cube(item-name, color, size)<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VII. Other Topics<br>22. Advanced Querying and <br>Information Retrieval<br>820<br>© The McGraw−Hill <br>Companies, 2001<br>22.2<br>Data Analysis and OLAP<br>827<br>The output is the same as in the version of the query without grouping, but with<br>three extra columns called item-name-ﬂag, color-ﬂag, and size-ﬂag. In each tuple, the<br>value of a ﬂag ﬁeld is 1 if the corresponding ﬁeld is a null representing all.<br>Instead of using tags to indicate nulls that represent all, we can replace the null<br>value by a value of our choice:<br>decode(grouping(item-name), 1, ’all’, item-name)<br>This expression returns the value “all” if the value of item-name is a null correspond-<br>ing to all, and returns the actual value of item-name otherwise. This expression can be<br>used in place of item-name in the select clause to get “all” in the output of the query,<br>in place of nulls representing all.<br>Neither the rollup nor the cube clause gives complete control on the groupings<br>that are generated. For instance, we cannot use them to specify that we want only<br>groupings {(color, size), (size, item-name)}. Such restricted groupings can be generated<br>by using the grouping construct in the having clause; we leave the details as an<br>exercise for you.<br>22.2.4<br>Ranking<br>Finding the position of a value in a larger set is a common operation. For instance,<br>we may wish to assign students a rank in class based on their total marks, with the<br>rank 1 going to the student with the highest marks, the rank 2 to the student with<br>the next highest marks, and so on. While such queries can be expressed in SQL-92,<br>they are difﬁcult to express and inefﬁcient to evaluate. Programmers often resort to<br>writing the query partly in SQL and partly in a programming language. A related<br>type of query is to ﬁnd the percentile in which a value in a (multi)set belongs, for<br>example, the bottom third, middle third, or top third. We study SQL:1999 support for<br>these types of queries here.<br>Ranking is done in conjunction with an order by speciﬁcation. Suppose we are<br>given a relation student-marks(student-id, marks) which stores the marks obtained by<br>each student. The following query gives the rank of each student.<br>select student-id, rank() over (order by (marks) desc) as s-rank<br>from student-marks<br>Note that the order of tuples in the output is not deﬁned, so they may not be sorted<br>by rank. An extra order</span><br><br><span style="background-color: #FDFFB6;" title="Chunk 106 | Start: 2120212 | End: 2140212 | Tokens: 3318"> by clause is needed to get them in sorted order, as shown<br>below.<br>select student-id, rank () over (order by (marks) desc) as s-rank<br>from student-marks order by s-rank<br>A basic issue with ranking is how to deal with the case of multiple tuples that are<br>the same on the ordering attribute(s). In our example, this means deciding what to<br>do if there are two students with the same marks. The rank function gives the same<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VII. Other Topics<br>22. Advanced Querying and <br>Information Retrieval<br>821<br>© The McGraw−Hill <br>Companies, 2001<br>828<br>Chapter 22<br>Advanced Querying and Information Retrieval<br>rank to all tuples that are equal on the order by attributes. For instance, if the highest<br>mark is shared by two students, both would get rank 1. The next rank given would<br>be 3, not 2, so if three students get the next highest mark, they would all get rank<br>3, and the next student(s) would get rank 5, and so on. There is also a dense rank<br>function that does not create gaps in the ordering. In the above example, the tuples<br>with the second highest value all get rank 2, and tuples with the third highest value<br>get rank 3, and so on.<br>Ranking can be done within partitions of the data. For instance, suppose we have<br>an additional relation student-section(student-id, section) that stores for each student<br>the section in which the student studies. The following query then gives the rank of<br>students within each section.<br>select student-id, section,<br>rank () over (partition by section order by marks desc) as sec-rank<br>from student-marks, student-section<br>where student-marks.student-id = student-section.student-id<br>order by section, sec-rank<br>The outer order by clause orders the result tuples by section, and within each section<br>by the rank.<br>Multiple rank expressions can be used within a single select statement; thus we<br>can obtain the overall rank and the rank within the section by using two rank expres-<br>sions in the same select clause. An interesting question is what happens when rank-<br>ing (possibly with partitioning) occurs along with a group by clause. In this case, the<br>group by clause is applied ﬁrst, and partitioning and ranking are done on the results<br>of the group by. Thus aggregate values can then be used for ranking. For example,<br>suppose we had marks for each student for each of several subjects. To rank students<br>by the sum of their marks in different subjects, we can use a group by clause to com-<br>pute the aggregate marks for each student, and then rank students by the aggregate<br>sum. We leave details as an exercise for you.<br>The ranking functions can be used to ﬁnd the top n tuples by embedding a ranking<br>query within an outer-level query; we leave details as an exercise. Note that bottom<br>n is simply the same as top n with a reverse sorting order. Several database systems<br>provide nonstandard SQL extensions to specify directly that only the top n results are<br>required; such extensions do not require the rank function, and simplify the job of the<br>optimizer, but are (currently) not as general since they do not support partitioning.<br>SQL:1999 also speciﬁes several other functions that can be used in place of rank.<br>For instance, percent rank of a tuple gives the rank of the tuple as a fraction. If there<br>are n tuples in the partition3 and the rank of the tuple is r, then its percent rank is<br>deﬁned as (r −1)/(n −1) (and as null if there is only one tuple in the partition). The<br>function cume dist, short for cumulative distribution, for a tuple is deﬁned as p/n<br>where p is the number of tuples in the partition with ordering values preceding or<br>equal to the ordering value of the tuple, and n is the number of tuples in the parti-<br>3.<br>The entire set is treated as a single partition if no explicit partition is used.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VII. Other Topics<br>22. Advanced Querying and <br>Information Retrieval<br>822<br>© The McGraw−Hill <br>Companies, 2001<br>22.2<br>Data Analysis and OLAP<br>829<br>tion. The function row number sorts the rows and gives each row a unique number<br>corresponding to its position in the sort order; different rows with the same ordering<br>value would get different row numbers, in a nondeterministic fashion.<br>Finally, for a given constant n, the ranking function ntile(n) takes the tuples in each<br>partition in the speciﬁed order, and divides them into n buckets with equal numbers<br>of tuples.4 For each tuple, ntile(n) then gives the number of the bucket in which it is<br>placed, with bucket numbers starting with 1. This function is particularly useful for<br>constructing histograms based on percentiles. For instance, we can sort employees<br>by salary, and use ntile(3) to ﬁnd which range (bottom third, middle third, or top<br>third) each employee is in, and compute the total salary earned by employees in each<br>range:<br>select threetile, sum(salary)<br>from (<br>select salary, ntile(3) over (order by (salary)) as threetile<br>from employee) as s<br>group by threetile.<br>The presence of null values can complicate the deﬁnition of rank, since it is not<br>clear where they should occur ﬁrst in the sort order. SQL:1999 permits the user to<br>specify where they should occur by using nulls ﬁrst or nulls last, for instance<br>select student-id, rank () over (order by marks desc nulls last) as s-rank<br>from student-marks<br>22.2.5<br>Windowing<br>An example of a window query is query that, given sales values for each date, cal-<br>culates for each date the average of the sales on that day, the previous day, and the<br>next day; such moving average queries are used to smooth out random variations.<br>Another example of a window query is one that ﬁnds the cumulative balance in an<br>account, given a relation specifying the deposits and withdrawals on an account.<br>Such queries are either hard or impossible (depending on the exact query) to express<br>in basic SQL.<br>SQL:1999 provides a windowing feature to support such queries. In contrast to<br>group by, the same tuple can exist in multiple windows. Suppose we are given a<br>relation transaction(account-number, date-time, value), where value is positive for a de-<br>posit and negative for a withdrawal. We assume there is at most one transaction per<br>date-time value.<br>Consider the query<br>4.<br>If the total number of tuples in a partition is not divisible by n, then the number of tuples in each<br>bucket can differ by at most 1. Tuples with the same value for the ordering attribute may be assigned to<br>different buckets, nondeterministically, in order to make the number of tuples in each bucket equal.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VII. Other Topics<br>22. Advanced Querying and <br>Information Retrieval<br>823<br>© The McGraw−Hill <br>Companies, 2001<br>830<br>Chapter 22<br>Advanced Querying and Information Retrieval<br>select account-number, date-time,<br>sum(value) over<br>(partition by account-number<br>order by date-time<br>rows unbounded preceding)<br>as balance<br>from transaction<br>order by account-number, date-time<br>The query gives the cumulative balances on each account just before each transaction<br>on the account; the cumulative balance of the account is the sum of values of all<br>earlier transactions on the account.<br>The partition by clause partitions tuples by account number, so for each row only<br>the tuples in its partition are considered. A window is created for each tuple; the key-<br>words rows unbounded preceding specify that the window for each tuple consists<br>of all tuples in the partition that precede it in the speciﬁed order (here, increasing<br>order of date-time). The aggregate function sum(value) is applied on all the tuples in<br>the window. Observe that the query does not use a group by clause, since there is an<br>output tuple for each tuple in the transaction relation.<br>While the query could be written without these extended constructs, it would be<br>rather difﬁcult to formulate. Note also that different windows can overlap, that is, a<br>tuple may be present in more than one window.<br>Other types of windows can be speciﬁed. For instance, to get a window containing<br>the previous 10 rows for each row, we can specify rows 10 preceding. To get a win-<br>dow containing the current, previous, and following row, we can use between rows<br>1 preceding and 1 following. To get the previous rows and the current row, we can<br>say between rows unbounded preceding and current. Note that if the ordering is on<br>a nonkey attribute, the result is not deterministic, since the order of tuples is not fully<br>deﬁned.<br>We can even specify windows by ranges of values, instead of numbers of rows. For<br>instance, suppose the ordering value of a tuple is v; then range between 10 preceding<br>and current row would give tuples whose ordering value is between v −10 and v<br>(both values inclusive). When dealing with dates, we can use range interval 10 day<br>preceding to get a window containing tuples within the previous 10 days, but not<br>including the date of the tuple.<br>Clearly, the windowing functionality of SQL:1999 is very rich and can be used to<br>write rather complex queries with a small amount of effort.<br>22.3<br>Data Mining<br>The term data mining refers loosely to the process of semiautomatically analyzing<br>large databases to ﬁnd useful patterns. Like knowledge discovery in artiﬁcial intelli-<br>gence (also called machine learning), or statistical analysis, data mining attempts to<br>discover rules and patterns from data. However, data mining differs from machine<br>learning and statistics in that it deals with large volumes of data, stored primarily on<br>disk. That is, data mining deals with “knowledge discovery in databases.”<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VII. Other Topics<br>22. Advanced Querying and <br>Information Retrieval<br>824<br>© The McGraw−Hill <br>Companies, 2001<br>22.3<br>Data Mining<br>831<br>Some types of knowledge discovered from a database can be represented by a set<br>of rules. The following is an example of a rule, stated informally: “Young women<br>with annual incomes greater than $50,000 are the most likely people to buy small<br>sports cars.” Of course such rules are not universally true, and have degrees of “sup-<br>port” and “conﬁdence,” as we shall see. Other types of knowledge are represented<br>by equations relating different variables to each other, or by other mechanisms for<br>predicting outcomes when the values of some variables are known.<br>There are a variety of possible types of patterns that may be useful, and different<br>techniques are used to ﬁnd different types of patterns. We shall study a few examples<br>of patterns and see how they may be automatically derived from a database.<br>Usually there is a manual component to data mining, consisting of preprocessing<br>data to a form acceptable to the algorithms, and postprocessing of discovered pat-<br>terns to ﬁnd novel ones that could be useful. There may also be more than one type<br>of pattern that can be discovered from a given database, and manual interaction may<br>be needed to pick useful types of patterns. For this reason, data mining is really a<br>semiautomatic process in real life. However, in our description we concentrate on<br>the automatic aspect of mining.<br>22.3.1<br>Applications of Data Mining<br>The discovered knowledge has numerous applications. The most widely used appli-<br>cations are those that require some sort of prediction. For instance, when a person<br>applies for a credit card, the credit-card company wants to predict if the person is a<br>good credit risk. The prediction is to be based on known attributes of the person, such<br>as age, income, debts, and past debt repayment history. Rules for making the predic-<br>tion are derived from the same attributes of past and current credit card holders,<br>along with their observed behavior, such as whether they defaulted on their credit-<br>card dues. Other types of prediction include predicting which customers may switch<br>over to a competitor (these customers may be offered special discounts to tempt them<br>not to switch), predicting which people are likely to respond to promotional mail<br>(“junk mail”), or predicting what types of phone calling card usage are likely to be<br>fraudulent.<br>Another class of applications looks for associations, for instance, books that tend<br>to be bought together. If a customer buys a book, an online bookstore may suggest<br>other associated books. If a person buys a camera, the system may suggest accessories<br>that tend to be bought along with cameras. A good salesperson is aware of such pat-<br>terns and exploits them to make additional sales. The challenge is to automate the<br>process. Other types of associations may lead to discovery of causation. For instance,<br>discovery of unexpected associations between a newly introduced medicine and car-<br>diac problems led to the ﬁnding that the medicine may cause cardiac problems in<br>some people. The medicine was then withdrawn from the market.<br>Associations are an example of descriptive patterns. Clusters are another example<br>of such patterns. For example, over a century ago a cluster of typhoid cases was found<br>around a well, which led to the discovery that the water in the well was contaminated<br>and was spreading typhoid. Detection of clusters of disease remains important even<br>today.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VII. Other Topics<br>22. Advanced Querying and <br>Information Retrieval<br>825<br>© The McGraw−Hill <br>Companies, 2001<br>832<br>Chapter 22<br>Advanced Querying and Information Retrieval<br>22.3.2<br>Classiﬁcation<br>As mentioned in Section 22.3.1, prediction is one of the most important types of data<br>mining. We outline what is classiﬁcation, study techniques for building one type of<br>classiﬁers, called decision tree classiﬁers, and then study other prediction techniques.<br>Abstractly, the classiﬁcation problem is this: Given that items belong to one of<br>several classes, and given past instances (called training instances) of items along<br>with the classes to which they belong, the problem is to predict the class to which a<br>new item belongs. The class of the new instance is not known, so other attributes of<br>the instance must be used to predict the class.<br>Classiﬁcation can be done by ﬁnding rules that partition the given data into<br>disjoint groups. For instance, suppose that a credit-card company wants to decide<br>whether or not to give a credit card to an applicant. The company has a variety of<br>information about the person, such as her age, educational background, annual in-<br>come, and current debts, that it can use for making a decision.<br>Some of this information could be relevant to the credit worthiness of the appli-<br>cant, whereas some may not be. To make the decision, the company assigns a credit-<br>worthiness level of excellent, good, average, or bad to each of a sample set of cur-<br>rent customers according to each customer’s payment history. Then, the company<br>attempts to ﬁnd rules that classify its current customers into excellent, good, aver-<br>age, or bad, on the basis of the information about the person, other than the actual<br>payment history (which is unavailable for new customers). Let us consider just two<br>attributes: education level (highest degree earned) and income. The rules may be of<br>the following form:<br>∀person P, P.degree = masters and P.income &gt; 75, 000<br>⇒P.credit = excellent<br>∀person P, P.degree = bachelors or<br>(P.income ≥25, 000 and P.income ≤75, 000) ⇒P.credit = good<br>Similar rules would also be present for the other credit worthiness levels (average<br>and bad).<br>The process of building a classiﬁer starts from a sample of data, called a training<br>set. For each tuple in the training set, the class to which the tuple belongs is already<br>known. For instance, the training set for a credit-card application may be the existing<br>customers, with their credit worthiness determined from their payment history. The<br>actual data, or population, may consist of all people, including those who are not<br>existing customers. There are several ways of building a classiﬁer, as we shall see.<br>22.3.2.1<br>Decision Tree Classiﬁers<br>The decision tree classiﬁer is a widely used technique for classiﬁcation. As the name<br>suggests, decision tree classiﬁers use a tree; each leaf node has an associated class,<br>and each internal node has a predicate (or more generally, a function) associated with<br>it. Figure 22.6 shows an example of a decision tree.<br>To classify a new instance, we start at the root, and traverse the tree to reach a<br>leaf; at an internal node we evaluate the predicate (or function) on the data instance,<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VII. Other Topics<br>22. Advanced Querying and <br>Information Retrieval<br>826<br>© The McGraw−Hill <br>Companies, 2001<br>22.3<br>Data Mining<br>833<br>degree<br>income<br>income<br>income<br>income<br>bachelors<br>masters<br>doctorate<br>none<br>bad<br>average<br>good<br>bad<br>average<br>good<br>excellent<br>&lt;50K<br>&gt;100K<br>&lt;25K<br>&gt;=25K<br>&gt;=50K<br>&lt;50K<br>&lt;25K<br>&gt;75K<br>25 to 75K<br>50 to 100K<br>Figure 22.6<br>Classiﬁcation tree.<br>to ﬁnd which child to go to. The process continues till we reach a leaf node. For<br>example, if the degree level of a person is masters, and the persons income is 40K,<br>starting from the root we follow the edge labeled “masters,” and from there the edge<br>labeled “25K to 75K,” to reach a leaf. The class at the leaf is “good,” so we predict that<br>the credit risk of that person is good.<br>Building Decision Tree Classiﬁers<br>The question then is how to build a decision tree classiﬁer, given a set of training<br>instances. The most common way of doing so is to use a greedy algorithm, which<br>works recursively, starting at the root and building the tree downward. Initially there<br>is only one node, the root, and all training instances are associated with that node.<br>At each node, if all, or “almost all” training instances associated with the node be-<br>long to the same class, then the node becomes a leaf node associated with that class.<br>Otherwise, a partitioning attribute and partitioning conditions must be selected to<br>create child nodes. The data associated with each child node is the set of training<br>instances that satisfy the partitioning condition for that child node. In our example,<br>the attribute degree is chosen, and four children, one for each value of degree, are cre-<br>ated. The conditions for the four children nodes are degree = none, degree = bachelors,<br>degree = masters, and degree = doctorate, respectively. The data associated with each<br>child consist of training instances satisfying the condition associated with that child.<br>At the node corresponding to masters, the attribute income is chosen, with the range<br>of values partitioned into intervals 0 to 25,000, 25,000 to 50,000, 50,000 to 75,000, and<br>over 75,000. The data associated with each node consist of training instances with the<br>degree attribute being masters, and the income attribute being in each of these ranges,<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VII. Other Topics<br>22. Advanced Querying and <br>Information Retrieval<br>827<br>© The McGraw−Hill <br>Companies, 2001<br>834<br>Chapter 22<br>Advanced Querying and Information Retrieval<br>respectively. As an optimization, since the class for the range 25,000 to 50,000 and the<br>range 50,000 to 75,000 is the same under the node degree = masters, the two ranges<br>have been merged into a single range 25,000 to 75,000.<br>Best Splits<br>Intuitively, by choosing a sequence of partitioning attributes, we start with the set<br>of all training instances, which is “impure” in the sense that it contains instances<br>from many classes, and end up with leaves which are “pure” in the sense that at<br>each leaf all training instances belong to only one class. We shall see shortly how to<br>measure purity quantitatively. To judge the beneﬁt of picking a particular attribute<br>and condition for partitioning of the data at a node, we measure the purity of the<br>data at the children resulting from partitioning by that attribute. The attribute and<br>condition that result in the maximum purity are chosen.<br>The purity of a set S of training instances can be measured quantitatively in several<br>ways. Suppose there are k classes, and of the instances in S the fraction of instances<br>in class i is</span><br><br><span style="background-color: #CAFFBF;" title="Chunk 107 | Start: 2140214 | End: 2160214 | Tokens: 3416"> pi. One measure of purity, the Gini measure is deﬁned as<br>Gini(S) = 1 −<br>k<br><br>i−1<br>p2<br>i<br>When all instances are in a single class, the Gini value is 0, while it reaches its max-<br>imum (of 1 −1/k) if each class has the same number of instances. Another measure<br>of purity is the entropy measure, which is deﬁned as<br>Entropy(S) = −<br>k<br><br>i−1<br>pi log2 pi<br>The entropy value is 0 if all instances are in a single class, and reaches its maximum<br>when each class has the same number of instances. The entropy measure derives<br>from information theory.<br>When a set S is split into multiple sets Si, i = 1, 2, . . . , r, we can measure the purity<br>of the resultant set of sets as:<br>Purity(S1, S2, . . . , Sr) =<br>r<br><br>i=1<br>|Si|<br>|S| purity(Si)<br>That is, the purity is the weighted average of the purity of the sets Si. The above<br>formula can be used with both the Gini measure and the entropy measure of purity.<br>The information gain due to a particular split of S into Si, i = 1, 2, . . . , r is then<br>Information-gain(S, {S1, S2, . . . , Sr}) = purity(S) −purity(S1, S2, . . . , Sr)<br>Splits into fewer sets are preferable to splits into many sets, since they lead to<br>simpler and more meaningful decision trees. The number of elements in each of the<br>sets Si may also be taken into account; otherwise, whether a set Si has 0 elements or<br>1 element would make a big difference in the number of sets, although the split is the<br>same for almost all the elements. The information content of a particular split can be<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VII. Other Topics<br>22. Advanced Querying and <br>Information Retrieval<br>828<br>© The McGraw−Hill <br>Companies, 2001<br>22.3<br>Data Mining<br>835<br>deﬁned in terms of entropy as<br>Information-content(S, {S1, S2, . . . , Sr})) = −<br>r<br><br>i−1<br>|Si|<br>|S| log2<br>|Si|<br>|S|<br>All of this leads to a deﬁnition: The best split for an attribute is the one that gives<br>the maximum information gain ratio, deﬁned as<br>Information-gain(S, {S1, S2, . . . , Sr})<br>Information-content(S, {S1, S2, . . . , Sr})<br>Finding Best Splits<br>How do we ﬁnd the best split for an attribute? How to split an attribute depends<br>on the type of the attribute. Attributes can be either continuous valued, that is, the<br>values can be ordered in a fashion meaningful to classiﬁcation, such as age or income,<br>or can be categorical, that is, they have no meaningful order, such as department<br>names or country names. We do not expect the sort order of department names or<br>country names to have any signiﬁcance to classiﬁcation.<br>Usually attributes that are numbers (integers/reals) are treated as continuous val-<br>ued while character string attributes are treated as categorical, but this may be con-<br>trolled by the user of the system. In our example, we have treated the attribute degree<br>as categorical, and the attribute income as continuous valued.<br>We ﬁrst consider how to ﬁnd best splits for continuous-valued attributes. For sim-<br>plicity, we shall only consider binary splits of continuous-valued attributes, that is,<br>splits that result in two children. The case of multiway splits is more complicated;<br>see the bibliographical notes for references on the subject.<br>To ﬁnd the best binary split of a continuous-valued attribute, we ﬁrst sort the at-<br>tribute values in the training instances. We then compute the information gain ob-<br>tained by splitting at each value. For example, if the training instances have values<br>1, 10, 15, and 25 for an attribute, the split points considered are 1, 10, and 15; in each<br>case values less than or equal to the split point form one partition and the rest of the<br>values form the other partition. The best binary split for the attribute is the split that<br>gives the maximum information gain.<br>For a categorical attribute, we can have a multiway split, with a child for each<br>value of the attribute. This works ﬁne for categorical attributes with only a few dis-<br>tinct values, such as degree or gender. However, if the attribute has many distinct<br>values, such as department names in a large company, creating a child for each value<br>is not a good idea. In such cases, we would try to combine multiple values into each<br>child, to create a smaller number of children. See the bibliographical notes for refer-<br>ences on how to do so.<br>Decision-Tree Construction Algorithm<br>The main idea of decision tree construction is to evaluate different attributes and dif-<br>ferent partitioning conditions, and pick the attribute and partitioning condition that<br>results in the maximum information gain ratio. The same procedure works recur-<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VII. Other Topics<br>22. Advanced Querying and <br>Information Retrieval<br>829<br>© The McGraw−Hill <br>Companies, 2001<br>836<br>Chapter 22<br>Advanced Querying and Information Retrieval<br>procedure GrowTree(S)<br>Partition(S);<br>procedure Partition (S)<br>if (purity(S) &gt; δp or |S| &lt; δs ) then<br>return;<br>for each attribute A<br>evaluate splits on attribute A;<br>Use best split found (across all attributes) to partition<br>S into S1, S2, . . . , Sr;<br>for i = 1, 2, . . . , r<br>Partition(Si);<br>Figure 22.7<br>Recursive construction of a decision tree.<br>sively on each of the sets resulting from the split, thereby recursively constructing<br>a decision tree. If the data can be perfectly classiﬁed, the recursion stops when the<br>purity of a set is 0. However, often data are noisy, or a set may be so small that par-<br>titioning it further may not be justiﬁed statistically. In this case, the recursion stops<br>when the purity of a set is “sufﬁciently high,” and the class of resulting leaf is deﬁned<br>as the class of the majority of the elements of the set. In general, different branches of<br>the tree could grow to different levels.<br>Figure 22.7 shows pseudocode for a recursive tree construction procedure, which<br>takes a set of training instances S as parameter. The recursion stops when the set is<br>sufﬁciently pure or the set S is too small for further partitioning to be statistically<br>signiﬁcant. The parameters δp and δs deﬁne cutoffs for purity and size; the system<br>may give them default values, that may be overridden by users.<br>There are a wide variety of decision tree construction algorithms, and we outline<br>the distinguishing features of a few of them. See the bibliographical notes for details.<br>With very large data sets, partitioning may be expensive, since it involves repeated<br>copying. Several algorithms have therefore been developed to minimize the I/O and<br>computation cost when the training data are larger than available memory.<br>Several of the algorithms also prune subtrees of the generated decision tree to<br>reduce overﬁtting: A subtree is overﬁtted if it has been so highly tuned to the speciﬁcs<br>of the training data that it makes many classiﬁcation errors on other data. A subtree<br>is pruned by replacing it with a leaf node. There are different pruning heuristics;<br>one heuristic uses part of the training data to build the tree and another part of the<br>training data to test it. The heuristic prunes a subtree if it ﬁnds that misclassiﬁcation<br>on the test instances would be reduced if the subtree were replaced by a leaf node.<br>We can generate classiﬁcation rules from a decision tree, if we so desire. For each<br>leaf we generate a rule as follows: The left-hand side is the conjunction of all the split<br>conditions on the path to the leaf, and the class is the class of the majority of the<br>training instances at the leaf. An example of such a classiﬁcation rule is<br>degree = masters and income &gt; 75, 000 ⇒excellent<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VII. Other Topics<br>22. Advanced Querying and <br>Information Retrieval<br>830<br>© The McGraw−Hill <br>Companies, 2001<br>22.3<br>Data Mining<br>837<br>22.3.2.2<br>Other Types of Classiﬁers<br>There are several types of classiﬁers other than decision tree classiﬁers. Two types<br>that have been quite useful are neural net classiﬁers and Bayesian classiﬁers. Neural net<br>classiﬁers use the training data to train artiﬁcial neural nets. There is a large body of<br>literature on neural nets, and we do not consider them further here.<br>Bayesian classiﬁers ﬁnd the distribution of attribute values for each class in the<br>training data; when given a new instance d, they use the distribution information to<br>estimate, for each class cj, the probability that instance d belongs to class cj, denoted<br>by p(cj|d), in a manner outlined here. The class with maximum probability becomes<br>the predicted class for instance d.<br>To ﬁnd the probability p(cj|d) of instance d being in class cj, Bayesian classiﬁers<br>use Bayes’ theorem, which says<br>p(cj|d) = p(d|cj)p(cj)<br>p(d)<br>where p(d|cj) is the probability of generating instance d given class cj, p(cj) is the<br>probability of occurrence of class cj, and p(d) is the probability of instance d occur-<br>ring. Of these, p(d) can be ignored since it is the same for all classes. p(cj) is simply<br>the fraction of training instances that belong to class cj.<br>Finding p(d|cj) exactly is difﬁcult, since it requires a complete distribution of in-<br>stances of cj. To simplify the task, naive Bayesian classiﬁers assume attributes have<br>independent distributions, and thereby estimate<br>p(d|cj) = p(d1|cj) ∗p(d2|cj) ∗. . . ∗p(dn|cj)<br>That is, the probability of the instance d occurring is the product of the probability of<br>occurrence of each of the attribute values di of d, given the class is cj.<br>The probabilities p(di|cj) derive from the distribution of values for each attribute i,<br>for each class class cj. This distribution is computed from the training instances that<br>belong to each class cj; the distribution is usually approximated by a histogram. For<br>instance, we may divide the range of values of attribute i into equal intervals, and<br>store the fraction of instances of class cj that fall in each interval. Given a value di for<br>attribute i, the value of p(di|cj) is simply the fraction of instances belonging to class<br>cj that fall in the interval to which di belongs.<br>A signiﬁcant beneﬁt of Bayesian classiﬁers is that they can classify instances with<br>unknown and null attribute values—unknown or null attributes are just omitted<br>from the probability computation. In contrast, decision tree classiﬁers cannot mean-<br>ingfully handle situations where an instance to be classiﬁed has a null value for a<br>partitioning attribute used to traverse further down the decision tree.<br>22.3.2.3<br>Regression<br>Regression deals with the prediction of a value, rather than a class. Given values for<br>a set of variables, X1, X2, . . . , Xn, we wish to predict the value of a variable Y . For<br>instance, we could treat the level of education as a number and income as another<br>number, and, on the basis of these two variables, we wish to predict the likelihood of<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VII. Other Topics<br>22. Advanced Querying and <br>Information Retrieval<br>831<br>© The McGraw−Hill <br>Companies, 2001<br>838<br>Chapter 22<br>Advanced Querying and Information Retrieval<br>default, which could be a percentage chance of defaulting, or the amount involved in<br>the default.<br>One way is to infer coefﬁcients a0, a1, a1, . . . , an such that<br>Y = a0 + a1 ∗X1 + a2 ∗X2 + · · · + an ∗Xn<br>Finding such a linear polynomial is called linear regression. In general, we wish to<br>ﬁnd a curve (deﬁned by a polynomial or other formula) that ﬁts the data; the process<br>is also called curve ﬁtting.<br>The ﬁt may only be approximate, because of noise in the data or because the rela-<br>tionship is not exactly a polynomial, so regression aims to ﬁnd coefﬁcients that give<br>the best possible ﬁt. There are standard techniques in statistics for ﬁnding regression<br>coefﬁcients. We do not discuss these techniques here, but the bibliographical notes<br>provide references.<br>22.3.3<br>Association Rules<br>Retail shops are often interested in associations between different items that people<br>buy. Examples of such associations are:<br>• Someone who buys bread is quite likely also to buy milk<br>• A person who bought the book Database System Concepts is quite likely also to<br>buy the book Operating System Concepts.<br>Association information can be used in several ways. When a customer buys a partic-<br>ular book, an online shop may suggest associated books. A grocery shop may decide<br>to place bread close to milk, since they are often bought together, to help shoppers ﬁn-<br>ish their task faster. Or the shop may place them at opposite ends of a row, and place<br>other associated items in between to tempt people to buy those items as well, as the<br>shoppers walk from one end of the row to the other. A shop that offers discounts on<br>one associated item may not offer a discount on the other, since the customer will<br>probably buy the other anyway.<br>Association Rules<br>An example of an association rule is<br>bread ⇒milk<br>In the context of grocery-store purchases, the rule says that customers who buy bread<br>also tend to buy milk with a high probability. An association rule must have an asso-<br>ciated population: the population consists of a set of instances. In the grocery-store<br>example, the population may consist of all grocery store purchases; each purchase is<br>an instance. In the case of a bookstore, the population may consist of all people who<br>made purchases, regardless of when they made a purchase. Each customer is an in-<br>stance. Here, the analyst has decided that when a purchase is made is not signiﬁcant,<br>whereas for the grocery-store example, the analyst may have decided to concentrate<br>on single purchases, ignoring multiple visits by the same customer.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VII. Other Topics<br>22. Advanced Querying and <br>Information Retrieval<br>832<br>© The McGraw−Hill <br>Companies, 2001<br>22.3<br>Data Mining<br>839<br>Rules have an associated support, as well as an associated conﬁdence. These are<br>deﬁned in the context of the population:<br>• Support is a measure of what fraction of the population satisﬁes both the an-<br>tecedent and the consequent of the rule.<br>For instance, suppose only 0.001 percent of all purchases include milk and<br>screwdrivers. The support for the rule<br>milk ⇒screwdrivers<br>is low. The rule may not even be statistically signiﬁcant—perhaps there was<br>only a single purchase that included both milk and screwdrivers. Businesses<br>are usually not interested in rules that have low support, since they involve<br>few customers, and are not worth bothering about.<br>On the other hand, if 50 percent of all purchases involve milk and bread,<br>then support for rules involving bread and milk (and no other item) is rela-<br>tively high, and such rules may be worth attention. Exactly what minimum<br>degree of support is considered desirable depends on the application.<br>• Conﬁdence is a measure of how often the consequent is true when the an-<br>tecedent is true. For instance, the rule<br>bread ⇒milk<br>has a conﬁdence of 80 percent if 80 percent of the purchases that include bread<br>also include milk. A rule with a low conﬁdence is not meaningful. In busi-<br>ness applications, rules usually have conﬁdences signiﬁcantly less than 100<br>percent, whereas in other domains, such as in physics, rules may have high<br>conﬁdences.<br>Note that the conﬁdence of bread ⇒milk may be very different from the<br>conﬁdence of milk ⇒bread, although both have the same support.<br>Finding Association Rules<br>To discover association rules of the form<br>i1, i2, . . . , in ⇒i0<br>we ﬁrst ﬁnd sets of items with sufﬁcient support, called large itemsets. In our exam-<br>ple we ﬁnd sets of items that are included in a sufﬁciently large number of instances.<br>We will shortly see how to compute large itemsets.<br>For each large itemset, we then output all rules with sufﬁcient conﬁdence that<br>involve all and only the elements of the set. For each large itemset S, we output a<br>rule S −s ⇒s for every subset s ⊂S, provided S −s ⇒s has sufﬁcient conﬁdence;<br>the conﬁdence of the rule is given by support of s divided by support of S.<br>We now consider how to generate all large itemsets. If the number of possible sets<br>of items is small, a single pass over the data sufﬁces to detect the level of support<br>for all the sets. A count, initialized to 0, is maintained for each set of items. When a<br>purchase record is fetched, the count is incremented for each set of items such that<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VII. Other Topics<br>22. Advanced Querying and <br>Information Retrieval<br>833<br>© The McGraw−Hill <br>Companies, 2001<br>840<br>Chapter 22<br>Advanced Querying and Information Retrieval<br>all items in the set are contained in the purchase. For instance, if a purchase included<br>items a, b, and c, counts would be incremented for {a}, {b}, {c}, {a, b}, {b, c}, {a, c},<br>and {a, b, c}. Those sets with a sufﬁciently high count at the end of the pass corre-<br>spond to items that have a high degree of association.<br>The number of sets grows exponentially, making the procedure just described in-<br>feasible if the number of items is large. Luckily, almost all the sets would normally<br>have very low support; optimizations have been developed to eliminate most such<br>sets from consideration. These techniques use multiple passes on the database, con-<br>sidering only some sets in each pass.<br>In the a priori technique for generating large itemsets, only sets with single items<br>are considered in the ﬁrst pass. In the second pass, sets with two items are considered,<br>and so on.<br>At the end of a pass all sets with sufﬁcient support are output as large itemsets.<br>Sets found to have too little support at the end of a pass are eliminated. Once a set is<br>eliminated, none of its supersets needs to be considered. In other words, in pass i we<br>need to count only supports for sets of size i such that all subsets of the set have been<br>found to have sufﬁciently high support; it sufﬁces to test all subsets of size i −1 to<br>ensure this property. At the end of some pass i, we would ﬁnd that no set of size i has<br>sufﬁcient support, so we do not need to consider any set of size i + 1. Computation<br>then terminates.<br>22.3.4<br>Other Types of Associations<br>Using plain association rules has several shortcomings. One of the major shortcom-<br>ings is that many associations are not very interesting, since they can be predicted.<br>For instance, if many people buy cereal and many people buy bread, we can predict<br>that a fairly large number of people would buy both, even if there is no connection be-<br>tween the two purchases. What would be interesting is a deviation from the expected<br>co-occurrence of the two. In statistical terms, we look for correlations between items;<br>correlations can be positive, in that the co-occurrence is higher than would have been<br>expected, or negative, in that the items co-occur less frequently than predicted. See a<br>standard textbook on statistics for more information about correlations.<br>Another important class of data-mining applications is sequence associations (or<br>correlations). Time-series data, such as stock prices on a sequence of days, form an<br>example of sequence data. Stock-market analysts want to ﬁnd associations among<br>stock-market price sequences. An example of such a association is the following rule:<br>“Whenever bond rates go up, the stock prices go down within 2 days.” Discover-<br>ing such association between sequences can help us to make intelligent investment<br>decisions. See the bibliographical notes for references to research on this topic.<br>Deviations from temporal patterns are often interesting. For instance, if a company<br>has been growing at a steady rate each year, a deviation from the usual growth rate<br>is surprising. If sales of winter clothes go down in summer, it is not surprising, since<br>we can predict it from past years; a deviation that we could not have predicted from<br>past experience would be considered interesting. Mining techniques can ﬁnd devia-<br>tions from what one would have expected on the basis of past temporal/sequential<br>patterns. See the bibliographical notes for references to research on this topic.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VII. Other Topics<br>22. Advanced Querying and <br>Information Retrieval<br>834<br>© The McGraw−Hill <br>Companies</span><br><br><span style="background-color: #9BF6FF;" title="Chunk 108 | Start: 2160216 | End: 2180216 | Tokens: 3137">, 2001<br>22.3<br>Data Mining<br>841<br>22.3.5<br>Clustering<br>Intuitively, clustering refers to the problem of ﬁnding clusters of points in the given<br>data. The problem of clustering can be formalized from distance metrics in several<br>ways. One way is to phrase it as the problem of grouping points into k sets (for a<br>given k) so that the average distance of points from the centroid of their assigned<br>cluster is minimized.5 Another way is to group points so that the average distance<br>between every pair of points in each cluster is minimized. There are other deﬁni-<br>tions too; see the bibliographical notes for details. But the intuition behind all these<br>deﬁnitions is to group similar points together in a single set.<br>Another type of clustering appears in classiﬁcation systems in biology. (Such clas-<br>siﬁcation systems do not attempt to predict classes, rather they attempt to cluster re-<br>lated items together.) For instance, leopards and humans are clustered under the class<br>mammalia, while crocodiles and snakes are clustered under reptilia. Both mammalia<br>and reptilia come under the common class chordata. The clustering of mammalia has<br>further subclusters, such as carnivora and primates. We thus have hierarchical clus-<br>tering. Given characteristics of different species, biologists have created a complex<br>hierarchical clustering scheme grouping related species together at different levels of<br>the hierarchy.<br>Hierarchical clustering is also useful in other domains—for clustering documents,<br>for example. Internet directory systems (such as Yahoo’s) cluster related documents<br>in a hierarchical fashion (see Section 22.5.5). Hierarchical clustering algorithms can<br>be classiﬁed as agglomerative clustering algorithms, which start by building small<br>clusters and then creater higher levels, or divisive clustering algorithms, which ﬁrst<br>create higher levels of the hierarchical clustering, then reﬁne each resulting cluster<br>into lower level clusters.<br>The statistics community has studied clustering extensively. Database research has<br>provided scalable clustering algorithms that can cluster very large data sets (that may<br>not ﬁt in memory). The Birch clustering algorithm is one such algorithm. Intuitively,<br>data points are inserted into a multidimensional tree structure (based on R-trees, de-<br>scribed in Section 23.3.5.3), and guided to appropriate leaf nodes based on nearness<br>to representative points in the internal nodes of the tree. Nearby points are thus clus-<br>tered together in leaf nodes, and summarized if there are more points than ﬁt in<br>memory. Some postprocessing after insertion of all points gives the desired overall<br>clustering. See the bibliographical notes for references to the Birch algorithm, and<br>other techniques for clustering, including algorithms for hierarchical clustering.<br>An interesting application of clustering is to predict what new movies (or books,<br>or music) a person is likely to be interested in, on the basis of:<br>1. The person’s past preferences in movies<br>2. Other people with similar past preferences<br>3. The preferences of such people for new movies<br>5.<br>The centroid of a set of points is deﬁned as a point whose coordinate on each dimension is the average<br>of the coordinates of all the points of that set on that dimension. For example in two dimensions, the<br>centroid of a set of points { (x1, y1), (x2, y2), . . ., (xn, yn) } is given by (<br>Pn<br>i=1 xi<br>n<br>,<br>Pn<br>i=1 yi<br>n<br>)<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VII. Other Topics<br>22. Advanced Querying and <br>Information Retrieval<br>835<br>© The McGraw−Hill <br>Companies, 2001<br>842<br>Chapter 22<br>Advanced Querying and Information Retrieval<br>One approach to this problem is as follows. To ﬁnd people with similar past prefer-<br>ences we create clusters of people based on their preferences for movies. The accuracy<br>of clustering can be improved by previously clustering movies by their similarity, so<br>even if people have not seen the same movies, if they have seen similar movies they<br>would be clustered together. We can repeat the clustering, alternately clustering peo-<br>ple, then movies, then people, and so on till we reache an equilibrium. Given a new<br>user, we ﬁnd a cluster of users most similar to that user, on the basis of the user’s<br>preferences for movies already seen. We then predict movies in movie clusters that<br>are popular with that user’s cluster as likely to be interesting to the new user. In fact,<br>this problem is an instance of collaborative ﬁltering, where users collaborate in the task<br>of ﬁltering information to ﬁnd information of interest.<br>22.3.6<br>Other Types of Mining<br>Text mining applies data mining techniques to textual documents. For instance, there<br>are tools that form clusters on pages that a user has visited; this helps users when<br>they browse the history of their browsing to ﬁnd pages they have visited earlier. The<br>distance between pages can be based, for instance, on common words in the pages<br>(see Section 22.5.1.3). Another application is to classify pages into a Web directory<br>automatically, according to their similarity with other pages (see Section 22.5.5).<br>Data-visualization systems help users to examine large volumes of data, and to<br>detect patterns visually. Visual displays of data—such as maps, charts, and other<br>graphical representations—allow data to be presented compactly to users. A sin-<br>gle graphical screen can encode as much information as a far larger number of text<br>screens. For example, if the user wants to ﬁnd out whether production problems at<br>plants are correlated to the locations of the plants, the problem locations can be en-<br>coded in a special color—say, red—on a map. The user can then quickly discover<br>locations where problems are occurring. The user may then form hypotheses about<br>why problems are occurring in those locations, and may verify the hypotheses quan-<br>titatively against the database.<br>As another example, information about values can be encoded as a color, and can<br>be displayed with as little as one pixel of screen area. To detect associations between<br>pairs of items, we can use a two-dimensional pixel matrix, with each row and each<br>column representing an item. The percentage of transactions that buy both items can<br>be encoded by the color intensity of the pixel. Items with high association will show<br>up as bright pixels in the screen—easy to detect against the darker background.<br>Data visualization systems do not automatically detect patterns, but provide sys-<br>tem support for users to detect patterns. Since humans are very good at detecting<br>visual patterns, data visualization is an important component of data mining.<br>22.4<br>Data Warehousing<br>Large companies have presences in many places, each of which may generate a large<br>volume of data. For instance, large retail chains have hundreds or thousands of stores,<br>whereas insurance companies may have data from thousands of local branches. Fur-<br>ther, large organizations have a complex internal organization structure, and there-<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VII. Other Topics<br>22. Advanced Querying and <br>Information Retrieval<br>836<br>© The McGraw−Hill <br>Companies, 2001<br>22.4<br>Data Warehousing<br>843<br>data<br>loaders<br>DBMS<br>data warehouse<br>query and<br>analysis tools<br>data source n<br>data source 2<br>data source 1<br>…<br>Figure 22.8<br>Data-warehouse architecture.<br>fore different data may be present in different locations, or on different operational<br>systems, or under different schemas. For instance, manufacturing-problem data and<br>customer-complaint data may be stored on different database systems. Corporate de-<br>cision makers require access to information from all such sources. Setting up queries<br>on individual sources is both cumbersome and inefﬁcient. Moreover, the sources of<br>data may store only current data, whereas decision makers may need access to past<br>data as well; for instance, information about how purchase patterns have changed in<br>the past year could be of great importance. Data warehouses provide a solution to<br>these problems.<br>A data warehouse is a repository (or archive) of information gathered from mul-<br>tiple sources, stored under a uniﬁed schema, at a single site. Once gathered, the data<br>are stored for a long time, permitting access to historical data. Thus, data warehouses<br>provide the user a single consolidated interface to data, making decision-support<br>queries easier to write. Moreover, by accessing information for decision support from<br>a data warehouse, the decision maker ensures that online transaction-processing sys-<br>tems are not affected by the decision-support workload.<br>22.4.1<br>Components of a Data Warehouse<br>Figure 22.8 shows the architecture of a typical data warehouse, and illustrates the<br>gathering of data, the storage of data, and the querying and data-analysis support.<br>Among the issues to be addressed in building a warehouse are the following:<br>• When and how to gather data. In a source-driven architecture for gather-<br>ing data, the data sources transmit new information, either continually (as<br>transaction processing takes place), or periodically (nightly, for example). In<br>a destination-driven architecture, the data warehouse periodically sends re-<br>quests for new data to the sources.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VII. Other Topics<br>22. Advanced Querying and <br>Information Retrieval<br>837<br>© The McGraw−Hill <br>Companies, 2001<br>844<br>Chapter 22<br>Advanced Querying and Information Retrieval<br>Unless updates at the sources are replicated at the warehouse via two-phase<br>commit, the warehouse will never be quite up to date with the sources. Two-<br>phase commit is usually far too expensive to be an option, so data warehouses<br>typically have slightly out-of-date data. That, however, is usually not a prob-<br>lem for decision-support systems.<br>• What schema to use. Data sources that have been constructed independently<br>are likely to have different schemas. In fact, they may even use different data<br>models. Part of the task of a warehouse is to perform schema integration, and<br>to convert data to the integrated schema before they are stored. As a result, the<br>data stored in the warehouse are not just a copy of the data at the sources. In-<br>stead, they can be thought of as a materialized view of the data at the sources.<br>• Data cleansing. The task of correcting and preprocessing data is called data<br>cleansing. Data sources often deliver data with numerous minor inconsisten-<br>cies, that can be corrected. For example, names are often misspelled, and ad-<br>dresses may have street/area/city names misspelled, or zip codes entered in-<br>correctly. These can be corrected to a reasonable extent by consulting a data-<br>base of street names and zip codes in each city. Address lists collected from<br>multiple sources may have duplicates that need to be eliminated in a merge–<br>purge operation. Records for multiple individuals in a house may be grouped<br>together so only one mailing is sent to each house; this operation is called<br>householding.<br>• How to propagate updates. Updates on relations at the data sources must<br>be propagated to the data warehouse. If the relations at the data warehouse<br>are exactly the same as those at the data source, the propagation is straight-<br>forward. If they are not, the problem of propagating updates is basically the<br>view-maintenance problem, which was discussed in Section 14.5.<br>• What data to summarize. The raw data generated by a transaction-processing<br>system may be too large to store online. However, we can answer many queries<br>by maintaining just summary data obtained by aggregation on a relation,<br>rather than maintaining the entire relation. For example, instead of storing<br>data about every sale of clothing, we can store total sales of clothing by item-<br>name and category.<br>Suppose that a relation r has been replaced by a summary relation s. Users<br>may still be permitted to pose queries as though the relation r were available<br>online. If the query requires only summary data, it may be possible to trans-<br>form it into an equivalent one using s instead; see Section 14.5.<br>22.4.2<br>Warehouse Schemas<br>Data warehouses typically have schemas that are designed for data analysis, using<br>tools such as OLAP tools. Thus, the data are usually multidimensional data, with di-<br>mension attributes and measure attributes. Tables containing multidimensional data<br>are called fact tables and are usually very large. A table recording sales information<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VII. Other Topics<br>22. Advanced Querying and <br>Information Retrieval<br>838<br>© The McGraw−Hill <br>Companies, 2001<br>22.4<br>Data Warehousing<br>845<br>for a retail store, with one tuple for each item that is sold, is a typical example of a fact<br>table. The dimensions of the sales table would include what the item is (usually an<br>item identiﬁer such as that used in bar codes), the date when the item is sold, which<br>location (store) the item was sold from, which customer bought the item, and so on.<br>The measure attributes may include the number of items sold and the price of the<br>items.<br>To minimize storage requirements, dimension attributes are usually short identi-<br>ﬁers that are foreign keys into other other tables called dimension tables. For<br>instance, a fact table sales would have attributes item-id, store-id, customer-id, and date,<br>and measure attributes number and price. The attribute store-id is a foreign key into<br>a dimension table store, which has other attributes such as store location (city, state,<br>country). The item-id attribute of the sales table would be a foreign key into a di-<br>mension table item-info, which would contain information such as the name of the<br>item, the category to which the item belongs, and other item details such as color and<br>size. The customer-id attribute would be a foreign key into a customer table containing<br>attributes such as name and address of the customer. We can also view the date at-<br>tribute as a foreign key into a date-info table giving the month, quarter, and year of<br>each date.<br>The resultant schema appears in Figure 22.9. Such a schema, with a fact table,<br>multiple dimension tables, and foreign keys from the fact table to the dimension ta-<br>bles, is called a star schema. More complex data warehouse designs may have multi-<br>ple levels of dimension tables; for instance, the item-info table may have an attribute<br>manufacturer-id that is a foreign key into another table giving details of the manufac-<br>turer. Such schemas are called snowﬂake schemas. Complex data warehouse designs<br>may also have more than one fact table.<br>item-id<br>store-id<br>store-id<br>item-id<br>itemname<br>color<br>size<br>item-info<br>sales<br>store<br>city<br>state<br>country<br>date<br>month<br>quarter<br>year<br>date-info<br>number<br>date<br>customer-id<br>customer<br>customer-id<br>name<br>street<br>city<br>state<br>zipcode<br>country<br>category<br>price<br>Figure 22.9<br>Star schema for a data warehouse.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VII. Other Topics<br>22. Advanced Querying and <br>Information Retrieval<br>839<br>© The McGraw−Hill <br>Companies, 2001<br>846<br>Chapter 22<br>Advanced Querying and Information Retrieval<br>22.5<br>Information-Retrieval Systems<br>The ﬁeld of information retrieval has developed in parallel with the ﬁeld of databases.<br>In the traditional model used in the ﬁeld of information retrieval, information is orga-<br>nized into documents, and it is assumed that there is a large number of documents.<br>Data contained in documents is unstructured, without any associated schema. The<br>process of information retrieval consists of locating relevant documents, on the basis<br>of user input, such as keywords or example documents.<br>The Web provides a convenient way to get to, and to interact with, information<br>sources across the Internet. However, a persistent problem facing the Web is the ex-<br>plosion of stored information, with little guidance to help the user to locate what<br>is interesting. Information retrieval has played a critical role in making the Web a<br>productive and useful tool, especially for researchers.<br>Traditional examples of information-retrieval systems are online library catalogs<br>and online document-management systems such as those that store newspaper arti-<br>cles. The data in such systems are organized as a collection of documents; a newspaper<br>article or a catalog entry (in a library catalog) are examples of documents. In the con-<br>text of the Web, usually each HTML page is considered to be a document.<br>A user of such a system may want to retrieve a particular document or a particular<br>class of documents. The intended documents are typically described by a set of key-<br>words—for example, the keywords “database system” may be used to locate books<br>on database systems, and the keywords “stock” and “scandal” may be used to locate<br>articles about stock-market scandals. Documents have associated with them a set of<br>keywords, and documents whose keywords contain those supplied by the user are<br>retrieved.<br>Keyword-based information retrieval can be used not only for retrieving textual<br>data, but also for retrieving other types of data, such as video or audio data, that<br>have descriptive keywords associated with them. For instance, a video movie may<br>have associated with it keywords such as its title, director, actors, type, and so on.<br>There are several differences between this model and the models used in tradi-<br>tional database systems.<br>• Database systems deal with several operations that are not addressed in infor-<br>mation-retrieval systems. For instance, database systems deal with updates<br>and with the associated transactional requirements of concurrency control<br>and durability. These matters are viewed as less important in information sys-<br>tems. Similarly, database systems deal with structured information organized<br>with relatively complex data models (such as the relational model or object-<br>oriented data models), whereas information-retrieval systems traditionally<br>have used a much simpler model, where the information in the database is<br>organized simply as a collection of unstructured documents.<br>• Information-retrieval systems deal with several issues that have not been ad-<br>dressed adequately in database systems. For instance, the ﬁeld of information<br>retrieval has dealt with the problems of managing unstructured documents,<br>such as approximate searching by keywords, and of ranking of documents on<br>estimated degree of relevance of the documents to the query.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VII. Other Topics<br>22. Advanced Querying and <br>Information Retrieval<br>840<br>© The McGraw−Hill <br>Companies, 2001<br>22.5<br>Information-Retrieval Systems<br>847<br>22.5.1<br>Keyword Search<br>Information-retrieval systems typically allow query expressions formed using key-<br>words and the logical connectives and, or, and not. For example, a user could ask<br>for all documents that contain the keywords “motorcycle and maintenance,” or docu-<br>ments that contain the keywords “computer or microprocessor,” or even documents<br>that contain the keyword “computer but not database.” A query containing keywords<br>without any of the above connectives is assumed to have ands implicitly connecting<br>the keywords.<br>In full text retrieval, all the words in each document are considered to be key-<br>words. For unstructured documents, full text retrieval is essential since there may be<br>no information about what words in the document are keywords. We shall use the<br>word term to refer to the words in a document, since all words are keywords.<br>In its simplest form an information retrieval system locates and returns all doc-<br>uments that contain all the keywords in the query, if the query has no connectives;<br>connectives are handled as you would expect. More sophisticated systems estimate<br>relevance of documents to a query so that the documents can be shown in order of<br>estimated relevance. They use information about term occurrences, as well as hyper-<br>link information, to estimate relevance; Section 22.5.1.1 and 22.5.1.2 outline how to do<br>so. Section 22.5.1.3 outlines how to deﬁne similarity of documents, and use similarity<br>for searching. Some systems a</span><br><br><span style="background-color: #A0C4FF;" title="Chunk 109 | Start: 2180218 | End: 2200218 | Tokens: 3363">lso attempt to provide a better set of answers by using<br>the meanings of terms, rather than just the syntactic occurrence of terms, as outlined<br>in Section 22.5.1.4.<br>22.5.1.1<br>Relevance Ranking Using Terms<br>The set of all documents that satisfy a query expression may be very large; in par-<br>ticular, there are billions of documents on the Web, and most keyword queries on<br>a Web search engine ﬁnd hundreds of thousands of documents containing the key-<br>words. Full text retrieval makes this problem worse: Each document may contain<br>many terms, and even terms that are only mentioned in passing are treated equiva-<br>lently with documents where the term is indeed relevant. Irrelevant documents may<br>get retrieved as a result.<br>Information retrieval systems therefore estimate relevance of documents to a query,<br>and return only highly ranked documents as answers. Relevance ranking is not an<br>exact science, but there are some well-accepted approaches.<br>The ﬁrst question to address is, given a particular term t, how relevant is a partic-<br>ular document d to the term. One approach is to use the the number of occurrences<br>of the term in the document as a measure of its relevance, on the assumption that<br>relevant terms are likely to be mentioned many times in a document. Just counting<br>the number of occurrences of a term is usually not a good indicator: First, the num-<br>ber of occurrences depends on the length of the document, and second, a document<br>containing 10 occurrences of a term may not be 10 times as relevant as a document<br>containing one occurrence.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VII. Other Topics<br>22. Advanced Querying and <br>Information Retrieval<br>841<br>© The McGraw−Hill <br>Companies, 2001<br>848<br>Chapter 22<br>Advanced Querying and Information Retrieval<br>One way of measuring r(d, t), the relevance of a document d to a term t, is<br>r(d, t) = log<br><br>1 + n(d, t)<br>n(d)<br><br>where n(d) denotes the number of terms in the document and n(d, t) denotes the<br>number of occurrences of term t in the document d. Observe that this metric takes<br>the length of the document into account. The relevance grows with more occurrences<br>of a term in the document, although it is not directly proportional to the number of<br>occurrences.<br>Many systems reﬁne the above metric by using other information. For instance, if<br>the term occurs in the title, or the author list, or the abstract, the document would be<br>considered more relevant to the term. Similarly, if the ﬁrst occurrence of a term is late<br>in the document, the document may be considered less relevant than if the ﬁrst oc-<br>currence is early in the document. The above notions can be formalized by extensions<br>of the formula we have shown for r(d, t). In the information retrieval community, the<br>relevance of a document to a term is referred to as term frequency, regardless of the<br>exact formula used.<br>A query Q may contain multiple keywords. The relevance of a document to a<br>query with two or more keywords is estimated by combining the relevance measures<br>of the document to each keyword. A simple way of combining the measures is to<br>add them up. However, not all terms used as keywords are equal. Suppose a query<br>uses two terms, one of which occurs frequently, such as “web,” and another that is<br>less frequent, such as “Silberschatz.” A document containing “Silberschatz” but not<br>“web” should be ranked higher than a document containing the term “web” but not<br>“Silberschatz.”<br>To ﬁx the above problem, weights are assigned to terms using the inverse doc-<br>ument frequency, deﬁned as 1/n(t), where n(t) denotes the number of documents<br>(among those indexed by the system) that contain the term t. The relevance of a doc-<br>ument d to a set of terms Q is then deﬁned as<br>r(d, Q) =<br><br>t∈Q<br>r(d, t)<br>n(t)<br>This measure can be further reﬁned if the user is permitted to specify weights w(t)<br>for terms in the query, in which case the user-speciﬁed weights are also taken into<br>account by using w(t)/n(t) in place of 1/n(t).<br>Almost all text documents (in English) contain words such as “and,” “or,” “a,”<br>and so on, and hence these words are useless for querying purposes since their in-<br>verse document frequency is extremely low. Information-retrieval systems deﬁne a<br>set of words, called stop words, containing 100 or so of the most common words,<br>and remove this set from the document when indexing; such words are not used as<br>keywords, and are discarded if present in the keywords supplied by the user.<br>Another factor taken into account when a query contains multiple terms is the<br>proximity of the term in the document. If the terms occur close to each other in the<br>document, the document would be ranked higher than if they occur far apart. The<br>formula for r(d, Q) can be modiﬁed to take proximity into account.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VII. Other Topics<br>22. Advanced Querying and <br>Information Retrieval<br>842<br>© The McGraw−Hill <br>Companies, 2001<br>22.5<br>Information-Retrieval Systems<br>849<br>Given a query Q, the job of an information retrieval system is to return documents<br>in descending order of their relevance to Q. Since there may be a very large number<br>of documents that are relevant, information retrieval systems typically return only<br>the ﬁrst few documents with the highest degree of estimated relevance, and permit<br>users to interactively request further documents.<br>22.5.1.2<br>Relevance Using Hyperlinks<br>Early Web search engines ranked documents by using only relevance measures simi-<br>lar to those described in Section 22.5.1.1. However, researchers soon realized that Web<br>documents have information that plain text documents do not have, namely hyper-<br>links. And in fact, the relevance ranking of a document is affected more by hyperlinks<br>that point to the document, than by hyperlinks going out of the document.<br>The basic idea of site ranking is to ﬁnd sites that are popular, and to rank pages<br>from such sites higher than pages from other sites. A site is identiﬁed by the in-<br>ternet address part of the URL, such as www.bell-labs.com in a URL http://www.bell-<br>labs.com/topic/books/db-book. A site usually contains multiple Web pages. Since most<br>searches are intended to ﬁnd information from popular sites, ranking pages from<br>popular sites higher is generally a good idea. For instance, the term “google” may oc-<br>cur in vast numbers of pages, but the site google.com is the most popular among the<br>sites with pages that contain the term “google”. Documents from google.com con-<br>taining the term “google” would therefore be ranked as the most relevant to the term<br>“google”.<br>This raises the question of how to deﬁne the popularity of a site. One way would<br>be to ﬁnd how many times a site is accessed. However, getting such information<br>is impossible without the cooperation of the site, and is infeasible for a Web search<br>engine to implement. A very effective alternative uses hyperlinks; it deﬁnes p(s), the<br>popularity of a site s, as the number of sites that contain at least one page with a link<br>to site s.<br>Traditional measures of relevance of the page (which we saw in Section 22.5.1.2)<br>can be combined with the popularity of the site containing the page to get an overall<br>measure of the relevance of the page. Pages with high overall relevance value are<br>returned as answers to a query, as before.<br>Note also that we used the popularity of a site as a measure of relevance of indi-<br>vidual pages at the site, not the popularity of individual pages. There are at least two<br>reasons for this. First, most sites contain only links to root pages of other sites, so all<br>other pages would appear to have almost zero popularity, when in fact they may be<br>accessed quite frequently by following links from the root page. Second, there are far<br>fewer sites than pages, so computing and using popularity of sites is cheaper than<br>computing and using popularity of pages.<br>There are more reﬁned notions of popularity of sites. For instance, a link from<br>a popular site to another site s may be considered to be a better indication of the<br>popularity of s than a link to s from a less popular site.6 This notion of popularity<br>6.<br>This is similar in some sense to giving extra weight to endorsements of products by celebrities (such<br>as ﬁlm stars), so its signiﬁcance is open to question!<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VII. Other Topics<br>22. Advanced Querying and <br>Information Retrieval<br>843<br>© The McGraw−Hill <br>Companies, 2001<br>850<br>Chapter 22<br>Advanced Querying and Information Retrieval<br>is in fact circular, since the popularity of a site is deﬁned by the popularity of other<br>sites, and there may be cycles of links between sites. However, the popularity of sites<br>can be deﬁned by a system of simultaneous linear equations, which can be solved by<br>matrix manipulation techniques. The linear equations are deﬁned in such a way that<br>they have a unique and well-deﬁned solution.<br>The popular Web search engine google.com uses the referring-site popularity idea<br>in its deﬁnition page rank, which is a measure of popularity of a page. This approach<br>of ranking of pages gave results so much better than previously used ranking tech-<br>niques, that google.com became a widely used search engine, in a rather short period<br>of time.<br>There is another, somewhat similar, approach, derived interestingly from a theory<br>of social networking developed by sociologists in the 1950s. In the social networking<br>context, the goal was to deﬁne the prestige of people. For example, the president<br>of the United States has high prestige since a large number of people know him. If<br>someone is known by multiple prestigious people, then she also has high prestige,<br>even if she is not known by as large a number of people.<br>The above idea was developed into a notion of hubs and authorities that takes into<br>account the presence of directories that link to pages containing useful information.<br>A hub is a page that stores links to many pages; it does not in itself contain actual<br>information on a topic, but points to pages that contain actual information. In con-<br>trast, an authority is a page that contains actual information on a topic, although<br>it may not be directly pointed to by many pages. Each page then gets a prestige<br>value as a hub (hub-prestige), and another prestige value as an authority (authority-<br>prestige). The deﬁnitions of prestige, as before, are cyclic and are deﬁned by a set of<br>simultaneous linear equations. A page gets higher hub-prestige if it points to many<br>pages with high authority-prestige, while a page gets higher authority-prestige if it is<br>pointed to by many pages with high hub-prestige. Given a query, pages with highest<br>authority-prestige are ranked higher than other pages. See the bibliographical notes<br>for references giving further details.<br>22.5.1.3<br>Similarity-Based Retrieval<br>Certain information-retrieval systems permit similarity-based retrieval. Here, the<br>user can give the system document A, and ask the system to retrieve documents<br>that are “similar” to A. The similarity of a document to another may be deﬁned, for<br>example, on the basis of common terms. One approach is to ﬁnd k terms in A with<br>highest values of r(d, t), and to use these k terms as a query to ﬁnd relevance of other<br>documents. The terms in the query are themselves weighted by r(d, t).<br>If the set of documents similar to A is large, the system may present the user a<br>few of the similar documents, allow him to choose the most relevant few, and start a<br>new search based on similarity to A and to the chosen documents. The resultant set<br>of documents is likely to be what the user intended to ﬁnd.<br>The same idea is also used to help users who ﬁnd many documents that appear to<br>be relevant on the basis of the keywords, but are not. In such a situation, instead of<br>adding further keywords to the query, users may be allowed to identify one or a few<br>of the returned documents as relevant; the system then uses the identiﬁed documents<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VII. Other Topics<br>22. Advanced Querying and <br>Information Retrieval<br>844<br>© The McGraw−Hill <br>Companies, 2001<br>22.5<br>Information-Retrieval Systems<br>851<br>to ﬁnd other similar ones. The resultant set of documents is likely to be what the user<br>intended to ﬁnd.<br>22.5.1.4<br>Synonyms and Homonyms<br>Consider the problem of locating documents about motorcycle maintenance for the<br>keywords “motorcycle” and “maintenance.” Suppose that the keywords for each doc-<br>ument are the words in the title and the names of the authors. The document titled<br>Motorcycle Repair would not be retrieved, since the word “maintenance” does not oc-<br>cur in its title.<br>We can solve that problem by making use of synonyms. Each word can have a set<br>of synonyms deﬁned, and the occurrence of a word can be replaced by the or of all<br>its synonyms (including the word itself). Thus, the query “motorcycle and repair” can<br>be replaced by “motorcycle and (repair or maintenance).” This query would ﬁnd the<br>desired document.<br>Keyword-based queries also suffer from the opposite problem, of homonyms, that<br>is single words with multiple meanings. For instance, the word object has different<br>meanings as a noun and as a verb. The word table may refer to a dinner table, or to a<br>relational table. Some keyword query systems attempt to disambiguate the meaning<br>of words in documents, and when a user poses a query, they ﬁnd out the intended<br>meaning by asking the user. The returned documents are those that use the term in<br>the intended meaning of the user. However, disambiguating meanings of words in<br>documents is not an easy task, so not many systems implement this idea.<br>In fact, a danger even with using synonyms to extend queries is that the synonyms<br>may themselves have different meanings. Documents that use the synonyms with an<br>alternative intended meaning would be retrieved. The user is then left wondering<br>why the system thought that a particular retrieved document is relevant, if it contains<br>neither the keywords the user speciﬁed, nor words whose intended meaning in the<br>document is synonymous with speciﬁed keywords! It is therefore advisable to verify<br>synonyms with the user, before using them to extend a query submitted by the user.<br>22.5.2<br>Indexing of Documents<br>An effective index structure is important for efﬁcient processing of queries in an<br>information-retrieval system. Documents that contain a speciﬁed keyword can be<br>efﬁciently located by using an inverted index, which maps each keyword Ki to the<br>set Si of (identiﬁers of) the documents that contain Ki. To support relevance ranking<br>based on proximity of keywords, such an index may provide not just identiﬁers of<br>documents, but also a list of locations in the document where the keyword appears.<br>Since such indices must be stored on disk, the index organization also attempts to<br>minimize the number of I/O operations to retrieve the set of (identiﬁers of) docu-<br>ments that contain a keyword. Thus, the system may attempt to keep the set of doc-<br>uments for a keyword in consecutive disk pages.<br>The and operation ﬁnds documents that contain all of a speciﬁed set of keywords<br>K1, K2, . . . , Kn. We implement the and operation by ﬁrst retrieving the sets of docu-<br>ment identiﬁers S1, S2, . . . , Sn of all documents that contain the respective keywords.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VII. Other Topics<br>22. Advanced Querying and <br>Information Retrieval<br>845<br>© The McGraw−Hill <br>Companies, 2001<br>852<br>Chapter 22<br>Advanced Querying and Information Retrieval<br>The intersection, S1 ∩S2 ∩· · · ∩Sn, of the sets gives the document identiﬁers of the<br>desired set of documents. The or operation gives the set of all documents that contain<br>at least one of the keywords K1, K2, . . . , Kn. We implement the or operation by com-<br>puting the union, S1∪S2∪· · ·∪Sn, of the sets. The not operation ﬁnds documents that<br>do not contain a speciﬁed keyword Ki. Given a set of document identiﬁers S, we can<br>eliminate documents that contain the speciﬁed keyword Ki by taking the difference<br>S −Si, where Si is the set of identiﬁers of documents that contain the keyword Ki.<br>Given a set of keywords in a query, many information retrieval systems do not<br>insist that the retrieved documents contain all the keywords (unless an and operation<br>is explicitly used). In this case, all documents containing at least one of the words are<br>retrieved (as in the or operation), but are ranked by their relevance measure.<br>To use term frequency for ranking, the index structure should additionally main-<br>tain the number of times terms occur in each document. To reduce this effort, they<br>may use a compressed representation with only a few bits, which approximates the<br>term frequency. The index should also store the document frequency of each term<br>(that is, the number of documents in which the term appears).<br>22.5.3<br>Measuring Retrieval Effectiveness<br>Each keyword may be contained in a large number of documents; hence, a compact<br>representation is critical to keep space usage of the index low. Thus, the sets of doc-<br>uments for a keyword are maintained in a compressed form. So that storage space<br>is saved, the index is sometimes stored such that the retrieval is approximate; a few<br>relevant documents may not be retrieved (called a false drop or false negative), or<br>a few irrelevant documents may be retrieved (called a false positive). A good index<br>structure will not have any false drops, but may permit a few false positives; the sys-<br>tem can ﬁlter them away later by looking at the keywords that they actually contain.<br>In Web indexing, false positives are not desirable either, since the actual document<br>may not be quickly accessible for ﬁltering.<br>Two metrics are used to measure how well an information-retrieval system is able<br>to answer queries. The ﬁrst, precision, measures what percentage of the retrieved<br>documents are actually relevant to the query. The second, recall, measures what per-<br>centage of the documents relevant to the query were retrieved. Ideally both should<br>be 100 percent.<br>Precision and recall are also important measures for understanding how well a<br>particular document ranking strategy performs. Ranking strategies can result in false<br>negatives and false positives, but in a more subtle sense.<br>• False negatives may occur when documents are ranked, because relevant doc-<br>uments get low rankings; if we fetched all documents down to documents<br>with very low ranking there would be very few false negatives. However, hu-<br>mans would rarely look beyond the ﬁrst few tens of returned documents, and<br>may thus miss relevant documents because they are not ranked among the<br>top few. Exactly what is a false negative depends on how many documents<br>are examined.<br>Therefore instead of having a single number as the measure of recall, we<br>can measure the recall as a function of the number of documents fetched.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VII. Other Topics<br>22. Advanced Querying and <br>Information Retrieval<br>846<br>© The McGraw−Hill <br>Companies, 2001<br>22.5<br>Information-Retrieval Systems<br>853<br>• False positives may occur because irrelevant documents get higher rankings<br>than relevant documents. This too depends on how many documents are ex-<br>amined. One option is to measure precision as a function of number of docu-<br>ments fetched.<br>A better and more intuitive alternative for measuring precision is to measure it<br>as a function of recall. With this combined measure, both precision and recall can be<br>computed as a function of number of documents, if required.<br>For instance, we can say that with a recall of 50 percent the precision was 75 per-<br>cent, whereas at a recall of 75 percent the precision dropped to 60 percent. In general,<br>we can draw a graph relating precision to recall. These measures can be computed for<br>individual queries, then averaged out across a suite of queries in a query benchmark.<br>Yet another problem with measuring precision and recall lies in how to deﬁne<br>which documents are really relevant and which are not. In fact, </span><br><br><span style="background-color: #BDB2FF;" title="Chunk 110 | Start: 2200220 | End: 2220220 | Tokens: 3206">it requires under-<br>standing of natural language, and understanding of the intent of the query, to decide<br>if a document is relevant or not. Researchers therefore have created collections of doc-<br>uments and queries, and have manually tagged documents as relevant or irrelevant<br>to the queries. Different ranking systems can be run on these collections to measure<br>their average precision and recall across multiple queries.<br>22.5.4<br>Web Search Engines<br>Web crawlers are programs that locate and gather information on the Web. They<br>recursively follow hyperlinks present in known documents to ﬁnd other documents.<br>A crawler retrieves the documents and adds information found in the documents to a<br>combined index; the document is generally not stored, although some search engines<br>do cache a copy of the document to give clients faster access to the documents.<br>Since the number of documents on the Web is very large, it is not possible to crawl<br>the whole Web in a short period of time; and in fact, all search engines cover only<br>some portions of the Web, not all of it, and their crawlers may take weeks or months<br>to perform a single crawl of all the pages they cover. There are usually many pro-<br>cesses, running on multiple machines, involved in crawling. A database stores a set<br>of links (or sites) to be crawled; it assigns links from this set to each crawler process.<br>New links found during a crawl are added to the database, and may be crawled later<br>if they are not crawled immediately. Pages found during a crawl are also handed over<br>to an indexing system, which may be running on a different machine. Pages have to<br>be refetched (that is, links recrawled) periodically to obtain updated information, and<br>to discard sites that no longer exist, so that the information in the search index is kept<br>reasonably up to date.<br>The indexing system itself runs on multiple machines in parallel. It is not a good<br>idea to add pages to the same index that is being used for queries, since doing so<br>would require concurrency control on the index, and affect query and update perfor-<br>mance. Instead, one copy of the index is used to answer queries while another copy<br>is updated with newly crawled pages. At periodic intervals the copies switch over,<br>with the old one being updated while the new copy is being used for queries.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VII. Other Topics<br>22. Advanced Querying and <br>Information Retrieval<br>847<br>© The McGraw−Hill <br>Companies, 2001<br>854<br>Chapter 22<br>Advanced Querying and Information Retrieval<br>To support very high query rates, the indices may be kept in main memory, and<br>there are multiple machines; the system selectively routes queries to the machines to<br>balance the load among them.<br>22.5.5<br>Directories<br>A typical library user may use a catalog to locate a book for which she is looking.<br>When she retrieves the book from the shelf, however, she is likely to browse through<br>other books that are located nearby. Libraries organize books in such a way that re-<br>lated books are kept close together. Hence, a book that is physically near the desired<br>book may be of interest as well, making it worthwhile for users to browse through<br>such books.<br>To keep related books close together, libraries use a classiﬁcation hierarchy. Books<br>on science are classiﬁed together. Within this set of books, there is a ﬁner classiﬁca-<br>tion, with computer-science books organized together, mathematics books organized<br>together, and so on. Since there is a relation between mathematics and computer sci-<br>ence, relevant sets of books are stored close to each other physically. At yet another<br>level in the classiﬁcation hierarchy, computer-science books are broken down into<br>subareas, such as operating systems, languages, and algorithms. Figure 22.10 illus-<br>trates a classiﬁcation hierarchy that may be used by a library. Because books can be<br>kept at only one place, each book in a library is classiﬁed into exactly one spot in the<br>classiﬁcation hierarchy.<br>In an information retrieval system, there is no need to store related documents<br>close together. However, such systems need to organize documents logically so as to<br>permit browsing. Thus, such a system could use a classiﬁcation hierarchy similar to<br>books<br>algorithms<br>graph algorithms<br>math<br>science<br>fiction<br>engineering<br>computer science<br>…<br>…<br>…<br>…<br>…<br>…<br>…<br>…<br>Figure 22.10<br>A classiﬁcation hierarchy for a library system.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VII. Other Topics<br>22. Advanced Querying and <br>Information Retrieval<br>848<br>© The McGraw−Hill <br>Companies, 2001<br>22.5<br>Information-Retrieval Systems<br>855<br>one that libraries use, and, when it displays a particular document, it can also display<br>a brief description of documents that are close in the hierarchy.<br>In an information retrieval system, there is no need to keep a document in a single<br>spot in the hierarchy. A document that talks of mathematics for computer scientists<br>could be classiﬁed under mathematics as well as under computer science. All that is<br>stored at each spot is an identiﬁer of the document (that is, a pointer to the document),<br>and it is easy to fetch the contents of the document by using the identiﬁer.<br>As a result of this ﬂexibility, not only can a document be classiﬁed under two lo-<br>cations, but also a subarea in the classiﬁcation hierarchy can itself occur under two<br>areas. The class of “graph algorithm” document can appear both under mathemat-<br>ics and under computer science. Thus, the classiﬁcation hierarchy is now a directed<br>acyclic graph (DAG), as shown in Figure 22.11. A graph-algorithm document may<br>appear in a single location in the DAG, but can be reached via multiple paths.<br>A directory is simply a classiﬁcation DAG structure. Each leaf of the directory<br>stores links to documents on the topic represented by the leaf. Internal nodes may<br>also contain links, for example to documents that cannot be classiﬁed under any of<br>the child nodes.<br>To ﬁnd information on a topic, a user would start at the root of the directory and<br>follow paths down the DAG until reaching a node representing the desired topic.<br>While browsing down the directory, the user can ﬁnd not only documents on the<br>topic he is interested in, but also ﬁnd related documents and related classes in the<br>classiﬁcation hierarchy. The user may learn new information by browsing through<br>documents (or subclasses) within the related classes.<br>Organizing the enormous amount of information available on the Web into a di-<br>rectory structure is a daunting task.<br>books<br>algorithms<br>graph algorithms<br>math<br>science<br>fiction<br>engineering<br>computer science<br>…<br>…<br>…<br>…<br>…<br>…<br>…<br>…<br>Figure 22.11<br>A classiﬁcation DAG for a library information retrieval system.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VII. Other Topics<br>22. Advanced Querying and <br>Information Retrieval<br>849<br>© The McGraw−Hill <br>Companies, 2001<br>856<br>Chapter 22<br>Advanced Querying and Information Retrieval<br>• The ﬁrst problem is determining what exactly the directory hierarchy should<br>be.<br>• The second problem is, given a document, deciding which nodes of the direc-<br>tory are categories relevant to the document.<br>To tackle the ﬁrst problem, portals such as Yahoo have teams of “internet librar-<br>ians” who come up with the classiﬁcation hierarchy and continually reﬁne it. The<br>Open Directory Project is a large collaborative effort, with different volunteers being<br>responsible for organizing different branches of the directory.<br>The second problem can also be tackled manually by librarians, or Web site main-<br>tainers may be responsible for deciding where their sites should lie in the hierarchy.<br>There are also techniques for automatically deciding the location of documents based<br>on computing their similarity to documents that have already been classiﬁed.<br>22.6<br>Summary<br>• Decision-support systems analyze online data collected by transaction-<br>processing systems, to help people make business decisions. Since most or-<br>ganizations are extensively computerized today, a very large body of infor-<br>mation is available for decision support. Decision-support systems come in<br>various forms, including OLAP systems and data mining systems.<br>• Online analytical processing (OLAP) tools help analysts view data summa-<br>rized in different ways, so that they can gain insight into the functioning of an<br>organization.<br>  OLAP tools work on multidimensional data, characterized by dimension<br>attributes and measure attributes.<br>  The data cube consists of multidimensional data summarized in different<br>ways. Precomputing the data cube helps speed up queries on summaries<br>of data.<br>  Cross-tab displays permit users to view two dimensions of multidimen-<br>sional data at a time, along with summaries of the data.<br>  Drill down, rollup, slicing, and dicing are among the operations that users<br>perform with OLAP tools.<br>• The OLAP component of the SQL:1999 standard provides a variety of new func-<br>tionality for data analysis, including new aggregate functions, cube and rollup<br>operations, ranking functions, windowing functions, which support summa-<br>rization on moving windows, and partitioning, with windowing and ranking<br>applied inside each partition.<br>• Data mining is the process of semiautomatically analyzing large databases<br>to ﬁnd useful patterns. There are a number of applications of data mining,<br>such as prediction of values based on past examples, ﬁnding of associations<br>between purchases, and automatic clustering of people and movies.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VII. Other Topics<br>22. Advanced Querying and <br>Information Retrieval<br>850<br>© The McGraw−Hill <br>Companies, 2001<br>22.6<br>Summary<br>857<br>• Classiﬁcation deals with predicting the class of test instances, by using at-<br>tributes of the test instances, based on attributes of training instances, and the<br>actual class of training instances. Classiﬁcation can be used, for instance, to<br>predict credit-worthiness levels of new applicants, or to predict the perfor-<br>mance of applicants to a university.<br>There are several types of classiﬁers, such as<br>  Decision-tree classiﬁers. These perform classiﬁcation by constructing a<br>tree based on training instances with leaves having class labels. The tree<br>is traversed for each test instance to ﬁnd a leaf, and the class of the leaf is<br>the predicted class.<br>Several techniques are available to construct decision trees, most of<br>them based on greedy heuristics.<br>  Bayesian classiﬁers are simpler to construct than decision-tree classiﬁers,<br>and work better in the case of missing/null attribute values.<br>• Association rules identify items that co-occur frequently, for instance, items<br>that tend to be bought by the same customer. Correlations look for deviations<br>from expected levels of association.<br>• Other types of data mining include clustering, text mining, and data visual-<br>ization.<br>• Data warehouses help gather and archive important operational data. Ware-<br>houses are used for decision support and analysis on historical data, for in-<br>stance to predict trends. Data cleansing from input data sources is often a<br>major task in data warehousing. Warehouse schemas tend to be multidimen-<br>sional, involving one or a few very large fact tables and several much smaller<br>dimension tables.<br>• Information retrieval systems are used to store and query textual data such<br>as documents. They use a simpler data model than do database systems, but<br>provide more powerful querying capabilities within the restricted model.<br>Queries attempt to locate documents that are of interest by specifying, for<br>example, sets of keywords. The query that a user has in mind usually cannot<br>be stated precisely; hence, information-retrieval systems order answers on the<br>basis of potential relevance.<br>• Relevance ranking makes use of several types of information, such as:<br>  Term frequency: how important each term is to each document.<br>  Inverse document frequency.<br>  Site popularity. Page rank and hub/authority rank are two ways to assign<br>importance to sites on the basis of links to the site.<br>• Similarity of documents is used to retrieve documents similar to an example<br>document. Synonyms and homonyms complicate the task of information re-<br>trieval.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VII. Other Topics<br>22. Advanced Querying and <br>Information Retrieval<br>851<br>© The McGraw−Hill <br>Companies, 2001<br>858<br>Chapter 22<br>Advanced Querying and Information Retrieval<br>• Precision and recall are two measures of the effectiveness of an information<br>retrieval system.<br>• Directory structures are used to classify documents with other similar docu-<br>ments.<br>Review Terms<br>• Decision-support systems<br>• Statistical analysis<br>• Multidimensional data<br>  Measure attributes<br>  Dimension attributes<br>• Cross-tabulation<br>• Data cube<br>• Online analytical processing<br>(OLAP)<br>  Pivoting<br>  Slicing and dicing<br>  Rollup and drill down<br>• Multidimensional OLAP (MOLAP)<br>• Relational OLAP (ROLAP)<br>• Hybrid OLAP (HOLAP)<br>• Extended aggregation<br>  Variance<br>  Standard deviation<br>  Correlation<br>  Regression<br>• Ranking functions<br>  Rank<br>  Dense rank<br>  Partition by<br>• Windowing<br>• Data mining<br>• Prediction<br>• Associations<br>• Classiﬁcation<br>  Training data<br>  Test data<br>• Decision-tree classiﬁers<br>  Partitioning attribute<br>  Partitioning condition<br>  Purity<br>–– Gini measure<br>–– Entropy measure<br>  Information gain<br>  Information content<br>  Information gain ratio<br>  Continuous-valued attribute<br>  Categorical attribute<br>  Binary split<br>  Multiway split<br>  Overﬁtting<br>• Bayesian classiﬁers<br>  Bayes theorem<br>  Naive Bayesian classiﬁers<br>• Regression<br>  Linear regression<br>  Curve ﬁtting<br>• Association rules<br>  Population<br>  Support<br>  Conﬁdence<br>  Large itemsets<br>• Other types of associations<br>• Clustering<br>  Hierarchical clustering<br>  Agglomerative clustering<br>  Divisive clustering<br>• Text mining<br>• Data visualization<br>• Data warehousing<br>  Gathering data<br>  Source-driven architecture<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VII. Other Topics<br>22. Advanced Querying and <br>Information Retrieval<br>852<br>© The McGraw−Hill <br>Companies, 2001<br>Exercises<br>859<br>  Destination-driven architec-<br>ture<br>  Data cleansing<br>–– Merge–purge<br>–– Householding<br>• Warehouse schemas<br>  Fact table<br>  Dimension tables<br>  Star schema<br>• Information retrieval systems<br>• Keyword search<br>• Full text retrieval<br>• Term<br>• Relevance ranking<br>  Term frequency<br>  Inverse document frequency<br>  Relevance<br>  Proximity<br>• Stop words<br>• Relevance using hyperlinks<br>  Site popularity<br>  Page rank<br>  Hub/authority ranking<br>• Similarity-based retrieval<br>• Synonyms<br>• Homonyms<br>• Inverted index<br>• False drop<br>• False negative<br>• False positive<br>• Precision<br>• Recall<br>• Web crawlers<br>• Directories<br>• Classiﬁcation hierarchy<br>Exercises<br>22.1 For each of the SQL aggregate functions sum, count, min and max, show how<br>to compute the aggregate value on a multiset S1 ∪S2, given the aggregate<br>values on multisets S1 and S2.<br>Based on the above, give expressions to compute aggregate values with<br>grouping on a subset S of the attributes of a relation r(A, B, C, D, E), given<br>aggregate values for grouping on attributes T ⊇S, for the following aggregate<br>functions:<br>a. sum, count, min and max<br>b. avg<br>c. standard deviation<br>22.2 Show how to express group by cube(a, b, c, d) using rollup; your answer should<br>have only one group by clause.<br>22.3 Give an example of a pair of groupings that cannot be expressed by using a<br>single group by clause with cube and rollup.<br>22.4 Given a relation S(student, subject, marks), write a query to ﬁnd the top n<br>students by total marks, by using ranking.<br>22.5 Given relation r(a, b, d, d), Show how to use the extended SQL features to gen-<br>erate a histogram of d versus a, dividing a into 20 equal-sized partitions (that<br>is, where each partition contains 5 percent of the tuples in r, sorted by a).<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VII. Other Topics<br>22. Advanced Querying and <br>Information Retrieval<br>853<br>© The McGraw−Hill <br>Companies, 2001<br>860<br>Chapter 22<br>Advanced Querying and Information Retrieval<br>22.6 Write a query to ﬁnd cumulative balances, equivalent to that shown in Sec-<br>tion 22.2.5, but without using the extended SQL windowing constructs.<br>22.7 Consider the balance attribute of the account relation. Write an SQL query to<br>compute a histogram of balance values, dividing the range 0 to the maximum<br>account balance present, into three equal ranges.<br>22.8 Consider the sales relation from Section 22.2. Write an SQL query to compute<br>the cube operation on the relation, giving the relation in Figure 22.2. Do not<br>use the with cube construct.<br>22.9 Construct a decision tree classiﬁer with binary splits at each node, using tu-<br>ples in relation r(A, B, C) shown below as training data; attribute C denotes<br>the class. Show the ﬁnal tree, and with each node show the best split for each<br>attribute along with its information gain value.<br>(1, 2, a), (2, 1, a), (2, 5, b), (3, 3, b), (3, 6, b), (4, 5, b), (5, 5, c), (6, 3, b), (6, 7, c)<br>22.10 Suppose there are two classiﬁcation rules, one that says that people with salaries<br>between $10,000 and $20,000 have a credit rating of good, and another that says<br>that people with salaries between $20,000 and $30,000 have a credit rating of<br>good. Under what conditions can the rules be replaced, without any loss of in-<br>formation, by a single rule that says people with salaries between $10,000 and<br>$30,000 have a credit rating of good.<br>22.11 Suppose half of all the transactions in a clothes shop purchase jeans, and one<br>third of all transactions in the shop purchase T-shirts. Suppose also that half<br>of the transactions that purchase jeans also purchase T-shirts. Write down all<br>the (nontrivial) association rules you can deduce from the above information,<br>giving support and conﬁdence of each rule.<br>22.12 Consider the problem of ﬁnding large itemsets.<br>a. Describe how to ﬁnd the support for a given collection of itemsets by using<br>a single scan of the data. Assume that the itemsets and associated informa-<br>tion, such as counts, will ﬁt in memory.<br>b. Suppose an itemset has support less than j. Show that no superset of this<br>itemset can have support greater than or equal to j.<br>22.13 Describe beneﬁts and drawbacks of a source-driven architecture for gathering<br>of data at a data-warehouse, as compared to a destination-driven architecture.<br>22.14 Consider the schema depicted in Figure 22.9. Give an SQL:1999 query to sum-<br>marize sales numbers and price by store and date, along with the hierarchies<br>on store and date.<br>22.15 Compute the relevance (using appropriate deﬁnitions of term frequency and<br>inverse document frequency) of each of the questions in this chapter to the<br>query “SQL relation.”<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VII. Other Topics<br>22. Advanced Querying and <br>Information Retrieval<br>854<br>© The McGraw−Hill <br>Companies, 2001<br>Bibliographical Notes<br>861<br>22.16 What is the difference between a false positive and a false drop? If it is essential<br>that no relevant information be missed by an information retrieval query, is it<br>acceptable to have either false positives or false drops? Why?<br>22.17 Suppose you want to ﬁnd documents that contain at least k of a given set of n<br>keywords. Suppose also you have a keyword index that gives you a (sorted) list<br>of identiﬁers of documents that contain a speciﬁed keyword. Give an efﬁcient<br>algorithm to ﬁnd the desired set of documents.<br>Bibliographical Notes<br>Gray et al. [1995] and Gray et al. [1997] describe the data-cube operator. Efﬁcient algo-<br>rithms for computing data cubes are described by Agarwal et al. [1996], Harinarayan<br>et al. [1996] and Ross and Srivastava [1997]. Descriptions of extended aggregation<br>support in SQL:1999 can be found in the product manuals of database systems such<br>as Oracle and IBM DB2. Deﬁnitions of statistical functions can be found in standard<br>statistics textbooks such as Bulmer [1979] and Ross [1999].<br>Witten and Frank [1999] and Han and Kamber [2000] provide textbook coverage<br>of data mining. Mitchell [1997] is a classic textbook on machine learning, and c</span><br><br><span style="background-color: #FFC6FF;" title="Chunk 111 | Start: 2220222 | End: 2240222 | Tokens: 3153">overs<br>classiﬁcation techniques in detail. Fayyad et al. [1995] presents an extensive collec-<br>tion of articles on knowledge discovery and data mining. Kohavi and Provost [2001]<br>presents a collection of articles on applications of data mining to electronic commerce.<br>Agrawal et al. [1993] provides an early overview of data mining in databases. Al-<br>gorithms for computing classiﬁers with large training sets are described by Agrawal<br>et al. [1992] and Shafer et al. [1996]; the decision tree construction algorithm described<br>in this chapter is based on the SPRINT algorithm of Shafer et al. [1996]. Agrawal and<br>Srikant [1994] was an early paper on association rule mining. Algorithms for mining<br>of different forms of association rules are described by Srikant and Agrawal [1996a]<br>and Srikant and Agrawal [1996b]. Chakrabarti et al. [1998] describes techniques for<br>mining surprising temporal patterns.<br>Clustering has long been studied in the area of statistics, and Jain and Dubes [1988]<br>provides textbook coverage of clustering. Ng and Han [1994] describes spatial clus-<br>tering techniques. Clustering techniques for large datasets are described by Zhang<br>et al. [1996]. Breese et al. [1998] provides an empirical analysis of different algorithms<br>for collaborative ﬁltering. Techniques for collaborative ﬁltering of news articles are<br>described by Konstan et al. [1997].<br>Chakrabarti [2000] provides a survey of hypertext mining techniques such as hy-<br>pertext classiﬁcation and clustering. Chakrabarti [1999] provides a survey of Web<br>resource discovery. Techniques for integrating data cubes with data mining are de-<br>scribed by Sarawagi [2000].<br>Poe [1995] and Mattison [1996] provide textbook coverage of data warehousing.<br>Zhuge et al. [1995] describes view maintenance in a data-warehousing environment.<br>Witten et al. [1999], Grossman and Frieder [1998], and Baeza-Yates and Ribeiro-<br>Neto [1999] provide textbook descriptions of information retrieval. Indexing of docu-<br>ments is covered in detail by Witten et al. [1999]. Jones and Willet [1997] is a collection<br>of articles on information retrieval. Salton [1989] is an early textbook on information-<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VII. Other Topics<br>22. Advanced Querying and <br>Information Retrieval<br>855<br>© The McGraw−Hill <br>Companies, 2001<br>862<br>Chapter 22<br>Advanced Querying and Information Retrieval<br>retrieval systems. The TREC benchmark (trec.nist.gov) is a benchmark for measuring<br>retrieval effectiveness.<br>Brin and Page [1998] describes the anatomy of the Google search engine, includ-<br>ing the PageRank technique, while a hubs and authorities based ranking technique<br>called HITS is described by Kleinberg [1999]. Bharat and Henzinger [1998] presents a<br>reﬁnement of the HITS ranking technique. A point worth noting is that the PageRank<br>of a page is computed independent of any query, and as a result a highly ranked page<br>which just happens to contain some irrelevant keywords would ﬁgure among the top<br>answers for a query on the irrelevant keywords. In contrast, the HITS algorithm takes<br>the query keywords into account when computing prestige, but has a higher cost for<br>answering queries.<br>Tools<br>A variety of tools are available for each of the applications we have studied in this<br>chapter. Most database vendors provide OLAP tools as part of their database sys-<br>tem, or as add-on applications. These include OLAP tools from Microsoft Corp., Or-<br>acle Express, Informix Metacube. The Arbor Essbase OLAP tool is from an indepen-<br>dent software vendor. The site www.databeacon.com provides an online demo of the<br>databeacon OLAP tools for use on Web and text ﬁle data sources. Many companies<br>also provide analysis tools specialized for speciﬁc applications, such as customer re-<br>lationship management.<br>There is also a wide variety of general purpose data mining tools, including min-<br>ing tools from the SAS Institute, IBM Intelligent Miner, and SGI Mineset. A good<br>deal of expertise is required to apply general purpose mining tools for speciﬁc appli-<br>cations. As a result a large number of mining tools have been developed to address<br>specialized applications. The Web site www.kdnuggets.com provides an extensive di-<br>rectory of mining software, solutions, publications, and so on.<br>Major database vendors also offer data warehousing products coupled with their<br>database systems. These provide support functionality for data modeling, cleans-<br>ing, loading, and querying. The Web site www.dwinfocenter.org provides information<br>datawarehousing products.<br>Google (www.google.com) is a popular search engine. Yahoo (www.yahoo.com)<br>and the Open Directory Project (dmoz.org) provide classiﬁcation hierarchies for Web<br>sites.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VII. Other Topics<br>23. Advanced Data Types <br>and New Applications<br>856<br>© The McGraw−Hill <br>Companies, 2001<br>C<br>H<br>A<br>P<br>T<br>E<br>R<br>2<br>3<br>Advanced Data Types<br>and New Applications<br>For most of the history of databases, the types of data stored in databases were rel-<br>atively simple, and this was reﬂected in the rather limited support for data types<br>in earlier versions of SQL. In the past few years, however, there has been increasing<br>need for handling new data types in databases, such as temporal data, spatial data.<br>and multimedia data.<br>Another major trend in the last decade has created its own issues: the growth<br>of mobile computers, starting with laptop computers and pocket organizers, and in<br>more recent years growing to include mobile phones with built-in computers, and a<br>variety of wearable computers that are increasingly used in commercial applications.<br>In this chapter we study several new data types, and also study database issues<br>dealing with mobile computers.<br>23.1<br>Motivation<br>Before we address each of the topics in detail, we summarize the motivation for, and<br>some important issues in dealing with, each of these types of data.<br>• Temporal data. Most database systems model the current state of the world,<br>for instance, current customers, current students, and courses currently being<br>offered. In many applications, it is very important to store and retrieve infor-<br>mation about past states. Historical information can be incorporated manu-<br>ally into a schema design. However, the task is greatly simpliﬁed by database<br>support for temporal data, which we study in Section 23.2.<br>• Spatial data. Spatial data include geographic data, such as maps and associ-<br>ated information, and computer-aided-design data, such as integrated-circuit<br>designs or building designs. Applications of spatial data initially stored data<br>as ﬁles in a ﬁle system, as did early-generation business applications. But as<br>the complexity and volume of the data, and the number of users, have grown,<br>863<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VII. Other Topics<br>23. Advanced Data Types <br>and New Applications<br>857<br>© The McGraw−Hill <br>Companies, 2001<br>864<br>Chapter 23<br>Advanced Data Types and New Applications<br>ad hoc approaches to storing and retrieving data in a ﬁle system have proved<br>insufﬁcient for the needs of many applications that use spatial data.<br>Spatial-data applications require facilities offered by a database system—<br>in particular, the ability to store and query large amounts of data efﬁciently.<br>Some applications may also require other database features, such as atomic<br>updates to parts of the stored data, durability, and concurrency control. In<br>Section 23.3, we study the extensions needed to traditional database systems<br>to support spatial data.<br>• Multimedia data. In Section 23.4, we study the features required in database<br>systems that store multimedia data such as image, video, and audio data. The<br>main distinguishing feature of video and audio data is that the display of the<br>data requires retrieval at a steady, predetermined rate; hence, such data are<br>called continuous-media data.<br>• Mobile databases. In Section 23.5, we study the database requirements of the<br>new generation of mobile computing systems, such as notebook computers<br>and palmtop computing devices, which are connected to base stations via<br>wireless digital communication networks. Such computers need to be able to<br>operate while disconnected from the network, unlike the distributed database<br>systems discussed in Chapter 19. They also have limited storage capacity, and<br>thus require special techniques for memory management.<br>23.2<br>Time in Databases<br>A database models the state of some aspect of the real world outside itself. Typically,<br>databases model only one state—the current state—of the real world, and do not<br>store information about past states, except perhaps as audit trails. When the state of<br>the real world changes, the database gets updated, and information about the old<br>state gets lost. However, in many applications, it is important to store and retrieve<br>information about past states. For example, a patient database must store informa-<br>tion about the medical history of a patient. A factory monitoring system may store<br>information about current and past readings of sensors in the factory, for analysis.<br>Databases that store information about states of the real world across time are called<br>temporal databases.<br>When considering the issue of time in database systems, we must distinguish be-<br>tween time as measured by the system and time as observed in the real world. The<br>valid time for a fact is the set of time intervals during which the fact is true in the<br>real world. The transaction time for a fact is the time interval during which the fact<br>is current within the database system. This latter time is based on the transaction se-<br>rialization order and is generated automatically by the system. Note that valid-time<br>intervals, being a real-world concept, cannot be generated automatically and must be<br>provided to the system.<br>A temporal relation is one where each tuple has an associated time when it is<br>true; the time may be either valid time or transaction time. Of course, both valid<br>time and transaction time can be stored, in which case the relation is said to be a<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VII. Other Topics<br>23. Advanced Data Types <br>and New Applications<br>858<br>© The McGraw−Hill <br>Companies, 2001<br>23.2<br>Time in Databases<br>865<br>account-<br>branch-name<br>balance<br>from<br>to<br>number<br>A-101<br>Downtown<br>500<br>1999/1/1<br>9:00<br>1999/1/24 11:30<br>A-101<br>Downtown<br>100<br>1999/1/24 11:30<br>*<br>A-215<br>Mianus<br>700<br>2000/6/2<br>15:30<br>2000/8/8<br>10:00<br>A-215<br>Mianus<br>900<br>2000/8/8<br>10:00<br>2000/9/5<br>8:00<br>A-215<br>Mianus<br>700<br>2000/9/5<br>8:00<br>*<br>A-217<br>Brighton<br>750<br>1999/7/5<br>11:00<br>2000/5/1<br>16:00<br>Figure 23.1<br>A temporal account relation.<br>bitemporal relation. Figure 23.1 shows an example of a temporal relation. To simplify<br>the representation, each tuple has only one time interval associated with it; thus, a<br>tuple is represented once for every disjoint time interval in which it is true. Intervals<br>are shown here as a pair of attributes from and to; an actual implementation would<br>have a structured type, perhaps called Interval, that contains both ﬁelds. Note that<br>some of the tuples have a “*” in the to time column; these asterisks indicate that<br>the tuple is true until the value in the to time column is changed; thus, the tuple is<br>true at the current time. Although times are shown in textual form, they are stored<br>internally in a more compact form, such as the number of seconds since some ﬁxed<br>time on a ﬁxed date (such as 12:00 AM, January 1, 1900) that can be translated back<br>to the normal textual form.<br>23.2.1<br>Time Speciﬁcation in SQL<br>The SQL standard deﬁnes the types date, time, and timestamp. The type date con-<br>tains four digits for the year (1–9999), two digits for the month (1–12), and two digits<br>for the date (1–31). The type time contains two digits for the hour, two digits for the<br>minute, and two digits for the second, plus optional fractional digits. The seconds<br>ﬁeld can go beyond 60, to allow for leap seconds that are added during some years<br>to correct for small variations in the speed of rotation of Earth. The type timestamp<br>contains the ﬁelds of date and time, with six fractional digits for the seconds ﬁeld.<br>Since different places in the world have different local times, there is often a need<br>for specifying the time zone along with the time. The Universal Coordinated Time<br>(UTC), is a standard reference point for specifying time, with local times deﬁned as<br>offsets from UTC. (The standard abbreviation is UTC, rather than UCT, since it is an<br>abbreviation of “Universal Coordinated Time” written in French as universel temps<br>coordonn´e.) SQL also supports two types, time with time zone, and timestamp with<br>time zone, which specify the time as a local time plus the offset of the local time from<br>UTC. For instance, the time could be expressed in terms of U.S. Eastern Standard<br>Time, with an offset of −6:00, since U.S. Eastern Standard time is 6 hours behind<br>UTC.<br>SQL supports a type called interval, which allows us to refer to a period of time<br>such as “1 day” or “2 days and 5 hours,” without specifying a particular time when<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VII. Other Topics<br>23. Advanced Data Types <br>and New Applications<br>859<br>© The McGraw−Hill <br>Companies, 2001<br>866<br>Chapter 23<br>Advanced Data Types and New Applications<br>this period starts. This notion differs from the notion of interval we used previously,<br>which refers to an interval of time with speciﬁc starting and ending times.1<br>23.2.2<br>Temporal Query Languages<br>A database relation without temporal information is sometimes called a snapshot<br>relation, since it reﬂects the state in a snapshot of the real world. Thus, a snapshot of<br>a temporal relation at a point in time t is the set of tuples in the relation that are true<br>at time t, with the time-interval attributes projected out. The snapshot operation on a<br>temporal relation gives the snapshot of the relation at a speciﬁed time (or the current<br>time, if the time is not speciﬁed).<br>A temporal selection is a selection that involves the time attributes; a temporal<br>projection is a projection where the tuples in the projection inherit their times from<br>the tuples in the original relation. A temporal join is a join, with the time of a tuple<br>in the result being the intersection of the times of the tuples from which it is derived.<br>If the times do not intersect, the tuple is removed from the result.<br>The predicates precedes, overlaps, and contains can be applied on intervals; their<br>meanings should be clear. The intersect operation can be applied on two intervals, to<br>give a single (possibly empty) interval. However, the union of two intervals may or<br>may not be a single interval.<br>Functional dependencies must be used with care in a temporal relation. Although<br>the account number may functionally determine the balance at any given point in<br>time, obviously the balance can change over time. A temporal functional depen-<br>dency X<br>τ→Y holds on a relation schema R if, for all legal instances r of R, all<br>snapshots of r satisfy the functional dependency X →Y .<br>Several proposals have been made for extending SQL to improve its support of<br>temporal data. SQL:1999 Part 7 (SQL/Temporal), which is currently under develop-<br>ment, is the proposed standard for temporal extensions to SQL.<br>23.3<br>Spatial and Geographic Data<br>Spatial data support in databases is important for efﬁciently storing, indexing, and<br>querying of data based on spatial locations. For example, suppose that we want to<br>store a set of polygons in a database, and to query the database to ﬁnd all polygons<br>that intersect a given polygon. We cannot use standard index structures, such as B-<br>trees or hash indices, to answer such a query efﬁciently. Efﬁcient processing of the<br>above query would require special-purpose index structures, such as R-trees (which<br>we study later) for the task.<br>Two types of spatial data are particularly important:<br>• Computer-aided-design (CAD) data, which includes spatial information<br>about how objects—such as buildings, cars, or aircraft—are constructed.<br>Other important examples of computer-aided-design databases are integrated-<br>circuit and electronic-device layouts.<br>1.<br>Many temporal database researchers feel this type should have been called span since it does not<br>specify an exact start or end time, only the time span between the two.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VII. Other Topics<br>23. Advanced Data Types <br>and New Applications<br>860<br>© The McGraw−Hill <br>Companies, 2001<br>23.3<br>Spatial and Geographic Data<br>867<br>• Geographic data such as road maps, land-usage maps, topographic elevation<br>maps, political maps showing boundaries, land ownership maps, and so on.<br>Geographic information systems are special-purpose databases tailored for<br>storing geographic data.<br>Support for geographic data has been added to many database systems, such as the<br>IBM DB2 Spatial Extender, the Informix Spatial Datablade, and Oracle Spatial.<br>23.3.1<br>Representation of Geometric Information<br>Figure 23.2 illustrates how various geometric constructs can be represented in a data-<br>base, in a normalized fashion. We stress here that geometric information can be rep-<br>resented in several different ways, only some of which we describe.<br>A line segment can be represented by the coordinates of its endpoints. For example,<br>in a map database, the two coordinates of a point would be its latitude and longi-<br>1<br>2<br>1<br>1<br>3<br>3<br>4<br>5<br>2<br>2<br>1<br>3<br>4<br>5<br>2<br>line segment<br>triangle<br>polygon<br>polygon<br>{(x1,y1), (x2,y2)}<br>{(x1,y1), (x2,y2), (x3,y3)}<br>{(x1,y1), (x2,y2), (x3,y3), (x4,y4), (x5,y5)}<br>{(x1,y1), (x2,y2), (x3,y3), ID1}<br>{(x1,y1), (x3,y3), (x4,y4), ID1}<br>{(x1,y1), (x4,y4), (x5,y5), ID1}<br>object<br>representation<br>Figure 23.2<br>Representation of geometric constructs.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VII. Other Topics<br>23. Advanced Data Types <br>and New Applications<br>861<br>© The McGraw−Hill <br>Companies, 2001<br>868<br>Chapter 23<br>Advanced Data Types and New Applications<br>tude. A polyline (also called a linestring) consists of a connected sequence of line seg-<br>ments, and can be represented by a list containing the coordinates of the endpoints<br>of the segments, in sequence. We can approximately represent an arbitrary curve by<br>polylines, by partitioning the curve into a sequence of segments. This representation<br>is useful for two-dimensional features such as roads; here, the width of the road is<br>small enough relative to the size of the full map that it can be considered two dimen-<br>sional. Some systems also support circular arcs as primitives, allowing curves to be<br>represented as sequences of arcs.<br>We can represent a polygon by listing its vertices in order, as in Figure 23.2.2 The<br>list of vertices speciﬁes the boundary of a polygonal region. In an alternative repre-<br>sentation, a polygon can be divided into a set of triangles, as shown in Figure 23.2.<br>This process is called triangulation, and any polygon can be triangulated. The com-<br>plex polygon can be given an identiﬁer, and each of the triangles into which it is<br>divided carries the identiﬁer of the polygon. Circles and ellipses can be represented<br>by corresponding types, or can be approximated by polygons.<br>List-based representations of polylines or polygons are often convenient for query<br>processing. Such non-ﬁrst-normal-form representations are used when supported by<br>the underlying database. So that we can use ﬁxed-size tuples (in ﬁrst-normal form)<br>for representing polylines, we can give the polyline or curve an identiﬁer, and can<br>represent each segment as a separate tuple that also carries with it the identiﬁer of<br>the polyline or curve. Similarly, the triangulated representation of polygons allows a<br>ﬁrst-normal-form relational representation of polygons.<br>The representation of points and line segments in three-dimensional space is sim-<br>ilar to their representation in two-dimensional space, the only difference being that<br>points have an extra z component. Similarly, the representation of planar ﬁgures—<br>such as triangles, rectangles, and other polygons—does not change much whe</span><br><br><span style="background-color: #FFADAD;" title="Chunk 112 | Start: 2240224 | End: 2260224 | Tokens: 3219">n we<br>move to three dimensions. Tetrahedrons and cuboids can be represented in the same<br>way as triangles and rectangles. We can represent arbitrary polyhedra by dividing<br>them into tetrahedrons, just as we triangulate polygons. We can also represent them<br>by listing their faces, each of which is itself a polygon, along with an indication of<br>which side of the face is inside the polyhedron.<br>23.3.2<br>Design Databases<br>Computer-aided-design (CAD) systems traditionally stored data in memory during<br>editing or other processing, and wrote the data back to a ﬁle at the end of a session of<br>editing. The drawbacks of such a scheme include the cost (programming complexity,<br>as well as time cost) of transforming data from one form to another, and the need<br>to read in an entire ﬁle even if only parts of it are required. For large designs, such<br>as the design of a large-scale integrated circuit, or the design of an entire airplane,<br>it may be impossible to hold the complete design in memory. Designers of object-<br>oriented databases were motivated in large part by the database requirements of CAD<br>2.<br>Some references use the term closed polygon to refer to what we call polygons, and refer to polylines as<br>open polygons.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VII. Other Topics<br>23. Advanced Data Types <br>and New Applications<br>862<br>© The McGraw−Hill <br>Companies, 2001<br>23.3<br>Spatial and Geographic Data<br>869<br>systems. Object-oriented databases represent components of the design as objects,<br>and the connections between the objects indicate how the design is structured.<br>The objects stored in a design database are generally geometric objects. Simple<br>two-dimensional geometric objects include points, lines, triangles, rectangles, and,<br>in general, polygons. Complex two-dimensional objects can be formed from simple<br>objects by means of union, intersection, and difference operations. Similarly, com-<br>plex three-dimensional objects may be formed from simpler objects such as spheres,<br>cylinders, and cuboids, by union, intersection, and difference operations, as in Fig-<br>ure 23.3. Three-dimensional surfaces may also be represented by wireframe models,<br>which essentially model the surface as a set of simpler objects, such as line segments,<br>triangles, and rectangles.<br>Design databases also store nonspatial information about objects, such as the ma-<br>terial from which the objects are constructed. We can usually model such information<br>by standard data-modeling techniques. We concern ourselves here with only the spa-<br>tial aspects.<br>Various spatial operations must be performed on a design. For instance, the de-<br>signer may want to retrieve that part of the design that corresponds to a particu-<br>lar region of interest. Spatial-index structures, discussed in Section 23.3.5, are useful<br>for such tasks. Spatial-index structures are multidimensional, dealing with two- and<br>three-dimensional data, rather than dealing with just the simple one-dimensional or-<br>dering provided by the B+-trees.<br>Spatial-integrity constraints, such as “two pipes should not be in the same loca-<br>tion,” are important in design databases to prevent interference errors. Such errors<br>often occur if the design is performed manually, and are detected only when a proto-<br>type is being constructed. As a result, these errors can be expensive to ﬁx. Database<br>support for spatial-integrity constraints helps people to avoid design errors, thereby<br>keeping the design consistent. Implementing such integrity checks again depends on<br>the availability of efﬁcient multidimensional index structures.<br>(a) Difference of cylinders<br>(b) Union of cylinders<br>Figure 23.3<br>Complex three-dimensional objects.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VII. Other Topics<br>23. Advanced Data Types <br>and New Applications<br>863<br>© The McGraw−Hill <br>Companies, 2001<br>870<br>Chapter 23<br>Advanced Data Types and New Applications<br>23.3.3<br>Geographic Data<br>Geographic data are spatial in nature, but differ from design data in certain ways.<br>Maps and satellite images are typical examples of geographic data. Maps may<br>provide not only location information—about boundaries, rivers, and roads, for<br>example—but also much more detailed information associated with locations, such<br>as elevation, soil type, land usage, and annual rainfall.<br>Geographic data can be categorized into two types:<br>• Raster data. Such data consist of bit maps or pixel maps, in two or more di-<br>mensions. A typical example of a two-dimensional raster image is a satellite<br>image of cloud cover, where each pixel stores the cloud visibility in a partic-<br>ular area. Such data can be three-dimensional—for example, the temperature<br>at different altitudes at different regions, again measured with the help of a<br>satellite. Time could form another dimension—for example, the surface tem-<br>perature measurements at different points in time. Design databases generally<br>do not store raster data.<br>• Vector data. Vector data are constructed from basic geometric objects, such as<br>points, line segments, triangles, and other polygons in two dimensions, and<br>cylinders, spheres, cuboids, and other polyhedrons in three dimensions.<br>Map data are often represented in vector format. Rivers and roads may be<br>represented as unions of multiple line segments. States and countries may be<br>represented as polygons. Topological information, such as height, may be rep-<br>resented by a surface divided into polygons covering regions of equal height,<br>with a height value associated with each polygon.<br>23.3.3.1<br>Representation of Geographic Data<br>Geographical features, such as states and large lakes, are represented as complex<br>polygons. Some features, such as rivers, may be represented either as complex curves<br>or as complex polygons, depending on whether their width is relevant.<br>Geographic information related to regions, such as annual rainfall, can be rep-<br>resented as an array—that is, in raster form. For space efﬁciency, the array can be<br>stored in a compressed form. In Section 23.3.5, we study an alternative representa-<br>tion of such arrays by a data structure called a quadtree.<br>As noted in Section 23.3.3, we can represent region information in vector form,<br>using polygons, where each polygon is a region within which the array value is the<br>same. The vector representation is more compact than the raster representation in<br>some applications. It is also more accurate for some tasks, such as depicting roads,<br>where dividing the region into pixels (which may be fairly large) leads to a loss of<br>precision in location information. However, the vector representation is unsuitable<br>for applications where the data are intrinsically raster based, such as satellite images.<br>23.3.3.2<br>Applications of Geographic Data<br>Geographic databases have a variety of uses, including online map services, vehicle-<br>navigation systems; distribution-network information for public-service utilities such<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VII. Other Topics<br>23. Advanced Data Types <br>and New Applications<br>864<br>© The McGraw−Hill <br>Companies, 2001<br>23.3<br>Spatial and Geographic Data<br>871<br>as telephone, electric-power, and water-supply systems; and land-usage information<br>for ecologists and planners.<br>Web-based road map services form a very widely used application of map data.<br>At the simplest level, these systems can be used to generate online road maps of<br>a desired region. An important beneﬁt of online maps is that it is easy to scale the<br>maps to the desired size—that is, to zoom in and out to locate relevant features. Road<br>map services also store information about roads and services, such as the layout of<br>roads, speed limits on roads, road conditions, connections between roads, and one-<br>way restrictions. With this additional information about roads, the maps can be used<br>for getting directions to go from one place to another and for automatic trip planning.<br>Users can query online information about services to locate, for example, hotels, gas<br>stations, or restaurants with desired offerings and price ranges.<br>Vehicle-navigation systems are systems mounted in automobiles, which provide<br>road maps and trip planning services. A useful addition to a mobile geographic infor-<br>mation system such as a vehicle navigation system is a Global Positioning System<br>(GPS) unit, which uses information broadcast from GPS satellites to ﬁnd the current<br>location with an accuracy of tens of meters. With such a system, a driver can never3<br>get lost—the GPS unit ﬁnds the location in terms of latitude, longitude, and elevation<br>and the navigation system can query the geographic database to ﬁnd where and on<br>which road the vehicle is currently located.<br>Geographic databases for public-utility information are becoming increasingly im-<br>portant as the network of buried cables and pipes grows. Without detailed maps,<br>work carried out by one utility may damage the cables of another utility, result-<br>ing in large-scale disruption of service. Geographic databases, coupled with accurate<br>location-ﬁnding systems, can help avoid such problems.<br>So far, we have explained why spatial databases are useful. In the rest of the sec-<br>tion, we shall study technical details, such as representation and indexing of spatial<br>information.<br>23.3.4<br>Spatial Queries<br>There are a number of types of queries that involve spatial locations.<br>• Nearness queries request objects that lie near a speciﬁed location. A query<br>to ﬁnd all restaurants that lie within a given distance of a given point is an<br>example of a nearness query. The nearest-neighbor query requests the object<br>that is nearest to a speciﬁed point. For example, we may want to ﬁnd the<br>nearest gasoline station. Note that this query does not have to specify a limit<br>on the distance, and hence we can ask it even if we have no idea how far the<br>nearest gasoline station lies.<br>• Region queries deal with spatial regions. Such a query can ask for objects that<br>lie partially or fully inside a speciﬁed region. A query to ﬁnd all retail shops<br>within the geographic boundaries of a given town is an example.<br>3.<br>Well, hardly ever!<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VII. Other Topics<br>23. Advanced Data Types <br>and New Applications<br>865<br>© The McGraw−Hill <br>Companies, 2001<br>872<br>Chapter 23<br>Advanced Data Types and New Applications<br>• Queries may also request intersections and unions of regions. For example,<br>given region information, such as annual rainfall and population density, a<br>query may request all regions with a low annual rainfall as well as a high<br>population density.<br>Queries that compute intersections of regions can be thought of as computing the<br>spatial join of two spatial relations—for example, one representing rainfall and the<br>other representing population density—with the location playing the role of join at-<br>tribute. In general, given two relations, each containing spatial objects, the spatial join<br>of the two relations generates either pairs of objects that intersect, or the intersection<br>regions of such pairs.<br>Several join algorithms efﬁciently compute spatial joins on vector data. Although<br>nested-loop join and indexed nested-loop join (with spatial indices) can be used, hash<br>joins and sort–merge joins cannot be used on spatial data. Researchers have pro-<br>posed join techniques based on coordinated traversal of spatial index structures on<br>the two relations. See the bibliographical notes for more information.<br>In general, queries on spatial data may have a combination of spatial and nonspa-<br>tial requirements. For instance, we may want to ﬁnd the nearest restaurant that has<br>vegetarian selections, and that charges less than $10 for a meal.<br>Since spatial data are inherently graphical, we usually query them by using a<br>graphical query language. Results of such queries are also displayed graphically,<br>rather than in tables. The user can invoke various operations on the interface, such as<br>choosing an area to be viewed (for example, by pointing and clicking on suburbs west<br>of Manhattan), zooming in and out, choosing what to display on the basis of selection<br>conditions (for example, houses with more than three bedrooms), overlay of multi-<br>ple maps (for example, houses with more than three bedrooms overlayed on a map<br>showing areas with low crime rates), and so on. The graphical interface constitutes<br>the front end. Extensions of SQL have been proposed to permit relational databases<br>to store and retrieve spatial information efﬁciently, and also allowing queries to mix<br>spatial and nonspatial conditions. Extensions include allowing abstract data types,<br>such as lines, polygons, and bit maps, and allowing spatial conditions, such as con-<br>tains or overlaps.<br>23.3.5<br>Indexing of Spatial Data<br>Indices are required for efﬁcient access to spatial data. Traditional index structures,<br>such as hash indices and B-trees, are not suitable, since they deal only with one-<br>dimensional data, whereas spatial data are typically of two or more dimensions.<br>23.3.5.1<br>k-d Trees<br>To understand how to index spatial data consisting of two or more dimensions, we<br>consider ﬁrst the indexing of points in one-dimensional data. Tree structures, such<br>as binary trees and B-trees, operate by successively dividing space into smaller parts.<br>For instance, each internal node of a binary tree partitions a one-dimensional interval<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VII. Other Topics<br>23. Advanced Data Types <br>and New Applications<br>866<br>© The McGraw−Hill <br>Companies, 2001<br>23.3<br>Spatial and Geographic Data<br>873<br>in two. Points that lie in the left partition go into the left subtree; points that lie in<br>the right partition go into the right subtree. In a balanced binary tree, the partition<br>is chosen so that approximately one-half of the points stored in the subtree fall in<br>each partition. Similarly, each level of a B-tree splits a one-dimensional interval into<br>multiple parts.<br>We can use that intuition to create tree structures for two-dimensional space, as<br>well as in higher-dimensional spaces. A tree structure called a k-d tree was one of<br>the early structures used for indexing in multiple dimensions. Each level of a k-d<br>tree partitions the space into two. The partitioning is done along one dimension at<br>the node at the top level of the tree, along another dimension in nodes at the next<br>level, and so on, cycling through the dimensions. The partitioning proceeds in such<br>a way that, at each node, approximately one-half of the points stored in the subtree<br>fall on one side, and one-half fall on the other. Partitioning stops when a node has<br>less than a given maximum number of points. Figure 23.4 shows a set of points in<br>two-dimensional space, and a k-d tree representation of the set of points. Each line<br>corresponds to a node in the tree, and the maximum number of points in a leaf node<br>has been set at 1. Each line in the ﬁgure (other than the outside box) corresponds to<br>a node in the k-d tree. The numbering of the lines in the ﬁgure indicates the level of<br>the tree at which the corresponding node appears.<br>The k-d-B tree extends the k-d tree to allow multiple child nodes for each internal<br>node, just as a B-tree extends a binary tree, to reduce the height of the tree. k-d-B trees<br>are better suited for secondary storage than k-d trees.<br>3<br>1<br>3<br>2<br>3<br>3<br>2<br>Figure 23.4<br>Division of space by a k-d tree.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VII. Other Topics<br>23. Advanced Data Types <br>and New Applications<br>867<br>© The McGraw−Hill <br>Companies, 2001<br>874<br>Chapter 23<br>Advanced Data Types and New Applications<br>23.3.5.2<br>Quadtrees<br>An alternative representation for two-dimensional data is a quadtree. An example<br>of the division of space by a quadtree appears in Figure 23.5. The set of points is the<br>same as that in Figure 23.4. Each node of a quadtree is associated with a rectangular<br>region of space. The top node is associated with the entire target space. Each non-<br>leaf node in a quadtree divides its region into four equal-sized quadrants, and cor-<br>respondingly each such node has four child nodes corresponding to the four quad-<br>rants. Leaf nodes have between zero and some ﬁxed maximum number of points.<br>Correspondingly, if the region corresponding to a node has more than the maximum<br>number of points, child nodes are created for that node. In the example in Figure 23.5,<br>the maximum number of points in a leaf node is set to 1.<br>This type of quadtree is called a PR quadtree, to indicate it stores points, and that<br>the division of space is divided based on regions, rather than on the actual set of<br>points stored. We can use region quadtrees to store array (raster) information. A<br>node in a region quadtree is a leaf node if all the array values in the region that it<br>covers are the same. Otherwise, it is subdivided further into four children of equal<br>area, and is therefore an internal node. Each node in the region quadtree corresponds<br>to a subarray of values. The subarrays corresponding to leaves either contain just<br>a single array element or have multiple array elements, all of which have the same<br>value.<br>Indexing of line segments and polygons presents new problems. There are exten-<br>sions of k-d trees and quadtrees for this task. However, a line segment or polygon<br>may cross a partitioning line. If it does, it has to be split and represented in each<br>of the subtrees in which its pieces occur. Multiple occurrences of a line segment or<br>polygon can result in inefﬁciencies in storage, as well as inefﬁciencies in querying.<br>Figure 23.5<br>Division of space by a quadtree.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VII. Other Topics<br>23. Advanced Data Types <br>and New Applications<br>868<br>© The McGraw−Hill <br>Companies, 2001<br>23.3<br>Spatial and Geographic Data<br>875<br>23.3.5.3<br>R-Trees<br>A storage structure called an R-tree is useful for indexing of rectangles and other<br>polygons. An R-tree is a balanced tree structure with the indexed polygons stored in<br>leaf nodes, much like a B+-tree. However, instead of a range of values, a rectangular<br>bounding box is associated with each tree node. The bounding box of a leaf node is<br>the smallest rectangle parallel to the axes that contains all objects stored in the leaf<br>node. The bounding box of internal nodes is, similarly, the smallest rectangle parallel<br>to the axes that contains the bounding boxes of its child nodes. The bounding box<br>of a polygon is deﬁned, similarly, as the smallest rectangle parallel to the axes that<br>contains the polygon.<br>Each internal node stores the bounding boxes of the child nodes along with the<br>pointers to the child nodes. Each leaf node stores the indexed polygons, and may<br>optionally store the bounding boxes of the polygons; the bounding boxes help speed<br>up checks for overlaps of the rectangle with the indexed polygons—if a query rect-<br>angle does not overlap with the bounding box of a polygon, it cannot overlap with<br>the polygon either. (If the indexed polygons are rectangles, there is of course no need<br>to store bounding boxes since they are identical to the rectangles.)<br>Figure 23.6 shows an example of a set of rectangles (drawn with a solid line) and<br>the bounding boxes (drawn with a dashed line) of the nodes of an R-tree for the set of<br>rectangles. Note that the bounding boxes are shown with extra space inside them, to<br>make them stand out pictorially. In reality, the boxes would be smaller and ﬁt tightly<br>on the objects that they contain; that is, each side of a bounding box B would touch<br>at least one of the objects or bounding boxes that are contained in B.<br>The R-tree itself is at the right side of Figure 23.6. The ﬁgure refers to the coordi-<br>nates of bounding box i as BBi in the ﬁgure.<br>3<br>BB1<br>BB2<br>BB<br>B<br>C<br>A<br>E<br>F<br>D<br>H I<br>G<br>A<br>B<br>C<br>G<br>I<br>D<br>E<br>F<br>H<br>1<br>0<br>3<br>2<br>Figure 23.6<br>An R-tree.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VII. Other Topics<br>23. Advanced Data Types <br>and New Applications<br>869<br>© The McGraw−Hill <br>Companies, 2001<br>876<br>Chapter 23<br>Advanced Data Types and New Applications<br>We shall now see how to imp</span><br><br><span style="background-color: #FFD6A5;" title="Chunk 113 | Start: 2260226 | End: 2280226 | Tokens: 3216">lement search, insert, and delete operations on an<br>R-tree.<br>• Search: As the ﬁgure shows, the bounding boxes associated with sibling nodes<br>may overlap; in B+-trees, k-d trees, and quadtrees, in contrast, the ranges do<br>not overlap. A search for polygons containing a point therefore has to follow<br>all child nodes whose associated bounding boxes contain the point; as a re-<br>sult, multiple paths may have to be searched. Similarly, a query to ﬁnd all<br>polygons that intersect a given polygon has to go down every node where the<br>associated rectangle intersects the polygon.<br>• Insert: When we insert a polygon into an R-tree, we select a leaf node to hold<br>the polygon. Ideally we should pick a leaf node that has space to hold a new<br>entry, and whose bounding box contains the bounding box of the polygon.<br>However, such a node may not exist; even if it did, ﬁnding the node may be<br>very expensive, since it is not possible to ﬁnd it by a single traversal down<br>from the root. At each internal node we may ﬁnd multiple children whose<br>bounding boxes contain the bounding box of the polygon, and each of these<br>children needs to be explored. Therefore, as a heuristic, in a traversal from the<br>root, if any of the child nodes has a bounding box containing the bounding<br>box of the polygon, the R-tree algorithm chooses one of them arbitrarily. If<br>none of the children satisfy this condition, the algorithm chooses a child node<br>whose bounding box has the maximum overlap with the bounding box of the<br>polygon for continuing the traversal.<br>Once the leaf node has been reached, if the node is already full, the algo-<br>rithm performs node splitting (and propagates splitting upward if required) in<br>a manner very similar to B+-tree insertion. Just as with B+-tree insertion, the<br>R-tree insertion algorithm ensures that the tree remains balanced. Addition-<br>ally, it ensures that the bounding boxes of leaf nodes, as well as internal nodes,<br>remain consistent; that is, bounding boxes of leaves contain all the bounding<br>boxes of the polygons stored at the leaf, while the bounding boxes for internal<br>nodes contain all the bounding boxes of the children nodes.<br>The main difference of the insertion procedure from the B+-tree insertion<br>procedure lies in how the node is split. In a B+-tree, it is possible to ﬁnd a value<br>such that half the entries are less than the midpoint and half are greater than<br>the value. This property does not generalize beyond one dimension; that is,<br>for more than one dimension, it is not always possible to split the entries into<br>two sets so that their bounding boxes do not overlap. Instead, as a heuristic,<br>the set of entries S can be split into two disjoint sets S1 and S2 so that the<br>bounding boxes of S1 and S2 have the minimum total area; another heuristic<br>would be to split the entries into two sets S1 and S2 in such a way that S1<br>and S2 have minimum overlap. The two nodes resulting from the split would<br>contain the entries in S1 and S2 respectively. The cost of ﬁnding splits with<br>minimum total area or overlap can itself be large, so cheaper heuristics, such<br>as the quadratic split heuristic are used. (The heuristic gets is name from the<br>fact that it takes time quadratic in the number of entries.)<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VII. Other Topics<br>23. Advanced Data Types <br>and New Applications<br>870<br>© The McGraw−Hill <br>Companies, 2001<br>23.4<br>Multimedia Databases<br>877<br>The quadratic split heuristic works this way: First, it picks a pair of entries<br>a and b from S such that putting them in the same node would result in a<br>bounding box with the maximum wasted space; that is, the area of the min-<br>imum bounding box of a and b minus the sum of the areas of a and b is the<br>largest. The heuristic places the entries a and b in sets S1 and S2 respectively.<br>It then iteratively adds the remaining entries, one entry per iteration, to one<br>of the two sets S1 or S2. At each iteration, for each remaining entry e, let ie,1<br>denote the increase in the size of the bounding box of S1 if e is added to S1 and<br>let ie,2 denote the corresponding increase for S2. In each iteration, the heuristic<br>chooses one of the entries with the maximum difference of ie,1 and ie,2 and<br>adds it to S1 if ie,1 is less than ie,2, and to S2 otherwise. That is, an entry with<br>“maximum preference” for one of S1 or S2 is chosen at each iteration. The<br>iteration stops when all entries have been assigned, or when one of the sets<br>S1 or S2 has enough entries that all remaining entries have to be added to<br>the other set so the nodes constructed from S1 and S2 both have the required<br>minimum occupancy. The heuristic then adds all unassigned entries to the set<br>with fewer entries.<br>• Deletion: Deletion can be performed like a B+-tree deletion, borrowing entries<br>from sibling nodes, or merging sibling nodes if a node becomes underfull. An<br>alternative approach redistributes all the entries of underfull nodes to sibling<br>nodes, with the aim of improving the clustering of entries in the R-tree.<br>See the bibliographical references for more details on insertion and deletion opera-<br>tions on R-trees, as well as on variants of R-trees, called R∗-trees or R+-trees.<br>The storage efﬁciency of R-trees is better than that of k-d trees or quadtrees, since<br>a polygon is stored only once, and we can ensure easily that each node is at least half<br>full. However, querying may be slower, since multiple paths have to be searched.<br>Spatial joins are simpler with quadtrees than with R-trees, since all quadtrees on a<br>region are partitioned in the same manner. However, because of their better storage<br>efﬁciency, and their similarity to B-trees, R-trees and their variants have proved pop-<br>ular in database systems that support spatial data.<br>23.4<br>Multimedia Databases<br>Multimedia data, such as images, audio, and video—an increasingly popular form<br>of data—are today almost always stored outside the database, in ﬁle systems. This<br>kind of storage is not a problem when the number of multimedia objects is relatively<br>small, since features provided by databases are usually not important.<br>However, database features become important when the number of multimedia<br>objects stored is large. Issues such as transactional updates, querying facilities, and<br>indexing then become important. Multimedia objects often have descriptive attri-<br>butes, such as those indicating when they were created, who created them, and to<br>what category they belong. One approach to building a database for such multimedia<br>objects is to use databases for storing the descriptive attributes and for keeping track<br>of the ﬁles in which the multimedia objects are stored.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VII. Other Topics<br>23. Advanced Data Types <br>and New Applications<br>871<br>© The McGraw−Hill <br>Companies, 2001<br>878<br>Chapter 23<br>Advanced Data Types and New Applications<br>However, storing multimedia outside the database makes it harder to provide<br>database functionality, such as indexing on the basis of actual multimedia data con-<br>tent. It can also lead to inconsistencies, such as a ﬁle that is noted in the database, but<br>whose contents are missing, or vice versa. It is therefore desirable to store the data<br>themselves in the database.<br>Several issues have to be addressed if multimedia data are to be stored in a database.<br>• The database must support large objects, since multimedia data such as videos<br>can occupy up to a few gigabytes of storage. Many database systems do not<br>support objects larger than a few gigabytes. Larger objects could be split into<br>smaller pieces and stored in the database. Alternatively, the multimedia ob-<br>ject may be stored in a ﬁle system, but the database may contain a pointer<br>to the object; the pointer would typically be a ﬁle name. The SQL/MED stan-<br>dard (MED stands for Management of External Data), which is under develop-<br>ment, allows external data, such as ﬁles, to be treated as if they are part of the<br>database. With SQL/MED, the object would appear to be part of the database,<br>but can be stored externally.<br>We discuss multimedia data formats in Section 23.4.1.<br>• The retrieval of some types of data, such as audio and video, has the require-<br>ment that data delivery must proceed at a guaranteed steady rate. Such data<br>are sometimes called isochronous data, or continuous-media data. For exam-<br>ple, if audio data are not supplied in time, there will be gaps in the sound. If<br>the data are supplied too fast, system buffers may overﬂow, resulting in loss<br>of data. We discuss continuous-media data in Section 23.4.2.<br>• Similarity-based retrieval is needed in many multimedia database applica-<br>tions. For example, in a database that stores ﬁngerprint images, a query ﬁnger-<br>print image is provided, and ﬁngerprints in the database that are similar to<br>the query ﬁngerprint must be retrieved. Index structures such as B+-trees and<br>R-trees cannot be used for this purpose; special index structures need to be<br>created. We discuss similarity-based retrieval in Section 23.4.3<br>23.4.1<br>Multimedia Data Formats<br>Because of the large number of bytes required to represent multimedia data, it is<br>essential that multimedia data be stored and transmitted in compressed form. For<br>image data, the most widely used format is JPEG, named after the standards body<br>that created it, the Joint Picture Experts Group. We can store video data by encod-<br>ing each frame of video in JPEG format, but such an encoding is wasteful, since<br>successive frames of a video are often nearly the same. The Moving Picture Experts<br>Group has developed the MPEG series of standards for encoding video and audio<br>data; these encodings exploit commonalities among a sequence of frames to achieve<br>a greater degree of compression. The MPEG-1 standard stores a minute of 30-frame-<br>per-second video and audio in approximately 12.5 megabytes (compared to approxi-<br>mately 75 megabytes for video in only JPEG). However, MPEG-1 encoding introduces<br>some loss of video quality, to a level roughly comparable to that of VHS video tape.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VII. Other Topics<br>23. Advanced Data Types <br>and New Applications<br>872<br>© The McGraw−Hill <br>Companies, 2001<br>23.4<br>Multimedia Databases<br>879<br>The MPEG-2 standard is designed for digital broadcast systems and digital video<br>disks (DVD); it introduces only a negligible loss of video quality. MPEG-2 compresses<br>1 minute of video and audio to approximately 17 megabytes. Several competing stan-<br>dards are used for audio encoding, including MP3, which stands for MPEG-1 Layer 3,<br>RealAudio, and other formats.<br>23.4.2<br>Continuous-Media Data<br>The most important types of continuous-media data are video and audio data (for ex-<br>ample, a database of movies). Continuous-media systems are characterized by their<br>real-time information-delivery requirements:<br>• Data must be delivered sufﬁciently fast that no gaps in the audio or video<br>result.<br>• Data must be delivered at a rate that does not cause overﬂow of system buffers.<br>• Synchronization among distinct data streams must be maintained. This need<br>arises, for example, when the video of a person speaking must show lips mov-<br>ing synchronously with the audio of the person speaking.<br>To supply data predictably at the right time to a large number of consumers of the<br>data, the fetching of data from disk must be carefully coordinated. Usually, data are<br>fetched in periodic cycles. In each cycle, say of n seconds, n seconds worth of data<br>is fetched for each consumer and stored in memory buffers, while the data fetched<br>in the previous cycle is being sent to the consumers from the memory buffers. The<br>cycle period is a compromise: A short period uses less memory but requires more<br>disk arm movement, which is a waste of resources, while a long period reduces disk<br>arm movement but increases memory requirements and may delay initial delivery<br>of data. When a new request arrives, admission control comes into play: That is, the<br>system checks if the request can be satisﬁed with available resources (in each period);<br>if so, it is admitted; otherwise it is rejected.<br>Extensive research on delivery of continuous media data has dealt with such issues<br>as handling arrays of disks and dealing with disk failure. See the bibliographical<br>references for details.<br>Several vendors offer video-on-demand servers. Current systems are based on ﬁle<br>systems, because existing database systems do not provide the real-time response<br>that these applications need. The basic architecture of a video-on-demand system<br>comprises:<br>• Video server. Multimedia data are stored on several disks (usually in a RAID<br>conﬁguration). Systems containing a large volume of data may use tertiary<br>storage for less frequently accessed data.<br>• Terminals. People view multimedia data through various devices, collectively<br>referred to as terminals. Examples are personal computers and televisions at-<br>tached to a small, inexpensive computer called a set-top box.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VII. Other Topics<br>23. Advanced Data Types <br>and New Applications<br>873<br>© The McGraw−Hill <br>Companies, 2001<br>880<br>Chapter 23<br>Advanced Data Types and New Applications<br>• Network. Transmission of multimedia data from a server to multiple termi-<br>nals requires a high-capacity network.<br>Video-on-demand service eventually will become ubiquitous, just as cable and<br>broadcast television are now. For the present, the main applications of video-server<br>technology are in ofﬁces (for training, viewing recorded talks and presentations, and<br>the like), in hotels, and in video-production facilities.<br>23.4.3<br>Similarity-Based Retrieval<br>In many multimedia applications, data are described only approximately in the data-<br>base. An example is the ﬁngerprint data in Section 23.4. Other examples are:<br>• Pictorial data. Two pictures or images that are slightly different as represented<br>in the database may be considered the same by a user. For instance, a database<br>may store trademark designs. When a new trademark is to be registered, the<br>system may need ﬁrst to identify all similar trademarks that were registered<br>previously.<br>• Audio data. Speech-based user interfaces are being developed that allow the<br>user to give a command or identify a data item by speaking. The input from<br>the user must then be tested for similarity to those commands or data items<br>stored in the system.<br>• Handwritten data. Handwritten input can be used to identify a handwritten<br>data item or command stored in the database. Here again, similarity testing is<br>required.<br>The notion of similarity is often subjective and user speciﬁc. However, similarity<br>testing is often more successful than speech or handwriting recognition, because the<br>input can be compared to data already in the system and, thus, the set of choices<br>available to the system is limited.<br>Several algorithms exist for ﬁnding the best matches to a given input by similarity<br>testing. Some systems, including a dial-by-name, voice-activated telephone system,<br>have been deployed commercially. See the bibliographical notes for references.<br>23.5<br>Mobility and Personal Databases<br>Large-scale, commercial databases have traditionally been stored in central comput-<br>ing facilities. In distributed database applications, there has usually been strong cen-<br>tral database and network administration. Two technology trends have combined to<br>create applications in which this assumption of central control and administration is<br>not entirely correct:<br>1. The increasingly widespread use of personal computers, and, more important,<br>of laptop or notebook computers.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VII. Other Topics<br>23. Advanced Data Types <br>and New Applications<br>874<br>© The McGraw−Hill <br>Companies, 2001<br>23.5<br>Mobility and Personal Databases<br>881<br>2. The development of a relatively low-cost wireless digital communication in-<br>frastructure, based on wireless local-area networks, cellular digital packet net-<br>works, and other technologies.<br>Mobile computing has proved useful in many applications. Many business trav-<br>elers use laptop computers so that they can work and access data en route. Delivery<br>services use mobile computers to assist in package tracking. Emergency-response ser-<br>vices use mobile computers at the scene of disasters, medical emergencies, and the<br>like to access information and to enter data pertaining to the situation. New applica-<br>tions of mobile computers continue to emerge.<br>Wireless computing creates a situation where machines no longer have ﬁxed loca-<br>tions and network addresses. Location-dependent queries are an interesting class of<br>queries that are motivated by mobile computers; in such queries, the location of the<br>user (computer) is a parameter of the query. The value of the location parameter is<br>provided either by the user or, increasingly, by a global positioning system (GPS). An<br>example is a traveler’s information system that provides data on hotels, roadside ser-<br>vices, and the like to motorists. Processing of queries about services that are ahead<br>on the current route must be based on knowledge of the user’s location, direction<br>of motion, and speed. Increasingly, navigational aids are being offered as a built-in<br>feature in automobiles.<br>Energy (battery power) is a scarce resource for most mobile computers. This limi-<br>tation inﬂuences many aspects of system design. Among the more interesting conse-<br>quences of the need for energy efﬁciency is the use of scheduled data broadcasts to<br>reduce the need for mobile systems to transmit queries.<br>Increasing amounts of data may reside on machines administered by users, rather<br>than by database administrators. Furthermore, these machines may, at times, be dis-<br>connected from the network. In many cases, there is a conﬂict between the user’s<br>need to continue to work while disconnected and the need for global data consis-<br>tency.<br>In Sections 23.5.1 through 23.5.4, we discuss techniques in use and under develop-<br>ment to deal with the problems of mobility and personal computing.<br>23.5.1<br>A Model of Mobile Computing<br>The mobile-computing environment consists of mobile computers, referred to as mo-<br>bile hosts, and a wired network of computers. Mobile hosts communicate with the<br>wired network via computers referred to as mobile support stations. Each mobile<br>support station manages those mobile hosts within its cell—that is, the geograph-<br>ical area that it covers. Mobile hosts may move between cells, thus necessitating a<br>handoff of control from one mobile support station to another. Since mobile hosts<br>may, at times, be powered down, a host may leave one cell and rematerialize later at<br>some distant cell. Therefore, moves between cells are not necessarily between adja-<br>cent cells. Within a small area, such as a building, mobile hosts may be connected by a<br>wireless local-area network (LAN) that provides lower-cost connectivity than would<br>a wide-area cellular network, and that reduces the overhead of handoffs.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VII. Other Topics<br>23. Advanced Data Types <br>and New Applications<br>875<br>© The McGraw−Hill <br>Companies, 2001<br>882<br>Chapter 23<br>Advanced Data Types and New Applications<br>It is possible for mobile hosts to communicate directly without the intervention<br>of a mobile support station. However, such communication can occur between only<br>nearby hosts. Such direct forms of communication are becoming more prevalent with<br>the advent of the Bluetooth standard. Bluetooth uses short-range digital radio to<br>allow wireless connectivity within a 10-meter range at high speed (up to 721 kilo-<br>bits per second). Initially conceived as a replacement for cables, Bluetooth’s greatest<br>promise is in easy ad hoc connection of mobile computers, PDAs, mobile phones, and<br>so-called intelligent appliances.<br>The network infrastructure for mobile computing consists in large part of two<br>technologies: wireless local-area networks (such as Avaya’s Orinoco wireless LAN),<br>and packet-based cellular telephony networks.</span><br><br><span style="background-color: #FDFFB6;" title="Chunk 114 | Start: 2280228 | End: 2300228 | Tokens: 3179"> Early cellular systems used analog<br>technology and were designed for voice communication. Second-generation digital<br>systems retained the focus on voice appliations. Third-generation (3G) and so-called<br>2.5G systems use packet-based networking and are more suited to data applications.<br>In these networks, voice is just one of many applications (albeit an economically im-<br>portant one).<br>Bluetooth, wireless LANs, and 2.5G and 3G cellular networks make it possible for<br>a wide variety of devices to communicate at low cost. While such communication<br>itself does not ﬁt the domain of a usual database application, the accounting, mon-<br>itoring, and management data pertaining to this communication will generate huge<br>databases. The immediacy of wireless communication generates a need for real-time<br>access to many of these databases. This need for timeliness adds another dimension<br>to the constraints on the system—a matter we shall discuss further in Section 24.3.<br>The size and power limitations of many mobile computers have led to alternative<br>memory hierarchies. Instead of, or in addition to, disk storage, ﬂash memory, which<br>we discussed in Section 11.1, may be included. If the mobile host includes a hard<br>disk, the disk may be allowed to spin down when it is not in use, to save energy. The<br>same considerations of size and energy limit the type and size of the display used<br>in a mobile device. Designers of mobile devices often create special-purpose user in-<br>terfaces to work within these constraints. However, the need to present Web-based<br>data has neccessitated the creation of presentation standards. Wireless application<br>protocol (WAP) is a standard for wireless internet access. WAP-based browsers ac-<br>cess special Web pages that use wireless markup lanaguge (WML), an XML-based<br>language designed for the constraints of mobile and wireless Web browsing.<br>23.5.2<br>Routing and Query Processing<br>The route between a pair of hosts may change over time if one of the two hosts is<br>mobile. This simple fact has a dramatic effect at the network level, since location-<br>based network addresses are no longer constants within the system.<br>Mobility also directly affects database query processing. As we saw in Chapter 19,<br>we must consider the communication costs when we choose a distributed query-<br>processing strategy. Mobility results in dynamically changing communication costs,<br>thus complicating the optimization process. Furthermore, there are competing no-<br>tions of cost to consider:<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VII. Other Topics<br>23. Advanced Data Types <br>and New Applications<br>876<br>© The McGraw−Hill <br>Companies, 2001<br>23.5<br>Mobility and Personal Databases<br>883<br>• User time is a highly valuable commodity in many business applications<br>• Connection time is the unit by which monetary charges are assigned in some<br>cellular systems<br>• Number of bytes, or packets, transferred is the unit by which charges are<br>computed in some digital cellular systems<br>• Time-of-day-based charges vary, depending on whether communication oc-<br>curs during peak or off-peak periods<br>• Energy is limited. Often, battery power is a scarce resource whose use must<br>be optimized. A basic principle of radio communication is that it requires less<br>energy to receive than to transmit radio signals. Thus, transmission and re-<br>ception of data impose different power demands on the mobile host.<br>23.5.3<br>Broadcast Data<br>It is often desirable for frequently requested data to be broadcast in a continuous<br>cycle by mobile support stations, rather than transmitted to mobile hosts on demand.<br>A typical application of such broadcast data is stock-market price information. There<br>are two reasons for using broadcast data. First, the mobile host avoids the energy cost<br>for transmitting data requests. Second, the broadcast data can be received by a large<br>number of mobile hosts at once, at no extra cost. Thus, the available transmission<br>bandwidth is utilized more effectively.<br>A mobile host can then receive data as they are transmitted, rather than consuming<br>energy by transmitting a request. The mobile host may have local nonvolatile storage<br>available to cache the broadcast data for possible later use. Given a query, the mobile<br>host may optimize energy costs by determining whether it can process that query<br>with only cached data. If the cached data are insufﬁcient, there are two options: Wait<br>for the data to be broadcast, or transmit a request for data. To make this decision, the<br>mobile host must know when the relevant data will be broadcast.<br>Broadcast data may be transmitted according to a ﬁxed schedule or a changeable<br>schedule. In the former case, the mobile host uses the known ﬁxed schedule to de-<br>termine when the relevant data will be transmitted. In the latter case, the broadcast<br>schedule must itself be broadcast at a well-known radio frequency and at well-known<br>time intervals.<br>In effect, the broadcast medium can be modeled as a disk with a high latency.<br>Requests for data can be thought of as being serviced when the requested data are<br>broadcast. The transmission schedules behave like indices on the disk. The biblio-<br>graphical notes list recent research papers in the area of broadcast data management.<br>23.5.4<br>Disconnectivity and Consistency<br>Since wireless communication may be paid for on the basis of connection time, there<br>is an incentive for certain mobile hosts to be disconnected for substantial periods.<br>Mobile computers without wireless connectivity are disconnected most of the time<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VII. Other Topics<br>23. Advanced Data Types <br>and New Applications<br>877<br>© The McGraw−Hill <br>Companies, 2001<br>884<br>Chapter 23<br>Advanced Data Types and New Applications<br>when they are being used, except periodically when they are connected to their host<br>computers, either physically or through a computer network.<br>During these periods of disconnection, the mobile host may remain in operation.<br>The user of the mobile host may issue queries and updates on data that reside or are<br>cached locally. This situation creates several problems, in particular:<br>• Recoverability: Updates entered on a disconnected machine may be lost if<br>the mobile host experiences a catastrophic failure. Since the mobile host rep-<br>resents a single point of failure, stable storage cannot be simulated well.<br>• Consistency: Locally cached data may become out of date, but the mobile<br>host cannot discover this situation until it is reconnected. Likewise, updates<br>occurring in the mobile host cannot be propagated until reconnection occurs.<br>We explored the consistency problem in Chapter 19, where we discussed network<br>partitioning, and we elaborate on it here. In wired distributed systems, partitioning is<br>considered to be a failure mode; in mobile computing, partitioning via disconnection<br>is part of the normal mode of operation. It is therefore necessary to allow data access<br>to proceed despite partitioning, even at the risk of some loss of consistency.<br>For data updated by only the mobile host, it is a simple matter to propagate the<br>updates when the mobile host reconnects. However, if the mobile host caches read-<br>only copies of data that may be updated by other computers, the cached data may<br>become inconsistent. When the mobile host is connected, it can be sent invalidation<br>reports that inform it of out-of-date cache entries. However, when the mobile host is<br>disconnected, it may miss an invalidation report. A simple solution to this problem is<br>to invalidate the entire cache on reconnection, but such an extreme solution is highly<br>costly. Several caching schemes are cited in the bibliographical notes.<br>If updates can occur at both the mobile host and elsewhere, detecting conﬂict-<br>ing updates is more difﬁcult. Version-numbering-based schemes allow updates of<br>shared ﬁles from disconnected hosts. These schemes do not guarantee that the up-<br>dates will be consistent. Rather, they guarantee that, if two hosts independently up-<br>date the same version of a document, the clash will be detected eventually, when the<br>hosts exchange information either directly or through a common host.<br>The version-vector scheme detects inconsistencies when copies of a document are<br>independently updated. This scheme allows copies of a document to be stored at mul-<br>tiple hosts. Although we use the term document, the scheme can be applied to any<br>other data items, such as tuples of a relation.<br>The basic idea is for each host i to store, with its copy of each document d, a version<br>vector—that is, a set of version numbers {Vd,i[j]}, with one entry for each other host<br>j on which the document could potentially be updated. When a host i updates a<br>document d, it increments the version number Vd,i[i] by one.<br>Whenever two hosts i and j connect with each other, they exchange updated docu-<br>ments, so that both obtain new versions of the documents. However, before exchang-<br>ing documents, the hosts have to discover whether the copies are consistent:<br>1. If the version vectors are the same on both hosts—that is, for each k, Vd,i[k] =<br>Vd,j[k]—then the copies of document d are identical.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VII. Other Topics<br>23. Advanced Data Types <br>and New Applications<br>878<br>© The McGraw−Hill <br>Companies, 2001<br>23.6<br>Summary<br>885<br>2. If, for each k, Vd,i[k] ≤Vd,j[k] and the version vectors are not identical, then<br>the copy of document d at host i is older than the one at host j. That is, the<br>copy of document d at host j was obtained by one or more modiﬁcations of<br>the copy of the document at host i. Host i replaces its copy of d, as well as its<br>copy of the version vector for d, with the copies from host j.<br>3. If there is a pair of hosts k and m such that Vd,i[k] &lt; Vd,j[k] and Vd,i[m] &gt;<br>Vd,j[m], then the copies are inconsistent; that is, the copy of d at i contains up-<br>dates performed by host k that have not been propagated to host j, and, sim-<br>ilarly, the copy of d at j contains updates performed by host m that have not<br>been propagated to host i. Then, the copies of d are inconsistent, since two or<br>more updates have been performed on d independently. Manual intervention<br>may be required to merge the updates.<br>The version-vector scheme was initially designed to deal with failures in distrib-<br>uted ﬁle systems. The scheme gained importance because mobile computers often<br>store copies of ﬁles that are also present on server systems, in effect constituting a<br>distributed ﬁle system that is often disconnected. Another application of the scheme<br>is in groupware systems, where hosts are connected periodically, rather than contin-<br>uously, and must exchange updated documents. The version-vector scheme also has<br>applications in replicated databases.<br>The version-vector scheme, however, fails to address the most difﬁcult and most<br>important issue arising from updates to shared data—the reconciliation of inconsis-<br>tent copies of data. Many applications can perform reconciliation automatically by<br>executing in each computer those operations that had performed updates on remote<br>computers during the period of disconnection. This solution works if update oper-<br>ations commute—that is, they generate the same result, regardless of the order in<br>which they are executed. Alternative techniques may be available in certain applica-<br>tions; in the worst case, however, it must be left to the users to resolve the inconsisten-<br>cies. Dealing with such inconsistency automatically, and assisting users in resolving<br>inconsistencies that cannot be handled automatically, remains an area of research.<br>Another weakness is that the version-vector scheme requires substantial commu-<br>nication between a reconnecting mobile host and that host’s mobile support station.<br>Consistency checks can be delayed until the data are needed, although this delay<br>may increase the overall inconsistency of the database.<br>The potential for disconnection and the cost of wireless communication limit the<br>practicality of transaction-processing techniques discussed in Chapter 19 for dis-<br>tributed systems. Often, it is preferable to let users prepare transactions on mobile<br>hosts, but to require that, instead of executing the transactions locally, they submit<br>transactions to a server for execution. Transactions that span more than one computer<br>and that include a mobile host face long-term blocking during transaction commit,<br>unless disconnectivity is rare or predictable.<br>23.6<br>Summary<br>• Time plays an important role in database systems. Databases are models of<br>the real world. Whereas most databases model the state of the real world at a<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VII. Other Topics<br>23. Advanced Data Types <br>and New Applications<br>879<br>© The McGraw−Hill <br>Companies, 2001<br>886<br>Chapter 23<br>Advanced Data Types and New Applications<br>point in time (at the current time), temporal databases model the states of the<br>real world across time.<br>• Facts in temporal relations have associated times when they are valid, which<br>can be represented as a union of intervals. Temporal query languages simplify<br>modeling of time, as well as time-related queries.<br>• Spatial databases are ﬁnding increasing use today to store computer-aided-<br>design data as well as geographic data.<br>• Design data are stored primarily as vector data; geographic data consist of a<br>combination of vector and raster data. Spatial-integrity constraints are impor-<br>tant for design data.<br>• Vector data can be encoded as ﬁrst-normal-form data, or can be stored using<br>non-ﬁrst-normal-form structures, such as lists. Special-purpose index struc-<br>tures are particularly important for accessing spatial data, and for processing<br>spatial queries.<br>• R-trees are a multidimensional extension of B-trees; with variants such as R+-<br>trees and R∗-trees, they have proved popular in spatial databases. Index struc-<br>tures that partition space in a regular fashion, such as quadtrees, help in pro-<br>cessing spatial join queries.<br>• Multimedia databases are growing in importance. Issues such as similarity-<br>based retrieval and delivery of data at guaranteed rates are topics of current<br>research.<br>• Mobile computing systems have become common, leading to interest in data-<br>base systems that can run on such systems. Query processing in such systems<br>may involve lookups on server databases. The query cost model must include<br>the cost of communication, including monetary cost and battery-power cost,<br>which is relatively high for mobile systems.<br>• Broadcast is much cheaper per recipient than is point-to-point communica-<br>tion, and broadcast of data such as stock-market data helps mobile systems to<br>pick up data inexpensively.<br>• Disconnected operation, use of broadcast data, and caching of data are three<br>important issues being addressed in mobile computing.<br>Review Terms<br>• Temporal data<br>• Valid time<br>• Transaction time<br>• Temporal relation<br>• Bitemporal relation<br>• Universal coordinated time (UTC)<br>• Snapshot relation<br>• Temporal query languages<br>• Temporal selection<br>• Temporal projection<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VII. Other Topics<br>23. Advanced Data Types <br>and New Applications<br>880<br>© The McGraw−Hill <br>Companies, 2001<br>Exercises<br>887<br>• Temporal join<br>• Spatial and geographic data<br>• Computer-aided-design (CAD)<br>data<br>• Geographic data<br>• Geographic information systems<br>• Triangulation<br>• Design databases<br>• Geographic data<br>• Raster data<br>• Vector data<br>• Global positioning system (GPS)<br>• Spatial queries<br>• Nearness queries<br>• Nearest-neighbor queries<br>• Region queries<br>• Spatial join<br>• Indexing of spatial data<br>• k-d trees<br>• k-d-B trees<br>• Quadtrees<br>  PR quadtree<br>  Region quadtree<br>• R-trees<br>  Bounding box<br>  Quadratic split<br>• Multimedia databases<br>• Isochronous data<br>• Continuous-media data<br>• Similarity-based retrieval<br>• Multimedia data formats<br>• Video servers<br>• Mobile computing<br>  Mobile hosts<br>  Mobile support stations<br>  Cell<br>  Handoff<br>• Location-dependent queries<br>• Broadcast data<br>• Consistency<br>  Invalidation reports<br>  Version-vector scheme<br>Exercises<br>23.1 What are the two types of time, and how are they different? Why does it make<br>sense to have both types of time associated with a tuple?<br>23.2 Will functional dependencies be preserved if a relation is converted to a tem-<br>poral relation by adding a time attribute? How is the problem handled in a<br>temporal database?<br>23.3 Suppose you have a relation containing the x, y coordinates and names of<br>restaurants. Suppose also that the only queries that will be asked are of the<br>following form: The query speciﬁes a point, and asks if there is a restaurant ex-<br>actly at that point. Which type of index would be preferable, R-tree or B-tree?<br>Why?<br>23.4 Consider two-dimensional vector data where the data items do not overlap.<br>Is it possible to convert such vector data to raster data? If so, what are the<br>drawbacks of storing raster data obtained by such conversion, instead of the<br>original vector data?<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VII. Other Topics<br>23. Advanced Data Types <br>and New Applications<br>881<br>© The McGraw−Hill <br>Companies, 2001<br>888<br>Chapter 23<br>Advanced Data Types and New Applications<br>23.5 Suppose you have a spatial database that supports region queries (with circu-<br>lar regions) but not nearest-neighbor queries. Describe an algorithm to ﬁnd the<br>nearest neighbor by making use of multiple region queries.<br>23.6 Suppose you want to store line segments in an R-tree. If a line segment is not<br>parallel to the axes, the bounding box for it can be large, containing a large<br>empty area.<br>• Describe the effect on performance of having large bounding boxes on<br>queries that ask for line segments intersecting a given region.<br>• Brieﬂy describe a technique to improve performance for such queries and<br>give an example of its beneﬁt. Hint: you can divide segments into smaller<br>pieces.<br>23.7 Give a recursive procedure to efﬁciently compute the spatial join of two re-<br>lations with R-tree indices. (Hint: Use bounding boxes to check if leaf entries<br>under a pair of internal nodes may intersect.)<br>23.8 Study the support for spatial data offered by the database system that you use,<br>and implement the following:<br>a. A schema to represent the geographic location of restaurants along with<br>features such as the cuisine served at the restaurant and the level of expen-<br>siveness.<br>b. A query to ﬁnd moderately priced restaurants that serve Indian food and<br>are within 5 miles of your house (assume any location for your house).<br>c. A query to ﬁnd for each restaurant the distance from the nearest restaurant<br>serving the same cuisine and with the same level of expensiveness.<br>23.9 What problems can occur in a continuous-media system if data is delivered<br>either too slowly or too fast?<br>23.10 Describe how the ideas behind the RAID organization (Section 11.3) can be used<br>in a broadcast-data environment, where there may occasionally be noise that<br>prevents reception of part of the data being transmitted.<br>23.11 List three main features of mobile computing over wireless networks that are<br>distinct from traditional distributed systems.<br>23.12 List three factors that need to be considered in query optimization for mobile<br>computing that are not considered in traditional query optimizers.<br>23.13 Deﬁne a model of repeatedly broadcast data in which the broadcast medium is<br>modeled as a virtual disk. Describe how access time and data-transfer rate for<br>this virtual disk differ from the corresponding values for a typical hard disk.<br>23.14 Consider a database of documents in which all documents are kept in a central<br>database. Copies of some documents are kept on mobile computers. Suppose<br>that mobile computer A updates a copy of document 1 while it is disconnected,<br>and, at the same time, mobile computer B updates a copy of document 2 while<br>it is disconnected. Show how the version-vector scheme can ensure proper up-<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VII. Other Topics<br>23. Advanced Data Types <br>and New Applications</span><br><br><span style="background-color: #CAFFBF;" title="Chunk 115 | Start: 2300230 | End: 2320230 | Tokens: 3019"><br>882<br>© The McGraw−Hill <br>Companies, 2001<br>Bibliographical Notes<br>889<br>dating of the central database and mobile computers when a mobile computer<br>reconnects.<br>23.15 Give an example to show that the version-vector scheme does not ensure se-<br>rializability. (Hint: Use the example from Exercise 23.14, with the assumption<br>that documents 1 and 2 are available on both mobile computers A and B, and<br>take into account the possibility that a document may be read without being<br>updated.)<br>Bibliographical Notes<br>The incorporation of time into the relational data model is discussed in Snodgrass<br>and Ahn [1985], Clifford and Tansel [1985], Gadia [1986], Gadia [1988], Snodgrass<br>[1987], Tansel et al. [1993], Snodgrass et al. [1994], and Tuzhilin and Clifford [1990].<br>Stam and Snodgrass [1988] and Soo [1991] provide surveys on temporal data man-<br>agement. Jensen et al. [1994] presents a glossary of temporal-database concepts, aimed<br>at unifying the terminology. a proposal that had signiﬁcant impact on the SQL stan-<br>dard. Tansel et al. [1993] is a collection of articles on different aspects of temporal<br>databases. Chomicki [1995] presents techniques for managing temporal integrity con-<br>straints. A concept of completeness for temporal query languages analogous to rela-<br>tional completeness (equivalence to the relational algebra) is given in Clifford et al.<br>[1994].<br>Samet [1995b] provides an overview of the large amount of work on spatial index<br>structures. Samet [1990] provides a textbook coverage of spatial data structures. An<br>early description of the quad tree is provided by Finkel and Bentley [1974]. Samet<br>[1990] and Samet [1995b] describe numerous variants of quad trees. Bentley [1975]<br>describes the k-d tree, and Robinson [1981] describes the k-d-B tree. The R-tree was<br>originally presented in Guttman [1984]. Extensions of the R-tree are presented by Sel-<br>lis et al. [1987], which describes the R+ tree; Beckmann et al. [1990], which describes<br>the R∗tree; and Kamel and Faloutsos [1992], which describes a parallel version of the<br>R-tree.<br>Brinkhoff et al. [1993] discusses an implementation of spatial joins using R-trees.<br>Lo and Ravishankar [1996] and Patel and DeWitt [1996] present partitioning-based<br>methods for computation of spatial joins. Samet and Aref [1995] provides an overview<br>of spatial data models, spatial operations, and the integration of spatial and nonspa-<br>tial data. Indexing of handwritten documents is discussed in Aref et al. [1995b], Aref<br>et al. [1995a], and Lopresti and Tomkins [1993]. Joins of approximate data are dis-<br>cussed in Barbar´a et al. [1992]. Evangelidis et al. [1995] presents a technique for con-<br>current access to indices on spatial data.<br>Samet [1995a] describes research issues in multimedia databases. Indexing of mul-<br>timedia data is discussed in Faloutsos and Lin [1995]. Video servers are discussed in<br>Anderson et al. [1992], Rangan et al. [1992], Ozden et al. [1994], Freedman and DeWitt<br>[1995], and Ozden et al. [1996b]. Fault tolerance is discussed in Berson et al. [1995] and<br>Ozden et al. [1996a]. Reason et al. [1996] suggests alternative compression schemes<br>for video transmission over wireless networks. Disk storage management techniques<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VII. Other Topics<br>23. Advanced Data Types <br>and New Applications<br>883<br>© The McGraw−Hill <br>Companies, 2001<br>890<br>Chapter 23<br>Advanced Data Types and New Applications<br>for video data are described in Chen et al. [1995], Chervenak et al. [1995], Ozden et al.<br>[1995a], and Ozden et al. [1995b].<br>Information management in systems that include mobile computers is studied in<br>Alonso and Korth [1993] and Imielinski and Badrinath [1994]. Imielinski and Korth<br>[1996] presents an introduction to mobile computing and a collection of research pa-<br>pers on the subject. Indexing of data broadcast over wireless media is considered in<br>Imielinski et al. [1995]. Caching of data in mobile environments is discussed in Bar-<br>bar´a and Imielinski [1994] and Acharya et al. [1995]. Disk management in mobile<br>computers is addressed in Douglis et al. [1994].<br>The version-vector scheme for detecting inconsistency in distributed ﬁle systems<br>is described by Popek et al. [1981] and Parker et al. [1983].<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VII. Other Topics<br>24. Advanced Transaction <br>Processing<br>884<br>© The McGraw−Hill <br>Companies, 2001<br>C<br>H<br>A<br>P<br>T<br>E<br>R<br>2<br>4<br>Advanced Transaction<br>Processing<br>In Chapters 15, 16, and 17, we introduced the concept of a transaction, which is a<br>program unit that accesses—and possibly updates—various data items, and whose<br>execution ensures the preservation of the ACID properties. We discussed in those<br>chapters a variety of schemes for ensuring the ACID properties in an environment<br>where failure can occur, and where the transactions may run concurrently.<br>In this chapter, we go beyond the basic schemes discussed previously, and cover<br>advanced transaction-processing concepts, including transaction-processing moni-<br>tors, transactional workﬂows, main-memory databases, real-time databases, long-<br>duration transactions, nested transactions, and multidatabase transactions.<br>24.1<br>Transaction-Processing Monitors<br>Transaction-processing monitors (TP monitors) are systems that were developed in<br>the 1970s and 1980s, initially in response to a need to support a large number of<br>remote terminals (such as airline-reservation terminals) from a single computer. The<br>term TP monitor initially stood for teleprocessing monitor.<br>TP monitors have since evolved to provide the core support for distributed trans-<br>action processing, and the term TP monitor has acquired its current meaning. The<br>CICS TP monitor from IBM was one of the earliest TP monitors, and has been very<br>widely used. Current-generation TP monitors include Tuxedo and Top End (both<br>now from BEA Systems), Encina (from Transarc, which is now a part of IBM), and<br>Transaction Server (from Microsoft).<br>24.1.1<br>TP-Monitor Architectures<br>Large-scale transaction processing systems are built around a client–server architec-<br>ture. One way of building such systems is to have a server process for each client; the<br>server performs authentication, and then executes actions requested by the client.<br>891<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VII. Other Topics<br>24. Advanced Transaction <br>Processing<br>885<br>© The McGraw−Hill <br>Companies, 2001<br>892<br>Chapter 24<br>Advanced Transaction Processing<br>remote<br>clients<br>server<br>files<br>remote<br>clients<br>(b) Single-server model<br>(a) Process-per-client model<br>server<br>files<br>remote<br>clients<br>router<br>servers<br>files<br>remote<br>clients<br>(d) Many-server, many-router model<br>(c) Many-server, single-router model<br>routers<br>monitor<br>servers<br>files<br>Figure 24.1<br>TP-monitor architectures.<br>This process-per-client model is illustrated in Figure 24.1a. This model presents sev-<br>eral problems with respect to memory utilization and processing speed:<br>• Per-process memory requirements are high. Even if memory for program code<br>is shared by all processes, each process consumes memory for local data and<br>open ﬁle descriptors, as well as for operating-system overhead, such as page<br>tables to support virtual memory.<br>• The operating system divides up available CPU time among processes by<br>switching among them; this technique is called multitasking. Each context<br>switch between one process and the next has considerable CPU overhead;<br>even on today’s fast systems, a context switch can take hundreds of microsec-<br>onds.<br>The above problems can be avoided by having a single-server process to which<br>all remote clients connect; this model is called the single-server model, illustrated in<br>Figure 24.1b. Remote clients send requests to the server process, which then executes<br>those requests. This model is also used in client–server environments, where clients<br>send requests to a single-server process. The server process handles tasks, such as<br>user authentication, that would normally be handled by the operating system. To<br>avoid blocking other clients when processing a long request for one client, the server<br>process is multithreaded: The server process has a thread of control for each client,<br>and, in effect, implements its own low-overhead multitasking. It executes code on<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VII. Other Topics<br>24. Advanced Transaction <br>Processing<br>886<br>© The McGraw−Hill <br>Companies, 2001<br>24.1<br>Transaction-Processing Monitors<br>893<br>behalf of one client for a while, then saves the internal context and switches to the<br>code for another client. Unlike the overhead of full multitasking, the cost of switching<br>between threads is low (typically only a few microseconds).<br>Systems based on the single-server model, such as the original version of the IBM<br>CICS TP monitor and ﬁle servers such as Novel’s NetWare, successfully provided<br>high transaction rates with limited resources. However, they had problems, espe-<br>cially when multiple applications accessed the same database:<br>• Since all the applications run as a single process, there is no protection among<br>them. A bug in one application can affect all the other applications as well. It<br>would be best to run each application as a separate process.<br>• Such systems are not suited for parallel or distributed databases, since a server<br>process cannot execute on multiple computers at once. (However, concurrent<br>threads within a process can be supported in a shared-memory multiproces-<br>sor system.) This is a serious drawback in large organizations, where parallel<br>processing is critical for handling large workloads, and distributed data are<br>becoming increasingly common.<br>One way to solve these problems is to run multiple application-server processes<br>that access a common database, and to let the clients communicate with the appli-<br>cation through a single communication process that routes requests. This model is<br>called the many-server, single-router model, illustrated in Figure 24.1c. This model<br>supports independent server processes for multiple applications; further, each ap-<br>plication can have a pool of server processes, any one of which can handle a client<br>session. The request can, for example, be routed to the most lightly loaded server in a<br>pool. As before, each server process can itself be multithreaded, so that it can handle<br>multiple clients concurrently. As a further generalization, the application servers can<br>run on different sites of a parallel or distributed database, and the communication<br>process can handle the coordination among the processes.<br>The above architecture is also widely used in Web servers. A Web server has a<br>main process that receives HTTP requests, and then assigns the task of handling each<br>request to a separate process (chosen from among a pool of processes). Each of the<br>processes is itself multithreaded, so that it can handle multiple requests.<br>A more general architecture has multiple processes, rather than just one, to com-<br>municate with clients. The client communication processes interact with one or more<br>router processes, which route their requests to the appropriate server. Later-<br>generation TP monitors therefore have a different architecture, called the many-server,<br>many-router model, illustrated in Figure 24.1d. A controller process starts up the<br>other processes, and supervises their functioning. Tandem Pathway is an example<br>of the later-generation TP monitors that use this architecture. Very high performance<br>Web server systems also adopt such an architecture.<br>The detailed structure of a TP monitor appears in Figure 24.2. A TP monitor does<br>more than simply pass messages to application servers. When messages arrive, they<br>may have to be queued; thus, there is a queue manager for incoming messages.<br>The queue may be a durable queue, whose entries survive system failures. Using a<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VII. Other Topics<br>24. Advanced Transaction <br>Processing<br>887<br>© The McGraw−Hill <br>Companies, 2001<br>894<br>Chapter 24<br>Advanced Transaction Processing<br>input queue<br>authorization<br>output queue<br>network<br>lock manager<br>recovery manager<br>log manager<br>application<br>servers<br>database and<br>resource managers <br>Figure 24.2<br>TP-monitor components.<br>durable queue helps ensure that once received and stored in the queue, the<br>messages will be processed eventually, regardless of system failures. Authorization<br>and application-server management (for example, server startup, and routing of mes-<br>sages to servers) are further functions of a TP monitor. TP monitors often provide<br>logging, recovery, and concurrency-control facilities, allowing application servers to<br>implement the ACID transaction properties directly if required.<br>Finally, TP monitors also provide support for persistent messaging. Recall that per-<br>sistent messaging (Section 19.4.3) provides a guarantee that the message will be de-<br>livered if (and only if) the transaction commits.<br>In addition to these facilities, many TP monitors also provided presentation facilities<br>to create menus/forms interfaces for dumb clients such as terminals; these facilities<br>are no longer important since dumb clients are no longer widely used.<br>24.1.2<br>Application Coordination Using TP monitors<br>Applications today often have to interact with multiple databases. They may also<br>have to interact with legacy systems, such as special-purpose data-storage systems<br>built directly on ﬁle systems. Finally, they may have to communicate with users or<br>other applications at remote sites. Hence, they also have to interact with commu-<br>nication subsystems. It is important to be able to coordinate data accesses, and to<br>implement ACID properties for transactions across such systems.<br>Modern TP monitors provide support for the construction and administration of<br>such large applications, built up from multiple subsystems such as databases, legacy<br>systems, and communication systems. A TP monitor treats each subsystem as a re-<br>source manager that provides transactional access to some set of resources. The in-<br>terface between the TP monitor and the resource manager is deﬁned by a set of trans-<br>action primitives, such as begin transaction, commit transaction, abort transaction, and<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VII. Other Topics<br>24. Advanced Transaction <br>Processing<br>888<br>© The McGraw−Hill <br>Companies, 2001<br>24.2<br>Transactional Workﬂows<br>895<br>prepare to commit transaction (for two-phase commit). Of course, the resource man-<br>ager must also provide other services, such as supplying data, to the application.<br>The resource-manager interface is deﬁned by the X/Open Distributed Transaction<br>Processing standard. Many database systems support the X/Open standards, and<br>can act as resource managers. TP monitors—as well as other products, such as SQL<br>systems, that support the X/Open standards—can connect to the resource managers.<br>In addition, services provided by a TP monitor, such as persistent messaging and<br>durable queues, act as resource managers supporting transactions. The TP monitor<br>can act as coordinator of two-phase commit for transactions that access these ser-<br>vices as well as database systems. For example, when a queued update transaction<br>is executed, an output message is delivered, and the request transaction is removed<br>from the request queue. Two-phase commit between the database and the resource<br>managers for the durable queue and persistent messaging helps ensure that, regard-<br>less of failures, either all these actions occur, or none occurs.<br>We can also use TP monitors to administer complex client–server systems consist-<br>ing of multiple servers and a large number of clients. The TP monitor coordinates<br>activities such as system checkpoints and shutdowns. It provides security and au-<br>thentication of clients. It administers server pools by adding servers or removing<br>servers without interruption of the system. Finally, it controls the scope of failures. If<br>a server fails, the TP monitor can detect this failure, abort the transactions in progress,<br>and restart the transactions. If a node fails, the TP monitor can migrate transactions to<br>servers at other nodes, again backing out incomplete transactions. When failed nodes<br>restart, the TP monitor can govern the recovery of the node’s resource managers.<br>TP monitors can be used to hide database failures in replicated systems; remote<br>backup systems (Section 17.10) are an example of replicated systems. Transaction<br>requests are sent to the TP monitor, which relays the messages to one of the database<br>replicas (the primary site, in case of remote backup systems). If one site fails, the TP<br>monitor can transparently route messages to a backup site, masking the failure of the<br>ﬁrst site.<br>In client–server systems, clients often interact with servers via a remote-procedure-<br>call (RPC) mechanism, where a client invokes a procedure call, which is actually ex-<br>ecuted at the server, with the results sent back to the client. As far as the client code<br>that invokes the RPC is concerned, the call looks like a local procedure-call invocation.<br>TP monitor systems, such as Encina, provide a transactional RPC interface to their<br>services. In such an interface, the RPC mechanism provides calls that can be used to<br>enclose a series of RPC calls within a transaction. Thus, updates performed by an RPC<br>are carried out within the scope of the transaction, and can be rolled back if there is<br>any failure.<br>24.2<br>Transactional Workﬂows<br>A workﬂow is an activity in which multiple tasks are executed in a coordinated way<br>by different processing entities. A task deﬁnes some work to be done and can be<br>speciﬁed in a number of ways, including a textual description in a ﬁle or electronic-<br>mail message, a form, a message, or a computer program. The processing entity that<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VII. Other Topics<br>24. Advanced Transaction <br>Processing<br>889<br>© The McGraw−Hill <br>Companies, 2001<br>896<br>Chapter 24<br>Advanced Transaction Processing<br>Workflow<br>Typical<br>Typical processing<br>application<br>task<br>entity<br>electronic-mail routing<br>electronic-mail message<br>mailers<br>loan processing<br>form processing<br>form processing<br>humans,<br>application software<br>purchase-order processing<br>humans, application<br>software, DBMSs<br>Figure 24.3<br>Examples of workﬂows.<br>performs the tasks may be a person or a software system (for example, a mailer, an<br>application program, or a database-management system).<br>Figure 24.3 shows examples of workﬂows. A simple example is that of an electronic-<br>mail system. The delivery of a single mail message may involve several mailer sys-<br>tems that receive and forward the mail message, until the message reaches its desti-<br>nation, where it is stored. Each mailer performs a task—forwarding the mail to the<br>next mailer—and the tasks of multiple mailers may be required to route mail from<br>source to destination. Other terms used in the database and related literature to refer<br>to workﬂows include task ﬂow and multisystem applications. Workﬂow tasks are<br>also sometimes called steps.<br>In general, workﬂows may involve one or more humans. For instance, consider<br>the processing of a loan. The relevant workﬂow appears in Figure 24.4. The person<br>who wants a loan ﬁlls out a form, which is then checked by a loan ofﬁcer. An em-<br>ployee who processes loan applications veriﬁes the data in the form, using sources<br>such as credit-reference bureaus. When all the required information has been col-<br>lected, the loan ofﬁcer may decide to approve the loan; that decision may then have<br>to be approved by one or more superior ofﬁcers, after which the loan can be made.<br>Each human here performs a task; in a bank that has not automated the task of loan<br>processing, the coordination of the tasks is typically carried out by passing of the<br>loan application, with attached notes and other information, from one employee to<br>customer<br>loan officer<br>verification<br>superior<br>officer<br>loan<br>disbursement<br>loan<br>application<br>reject<br>accept<br>Figure 24.4<br>Workﬂow in loan processing.<br>Silberschatz−Korth−Sudarshan: <br>Database S</span><br><br><span style="background-color: #9BF6FF;" title="Chunk 116 | Start: 2320232 | End: 2340232 | Tokens: 3100">ystem <br>Concepts, Fourth Edition<br>VII. Other Topics<br>24. Advanced Transaction <br>Processing<br>890<br>© The McGraw−Hill <br>Companies, 2001<br>24.2<br>Transactional Workﬂows<br>897<br>the next. Other examples of workﬂows include processing of expense vouchers, of<br>purchase orders, and of credit-card transactions.<br>Today, all the information related to a workﬂow is more than likely to be stored in<br>a digital form on one or more computers, and, with the growth of networking, infor-<br>mation can be easily transferred from one computer to another. Hence, it is feasible<br>for organizations to automate their workﬂows. For example, to automate the tasks<br>involved in loan processing, we can store the loan application and associated infor-<br>mation in a database. The workﬂow itself then involves handing of responsibility<br>from one human to the next, and possibly even to programs that can automatically<br>fetch the required information. Humans can coordinate their activities by means such<br>as electronic mail.<br>We have to address two activities, in general, to automate a workﬂow. The ﬁrst<br>is workﬂow speciﬁcation: detailing the tasks that must be carried out and deﬁning<br>the execution requirements. The second problem is workﬂow execution, which we<br>must do while providing the safeguards of traditional database systems related to<br>computation correctness and data integrity and durability. For example, it is not ac-<br>ceptable for a loan application or a voucher to be lost, or to be processed more than<br>once, because of a system crash. The idea behind transactional workﬂows is to use<br>and extend the concepts of transactions to the context of workﬂows.<br>Both activities are complicated by the fact that many organizations use several<br>independently managed information-processing systems that, in most cases, were<br>developed separately to automate different functions. Workﬂow activities may re-<br>quire interactions among several such systems, each performing a task, as well as<br>interactions with humans.<br>A number of workﬂow systems have been developed in recent years. Here, we<br>study properties of workﬂow systems at a relatively abstract level, without going<br>into the details of any particular system.<br>24.2.1<br>Workﬂow Speciﬁcation<br>Internal aspects of a task do not need to be modeled for the purpose of speciﬁcation<br>and management of a workﬂow. In an abstract view of a task, a task may use param-<br>eters stored in its input variables, may retrieve and update data in the local system,<br>may store its results in its output variables, and may be queried about its execution<br>state. At any time during the execution, the workﬂow state consists of the collection<br>of states of the workﬂow’s constituent tasks, and the states (values) of all variables in<br>the workﬂow speciﬁcation.<br>The coordination of tasks can be speciﬁed either statically or dynamically. A static<br>speciﬁcation deﬁnes the tasks—and dependencies among them—before the execu-<br>tion of the workﬂow begins. For instance, the tasks in an expense-voucher workﬂow<br>may consist of the approvals of the voucher by a secretary, a manager, and an accoun-<br>tant, in that order, and ﬁnally by the delivery of a check. The dependencies among<br>the tasks may be simple—each task has to be completed before the next begins.<br>A generalization of this strategy is to have a precondition for execution of each<br>task in the workﬂow, so that all possible tasks in a workﬂow and their dependen-<br>cies are known in advance, but only those tasks whose preconditions are satisﬁed<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VII. Other Topics<br>24. Advanced Transaction <br>Processing<br>891<br>© The McGraw−Hill <br>Companies, 2001<br>898<br>Chapter 24<br>Advanced Transaction Processing<br>are executed. The preconditions can be deﬁned through dependencies such as the<br>following:<br>• Execution states of other tasks—for example, “task ti cannot start until task tj<br>has ended,” or “task ti must abort if task tj has committed”<br>• Output values of other tasks—for example, “task ti can start if task tj re-<br>turns a value greater than 25,” or “the manager-approval task can start if the<br>secretary-approval task returns a value of OK”<br>• External variables modiﬁed by external events—for example, “task ti cannot<br>be started before 9 AM,” or “task ti must be started within 24 hours of the<br>completion of task tj”<br>We can combine the dependencies by the regular logical connectors (or, and, not) to<br>form complex scheduling preconditions.<br>An example of dynamic scheduling of tasks is an electronic-mail routing system.<br>The next task to be scheduled for a given mail message depends on what the desti-<br>nation address of the message is, and on which intermediate routers are functioning.<br>24.2.2<br>Failure-Atomicity Requirements of a Workﬂow<br>The workﬂow designer may specify the failure-atomicity requirements of a work-<br>ﬂow according to the semantics of the workﬂow. The traditional notion of failure<br>atomicity would require that a failure of any task results in the failure of the work-<br>ﬂow. However, a workﬂow can, in many cases, survive the failure of one of its tasks—<br>for example, by executing a functionally equivalent task at another site. Therefore,<br>we should allow the designer to deﬁne failure-atomicity requirements of a workﬂow.<br>The system must guarantee that every execution of a workﬂow will terminate in a<br>state that satisﬁes the failure-atomicity requirements deﬁned by the designer. We call<br>those states acceptable termination states of a workﬂow. All other execution states of<br>a workﬂow constitute a set of nonacceptable termination states, in which the failure-<br>atomicity requirements may be violated.<br>An acceptable termination state can be designated as committed or aborted. A<br>committed acceptable termination state is an execution state in which the objectives<br>of a workﬂow have been achieved. In contrast, an aborted acceptable termination<br>state is a valid termination state in which a workﬂow has failed to achieve its ob-<br>jectives. If an aborted acceptable termination state has been reached, all undesirable<br>effects of the partial execution of the workﬂow must be undone in accordance with<br>that workﬂow’s failure-atomicity requirements.<br>A workﬂow must reach an acceptable termination state even in the presence of system<br>failures. Thus, if a workﬂow was in a nonacceptable termination state at the time of<br>failure, during system recovery it must be brought to an acceptable termination state<br>(whether aborted or committed).<br>For example, in the loan-processing workﬂow, in the ﬁnal state, either the loan<br>applicant is told that a loan cannot be made or the loan is disbursed. In case of fail-<br>ures such as a long failure of the veriﬁcation system, the loan application could be<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VII. Other Topics<br>24. Advanced Transaction <br>Processing<br>892<br>© The McGraw−Hill <br>Companies, 2001<br>24.2<br>Transactional Workﬂows<br>899<br>returned to the loan applicant with a suitable explanation; this outcome would consti-<br>tute an aborted acceptable termination. A committed acceptable termination would<br>be either the acceptance or the rejection of the loan.<br>In general, a task can commit and release its resources before the workﬂow reaches<br>a termination state. However, if the multitask transaction later aborts, its failure atom-<br>icity may require that we undo the effects of already completed tasks (for example,<br>committed subtransactions) by executing compensating tasks (as subtransactions).<br>The semantics of compensation requires that a compensating transaction eventually<br>complete its execution successfully, possibly after a number of resubmissions.<br>In an expense-voucher-processing workﬂow, for example, a department-budget<br>balance may be reduced on the basis of an initial approval of a voucher by the man-<br>ager. If the voucher is later rejected, whether because of failure or for other reasons,<br>the budget may have to be restored by a compensating transaction.<br>24.2.3<br>Execution of Workﬂows<br>The execution of the tasks may be controlled by a human coordinator or by a soft-<br>ware system called a workﬂow-management system. A workﬂow-management sys-<br>tem consists of a scheduler, task agents, and a mechanism to query the state of the<br>workﬂow system. A task agent controls the execution of a task by a processing en-<br>tity. A scheduler is a program that processes workﬂows by submitting various tasks<br>for execution, monitoring various events, and evaluating conditions related to inter-<br>task dependencies. A scheduler may submit a task for execution (to a task agent),<br>or may request that a previously submitted task be aborted. In the case of multi-<br>database transactions, the tasks are subtransactions, and the processing entities are<br>local database management systems. In accordance with the workﬂow speciﬁcations,<br>the scheduler enforces the scheduling dependencies and is responsible for ensuring<br>that tasks reach acceptable termination states.<br>There are three architectural approaches to the development of a workﬂow-mana-<br>gement system. A centralized architecture has a single scheduler that schedules the<br>tasks for all concurrently executing workﬂows. The partially distributed architec-<br>ture has one scheduler instantiated for each workﬂow. When the issues of concurrent<br>execution can be separated from the scheduling function, the latter option is a natural<br>choice. A fully distributed architecture has no scheduler, but the task agents coordi-<br>nate their execution by communicating with one another to satisfy task dependencies<br>and other workﬂow execution requirements.<br>The simplest workﬂow-execution systems follow the fully distributed approach<br>just described and are based on messaging. Messaging may be implemented by per-<br>sistent messaging mechanisms, to provide guaranteed delivery. Some implementa-<br>tions use e-mail for messaging; such implementations provide many of the features<br>of persistent messaging, but generally do not guarantee atomicity of message deliv-<br>ery and transaction commit. Each site has a task agent that executes tasks received<br>through messages. Execution may also involve presenting messages to humans, who<br>have then to carry out some action. When a task is completed at a site, and needs<br>to be processed at another site, the task agent dispatches a message to the next site.<br>The message contains all relevant information about the task to be performed. Such<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VII. Other Topics<br>24. Advanced Transaction <br>Processing<br>893<br>© The McGraw−Hill <br>Companies, 2001<br>900<br>Chapter 24<br>Advanced Transaction Processing<br>message-based workﬂow systems are particularly useful in networks that may be<br>disconnected for part of the time, such as dial-up networks.<br>The centralized approach is used in workﬂow systems where the data are stored<br>in a central database. The scheduler notiﬁes various agents, such as humans or com-<br>puter programs, that a task has to be carried out, and keeps track of task completion.<br>It is easier to keep track of the state of a workﬂow with a centralized approach than<br>it is with a fully distributed approach.<br>The scheduler must guarantee that a workﬂow will terminate in one of the spec-<br>iﬁed acceptable termination states. Ideally, before attempting to execute a workﬂow,<br>the scheduler should examine that workﬂow to check whether the workﬂow may ter-<br>minate in a nonacceptable state. If the scheduler cannot guarantee that a workﬂow<br>will terminate in an acceptable state, it should reject such speciﬁcations without at-<br>tempting to execute the workﬂow. As an example, let us consider a workﬂow consist-<br>ing of two tasks represented by subtransactions S1 and S2, with the failure-atomicity<br>requirements indicating that either both or neither of the subtransactions should be<br>committed. If S1 and S2 do not provide prepared-to-commit states (for a two-phase<br>commit), and further do not have compensating transactions, then it is possible to<br>reach a state where one subtransaction is committed and the other aborted, and there<br>is no way to bring both to the same state. Therefore, such a workﬂow speciﬁcation is<br>unsafe, and should be rejected.<br>Safety checks such as the one just described may be impossible or impractical to<br>implement in the scheduler; it then becomes the responsibility of the person design-<br>ing the workﬂow speciﬁcation to ensure that the workﬂows are safe.<br>24.2.4<br>Recovery of a Workﬂow<br>The objective of workﬂow recovery is to enforce the failure atomicity of the work-<br>ﬂows. The recovery procedures must make sure that, if a failure occurs in any of the<br>workﬂow-processing components (including the scheduler), the workﬂow will even-<br>tually reach an acceptable termination state (whether aborted or committed). For ex-<br>ample, the scheduler could continue processing after failure and recovery, as though<br>nothing happened, thus providing forward recoverability. Otherwise, the scheduler<br>could abort the whole workﬂow (that is, reach one of the global abort states). In ei-<br>ther case, some subtransactions may need to be committed or even submitted for<br>execution (for example, compensating subtransactions).<br>We assume that the processing entities involved in the workﬂow have their own<br>local recovery systems and handle their local failures. To recover the execution-<br>environment context, the failure-recovery routines need to restore the state infor-<br>mation of the scheduler at the time of failure, including the information about the<br>execution states of each task. Therefore, the appropriate status information must be<br>logged on stable storage.<br>We also need to consider the contents of the message queues. When one agent<br>hands off a task to another, the handoff should be carried out exactly once: If the<br>handoff happens twice a task may get executed twice; if the handoff does not oc-<br>cur, the task may get lost. Persistent messaging (Section 19.4.3) provides exactly the<br>features to ensure positive, single handoff.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VII. Other Topics<br>24. Advanced Transaction <br>Processing<br>894<br>© The McGraw−Hill <br>Companies, 2001<br>24.3<br>Main-Memory Databases<br>901<br>24.2.5<br>Workﬂow Management Systems<br>Workﬂows are often hand coded as part of application systems. For instance, en-<br>terprise resource planning (ERP) systems, which help coordinate activities across an<br>entire enterprise, have numerous workﬂows built into them.<br>The goal of workﬂow management systems is to simplify the construction of work-<br>ﬂows and make them more reliable, by permitting them to be speciﬁed in a high-level<br>manner and executed in accordance with the speciﬁcation. There are a large number<br>of commercial workﬂow management systems; some, like FlowMark from IBM, are<br>general-purpose workﬂow management systems, while others are speciﬁc to partic-<br>ular workﬂows, such as order processing or bug/failure reporting systems.<br>In today’s world of interconnected organizations, it is not sufﬁcient to manage<br>workﬂows only within an organization. Workﬂows that cross organizational bound-<br>aries are becoming increasingly common. For instance, consider an order placed by<br>an organization and communicated to another organization that fulﬁlls the order.<br>In each organization there may be a workﬂow associated with the order, and it is<br>important that the workﬂows be able to interoperate, in order to minimize human<br>intervention.<br>The Workﬂow Management Coalition has developed standards for interoperation<br>between workﬂow systems. Current standardization efforts use XML as the under-<br>lying language for communicating information about the workﬂow. See the biblio-<br>graphical notes for more information.<br>24.3<br>Main-Memory Databases<br>To allow a high rate of transaction processing (hundreds or thousands of transactions<br>per second), we must use high-performance hardware, and must exploit parallelism.<br>These techniques alone, however, are insufﬁcient to obtain very low response times,<br>since disk I/O remains a bottleneck—about 10 milliseconds are required for each I/O<br>and this number has not decreased at a rate comparable to the increase in processor<br>speeds. Disk I/O is often the bottleneck for reads, as well as for transaction commits.<br>The long disk latency (about 10 milliseconds average) increases not only the time to<br>access a data item, but also limits the number of accesses per second.<br>We can make a database system less disk bound by increasing the size of the<br>database buffer. Advances in main-memory technology let us construct large main<br>memories at relatively low cost. Today, commercial 64-bit systems can support main<br>memories of tens of gigabytes.<br>For some applications, such as real-time control, it is necessary to store data in<br>main memory to meet performance requirements. The memory size required for<br>most such systems is not exceptionally large, although there are at least a few appli-<br>cations that require multiple gigabytes of data to be memory resident. Since memory<br>sizes have been growing at a very fast rate, an increasing number of applications can<br>be expected to have data that ﬁt into main memory.<br>Large main memories allow faster processing of transactions, since data are mem-<br>ory resident. However, there are still disk-related limitations:<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VII. Other Topics<br>24. Advanced Transaction <br>Processing<br>895<br>© The McGraw−Hill <br>Companies, 2001<br>902<br>Chapter 24<br>Advanced Transaction Processing<br>• Log records must be written to stable storage before a transaction is commit-<br>ted. The improved performance made possible by a large main memory may<br>result in the logging process becoming a bottleneck. We can reduce commit<br>time by creating a stable log buffer in main memory, using nonvolatile RAM<br>(implemented, for example, by battery backed-up memory). The overhead im-<br>posed by logging can also be reduced by the group-commit technique discussed<br>later in this section. Throughput (number of transactions per second) is still<br>limited by the data-transfer rate of the log disk.<br>• Buffer blocks marked as modiﬁed by committed transactions still have to be<br>written so that the amount of log that has to be replayed at recovery time is<br>reduced. If the update rate is extremely high, the disk data-transfer rate may<br>become a bottleneck.<br>• If the system crashes, all of main memory is lost. On recovery, the system has<br>an empty database buffer, and data items must be input from disk when they<br>are accessed. Therefore, even after recovery is complete, it takes some time be-<br>fore the database is fully loaded in main memory and high-speed processing<br>of transactions can resume.<br>On the other hand, a main-memory database provides opportunities for optimiza-<br>tions:<br>• Since memory is costlier than disk space, internal data structures in main-<br>memory databases have to be designed to reduce space requirements. How-<br>ever, data structures can have pointers crossing multiple pages unlike those in<br>disk databases, where the cost of the I/Os to traverse multiple pages would be<br>excessively high. For example, tree structures in main-memory databases can<br>be relatively deep, unlike B+-trees, but should minimize space requirements.<br>• There is no need to pin buffer pages in memory before data are accessed, since<br>buffer pages will never be replaced.<br>• Query-processing techniques should be designed to minimize space over-<br>head, so that main memory limits are not exceeded while a query is being<br>evaluated; that situation would result in paging to swap area, and would slow<br>down query processing.<br>• Once the disk I/O bottleneck is removed, operations such as locking and latch-<br>ing may become bottlenecks. Such bottlenecks must be eliminated by im-<br>provements in the implementation of these operations.<br>• Recovery algorithms can be optimized, since pages rarely need to be written<br>out to make space for other pages.<br>TimesTen and DataBlitz are two main-memory database products that exploit sev-<br>eral of these optimizations, while the Oracle database has added special features to<br>support very large main memories. A</span><br><br><span style="background-color: #A0C4FF;" title="Chunk 117 | Start: 2340234 | End: 2360234 | Tokens: 3111">dditional information on main-memory data-<br>bases is given in the references in the bibliographical notes.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VII. Other Topics<br>24. Advanced Transaction <br>Processing<br>896<br>© The McGraw−Hill <br>Companies, 2001<br>24.4<br>Real-Time Transaction Systems<br>903<br>The process of committing a transaction T requires these records to be written to<br>stable storage:<br>• All log records associated with T that have not been output to stable storage<br>• The &lt;T commit&gt; log record<br>These output operations frequently require the output of blocks that are only par-<br>tially ﬁlled. To ensure that nearly full blocks are output, we use the group-commit<br>technique. Instead of attempting to commit T when T completes, the system waits un-<br>til several transactions have completed, or a certain period of time has passed since<br>a transaction completed execution. It then commits the group of transactions that are<br>waiting, together. Blocks written to the log on stable storage would contain records<br>of several transactions. By careful choice of group size and maximum waiting time,<br>the system can ensure that blocks are full when they are written to stable storage<br>without making transactions wait excessively. This technique results, on average, in<br>fewer output operations per committed transaction.<br>Although group commit reduces the overhead imposed by logging, it results in a<br>slight delay in commit of transactions that perform updates. The delay can be made<br>quite small (say, 10 milliseconds), which is acceptable for many applications. These<br>delays can be eliminated if disks or disk controllers support nonvolatile RAM buffers<br>for write operations. Transactions can commit as soon as the write is performed on<br>the nonvolatile RAM buffer. In this case, there is no need for group commit.<br>Note that group commit is useful even in databases with disk-resident data.<br>24.4<br>Real-Time Transaction Systems<br>The integrity constraints that we have considered thus far pertain to the values stored<br>in the database. In certain applications, the constraints include deadlines by which a<br>task must be completed. Examples of such applications include plant management,<br>trafﬁc control, and scheduling. When deadlines are included, correctness of an exe-<br>cution is no longer solely an issue of database consistency. Rather, we are concerned<br>with how many deadlines are missed, and by how much time they are missed. Dead-<br>lines are characterized as follows:<br>• Hard deadline. Serious problems, such as system crash, may occur if a task is<br>not completed by its deadline.<br>• Firm deadline. The task has zero value if it is completed after the deadline.<br>• Soft deadlines. The task has diminishing value if it is completed after the<br>deadline, with the value approaching zero as the degree of lateness increases.<br>Systems with deadlines are called real-time systems.<br>Transaction management in real-time systems must take deadlines into account. If<br>the concurrency-control protocol determines that a transaction Ti must wait, it may<br>cause Ti to miss the deadline. In such cases, it may be preferable to pre-empt the<br>transaction holding the lock, and to allow Ti to proceed. Pre-emption must be used<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VII. Other Topics<br>24. Advanced Transaction <br>Processing<br>897<br>© The McGraw−Hill <br>Companies, 2001<br>904<br>Chapter 24<br>Advanced Transaction Processing<br>with care, however, because the time lost by the pre-empted transaction (due to roll-<br>back and restart) may cause the transaction to miss its deadline. Unfortunately, it is<br>difﬁcult to determine whether rollback or waiting is preferable in a given situation.<br>A major difﬁculty in supporting real-time constraints arises from the variance in<br>transaction execution time. In the best case, all data accesses reference data in the<br>database buffer. In the worst case, each access causes a buffer page to be written to<br>disk (preceded by the requisite log records), followed by the reading from disk of<br>the page containing the data to be accessed. Because the two or more disk accesses<br>required in the worst case take several orders of magnitude more time than the main-<br>memory references required in the best case, transaction execution time can be esti-<br>mated only very poorly if data are resident on disk. Hence, main-memory databases<br>are often used if real-time constraints have to be met.<br>However, even if data are resident in main memory, variances in execution time<br>arise from lock waits, transaction aborts, and so on. Researchers have devoted con-<br>siderable effort to concurrency control for real-time databases. They have extended<br>locking protocols to provide higher priority for transactions with early deadlines.<br>They have found that optimistic concurrency protocols perform well in real-time<br>databases; that is, these protocols result in fewer missed deadlines than even the<br>extended locking protocols. The bibliographical notes provide references to research<br>in the area of real-time databases.<br>In real-time systems, deadlines, rather than absolute speed, are the most important<br>issue. Designing a real-time system involves ensuring that there is enough processing<br>power to meet deadlines without requiring excessive hardware resources. Achieving<br>this objective, despite the variance in execution time resulting from transaction man-<br>agement, remains a challenging problem.<br>24.5<br>Long-Duration Transactions<br>The transaction concept developed initially in the context of data-processing applica-<br>tions, in which most transactions are noninteractive and of short duration. Although<br>the techniques presented here and earlier in Chapters 15, 16, and 17 work well in<br>those applications, serious problems arise when this concept is applied to database<br>systems that involve human interaction. Such transactions have these key properties:<br>• Long duration. Once a human interacts with an active transaction, that trans-<br>action becomes a long-duration transaction from the perspective of the com-<br>puter, since human response time is slow relative to computer speed. Further-<br>more, in design applications, the human activity may involve hours, days, or<br>an even longer period. Thus, transactions may be of long duration in human<br>terms, as well as in machine terms.<br>• Exposure of uncommitted data. Data generated and displayed to a user by a<br>long-duration transaction are uncommitted, since the transaction may abort.<br>Thus, users—and, as a result, other transactions—may be forced to read un-<br>committed data. If several users are cooperating on a project, user transactions<br>may need to exchange data prior to transaction commit.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VII. Other Topics<br>24. Advanced Transaction <br>Processing<br>898<br>© The McGraw−Hill <br>Companies, 2001<br>24.5<br>Long-Duration Transactions<br>905<br>• Subtasks. An interactive transaction may consist of a set of subtasks initiated<br>by the user. The user may wish to abort a subtask without necessarily causing<br>the entire transaction to abort.<br>• Recoverability. It is unacceptable to abort a long-duration interactive transac-<br>tion because of a system crash. The active transaction must be recovered to a<br>state that existed shortly before the crash so that relatively little human work<br>is lost.<br>• Performance. Good performance in an interactive transaction system is de-<br>ﬁned as fast response time. This deﬁnition is in contrast to that in a nonin-<br>teractive system, in which high throughput (number of transactions per sec-<br>ond) is the goal. Systems with high throughput make efﬁcient use of system<br>resources. However, in the case of interactive transactions, the most costly<br>resource is the user. If the efﬁciency and satisfaction of the user is to be op-<br>timized, response time should be fast (from a human perspective). In those<br>cases where a task takes a long time, response time should be predictable (that<br>is, the variance in response times should be low), so that users can manage<br>their time well.<br>In Sections 24.5.1 through 24.5.5, we shall see why these ﬁve properties are incompat-<br>ible with the techniques presented thus far, and shall discuss how those techniques<br>can be modiﬁed to accommodate long-duration interactive transactions.<br>24.5.1<br>Nonserializable Executions<br>The properties that we discussed make it impractical to enforce the requirement<br>used in earlier chapters that only serializable schedules be permitted. Each of the<br>concurrency-control protocols of Chapter 16 has adverse effects on long-duration<br>transactions:<br>• Two-phase locking. When a lock cannot be granted, the transaction request-<br>ing the lock is forced to wait for the data item in question to be unlocked. The<br>duration of this wait is proportional to the duration of the transaction holding<br>the lock. If the data item is locked by a short-duration transaction, we expect<br>that the waiting time will be short (except in case of deadlock or extraordinary<br>system load). However, if the data item is locked by a long-duration transac-<br>tion, the wait will be of long duration. Long waiting times lead to both longer<br>response time and an increased chance of deadlock.<br>• Graph-based protocols. Graph-based protocols allow for locks to be released<br>earlier than under the two-phase locking protocols, and they prevent dead-<br>lock. However, they impose an ordering on the data items. Transactions must<br>lock data items in a manner consistent with this ordering. As a result, a trans-<br>action may have to lock more data than it needs. Furthermore, a transaction<br>must hold a lock until there is no chance that the lock will be needed again.<br>Thus, long-duration lock waits are likely to occur.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VII. Other Topics<br>24. Advanced Transaction <br>Processing<br>899<br>© The McGraw−Hill <br>Companies, 2001<br>906<br>Chapter 24<br>Advanced Transaction Processing<br>• Timestamp-based protocols. Timestamp protocols never require a transac-<br>tion to wait. However, they do require transactions to abort under certain cir-<br>cumstances. If a long-duration transaction is aborted, a substantial amount of<br>work is lost. For noninteractive transactions, this lost work is a performance<br>issue. For interactive transactions, the issue is also one of user satisfaction. It<br>is highly undesirable for a user to ﬁnd that several hours’ worth of work have<br>been undone.<br>• Validation protocols. Like timestamp-based protocols, validation protocols<br>enforce serializability by means of transaction abort.<br>Thus, it appears that the enforcement of serializability results in long-duration waits,<br>in abort of long-duration transactions, or in both. There are theoretical results, cited<br>in the bibliographical notes, that substantiate this conclusion.<br>Further difﬁculties with the enforcement of serializability arise when we consider<br>recovery issues. We previously discussed the problem of cascading rollback, in which<br>the abort of a transaction may lead to the abort of other transactions. This phe-<br>nomenon is undesirable, particularly for long-duration transactions. If locking is<br>used, exclusive locks must be held until the end of the transaction, if cascading roll-<br>back is to be avoided. This holding of exclusive locks, however, increases the length<br>of transaction waiting time.<br>Thus, it appears that the enforcement of transaction atomicity must either lead to<br>an increased probability of long-duration waits or create a possibility of cascading<br>rollback.<br>These considerations are the basis for the alternative concepts of correctness of<br>concurrent executions and transaction recovery that we consider in the remainder of<br>this section.<br>24.5.2<br>Concurrency Control<br>The fundamental goal of database concurrency control is to ensure that concurrent<br>execution of transactions does not result in a loss of database consistency. The con-<br>cept of serializability can be used to achieve this goal, since all serializable schedules<br>preserve consistency of the database. However, not all schedules that preserve consis-<br>tency of the database are serializable. For an example, consider again a bank database<br>consisting of two accounts A and B, with the consistency requirement that the sum<br>A + B be preserved. Although the schedule of Figure 24.5 is not conﬂict serializable,<br>it nevertheless preserves the sum of A + B. It also illustrates two important points<br>about the concept of correctness without serializability.<br>• Correctness depends on the speciﬁc consistency constraints for the database.<br>• Correctness depends on the properties of operations performed by each trans-<br>action.<br>In general it is not possible to perform an automatic analysis of low-level operations<br>by transactions and check their effect on database consistency constraints. However,<br>there are simpler techniques. One is to use the database consistency constraints as<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VII. Other Topics<br>24. Advanced Transaction <br>Processing<br>900<br>© The McGraw−Hill <br>Companies, 2001<br>24.5<br>Long-Duration Transactions<br>907<br>T1<br>T2<br>read(A)<br>A := A<br>50<br>write(A)<br>read(B)<br>B := B<br>10<br>write(B)<br>read(B)<br>B := B + 50<br>write(B)<br>read(A)<br>A := A + 10<br>write(A)<br>–<br>–<br>Figure 24.5<br>A non-conﬂict-serializable schedule.<br>the basis for a split of the database into subdatabases on which concurrency can be<br>managed separately. Another is to treat some operations besides read and write as<br>fundamental low-level operations, and to extend concurrency control to deal with<br>them.<br>The bibliographical notes reference other techniques for ensuring consistency with-<br>out requiring serializability. Many of these techniques exploit variants of multiver-<br>sion concurrency control (see Section 17.6). For older data-processing applications<br>that need only one version, multiversion protocols impose a high space overhead<br>to store the extra versions. Since many of the new database applications require the<br>maintenance of versions of data, concurrency-control techniques that exploit multi-<br>ple versions are practical.<br>24.5.3<br>Nested and Multilevel Transactions<br>A long-duration transaction can be viewed as a collection of related subtasks or sub-<br>transactions. By structuring a transaction as a set of subtransactions, we are able to<br>enhance parallelism, since it may be possible to run several subtransactions in paral-<br>lel. Furthermore, it is possible to deal with failure of a subtransaction (due to abort,<br>system crash, and so on) without having to roll back the entire long-duration trans-<br>action.<br>A nested or multilevel transaction T consists of a set T = {t1, t2, . . ., tn} of sub-<br>transactions and a partial order P on T. A subtransaction ti in T may abort without<br>forcing T to abort. Instead, T may either restart ti or simply choose not to run ti. If<br>ti commits, this action does not make ti permanent (unlike the situation in Chap-<br>ter 17). Instead, ti commits to T, and may still abort (or require compensation—see<br>Section 24.5.4) if T aborts. An execution of T must not violate the partial order P. That<br>is, if an edge ti →tj appears in the precedence graph, then tj →ti must not be in<br>the transitive closure of P.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VII. Other Topics<br>24. Advanced Transaction <br>Processing<br>901<br>© The McGraw−Hill <br>Companies, 2001<br>908<br>Chapter 24<br>Advanced Transaction Processing<br>Nesting may be several levels deep, representing a subdivision of a transaction<br>into subtasks, subsubtasks, and so on. At the lowest level of nesting, we have the<br>standard database operations read and write that we have used previously.<br>If a subtransaction of T is permitted to release locks on completion, T is called<br>a multilevel transaction. When a multilevel transaction represents a long-duration<br>activity, the transaction is sometimes referred to as a saga. Alternatively, if locks held<br>by a subtransaction ti of T are automatically assigned to T on completion of ti, T is<br>called a nested transaction.<br>Although the main practical value of multilevel transactions arises in complex,<br>long-duration transactions, we shall use the simple example of Figure 24.5 to show<br>how nesting can create higher-level operations that may enhance concurrency. We<br>rewrite transaction T1, using subtransactions T1,1 and T1,2, which perform increment<br>or decrement operations:<br>• T1 consists of<br>  T1,1, which subtracts 50 from A<br>  T1,2, which adds 50 to B<br>Similarly, we rewrite transaction T2, using subtransactions T2,1 and T2,2, which also<br>perform increment or decrement operations:<br>• T2 consists of<br>  T2,1, which subtracts 10 from B<br>  T2,2, which adds 10 to A<br>No ordering is speciﬁed on T1,1, T1,2, T2,1, and T2,2. Any execution of these subtrans-<br>actions will generate a correct result. The schedule of Figure 24.5 corresponds to the<br>schedule &lt; T1,1, T2,1, T1,2, T2,2 &gt;.<br>24.5.4<br>Compensating Transactions<br>To reduce the frequency of long-duration waiting, we arrange for uncommitted up-<br>dates to be exposed to other concurrently executing transactions. Indeed, multilevel<br>transactions may allow this exposure. However, the exposure of uncommitted data<br>creates the potential for cascading rollbacks. The concept of compensating transac-<br>tions helps us to deal with this problem.<br>Let transaction T be divided into several subtransactions t1, t2, . . . , tn. After a sub-<br>transaction ti commits, it releases its locks. Now, if the outer-level transaction T has to<br>be aborted, the effect of its subtransactions must be undone. Suppose that subtrans-<br>actions t1, . . . , tk have committed, and that tk+1 was executing when the decision to<br>abort is made. We can undo the effects of tk+1 by aborting that subtransaction. How-<br>ever, it is not possible to abort subtransactions t1, . . . , tk, since they have committed<br>already.<br>Instead, we execute a new subtransaction cti, called a compensating transaction, to<br>undo the effect of a subtransaction ti. Each subtransaction ti is required to have a<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VII. Other Topics<br>24. Advanced Transaction <br>Processing<br>902<br>© The McGraw−Hill <br>Companies, 2001<br>24.5<br>Long-Duration Transactions<br>909<br>compensating transaction cti. The compensating transactions must be executed in<br>the inverse order ctk, . . . , ct1. Here are several examples of compensation:<br>• Consider the schedule of Figure 24.5, which we have shown to be correct,<br>although not conﬂict serializable. Each subtransaction releases its locks once<br>it completes. Suppose that T2 fails just prior to termination, after T2,2 has re-<br>leased its locks. We then run a compensating transaction for T2,2 that subtracts<br>10 from A and a compensating transaction for T2,1 that adds 10 to B.<br>• Consider a database insert by transaction Ti that, as a side effect, causes a<br>B+-tree index to be updated. The insert operation may have modiﬁed several<br>nodes of the B+-tree index. Other transactions may have read these nodes in<br>accessing data other than the record inserted by Ti. As in Section 17.9, we can<br>undo the insertion by deleting the record inserted by Ti. The result is a correct,<br>consistent B+-tree, but is not necessarily one with exactly the same structure<br>as the one we had before Ti started. Thus, deletion is a compensating action<br>for insertion.<br>• Consider a long-duration transaction Ti representing a travel reservation.<br>Transaction T has three subtransactions: Ti,1, which makes airline reserva-<br>tions; Ti,2, which reserves rental cars; and Ti,3, which reserves a hotel room.<br>Suppose that the hotel cancels the reservation. Instead of undoing all of Ti,<br>we compensate for the failure of Ti,3 by deleting the old hotel reservation and<br>making a new one.<br>If the system crashes in the middle of executing an outer-level transaction, its sub-<br>transactions must be rolled back when it recovers. The techniques described in Sec-<br>tion 17.9 can be used for this purpose.<br>Compensation for the failure of a transaction requires that the semantics of the<br>failed transaction be used. For certain operations, such as incrementation or insertion<br>into a B+-tree, the co</span><br><br><span style="background-color: #BDB2FF;" title="Chunk 118 | Start: 2360236 | End: 2380236 | Tokens: 2987">rresponding compensation is easily deﬁned. For more complex<br>transactions, the application programmers may have to deﬁne the correct form of<br>compensation at the time that the transaction is coded. For complex interactive trans-<br>actions, it may be necessary for the system to interact with the user to determine the<br>proper form of compensation.<br>24.5.5<br>Implementation Issues<br>The transaction concepts discussed in this section create serious difﬁculties for im-<br>plementation. We present a few of them here, and discuss how we can address these<br>problems.<br>Long-duration transactions must survive system crashes. We can ensure that they<br>will by performing a redo on committed subtransactions, and by performing either<br>an undo or compensation for any short-duration subtransactions that were active at<br>the time of the crash. However, these actions solve only part of the problem. In typical<br>database systems, such internal system data as lock tables and transactions time-<br>stamps are kept in volatile storage. For a long-duration transaction to be resumed<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VII. Other Topics<br>24. Advanced Transaction <br>Processing<br>903<br>© The McGraw−Hill <br>Companies, 2001<br>910<br>Chapter 24<br>Advanced Transaction Processing<br>after a crash, these data must be restored. Therefore, it is necessary to log not only<br>changes to the database, but also changes to internal system data pertaining to long-<br>duration transactions.<br>Logging of updates is made more complex when certain types of data items exist<br>in the database. A data item may be a CAD design, text of a document, or another<br>form of composite design. Such data items are physically large. Thus, storing both<br>the old and new values of the data item in a log record is undesirable.<br>There are two approaches to reducing the overhead of ensuring the recoverability<br>of large data items:<br>• Operation logging. Only the operation performed on the data item and the<br>data-item name are stored in the log. Operation logging is also called logi-<br>cal logging. For each operation, an inverse operation must exist. We perform<br>undo using the inverse operation, and redo using the operation itself. Recov-<br>ery through operation logging is more difﬁcult, since redo and undo are not<br>idempotent. Further, using logical logging for an operation that updates mul-<br>tiple pages is greatly complicated by the fact that some, but not all, of the<br>updated pages may have been written to the disk, so it is hard to apply either<br>the redo or the undo of the operation on the disk image during recovery.<br>Using physical redo logging and logical undo logging, as described in Sec-<br>tion 17.9, provides the concurrency beneﬁts of logical logging while avoiding<br>the above pitfalls.<br>• Logging and shadow paging. Logging is used for modiﬁcations to small data<br>items, but large data items are made recoverable via a shadow-page technique<br>(see Section 17.5). When we use shadowing, only those pages that are actually<br>modiﬁed need to be stored in duplicate.<br>Regardless of the technique used, the complexities introduced by long-duration trans-<br>actions and large data items complicate the recovery process. Thus, it is desirable<br>to allow certain noncritical data to be exempt from logging, and to rely instead on<br>ofﬂine backups and human intervention.<br>24.6<br>TransactionManagementinMultidatabases<br>Recall from Section 19.8 that a multidatabase system creates the illusion of logical<br>database integration, in a heterogeneous database system where the local database<br>systems may employ different logical data models and data-deﬁnition and data-<br>manipulation languages, and may differ in their concurrency-control and transac-<br>tion-management mechanisms.<br>A multidatabase system supports two types of transactions:<br>1. Local transactions. These transactions are executed by each local database<br>system outside of the multidatabase system’s control.<br>2. Global transactions. These transactions are executed under the multidatabase<br>system’s control.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VII. Other Topics<br>24. Advanced Transaction <br>Processing<br>904<br>© The McGraw−Hill <br>Companies, 2001<br>24.6<br>Transaction Management in Multidatabases<br>911<br>The multidatabase system is aware of the fact that local transactions may run at the<br>local sites, but it is not aware of what speciﬁc transactions are being executed, or of<br>what data they may access.<br>Ensuring the local autonomy of each database system requires that no changes<br>be made to its software. A database system at one site thus is not able to commu-<br>nicate directly with a one at any other site to synchronize the execution of a global<br>transaction active at several sites.<br>Since the multidatabase system has no control over the execution of local transac-<br>tions, each local system must use a concurrency-control scheme (for example, two-<br>phase locking or timestamping) to ensure that its schedule is serializable. In addition,<br>in case of locking, the local system must be able to guard against the possibility of lo-<br>cal deadlocks.<br>The guarantee of local serializability is not sufﬁcient to ensure global serializabil-<br>ity. As an illustration, consider two global transactions T1 and T2, each of which ac-<br>cesses and updates two data items, A and B, located at sites S1 and S2, respectively.<br>Suppose that the local schedules are serializable. It is still possible to have a situation<br>where, at site S1, T2 follows T1, whereas, at S2, T1 follows T2, resulting in a non-<br>serializable global schedule. Indeed, even if there is no concurrency among global<br>transactions (that is, a global transaction is submitted only after the previous one<br>commits or aborts), local serializability is not sufﬁcient to ensure global serializabil-<br>ity (see Exercise 24.14).<br>Depending on the implementation of the local database systems, a global trans-<br>action may not be able to control the precise locking behavior of its local substrans-<br>actions. Thus, even if all local database systems follow two-phase locking, it may<br>be possible only to ensure that each local transaction follows the rules of the pro-<br>tocol. For example, one local database system may commit its subtransaction and<br>release locks, while the subtransaction at another local system is still executing. If the<br>local systems permit control of locking behavior and all systems follow two-phase<br>locking, then the multidatabase system can ensure that global transactions lock in a<br>two-phase manner and the lock points of conﬂicting transactions would then deﬁne<br>their global serialization order. If different local systems follow different concurrency-<br>control mechanisms, however, this straightforward sort of global control does not<br>work.<br>There are many protocols for ensuring consistency despite concurrent execution<br>of global and local transactions in multidatabase systems. Some are based on impos-<br>ing sufﬁcient conditions to ensure global serializability. Others ensure only a form of<br>consistency weaker than serializability, but achieve this consistency by less restric-<br>tive means. We consider one of the latter schemes: two-level serializability. Section 24.5<br>describes further approaches to consistency without serializability; other approaches<br>are cited in the bibliographical notes.<br>A related problem in multidatabase systems is that of global atomic commit. If<br>all local systems follow the two-phase commit protocol, that protocol can be used<br>to achieve global atomicity. However, local systems not designed to be part of a dis-<br>tributed system may not be able to participate in such a protocol. Even if a local sys-<br>tem is capable of supporting two-phase commit, the organization owning the system<br>may be unwilling to permit waiting in cases where blocking occurs. In such cases,<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VII. Other Topics<br>24. Advanced Transaction <br>Processing<br>905<br>© The McGraw−Hill <br>Companies, 2001<br>912<br>Chapter 24<br>Advanced Transaction Processing<br>compromises may be made that allow for lack of atomicity in certain failure modes.<br>Further discussion of these matters appears in the literature (see the bibliographical<br>notes).<br>24.6.1<br>Two-Level Serializability<br>Two-level serializability (2LSR) ensures serializability at two levels of the system:<br>• Each local database system ensures local serializability among its local trans-<br>actions, including those that are part of a global transaction.<br>• The multidatabase system ensures serializability among the global transac-<br>tions alone—ignoring the orderings induced by local transactions.<br>Each of these serializability levels is simple to enforce. Local systems already offer<br>guarantees of serializability; thus, the ﬁrst requirement is easy to achieve. The second<br>requirement applies to only a projection of the global schedule in which local trans-<br>actions do not appear. Thus, the multidatabase system can ensure the second require-<br>ment using standard concurrency-control techniques (the precise choice of technique<br>does not matter).<br>The two requirements of 2LSR are not sufﬁcient to ensure global serializability.<br>However, under the 2LSR-based approach, we adopt a requirement weaker than se-<br>rializability, called strong correctness:<br>1. Preservation of consistency as speciﬁed by a set of consistency constraints<br>2. Guarantee that the set of data items read by each transaction is consistent<br>It can be shown that certain restrictions on transaction behavior, combined with 2LSR,<br>are sufﬁcient to ensure strong correctness (although not necessarily to ensure serial-<br>izability). We list several of these restrictions.<br>In each of the protocols, we distinguish between local data and global data. Local<br>data items belong to a particular site and are under the sole control of that site. Note<br>that there cannot be any consistency constraints between local data items at distinct<br>sites. Global data items belong to the multidatabase system, and, though they may<br>be stored at a local site, are under the control of the multidatabase system.<br>The global-read protocol allows global transactions to read, but not to update,<br>local data items, while disallowing all access to global data by local transactions. The<br>global-read protocol ensures strong correctness if all these conditions hold:<br>1. Local transactions access only local data items.<br>2. Global transactions may access global data items, and may read local data<br>items (although they must not write local data items).<br>3. There are no consistency constraints between local and global data items.<br>The local-read protocol grants local transactions read access to global data, but<br>disallows all access to local data by global transactions. In this protocol, we need to<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VII. Other Topics<br>24. Advanced Transaction <br>Processing<br>906<br>© The McGraw−Hill <br>Companies, 2001<br>24.6<br>Transaction Management in Multidatabases<br>913<br>introduce the notion of a value dependency. A transaction has a value dependency<br>if the value that it writes to a data item at one site depends on a value that it read for<br>a data item on another site.<br>The local-read protocol ensures strong correctness if all these conditions hold:<br>1. Local transactions may access local data items, and may read global data items<br>stored at the site (although they must not write global data items).<br>2. Global transactions access only global data items.<br>3. No transaction may have a value dependency.<br>The global-read–write/local-read protocol is the most generous in terms of data<br>access of the protocols that we have considered. It allows global transactions to read<br>and write local data, and allows local transactions to read global data. However, it<br>imposes both the value-dependency condition of the local-read protocol and the con-<br>dition from the global-read protocol that there be no consistency constraints between<br>local and global data.<br>The global-read–write/local-read protocol ensures strong correctness if all these<br>conditions hold:<br>1. Local transactions may access local data items, and may read global data items<br>stored at the site (although they must not write global data items).<br>2. Global transactions may access global data items as well as local data items<br>(that is, they may read and write all data).<br>3. There are no consistency constraints between local and global data items.<br>4. No transaction may have a value dependency.<br>24.6.2<br>Ensuring Global Serializability<br>Early multidatabase systems restricted global transactions to be read only. They thus<br>avoided the possibility of global transactions introducing inconsistency to the data,<br>but were not sufﬁciently restrictive to ensure global serializability. It is indeed pos-<br>sible to get such global schedules and to develop a scheme to ensure global serializ-<br>ability, and we ask you to do both in Exercise 24.15.<br>There are a number of general schemes to ensure global serializability in an envi-<br>ronment where update as well read-only transactions can execute. Several of these<br>schemes are based on the idea of a ticket. A special data item called a ticket is created<br>in each local database system. Every global transaction that accesses data at a site<br>must write the ticket at that site. This requirement ensures that global transactions<br>conﬂict directly at every site they visit. Furthermore, the global transaction manager<br>can control the order in which global transactions are serialized, by controlling the<br>order in which the tickets are accessed. References to such schemes appear in the<br>bibliographical notes.<br>If we want to ensure global serializability in an environment where no direct lo-<br>cal conﬂicts are generated in each site, some assumptions must be made about the<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VII. Other Topics<br>24. Advanced Transaction <br>Processing<br>907<br>© The McGraw−Hill <br>Companies, 2001<br>914<br>Chapter 24<br>Advanced Transaction Processing<br>schedules allowed by the local database system. For example, if the local schedules<br>are such that the commit order and serialization order are always identical, we can<br>ensure serializability by controlling only the order in which transactions commit.<br>The problem with schemes that ensure global serializability is that they may re-<br>strict concurrency unduly. They are particularly likely to do so because most trans-<br>actions submit SQL statements to the underlying database system, instead of submit-<br>ting individual read, write, commit, and abort steps. Although it is still possible to<br>ensure global serializability under this assumption, the level of concurrency may be<br>such that other schemes, such as the two-level serializability technique discussed in<br>Section 24.6.1, are attractive alternatives.<br>24.7<br>Summary<br>• Workﬂows are activities that involve the coordinated execution of multiple<br>tasks performed by different processing entities. They exist not just in com-<br>puter applications, but also in almost all organizational activities. With the<br>growth of networks, and the existence of multiple autonomous database sys-<br>tems, workﬂows provide a convenient way of carrying out tasks that involve<br>multiple systems.<br>• Although the usual ACID transactional requirements are too strong or are<br>unimplementable for such workﬂow applications, workﬂows must satisfy a<br>limited set of transactional properties that guarantee that a process is not left<br>in an inconsistent state.<br>• Transaction-processing monitors were initially developed as multithreaded<br>servers that could service large numbers of terminals from a single process.<br>They have since evolved, and today they provide the infrastructure for build-<br>ing and administering complex transaction-processing systems that have a<br>large number of clients and multiple servers. They provide services such as<br>durable queueing of client requests and server responses, routing of client<br>messages to servers, persistent messaging, load balancing, and coordination<br>of two-phase commit when transactions access multiple servers.<br>• Large main memories are exploited in certain systems to achieve high system<br>throughput. In such systems, logging is a bottleneck. Under the group-commit<br>concept, the number of outputs to stable storage can be reduced, thus releas-<br>ing this bottleneck.<br>• The efﬁcient management of long-duration interactive transactions is more<br>complex, because of the long-duration waits, and because of the possibility of<br>aborts. Since the concurrency-control techniques used in Chapter 16 use waits,<br>aborts, or both, alternative techniques must be considered. These techniques<br>must ensure correctness without requiring serializability.<br>• A long-duration transaction is represented as a nested transaction with atomic<br>database operations at the lowest level. If a transaction fails, only active short-<br>duration transactions abort. Active long-duration transactions resume once<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VII. Other Topics<br>24. Advanced Transaction <br>Processing<br>908<br>© The McGraw−Hill <br>Companies, 2001<br>24.7<br>Summary<br>915<br>any short-duration transactions have recovered. A compensating transaction<br>is needed to undo updates of nested transactions that have committed, if the<br>outer-level transaction fails.<br>• In systems with real-time constraints, correctness of execution involves not<br>only database consistency but also deadline satisfaction. The wide variance<br>of execution times for read and write operations complicates the transaction-<br>management problem for time-constrained systems.<br>• A multidatabase system provides an environment in which new database ap-<br>plications can access data from a variety of pre-existing databases located in<br>various heterogeneous hardware and software environments.<br>The local database systems may employ different logical models and data-<br>deﬁnition and data-manipulation languages, and may differ in their concur-<br>rency-control and transaction-management mechanisms. A multidatabase<br>system creates the illusion of logical database integration, without requiring<br>physical database integration.<br>Review Terms<br>• TP monitor<br>• TP-monitor architectures<br>  Process per client<br>  Single server<br>  Many server, single router<br>  Many server, many router<br>• Multitasking<br>• Context switch<br>• Multithreaded server<br>• Queue manager<br>• Application coordination<br>  Resource manager<br>  Remote procedure call (RPC)<br>• Transactional Workﬂows<br>  Task<br>  Processing entity<br>  Workﬂow speciﬁcation<br>  Workﬂow execution<br>• Workﬂow state<br>  Execution states<br>  Output values<br>  External variables<br>• Workﬂow failure atomicity<br>• Workﬂow termination states<br>  Acceptable<br>  Nonacceptable<br>  Committed<br>  Aborted<br>• Workﬂow recovery<br>• Workﬂow-management system<br>• Workﬂow-management system<br>architectures<br>  Centralized<br>  Partially distributed<br>  Fully distributed<br>• Main-memory databases<br>• Group commit<br>• Real-time systems<br>• Deadlines<br>  Hard deadline<br>  Firm deadline<br>  Soft deadline<br>• Real-time databases<br>• Long-duration transactions<br>• Exposure of uncommitted data<br>• Subtasks<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VII. Other Topics<br>24. Advanced Transaction <br>Processing<br>909<br>© The McGraw−Hill <br>Companies, 2001<br>916<br>Chapter 24<br>Advanced Transaction Processing<br>• Nonserializable executions<br>• Nested transactions<br>• Multilevel transactions<br>• Saga<br>• Compensating transactions<br>• Logical logging<br>• Multidatabase systems<br>• Autonomy<br>• Local transactions<br>• Global transactions<br>• Two-level serializability (2LSR)<br>• Strong correctness<br>• Local data<br>• Global data<br>• Protocols<br>  Global-read<br>  Local-read<br>  Value dependency<br>  Global-read–write/local-read<br>• Ensuring global serializability<br>• Ticket<br>Exercises<br>24.1 Explain how a TP monitor manages memory and processor resources more<br>effectively than a typical operating system.<br>24.2 Compare TP monitor features with those provided by Web servers supporting<br>servlets (such servers have been nicknamed TP-lite).<br>24.3 Consider the process of admitting new students at your university (or new<br>employees at your organization).<br>a. Give a high-level picture of the workﬂow</span><br><br><span style="background-color: #FFC6FF;" title="Chunk 119 | Start: 2380238 | End: 2387646 | Tokens: 1114"> starting from the student appli-<br>cation procedure.<br>b. Indicate acceptable termination states, and which steps involve human in-<br>tervention.<br>c. Indicate possible errors (including deadline expiry) and how they are dealt<br>with.<br>d. Study how much of the workﬂow has been automated at your university.<br>24.4 Like database systems, workﬂow systems also require concurrency and recov-<br>ery management. List three reasons why we cannot simply apply a relational<br>database system using 2PL, physical undo logging, and 2PC.<br>24.5 If the entire database ﬁts in main memory, do we still need a database system<br>to manage the data? Explain your answer.<br>24.6 Consider a main-memory database system recovering from a system crash.<br>Explain the relative merits of<br>• Loading the entire database back into main memory before resuming trans-<br>action processing<br>• Loading data as it is requested by transactions<br>24.7 In the group-commit technique, how many transactions should be part of a<br>group? Explain your answer.<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VII. Other Topics<br>24. Advanced Transaction <br>Processing<br>910<br>© The McGraw−Hill <br>Companies, 2001<br>Bibliographical Notes<br>917<br>24.8 Is a high-performance transaction system necessarily a real-time system? Why<br>or why not?<br>24.9 In a database system using write-ahead logging, what is the worst-case num-<br>ber of disk accesses required to read a data item? Explain why this presents a<br>problem to designers of real-time database systems.<br>24.10 Explain why it may be impractical to require serializability for long-duration<br>transactions.<br>24.11 Consider a multithreaded process that delivers messages from a durable queue<br>of persistent messages. Different threads may run concurrently, attempting to<br>deliver different messages. In case of a delivery failure, the message must be<br>restored in the queue. Model the actions that each thread carries out as a mul-<br>tilevel transaction, so that locks on the queue need not be held till a message is<br>delivered.<br>24.12 Discuss the modiﬁcations that need to be made in each of the recovery schemes<br>covered in Chapter 17 if we allow nested transactions. Also, explain any differ-<br>ences that result if we allow multilevel transactions.<br>24.13 What is the purpose of compensating transactions? Present two examples of<br>their use.<br>24.14 Consider a multidatabase system in which it is guaranteed that at most one<br>global transaction is active at any time, and every local site ensures local seri-<br>alizability.<br>a. Suggest ways in which the multidatabase system can ensure that there is<br>at most one active global transaction at any time.<br>b. Show by example that it is possible for a nonserializable global schedule<br>to result despite the assumptions.<br>24.15 Consider a multidatabase system in which every local site ensures local serial-<br>izability, and all global transactions are read only.<br>a. Show by example that nonserializable executions may result in such a sys-<br>tem.<br>b. Show how you could use a ticket scheme to ensure global serializability.<br>Bibliographical Notes<br>Gray and Edwards [1995] provides an overview of TP monitor architectures; Gray<br>and Reuter [1993] provides a detailed (and excellent) textbook description of tran-<br>saction-processing systems, including chapters on TP monitors. Our description of TP<br>monitors is modeled on these two sources. X/Open [1991] deﬁnes the X/Open XA<br>interface. Transaction processing in Tuxedo is described in Huffman [1993]. Wipﬂer<br>[1987] is one of several texts on application development using CICS.<br>Fischer [2001] is a handbook on workﬂow systems. A reference model for work-<br>ﬂows, proposed by the Workﬂow Management Coalition, is presented in Hollinsworth<br>Silberschatz−Korth−Sudarshan: <br>Database System <br>Concepts, Fourth Edition<br>VII. Other Topics<br>24. Advanced Transaction <br>Processing<br>911<br>© The McGraw−Hill <br>Companies, 2001<br>918<br>Chapter 24<br>Advanced Transaction Processing<br>[1994]. The Web site of the coalition is www.wfmc.org. Our description of workﬂows<br>follows the model of Rusinkiewicz and Sheth [1995].<br>Reuter [1989] presents ConTracts, a method for grouping transactions into multi-<br>transaction activities. Some issues related to workﬂows were addressed in the work<br>on long-running activities described by Dayal et al. [1990] and Dayal et al. [1991]. The<br>authors propose event–condition–action rules as a technique for specifying work-<br>ﬂows. Jin et al. [1993] describes workﬂow issues in telecommunication applications.<br>Garcia-Molina and Salem [1992] provides an overview of main-memory databases.<br>Jagadish et al. [1993] describes a recovery algorithm designed for main-memory data-<br>bases. A storage manager for main-memory databases is described in Jagadish et al.<br>[1994].<br>Transaction processing in real-time databases is discussed by Abbott and Garcia-<br>Molina [1999] and Dayal et al. [1990]. Barclay et al. [1982] describes a real-time data-<br>base system used in a telecommunications switching system. Complexity and<br>correctness issues in real-time databases are addressed by Korth et al. [1990b] and<br>Soparkar et al. [1995]. Concurrency control and scheduling in real-time databases are<br>discussed by Haritsa et al. [1990], Hong et al. [1993], and Pang et al. [1995]. Ozsoyoglu<br>and Snodgrass [1995] is a survey of research in real-time and temporal databases.<br>Nested and multilevel transactions are presented by Lynch [1983], Moss [1982],<br>Moss [1985], Lynch and Merritt [1986], Fekete et al. [1990b], Fekete et al. [1990a], Ko-<br>rth and Speegle [1994], and Pu et al. [1988]. Theoretical aspects of multilevel transac-<br>tions are presented in Lynch et al. [1988] and Weihl and Liskov [1990].<br>Several extended-transaction models have been deﬁned including Sagas (Garcia-<br>Molina and Salem [1987]), ACTA (Chrysanthis and Ramamritham [1994]), the Con-<br>Tract model (Wachter and Reuter [1992]), ARIES (Mohan et al. [1992] and Rothermel<br>and Mohan [1989]), and the NT/PV model (Korth and Speegle [1994]).<br>Splitting transactions to achieve higher performance is addressed in Shasha et al.<br>[1995]. A model for concurrency in nested transactions systems is presented in Beeri<br>et al. [1989]. Relaxation of serializability is discussed in Garcia-Molina [1983] and<br>Sha et al. [1988]. Recovery in nested transaction systems is discussed by Moss [1987],<br>Haerder and Rothermel [1987], Rothermel and Mohan [1989]. Multilevel transaction<br>management is discussed in Weikum [1991].<br>Gray [1981], Skarra and Zdonik [1989], Korth and Speegle [1988], and Korth and<br>Speegle [1990] discuss long-duration transactions. Transaction processing for<br>long-duration transactions is considered by Weikum and Schek [1984], Haerder and<br>Rothermel [1987], Weikum et al. [1990], and Korth et al. [1990a]. Salem et al. [1994]<br>presents an extension of 2PL for long-duration transactions by allowing the early<br>release of locks under certain circumstances. Transaction processing in design and<br>software-engineering applications is discussed in Korth et al. [1988], Kaiser [1990],<br>and Weikum [1991].<br>Transaction processing in multidatabase systems is discussed in Breitbart et al.<br>[1990], Breitbart et al. [1991], Breitbart et al. [1992], Soparkar et al. [1991], Mehrotra<br>et al. [1992b] and Mehrotra et al. [1992a]. The ticket scheme is presented in Geor-<br>gakopoulos et al. [1994]. 2LSR is introduced in Mehrotra et al. [1991]. An earlier ap-<br>proach, called quasi-serializability, is presented in Du and Elmagarmid [1989].<br></span></div>
</div>

    
<footer>
    Made with <span class="heart">🤎</span> by <a href="https://github.com/chonkie-inc/chonkie" target="_blank" rel="noopener noreferrer">🦛 Chonkie</a>
</footer>

</body>
</html>
