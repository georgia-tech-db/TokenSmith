from google import genai
from google.genai import types
import httpx
from typing import Optional, List
from pydantic import BaseModel, Field
from tests.metrics.base import MetricBase


class GradingResult(BaseModel):
    """Structured output for LLM as judge grading."""
    score: int = Field(
        description="Rating from 1-5 where 1 is completely incorrect/irrelevant and 5 is excellent/comprehensive",
        ge=1,
        le=5
    )
    accuracy: str = Field(
        description="Brief assessment of factual accuracy compared to the textbook"
    )
    completeness: str = Field(
        description="Brief assessment of whether the answer fully addresses the question"
    )
    clarity: str = Field(
        description="Brief assessment of how clear and well-organized the answer is"
    )
    overall_reasoning: str = Field(
        description="One sentence summary explaining the overall score"
    )


class LLMJudgeMetric(MetricBase):
    """LLM as Judge metric using Gemini for answer evaluation."""
    
    def __init__(self, doc_url: Optional[str] = None, use_structured_output: bool = True):
        """
        Initialize the LLM Judge metric.
        
        Args:
            doc_url: URL to the reference document (PDF). If None, uses default textbook.
            use_structured_output: Whether to use structured output (Pydantic model).
        """
        self.doc_url = doc_url or "https://my.uopeople.edu/pluginfile.php/57436/mod_book/chapter/37620/Database%20System%20Concepts%204th%20Edition%20By%20Silberschatz-Korth-Sudarshan.pdf"
        self.use_structured_output = use_structured_output
        self._doc_data = None
        self._client = None
        self._initialized = False
        self._available = None
    
    @property
    def name(self) -> str:
        return "llm_judge"
    
    @property
    def weight(self) -> float:
        return 1.0
    
    def _initialize(self) -> bool:
        """Initialize the Gemini client and load the reference document."""
        try:
            self._client = genai.Client()
            # Cache the document data
            self._doc_data = httpx.get(self.doc_url).content
            return True
        except Exception as e:
            print(f"LLM Judge metric initialization failed: {e}")
            return False
    
    def is_available(self) -> bool:
        """Check if LLM Judge is available (lazy check)."""
        if self._available is None:
            self._available = self._check_availability()
        return self._available
    
    def _check_availability(self) -> bool:
        """Check if the required dependencies are available without full initialization."""
        try:
            import google.genai
            return True
        except ImportError:
            return False
    
    def _build_grading_prompt(self, question: str, answer: str) -> str:
        """Build an enhanced grading prompt with clear evaluation criteria."""
        return f"""You are an expert evaluator for a database textbook Q&A system. Your task is to grade the quality of answers generated by an LLM pipeline.

**Reference Material:** The attached PDF is the authoritative source textbook on database systems.

**Evaluation Task:**
Question: {question}

Generated Answer: {answer}

**Grading Criteria:**
Evaluate the answer on the following dimensions:

1. **Accuracy (40%)**: Are the facts, concepts, and technical details correct according to the textbook?
2. **Completeness (30%)**: Does the answer fully address all aspects of the question?
3. **Clarity (20%)**: Is the answer well-organized, coherent, and easy to understand?
4. **Relevance (10%)**: Does the answer stay focused on the question without unnecessary tangents?

**Rating Scale:**
- 5 (Excellent): Highly accurate, complete, and clear; demonstrates deep understanding
- 4 (Good): Mostly accurate and complete with minor gaps or clarity issues
- 3 (Satisfactory): Correct core concepts but missing important details or has clarity problems
- 2 (Poor): Contains significant errors, omissions, or confusion
- 1 (Unacceptable): Fundamentally incorrect, irrelevant, or fails to address the question

**Instructions:**
- Base your evaluation ONLY on the attached textbook content
- Provide specific, actionable feedback
- Be fair but rigorous in your assessment"""
    
    def calculate(self, answer: str, expected: str, keywords: Optional[List[str]] = None) -> float:
        """
        Calculate LLM Judge score for the given answer.
        
        Args:
            answer: The generated answer to evaluate
            expected: The expected answer (used as the question context if available)
            keywords: Optional keywords (not used in this metric)
            
        Returns:
            float: Normalized score between 0.0 and 1.0
        """
        if not self.is_available():
            print("LLM Judge metric not available")
            return 0.0
        
        # Lazy initialization on first use
        if not self._initialized:
            if not self._initialize():
                print("LLM Judge metric initialization failed")
                return 0.0
            self._initialized = True
        
        if not answer.strip():
            return 0.0
        
        # Use expected as question context if it looks like a question
        question = expected if expected.strip() else "Evaluate this answer"
        
        try:
            prompt = self._build_grading_prompt(question, answer)
            
            if self.use_structured_output:
                # Use structured output with Pydantic model
                response = self._client.models.generate_content(
                    model="gemini-2.5-flash",
                    contents=[
                        types.Part.from_bytes(
                            data=self._doc_data,
                            mime_type='application/pdf',
                        ),
                        prompt
                    ],
                    config=types.GenerateContentConfig(
                        response_mime_type="application/json",
                        response_schema=GradingResult,
                    )
                )
                
                # Parse structured output
                result = GradingResult.model_validate_json(response.text)
                
                print("\n" + "="*60)
                print("*** LLM AS JUDGE EVALUATION ***")
                print("="*60)
                print(f"Score: {result.score}/5")
                print(f"\nAccuracy: {result.accuracy}")
                print(f"Completeness: {result.completeness}")
                print(f"Clarity: {result.clarity}")
                print(f"\nOverall: {result.overall_reasoning}")
                print("="*60 + "\n")
                
                # Normalize to 0-1 range
                normalized_score = (result.score - 1) / 4.0
                return normalized_score
                
            else:
                # Fallback to unstructured output
                response = self._client.models.generate_content(
                    model="gemini-2.5-flash",
                    contents=[
                        types.Part.from_bytes(
                            data=self._doc_data,
                            mime_type='application/pdf',
                        ),
                        prompt
                    ]
                )
                
                print("\n" + "="*60)
                print("*** LLM AS JUDGE EVALUATION ***")
                print("="*60)
                print(response.text)
                print("="*60 + "\n")
                
                # Try to extract score from text (basic parsing)
                score = self._extract_score_from_text(response.text)
                return (score - 1) / 4.0 if score else 0.5
                
        except Exception as e:
            print(f"LLM Judge evaluation failed: {e}")
            return 0.0
    
    def _extract_score_from_text(self, text: str) -> Optional[int]:
        """Extract numeric score from unstructured text response."""
        import re
        # Look for patterns like "Score: 4" or "Rating: 4/5" or "4 out of 5"
        patterns = [
            r'[Ss]core[:\s]+(\d)',
            r'[Rr]ating[:\s]+(\d)',
            r'(\d)\s*[/out of]+\s*5',
        ]
        for pattern in patterns:
            match = re.search(pattern, text)
            if match:
                score = int(match.group(1))
                if 1 <= score <= 5:
                    return score
        return None


def get_score(question: str, answer: str, use_structured_output: bool = True) -> Optional[float]:
    """
    Legacy function for backward compatibility.
    
    Args:
        question: The question that was asked
        answer: The generated answer to evaluate
        use_structured_output: Whether to use structured output
        
    Returns:
        Normalized score (0-1) or None if evaluation fails
    """
    judge = LLMJudgeMetric(use_structured_output=use_structured_output)
    return judge.calculate(answer=answer, expected=question)


if __name__ == "__main__":
    # Test the metric
    test_question = "What is a database transaction?"
    test_answer = "A database transaction is a unit of work that is performed against a database."
    
    score = get_score(test_question, test_answer)
    print(f"\nFinal normalized score: {score}")