benchmarks:
  - id: "aggregation_grouping"
    question: "How do aggregation with grouping work?"
    expected_answer: "Aggregation partitions tuples by grouping attributes and applies functions like sum, avg, min, max to each group, producing one result per group."
    keywords: ["aggregation", "grouping", "generalized projection", "ignore nulls", "attribute renaming", "duplicates"]
    similarity_threshold: 0.8
    ideal_retrieved_chunks: [1039, 1040, 1496, 1497, 714]
    
  - id: "acid_properties"
    question: "What are the ACID properties of transactions?"
    expected_answer: "Atomicity ensures a transaction's actions are all-or-nothing, enforced by abort/rollback and recovery that can undo partial effects; consistency requires each transaction to preserve database integrity when run alone and relies on the scheduler to admit serializable, recoverable, and preferably cascadeless schedules; isolation makes concurrent executions equivalent to some serial order, commonly achieved with two-phase locking variants that prevent reads of uncommitted data; durability guarantees committed effects persist across crashes via logging to stable storage and redo on restart;"
    keywords: ["atomicity", "consistency", "isolation", "durability", "recoverable", "checkpoints", "two-phase locking", "two-phase commit"]
    similarity_threshold: 0.82
    ideal_retrieved_chunks: [1143, 1142, 1145, 1146, 1148]

  - id: "bptree"
    question: "How does a B+ tree index organize keys and support search, insert, and delete, and why is it preferred over binary trees for disk-based access"
    expected_answer: "B+-trees match node size to a disk page, giving very high fan-out and a shallow, height-balanced tree, so searches/updates require few page I/Os."
    keywords: ["fan-out", "leaf linkage", "merge", "balanced height", "reduced height"]
    similarity_threshold: 0.78
    ideal_retrieved_chunks: [908, 909, 940, 937, 938]

  - id: "fd_normalization"
    question: "What are functional dependencies?"
    expected_answer: "A functional dependency X -> Y asserts that tuples agreeing on X must agree on Y."
    keywords: ["common key", "lossless join", "dependency preservation", "normalization", "superkey"]
    similarity_threshold: 0.7
    ideal_retrieved_chunks: [438, 439, 463, 451, 484]

  - id: "sql_isolation"
    question: "What isolation guarantees does SQL provide by default?"
    expected_answer: "Serializable. In the SQL standard, the default isolation level is Serializable, which guarantees that the outcome of concurrently executing transactions is equivalent to some serial (one-at-a-time) order of those transactionsâ€”thereby preventing dirty reads, nonrepeatable reads, and phantoms."
    keywords: ["serializable", "read committed", "repeatable read", "dirty read", "nonrepeatable read", "phantom", "two-phase locking", "predicate locking"]
    similarity_threshold: 0.7
    ideal_retrieved_chunks: [1173, 1174, 1142, 1143, 1172]

  - id: "primary_foreign_keys"
    question: "Explain primary keys and foreign keys"
    expected_answer: "A primary key is a set of one or more attributes that uniquely identifies each tuple in a relation, chosen from candidate keys which are minimal superkeys; primary key attributes are underlined in schema diagrams and cannot have null values. A foreign key is a set of attributes in one relation (the referencing relation) that references the primary key of another relation (the referenced relation), establishing a referential integrity constraint that requires values in the foreign key to match values in the referenced primary key, thereby linking related data across tables."
    keywords: ["primary key", "foreign key", "unique identifier", "referential integrity", "candidate key", "superkey"]
    similarity_threshold: 0.72
    ideal_retrieved_chunks: [90, 91, 119, 93, 94]

  - id: "database_schema"
    question: "What is a database schema"
    expected_answer: "A database schema is the overall logical design and structure of the database, analogous to variable declarations in a program, defining the relations, their attributes, data types, and constraints including primary keys and foreign keys. The schema remains relatively stable over time, while a database instance represents the actual collection of data stored at a particular moment, with values that change as information is inserted, deleted, or modified."
    keywords: ["database schema", "logical design", "structure", "database instance", "relations", "attributes", "constraints"]
    similarity_threshold: 0.70
    ideal_retrieved_chunks: [49, 50, 51, 250, 60]

  - id: "book_authors"
    question: "Tell me about the authors of the book"
    expected_answer: "The authors of Database System Concepts Seventh Edition are Abraham Silberschatz, a Professor at Yale University and former Bell Labs vice president who is an ACM and IEEE fellow; Henry F. Korth, a Professor at Lehigh University who previously worked at Bell Labs and is also an ACM and IEEE fellow; and S. Sudarshan, the Subrao M. Nilekani Chair Professor at the Indian Institute of Technology Bombay who received his Ph.D. from the University of Wisconsin and is an ACM fellow, with research focusing on query processing and optimization."
    keywords: ["Abraham Silberschatz", "Henry Korth", "Sudarshan", "Yale", "Lehigh", "IIT Bombay", "Bell Labs"]
    similarity_threshold: 0.65
    ideal_retrieved_chunks: [1, 2, 0, 3, 22]

  - id: "aries_atomicity"
    question: "How does the recovery manager use ARIES to ensure atomicity"
    expected_answer: "The ARIES recovery algorithm ensures atomicity by maintaining a write-ahead log where all updates are recorded before being applied to the database, with each log record containing transaction ID, data item, old value, and new value. During normal operation, log records are written to stable storage before the transaction commits; if a transaction aborts or the system crashes, the recovery manager uses the log to undo uncommitted transactions by applying the old values in reverse order, ensuring that partial effects of incomplete transactions are completely rolled back and the all-or-nothing property of atomicity is preserved."
    keywords: ["ARIES", "write-ahead log", "log records", "undo", "rollback", "stable storage", "atomicity", "recovery"]
    similarity_threshold: 0.75
    ideal_retrieved_chunks: [1355, 1356, 1358, 1353, 1359]

  - id: "oltp_vs_analytics"
    question: "Contrast the goals of Online Transaction Processing and data analytics"
    expected_answer: "Online Transaction Processing (OLTP) supports a large number of concurrent users performing small, fast transactions that retrieve and update relatively small amounts of data with requirements for high throughput, low latency, and immediate consistency, typically using normalized schemas optimized for transactional integrity. Data analytics, in contrast, processes large volumes of historical data to draw conclusions and infer patterns for business intelligence and decision support, involving complex queries that scan and aggregate data across many records, often using denormalized schemas like star schemas in data warehouses optimized for read-heavy analytical workloads rather than transactional updates."
    keywords: ["OLTP", "online transaction processing", "data analytics", "business intelligence", "decision support", "throughput", "data warehouse", "transactional", "analytical"]
    similarity_threshold: 0.73
    ideal_retrieved_chunks: [33, 34, 738, 739, 741]

  - id: "lossy_decomposition"
    question: "Show me what happens during a lossy decomposition"
    expected_answer: "A lossy decomposition occurs when a relation R is decomposed into smaller relations R1 and R2 such that joining them back together produces spurious tuples not present in the original relation, resulting in loss of information about which attribute combinations actually existed. This happens when the intersection of R1 and R2 does not form a superkey for either relation, violating the lossless-join condition; the natural join of the decomposed relations generates extra tuples from invalid combinations, making it impossible to reconstruct the original data accurately, which is why database design insists that all decompositions must be lossless."
    keywords: ["lossy decomposition", "spurious tuples", "lossless join", "superkey", "natural join", "information loss", "functional dependency"]
    similarity_threshold: 0.70
    ideal_retrieved_chunks: [431, 430, 432, 433, 440]
