Databases are essential in almost all enterprises, with use increasing in the last four decades. They form an integral part of banking, airlines, universities, and human resources. Today, data-base system vendors like Oracle are among the largest software companies and form a significant part of the product line of more diversified companies like Microsoft and IBM. [end of text]
The textbook section on Database Management Systems focuses on the fundamental concepts, data models, and technologies used in database systems. It covers the basics of database design, including data types, relationships, and normalization. It also delves into the implementation of database systems, including the use of programming languages and database management systems (DBMS). The textbook also discusses indexing, query optimization, and data management strategies. It emphasizes the importance of database design and implementation in modern computing. [end of text]
Conventional file-processing environments do not allow needed data to be retrieved in a convenient and efficient manner. Data isolation, integrity problems, and atomicity problems are major disadvantages of conventional file-processing systems. Database systems, such as DBMSs, are required for general use to address these issues. [end of text]
Database systems are designed to protect sensitive data by maintaining supervision, but this is challenging due to data access by different applications. Security problems arise, especially in banking systems, where access to payroll data is essential but not the entire database. This issue prompted the development of database systems, which enable them to solve file-processing problems. [end of text]
The textbook explains that a database system is a collection of interrelated files and programs that allow users to access and modify data. It emphasizes that the system hides data details, using complex data structures to represent data in the database. The physical level describes how data is stored, while the logical level describes what data is stored and relationships among those data. The system provides users with an abstract view of the data, hiding complexity through several levels of abstraction. [end of text]
The need for efﬁciency has led designers to use complex data structures, and developers hide complexity through several levels of abstraction to simplify user interactions. Database administrators use the logical level of abstraction to provide many views for the same database. [end of text]
Databases change over time and are structured at various levels of abstraction. Conceptual information about database schemas and instances can be understood by analogy to programming languages. Schemas and instances are hidden at the logical level and can be changed at the view level. Logical schemas are the most important for application programs, as they do not depend on physical schema changes. [end of text]
Databases change over time, with instances stored at a particular moment. Schemas are designed infrequently, while physical and logical schemas are hidden beneath them. Logical schemas are the most important, affecting application programs. Languages for describing schemas are used after introducing datamodels. [end of text]
The entity-relationship model is a collection of conceptual tools for describing data, data relationships, data semantics, and consistency constraints. It provides a way to design a database at the logical level. The entity-relationship model is based on a perception of a real world that consists of a collection of basic objects, relationships among these objects, and unique customer identiﬁers. [end of text]
The entity-relationship (E-R) data model is based on a perception of a real world that consists of entities and relationships among these objects. Entities are described by attributes, and relationships are associated with entities. The E-R model is used to design databases by building an E-R diagram, which includes rectangles for entity sets, ellipses for attributes, diamonds for relationships, and lines linking attributes to entity sets and entity sets to relationships. Constraints such as cardinalities are also considered. [end of text]
The relational model is an example of a record-based model, where records are stored in ﬁxed-format records of various types. It is at a lower level of abstraction than the E-R model, with tables representing entities and relationships. The relational model is widely used in databases and is often translated to the E-R model for easier design. It is also possible to create schemas with unnecessary duplication in the relational model. [end of text]
The relational model uses a collection of tables to represent both data and the relationships among those data. Each table has multiple columns, and each column has a unique name. The relational model is an example of a record-based model. Record-based models are so named because the database is structured in fixed-format records of different types. Each table contains records of a particular type. Each record type defines a fixed number of attributes. The columns of the table correspond to the attributes of the record type. The relational data model is the most widely used data model, and a vast majority of current database systems are based on the relational model. Chapters 3 through 7 cover the relational model in detail. The relational model is at a lower level of abstraction than the E-R model. Databasedesigns are often carried out in the E-R model, and then translated to the relational model; Chapter 2 describes the translation process. For example, it is easy to see that the tables customer and account correspond to the entity sets of the same name, while the table depositor corresponds to the relationship set depositor. [end of text]
The object-oriented data model extends the E-R model with concepts such as objects, classes, and relationships. [end of text]
The textbook discusses encapsulation, methods, and object identity, object-relational data modeling, structured data models, XML, and the history of data models. [end of text]
A database system provides a data deﬁnition language to specify the database schema and a data manipulation language to express database queries and updates. In practice, the data deﬁnition and data manipulation languages are not two separate languages; instead, they form a single database language, such as SQL. [end of text]
The textbook explains the concepts of database schema, data-deﬁnition language, data storage and deﬁnition language, and data values satisfying consistency constraints. [end of text]
Data manipulation is the retrieval, insertion, deletion, and modification of data in a database. Data manipulation languages (DML) enable users to access and modify data as defined by the database model. Declarative DMLs are easier to learn and use but require users to specify how to get data, while procedural DMLs do not require this information. Queries are statements that retrieve information and are part of DML. Queries can involve information from multiple tables. [end of text]
The textbook discusses the use of SQL, a commercially used query language, to access and manipulate database data. It also covers other query languages like ODBC and JDBC, which are used experimentally. The goal is to allow humans to interact efficiently with the database system. [end of text]
Application programs are programs used to interact with databases. They are typically written in a host language like Cobol, C, C++, or Java. Examples include payroll checks, debit accounts, credit accounts, or transferring funds between accounts. To access the database, application programs need to be executed from the host language. Two methods are used: by providing an application program interface (set of procedures) and retrieving results. Alternatively, by extending the host language syntax to embed DML calls. [end of text]
A database system is designed to retrieve and store information, with different types of users interacting with the system. Database users include naive users who use forms interfaces, and sophisticated users who use specialized database applications. [end of text]
The textbook summarizes the four types of database-system users, differentiated by the way they interact with the system, and the different types of user interfaces designed for each type. It also covers the roles of application programmers, sophisticated users, and specialized users in the database system. [end of text]
15base and expert systems, systems that store data with complex data types (forexample, graphics data and audio data), and environment-modeling systems. Chapters 8 and 9 cover several of these applications. Database Administrator, one of the main reasons for using DBMSs is to have central control of both the data and the programs that access those data. A DBA is a database administrator who creates the original database schema, modifies the schema and physical organization, grants access authorization, and performs routine maintenance. [end of text]
One of the main reasons for using DBMSs is to have central control of both the data and the programs that access those data. A person who has such central control over the system is called a database administrator (DBA). The functions of a DBA include: schema definition, storage structure and access-method deﬁnition, schema and physical organization modiﬁcation, granting of authorization for data access, routine maintenance. [end of text]
In database systems, transactions are collections of operations that perform a single logical function. Each transaction is a unit of both atomicity and consistency. Transactions must not violate database consistency constraints, and temporary inconsistency may lead to difficulty during execution. The data system's responsibility is to define transactions properly, ensuring atomicity and durability. When multiple transactions update the database concurrently, data consistency may be lost, even if each individual transaction is correct. The concurrency-control manager controls the interaction among concurrent transactions, ensuring database consistency. [end of text]
A database system is partitioned into modules that handle storage and query processing, with a focus on managing large amounts of data. The storage manager is crucial for storing and managing data, while the query processor manages the data retrieval process. Corporate databases vary in size from hundreds of gigabytes to terabytes, with a gigabyte being 1000 megabytes. [end of text]
The storage manager, query processor, and DML compiler are key components in a database system, facilitating data storage, retrieval, and updates while minimizing data movement between disk and main memory. The DML compiler translates DML statements into low-level instructions, while the query evaluation engine executes low-level instructions generated by the DML compiler. The DDL interpreter interprets DDL statements, and the DML compiler translates DML statements into evaluation plans. The query evaluation engine executes low-level instructions generated by the DML compiler. [end of text]
A storage manager is a program that translates database operations into file system commands, managing disk space and data structures to handle large data sets. It includes authorization and integrity management, transaction management, and file allocation. The storage manager is part of the database system and implements data structures such as data files, data dictionaries, and indices. [end of text]
The query processor components include DDL interpreter, DML compiler, and query evaluation engine.
Most users of a database system today connect to it through a network, and applications are partitioned into two or three parts, with a client machine acting as a frontend and communicating with an application server. Three-tier architectures are more appropriate for large applications and applications running on the World Wide Web. [end of text]
Data processing is crucial for the growth of computers, dating back to the early days of commercial computers. Punched cards and mechanical systems were used to record U.S. census data and Silberschatz-Korth-Sudarshan: Database System Concepts, Fourth Edition1. Introduction. [end of text]
The textbook discusses the evolution of database technology, including the use of magnetic tapes, hard disks, and modern databases, and the development of the relational model and non-procedural querying methods. [end of text]
The 1980s saw significant advancements in relational databases, including the development of System R by IBM Research, which revolutionized database technology. The 1990s saw the introduction of SQL, a language designed for decision support, and the emergence of parallel and distributed databases. The late 1990s saw the explosive growth of the World Wide Web and the need for more extensive database deployment. [end of text]
A database-management system (DBMS) is a collection of interrelated data and programs to access that data. It aims to provide an environment for people to use in retrieving and storing information. Database systems are ubiquitous today, and most people interact with databases many times every day. A major purpose of a database system is to provide users with an abstract view of the data, hiding details of how the data are stored. Underlying the structure of a database is the data model, which provides a convenient graphical representation. The overall design of the database is called the database schema, which is specified by a set of deﬁnitions using a data-deﬁnition language. A database system has several subsystems, including the transaction manager, query processor, storage manager, and metadata. [end of text]
are two disadvantages of using a database? Two main disadvantages include data redundancy and potential data loss. [end of text]
The responsibility for a task might be discharged if there were no clear guidelines or if the task was not well-defined. This could lead to confusion, misunderstandings, and potential errors. [end of text]
Procedural learning and use are easier for some groups than others.
Enterprise's Silberschatz, Korth, Sudarshan, 4th ed. Database System Concepts, McGraw-Hill, 2001. Chapter 1, Introduction. [end of text]
The entity-relationship (E-R) model is a high-level data model based on a perception of a real world consisting of entities and relationships. The relational model is a lower-level model using tables to represent both data and relationships among those data. The E-R model is useful for database design by facilitating the mapping of enterprise schemas onto conceptual schemas. The entity-relationship model extends the representation of entities by adding notions of encapsulation, methods, and object identity. The object-relational model combines features of the entity-relationship model and the relational model. [end of text]
An entity is a "thing" or "object" in the real world that is distinguishable from others, with a set of properties that uniquely identify it. Entities can be concrete or abstract, such as a person or a loan, and have attributes that describe their properties. Attributes are descriptive properties possessed by each entity, and their values uniquely identify the entity. Entities are represented by sets of attributes, which can be disjoint or include further attributes. Attributes can be characterized by different types, such as social-security numbers. [end of text]
An entity is a "thing" or "object" in the real world that is distinguishable from others. For example, each person in an enterprise is an entity. An entity has aset of properties, and the values for some set of properties may uniquely identify an entity. For instance, a person may have a person-id property whose value uniquely identifies that person. Thus, the value 677-89-9011 for person-id would uniquely identify one particular person in the enterprise. Similarly, loans can be thought of as entities, and loan number L-15 at the Perryridge branch uniquely identifies a loan entity. An entity set is a set of entities of the same type that share the same properties, or attributes. The set of all persons who are customers at a given bank, for example, can be deﬁned as the entity set customer. Similarly, the entity set loan might represent the 27Silberschatz−Korth−Sudarshan: Database System Concepts, Fourth EditionI. Data Models2. Entity−Relationship Model38© The McGraw−Hill Companies, 200128Chapter 2Entity-Relationship Modelset of all loans awarded by a particular bank. The individual entities that constitute an entity set are said to be the extension of the entity set. Thus, all the individual bank customers are the extension of the entity set customer. [end of text]
In our examples, the attributes are simple, with a single value for each entity. Composite attributes can be divided into subparts, making the modeling cleaner. Single-valued and multivalued attributes are used to group related attributes. Derived attributes are derived from other related attributes or entities. The null value indicates "not applicable." [end of text]
The entity set account represents customers and their balances, while branch entities are described by branch-name and city. Relationship sets are mathematical relations on n ≥2 entity sets, where each entity set is a subset of {(e1, e2, . . . , en) | e1 ∈E1, e2 ∈E2, . . . , en ∈En}. Relationship instances in an E-R schema represent associations between named entities. Descriptive attributes can be used to specify the most recent date on which a customer accessed an account. Relationships may have attributes called descriptive attributes, such as access-date, which can be used to record whether a customer has taken the course for credit or is auditing. [end of text]
A relationship is an association among several entities, such as customer Hayes with loan L-15. A relationship set is a subset of relationships of the same type. The association between customer and bank loan is represented by borrower. Relationships can have attributes and descriptive attributes, with roles implicit and not usually specified. Relationships may have multiple attributes, such as access-date, and relationships involving the same entity sets may participate in another relationship set, such as guarantor. [end of text]
The relationship sets borrower and loan-branch represent a binary relationship set, involving two entity sets. Ternary relationships involve more than two entity sets. Examples include employee, branch, and job, with attributes title and level. A ternary relationship among Jones, Perryridge, and manager indicates that Jones acts as manager at the Perryridge branch. [end of text]
Mapping cardinalities and participation constraints are two important types of constraints in E-R enterprise schemas. They describe binary relationship sets and are useful for describing binary relationship sets that involve more than two entity sets. In this section, we shall concentrate on binary relationship sets. [end of text]
Mapping cardinalities are used to describe binary relationship sets, such as one-to-many or many-to-many, to indicate the number of entities each can be associated with. [end of text]
The participation of an entity set in a relationship set is total if every entity participates in at least one relationship, while partial if only some entities participate. [end of text]
The relationship set borrower is total, and an individual can be a bank customer whether or not she has a loan with the bank. Hence, it is possible that only some of the customer entities are related to the loan entity set through the borrower relationship, and the participation of customer in the borrower relationship set is therefore partial. [end of text]
In a database, entities are distinct and can be uniquely identified by their attribute values. Keys, which are subsets of attributes, help uniquely identify relationships and distinguish them from each other. Candidate keys are chosen as primary keys, ensuring uniqueness and preventing extraneous attributes. The primary key should be chosen with care to avoid changes to its attributes. [end of text]
A superkey is a set of one or more attributes that uniquely identify an entity in an entity set. Candidate keys are minimal superkeys that can be formed from any subset of attributes. Key (primary, candidate, super) properties are used to represent the entity set rather than individual entities. Candidate keys should be chosen with care to prevent attribute changes. [end of text]
The primary key of an entity set allows us to distinguish among the various entities of the set. We need a similar mechanism to distinguish among the various relationships of a relationship set. Let R be a relationship set involving entity sets E1, E2, . . . , En. Let primary-key(Ei) denote the set of attributes that forms the primary key for entity set Ei. Assumefor now that the attribute names of all primary keys are unique, and each entity set participates only once in the relationship. The composition of the primary key fora relationship set depends on the set of attributes associated with the relationshipset R.If the relationship set R has no attributes associated with it, then the set of attributesprimary-key(E1) ∪primary-key(E2) ∪· · · ∪primary-key(En)describes an individual relationship in set R. If the relationship set R has attributes a1, a2, · · · , am associated with it, then the set of attributesprimary-key(E1) ∪primary-key(E2) ∪· · · ∪primary-key(En) ∪{a1, a2, . . . , am}describes an individual relationship in set R. In both of the above cases, the set of attributesprimary-key(E1) ∪primary-key(E2) ∪· · · ∪primary-key(En)forms a superkey for the relationship set. In case the attribute names
The structure of the primary key for the relationship set depends on the map-ping cardinality of the relationship set. For many-to-many relationships, the primary key is the union of the primary keys of customer and account. For many-to-one relationships, the primary key is the primary key of customer. For one-to-one relationships, the primary key is the primary key of account. For nonbinary relationships, the primary key can be formed as described earlier. For cardinality constraints, the choice of the primary key is more complicated. [end of text]
In the design of an E-R database schema, it is possible to deﬁne a set of entities and the relationships among them in different ways, such as treating a telephone as an attribute or an entity. The main difference between these two deﬁnitions is that treating a telephone as an entity better models the situation where one may want to keep extra information about a telephone, such as Silberschatz-Korth-Sudarshan: Database System Concepts, Fourth Edition. [end of text]
Treating a telephone as an entity better models situations where employees have multiple telephones, allowing for more detailed information about each telephone. Treating telephone as an attribute is more general and appropriate when the generality is useful. The main difference is that treating telephone as an entity better models situations where employees have multiple telephones, allowing for more detailed information about each telephone. Treating telephone as an attribute is more general and appropriate when the generality is useful. [end of text]
In Section 2.1.1, it was assumed that a bank loan is modeled as an entity. A separate relationship for each holder of a joint loan is needed to avoid replication of attributes such as loan-number and amount. [end of text]
The approach of using binary relationships can also be useful in deciding whether certain attributes might be better represented as relationships. Binary relationships can be more straightforward to express and reduce the complexity of the design and storage requirements. However, it is not always desirable to restrict the E-R model to include only binary relationships. The cardinality ratio of a relationship can affect the placement of relationship attributes, and attributes of one-to-one or one-to-many relationship sets can be associated with one of the participating entity sets, rather than with the relationship set. [end of text]
In databases, relationships are often binary, but relationships that appear binary could be better represented by several binary relationships. Using the two relationships mother and father allows us to record a child’s mother, even if we are not aware of the father’s identity. Using binary relationship sets is preferred in this case. Conceptsually, we can restrict the E-R model to include only binary relationships, but this restriction is not always desirable. An identifying attribute may be needed to represent the relationship set. This attribute increases complexity and storage requirements. [end of text]
In a one-to-many relationship, attributes of one entity set can be associated with another entity set, while in a one-to-one relationship, attributes can be associated with the participating entity set. This affects the placement of attributes in the database. [end of text]
In a one-to-many relationship set, access-date can be placed as an attribute of the depositor relationship set, while in a one-to-one relationship set, it can be associated with either one of the participating entities. For many-to-many relationship sets, access-date should be an attribute of the depositor relationship set, rather than either one of the participating entities. [end of text]
E-R diagrams represent the logical structure of a database graphically, consisting of rectangles, attributes, diamonds, lines, and double lines. They use rectangular sets for entities, ellipses for attributes, diamonds for relationships, and lines for attributes to entity sets and entity sets to relationship sets. Double ellipses denote derived attributes, and dashed ellipses indicate derived attributes. Relationships can be many-to-many, one-to-many, many-to-one, or one-to-one. [end of text]
An undirected line from borrower to loan specifies a many-to-many relationship set from customer to loan. If borrower were one-to-many, from customer to loan, the line would be directed with an arrow pointing to the customer entity set. Similarly, if borrower were many-to-one, the line would have an arrow pointing to the loan entity set. Finally, if borrower were one-to-one, both lines would have arrows pointing to customer and loan entities. [end of text]
The E-R diagram shows roles for manager and worker between the employee entity set and the works-for relationship set. Nonbinary relationships can be specified easily in an E-R diagram. The ternary relationship between entity sets A1, A2, and A3 has a candidate key formed by the union of the primary keys of A1, A2, and A3. The functional dependencies allow either interpretation of the relationship. [end of text]
The textbook explains that loan amounts and loan numbers are limited to a certain number of entries per relationship set, with a maximum of 1 entry per relationship set. [end of text]
A weak entity set may not have sufficient attributes to form a primary key, whereas a strong entity set must be associated with another entity set, called the identifying or owner entity set. Every weak entity must be associated with an identifying entity; the weak entity set is said to be existence dependent on the identifying entity set. The identifying entity set is said to own the weak entity set that it identifies. The relationship associating the weak entity set with the identifying entity set is called the identifying relationship. The identifying relationship is many to one from the weak entity set to the identifying entity set, and the participation of the weak entity set in the relationship is total. [end of text]
As another example of an entity set that can be modeled as a weak entity set, consider offerings of a course at a university. The same course may be offered in different semesters, and within a semester there may be multiple sections for the same course. Thus we can create a weak entity set course-offering, existence dependent on course; different offerings of the same course are identified by a semester and a section-number, which form a discriminator but not a primary key. [end of text]
Specialization, generalization, higher- and lower-level entity sets, attribute inheritance, and aggregation. [end of text]
An entity set may include subgroupings of entities that are distinct in some way from other entities in the set. For instance, a subset of entities within an entity set may have attributes that are not shared by all the entities in the entity set. The E-Rmodel provides a means for representing these distinctive entity groupings. Consider an entity set person, with attributes name, street, and city. A person maybe further classiﬁed as one of the following: customer, employee Each of these person types is described by a set of attributes that includes all the attributes of entity set person plus possibly additional attributes. For example, customer entities may be described further by the attribute customer-id, whereas employee enti-ties may be described further by the attributes employee-id and salary. The process of designingating subgroupings within an entity set is called specialization. The special-ization of person allows us to distinguish among persons according to whether theyare employees or customers. As another example, suppose the bank wishes to divide accounts into two categories, checking account and savings account. Savings accounts need a minimum balance, but the bank may set interest rates differently for different customers, offer better rates to favored customers. Checking accounts have a fixed interest rate, but offer an overdraft facility; the overdraft amount on a checking account must be recorded. [end of text]
The reﬁnement from an initial entity set into successive levels of entity subgroupings represents a top-down design process in which distinctions are made explicit. The process may also proceed in a bottom-up manner, in which multiple entity sets are synthesized into a higher-level entity set on the basis of common features. The database designer may have identiﬁed a customer entity set with attributes name, street, city, and customer-id, and an employee entity set with attributes name, street, city, employee-id, and salary. Person is the higher-level entity set and customer and employee are lower-level entity sets. The person entity set is the superclass of the customer and employee subclasses. Generalization is a containment relationship that exists between a higher-level entity set and one or more lower-level entity sets. The process of applying both processes, in combination, is used in the course of designing the E-R model. [end of text]
A higher-level entity set with attributes and relationships that apply to all of its lower-level entity sets, and a lower-level entity set with distinctive features that apply only within a specific lower-level entity set. Constraints on generalizations may involve membership evaluation based on explicit conditions or predicates. [end of text]
The higher- and lower-level entities created by specialization and generalization inherit attributes, leading to attribute inheritance. This property is crucial for entity sets participating in relationships and can be seen in the hierarchy of entity sets depicted in Figure 2.17. [end of text]
To model an enterprise more accurately, the database designer may choose to place constraints on a particular generalization, such as condition-deﬁned membership. [end of text]
All account entities are evaluated on the deﬁning account-type attribute. Only those entities that satisfy the condition account-type = “savings account” are allowed to belong to the lower-level entity set person. All entities that satisfy the condition account-type = “checking account” are included in checking account. Since all the lower-level entities are evaluated on the same attribute (account-type), the account generalization is attribute-deﬁned. User-deﬁned. User-deﬁned lower-level entity sets are not constrained by a membership condition; rather, the database user assigns entities to a given entity set. For instance, let us assume that, after 3 months of employment, bank employees are assigned to one of four work teams. We therefore represent the teams as four lower-level entity sets of the higher-level employee entity set. Given an employee is not assigned to a speciﬁc team entity automatically on the basis of an explicit deﬁning condition. Instead, the user in charge of this decision makes the team assignment on an individual basis. The assignment is implemented by an operation that adds an entity to an entity set. Second type of constraint relates to whether or not entities may belong to more than one lower-level entity set within a single generalization. The lower-level entity sets may be one of the following: disjoint, overlapping. Third type of constraint relates to whether or not entities may belong to more than one lower-level entity set within a single generalization. The lower-level entity sets may be
The E-R model cannot express relationships among relationships, as demonstrated by the ternary relationship works-on between an employee, branch, and job. To avoid this limitation, a quaternary relationship manages between employee, branch, job, and manager can be created. [end of text]
The textbook summarizes the use of E-R diagrams, aggregation, and alternative E-R notation to represent a situation where multiple entities are related through a single relationship. [end of text]
The set of symbols used in E-R diagrams includes boxes for entities, attributes, primary keys, and relationships. Entities are represented by boxes with names outside, attributes listed one below the other within the box, and primary keys are indicated by listing them at the top. Relationships are represented by lines between entity sets, with binary relationships shown by "crow's foot" notation. [end of text]
The E-R data model provides flexibility for database design, allowing entities to represent objects, real-world concepts, ternary relationships, and pair of binary relationships. [end of text]
The textbook outlines the steps in database design, including characterizing user requirements, choosing a data model, and translating these requirements into a conceptual schema. [end of text]
A high-level data model is used by database designers to specify data requirements and structure the database. The initial phase involves domain experts and users to characterize data needs. The final phase involves choosing a data model and translating requirements into a conceptual schema. [end of text]
In database design, the E-R model is used to translate user requirements into a conceptual schema, which is then used to develop a more realistic, but also more complicated, design than what was seen in earlier examples. The E-R model provides a foundation for the database, and helps ensure that data requirements are met and do not conflict with one another. The process of moving from an abstract data model to the implementation of the database proceeds in two final design phases, where the E-R model is used to map the high-level conceptual schema onto the implementation data model. The E-R model also serves as a basis for the functional requirements of the enterprise. In the logical-design phase, the E-R model is used to map the high-level conceptual schema onto the implementation data model of the database system that will be used, and in the physical-design phase, the physical features of the database are specified. The E-R model is also used to model the functional requirements of the banking enterprise. [end of text]
The textbook outlines the process of database design for a banking enterprise, focusing on the initial speciﬁcation of user requirements and the entity sets and their attributes. It begins by identifying entity sets and their attributes, then constructs a conceptual schema for the database. The text does not model every aspect of the database design for a bank but rather focuses on the initial requirements and entity sets. [end of text]
The initial speciﬁcation of user requirements may be based on interviews with the database users, and on the designer's analysis of the enterprise. The description that arises from this design phase serves as the basis for specifying the conceptual structure of the database. The banking enterprise is organized into branches, each with a unique name and location. Customers are identified by their customer-id, and employees by their employee-id. Accounts are held by customers and employees, with balances and interest rates. Loans originate at branches and can be held by customers. Deposits and withdrawals are tracked in the model. [end of text]
Our specification of data requirements serves as the starting point for constructing a conceptual schema for the database. From the characteristics listed in Section 2.8.2.1, we begin to identify entity sets and their attributes: branch entity set with attributes branch-name, branch-city, and assets, customer entity set with attributes customer-id, customer-name, customer-street, and customer-city, and employee entity set with attributes employee-id, employee-name, telephone-number, salary, and manager. Additional descriptive features include dependent-name, start-date, and employment-length. Silberschatz-Korth-Sudarshan: Database System Concepts, Fourth Edition. I. Data Models 70 © The McGraw-Hill Companies, 2001. [end of text]
The E-R diagram for a bank, expressed in terms of E-R concepts, includes the entity sets, attributes, relationship sets, and mapping cardinalities arrived at through the design processes of Sections 2.8.2.1 and 2.8.2.2, and reﬁned in Section 2.8.2.3. [end of text]
In the previous section, we redefined attributes of entity sets to improve the design scheme. Now we specify the relationships and mapping cardinalities for borrower, loan-branch, loan-payment, and depositor. We also redefined attributes of entity sets to make them more consistent with the new design. [end of text]
The E-R diagram for a banking enterprise, expressed in terms of E-R concepts. It includes entity sets, attributes, relationship sets, and mapping cardinalities. The diagram is from Chapter 2 of the book. [end of text]
We can represent a database that conforms to an E-R database schema by a collection of tables. For each entity set and for each relationship set in the database, there is a unique table to which we assign the name of the corresponding entity set or relation-ship set. Each table has multiple columns, each of which has a unique name. Both the E-R model and the relational-database model are abstract, logical representations of real-world enterprises. Because the two models employ similar design principles, we can convert an E-R design into a relational design. Converting adatabase representation from an E-R diagram to a table format is the way we arriveat a relational-database design from an E-R diagram. Although important differencesSilberschatz−Korth−Sudarshan: Database System Concepts, Fourth EditionI. Data Models2. Entity−Relationship Model72© The McGraw−Hill Companies, 2001 [end of text]
In an E-R diagram, an entity can be represented by a table with one column for each attribute of the entity set, and each row corresponds to one entity of the entity set. Constraints specified in an E-R diagram, such as primary keys and cardinality constraints, are mapped to constraints on the tables generated from the E-R diagram. [end of text]
The entity set E with descriptive attributes a1, a2, . . . , an is represented by a table called E with n distinct columns, each of which corresponds to one of the attributes of E. Each row in this table corresponds to one entity of the entityset E. The table represents the entity set by a table called loan, with two columns, as in Figure 2.23. The row(L-17, 1000)in the loan table means that loan number L-17 has a loan amount of $1000. The entity set customer of the E-R diagram in Fig-ure 2.8 has the attributes customer-id, customer-name, customer-street, and customer-city. The table corresponding to customer has four columns, as in Fig-ure 2.24. [end of text]
A weak entity set with attributes a1, a2, . . . , am, and a strong entity set B with attributes b1, b2, . . . , bn. The primary key of B consists of attributes b1, b2, . . . , bn. The entity set A is represented by a table with one column for each attribute of the set {a1, a2, . . . , am} ∪ {b1, b2, . . . , bn}. The entity set B has three attributes: payment-number, payment-date, and payment-amount. The primary key of the loan entity set, on which payment depends, is loan-number. [end of text]
Let R be a relationship set, a1, a2, ..., am be the set of attributes formed by the union of the primary keys of each entity set, and b1, b2, ..., bn be the descriptive attributes of R. The table R represents a relationship set with columns for each attribute of the set: {a1, a2, ..., am} ∪ {b1, b2, ..., bn}. The relationship set borrower in the E-R diagram of Fig. 2.8 involves the customer and loan entities. [end of text]
The relationship set customer-idloan-number019-28-3746L-11019-28-3746L-23244-66-8800L-93321-12-3123L-17335-57-7991L-16555-55-5555L-14677-89-9011L-15963-96-3963L-17L-931033 June 2001900L-9310413 June 2001200L-231117 May 200175L-931033 June 2001900L-9310413 June 2001200Figure 2.25The payment table.Since the relationship set has no attributes, the borrower table has two columns, la-beled customer-id and loan-number, as shown in Figure 2.26.2.9.3.1Redundancy of TablesA relationship set linking a weak entity set to the corresponding strong entity set istreated specially. As we noted in Section 2.6, these relationships are many
A relationship set linking a weak entity set to the corresponding strong entity set is treated specially, as described in Section 2.6. These relationships are many-to-one and have no descriptive attributes. The primary key of a weak entity set includes the primary key of the strong entity set. The E-R diagram of Figure 2.16 shows a weak entity set payment dependent on the strong entity set loan via the relation-ship set loan-payment. The primary key of payment is {loan-number, payment-number}, and the primary key of loan is {loan-number}. The loan-payment table has two columns, loan-number and payment-number. The table for the entity set payment has four columns, loan-number, payment-number, payment-date, and payment-amount. Every (loan-number, payment-number) combination in loan-payment would also be present in the payment table, and vice versa. Therefore, the loan-payment table is redundant. In general, the table for the relationship set customer-idloan-number019-28-3746L-11019-28-3746L-23244-66-8800L-93321-12-3123L-17335-57-7991L-16555-55-5555L-14677-89-901
Consider a many-to-one relationship set AB from entity set A to entity set B. Using table construction, we combine tables A and AB to form a single table. An account cannot exist without being associated with a branch, and the relationship set account-branch is many to one from account to branch. We combine the table for account-branch with the table for account and require only the following two tables: account, with attributes account-number, balance, and branch-name; branch, with attributes branch-name, branch-city, and assets. [end of text]
We handle composite attributes by creating separate attributes for each component, creating a separate column for the composite attribute itself. [end of text]
Multivalued attributes are an exception to the rule in an E-R diagram, where attributes map directly to columns for tables. These attributes are created into new tables for further organization. [end of text]
In E-R diagrams, a multivalued attribute is represented by a table with columns for each attribute of the entity set and primary key, and each dependent of an entity set is represented as a unique row in the table. The generalization is transformed into a tabular form by creating tables for lower-level entity sets and including attributes and primary keys of the higher-level entity set. The second method for overlapping generalization involves creating tables for lower-level entity sets and including attributes of the higher-level entity set. The third method for disjoint generalization involves creating tables for lower-level entity sets and including attributes of the higher-level entity set. The second method is simpler and more efficient for overlapping generalization. The third method is more complex and less efficient for overlapping generalization. The second method is simpler and more efficient for disjoint generalization. The third method is more complex and less efficient for disjoint generalization. The second method is simpler and more efficient for overlapping generalization. The third method is more complex and less efficient for overlapping generalization. The second method is simpler and more efficient for disjoint generalization. The third method is more complex and less efficient for disjoint generalization. The second method is simpler and more efficient for overlapping generalization. The third method is more complex and less efficient for overlapping generalization. The second method is simpler and more efficient for disjoint generalization. The third method is more complex and less efficient for disjoint generalization. The second method is simpler and more efficient for overlapping generalization
There are two different methods for transforming an E-R diagram to a tabular form that include generalization. The first method includes only the first tier of lower-level entity sets—savings-account and checking-account. The second method uses two tables, one for each lower-level entity set, with attributes for each entity set plus a column for the primary key of the higher-level entity set. The second method is used for an overlapping generalization, whereas the first method is used for an overlapping generalization that is disjoint and complete. [end of text]
Transforming an E-R diagram containing aggregation to a tabular form is straightforward. The table for the relationship setSilberschatz−Korth−Sudarshan: Database System Concepts, Fourth EditionI. Data Models2. Entity-Relationship Model77© The McGraw-Hill Companies, 200168Chapter 2Entity-Relationship Modelmanages between the aggregation of works-on and the entity set manager includes a column for each attribute in the primary keys of the entity set manager and the rela-tionship set works-on. It would also include a column for any descriptive attributes, if they exist, of the relationship set manages. We then transform the relationship sets and entity sets within the aggregated entity. [end of text]
Entity-relationship diagrams help model data representation in a software system. They form only one part, while other components include user interactions, module speculations, and hardware component interactions. UML, a standard for software system specifications, includes class diagrams, use case diagrams, activity diagrams, and implementation diagrams. UML features include class diagrams, which show objects and their attributes, and class diagrams can depict methods. [end of text]
In the UML class diagram, cardinality constraints are specified as l..h, where l denotes the minimum and h the maximum number of relationships an entity can participate in. Generalization and specialization are represented by connecting entity sets by a line with a triangle at the end corresponding to the more general entity set. UML diagrams can also represent explicit constraints of disjoint/overlapping. [end of text]
The entity-relationship (E-R) data model is a conceptual model based on a perception of real-world entities and relationships, designed primarily for database design. It facilitates the specification of an enterprise schema by representing the overall logical structure of the database. The model expresses the distinction between entities and relationships using attributes, and associates each entity with a set of attributes that describe it. Superkeys are used to identify a unique entity in an entity set, and relationships are identified by a set of attributes that allow identifying a unique relationship in a relationship set. [end of text]
In the E-R model, a database can be represented by a collection of tables. Each entity set and relationship set in the database has a unique table assigned to it. The UML provides a graphical means of modeling various components of a software system, including class diagrams based on E-R diagrams. [end of text]
Perkey is a system for managing and analyzing data in a database. It provides tools for data entry, data entry validation, and data entry correction. Perkey is often used in conjunction with other data management tools such as SQL and data warehousing systems. [end of text]
One or more cars each, each with associated accidents. [end of text]
A log is kept of all tests and examinations conducted on each patient. Associates with each patient a log of the various tests and exams conducted. [end of text]
E-R diagram for registrar's office: 
- Course: (Cno, Cname, Ccredits, Syllabus, Prerequisites)
- Student: (Id, Name, Program)
- Instructor: (Id, Name, Department, Title)
- Enrollment: (Cno, Sno, Ccredit, Cyear, Csemester, Csection, Cinstructor, Ctimetable, Cclassroom)
- Grades: (Cno, Sno, Cgrade, Cyear, Csemester, Csection, Cinstructor, Ctimetable, Cclassroom)
Assumptions about mapping constraints: 
- Courses can be assigned to multiple instructors.
- Students can be enrolled in multiple courses.
- Grades can be awarded to multiple students in multiple courses. [end of text]
An E-R diagram for the database with exams as entities and a ternary relationship for course-offerings. Only one binary relationship exists between students and course-offerings, with only one relationship per student and course-offering pair. [end of text]
The textbook defines a model for storing team matches, including match details, player statistics, and individual player statistics. [end of text]
For all teams in a league, the data is gathered and analyzed to determine team performance, identify trends, and make informed decisions. [end of text]
Weak entity sets arise because they lack sufficient attributes to uniquely identify entities, making it difficult to establish relationships among them. [end of text]
Usefulness of databases is a fundamental concept in database management.
In a bookstore, entity sets include books, music cassettes, and compact disks. Music items can be present in either cassette or compact disk format, with differing prices. The E-R diagram can be extended to model the addition of music cassettes and compact disks, and the possibility of containing any combination of books, music cassettes, or compact disks in a shopping basket. Generalization can be used to model the effect on shopping baskets when a combination of items is added. [end of text]
Redundancy in databases can lead to data inconsistencies and decreased efficiency, making it a bad practice to avoid. [end of text]
In this database, the entity set exam could be modeled as the single entity set exam, with attributes course-name, section-number, room-number, and time. Alternatively, one or more additional entity sets could be deﬁned, along with relationship sets to replace some of the attributes of the exam entity set. An E-R diagram illustrating the use of all three additional entity sets listed would show the relationship between the exam entity set and the additional entity sets, and explain the application characteristics that would influence a decision to include or not to include each of the additional entity sets. [end of text]
In making the appropriate choice, consider criteria such as functionality, scalability, and ease of use. Three alternative E-R diagrams for the university registrar's office of Exercise 2.4 are shown below. Each has its merits, and I argue in favor of the one that best represents the registrar's office's needs.
1.
The graph is disconnected, as the schema structure is not connected.
The graph is acyclic, as there are no cycles in the data flow. [end of text]
The McGraw-Hill Companies, 2001, discusses the relative merits of two alternative representations for a ternary relationship: binary relationships and entity-relationship models. Entity-relationship models are more suitable for binary relationships as they provide a more intuitive representation of data. [end of text]
In Section 2.4.3, we described an E-R diagram with entities A, B, C, and R. We showed a simple instance of E, A, B, C, RA, RB, and RC that cannot correspond to any instance of A, B, C, and R. We then modified the E-R diagram to introduce constraints that will guarantee that any instance of E, A, B, C, RA, RB, and RC that satisfies the constraints will correspond to an instance of A, B, C, and R. We also modified the translation to handle total participation constraints on the ternary relationship. The above representation requires that we create a primary key attribute for E. Finally, we showed how to treat E as a weak entity set so that a primary key attribute is not required. [end of text]
The primary key attribute of an entity set can lead to redundancy if not managed properly, as it may not uniquely identify each entity. [end of text]
The entity-relationship model is the primary data model for relational databases. It represents entities (such as motorcycles, passenger cars, vans, and buses) and their relationships (e.g., owning, being owned by). The model is hierarchical, with entities at the top and relationships at the bottom. Attributes at each level should be selected based on their importance to the business and data integrity. The entity-relationship model is a fundamental concept in database design and is used in many databases. [end of text]
The system can automatically check constraints such as unique constraints, primary key constraints, and foreign key constraints. These constraints ensure data integrity and prevent data redundancy. [end of text]
Inheritance of attributes from higher-level entities, handling attribute conflicts when X and Y have the same name. [end of text]
and 2.17. Conceptual information: In this section, we delve into the concept of "data type" in programming, focusing on the differences between primitive and composite data types. We discuss how these types determine the type of data they can hold, the methods available to manipulate these types, and the differences between primitive and composite data types. We also explore the importance of data types in programming and how they can be used to create more efficient and effective code. The textbook emphasizes the importance of understanding data types in programming and how they can be used to create more efficient and effective code. [end of text]
The E-R database schema for a merged bank would have a single database, but there are several potential problems: the possibility that the two original banks have branches with the same name, the possibility that some customers are customers of both banks, and the possibility that some loan or account numbers were used at both banks. For each of these potential problems, there is indeed a potential for difficulties. To address these issues, we would need to merge the data from both banks, change the names of the branches, and update the loan and account numbers. This would require changes to the E-R database schema and the data, but the overall structure of the database would remain the same. [end of text]
The relational model provides a simple yet powerful way of representing data, serving as the primary data model for commercial data-processing applications. It is simple and easy for programmers to use, compared to earlier data models such as the network model or the hierarchical model. The relational algebra formsthe basis of the widely used SQL query language, while the tuple relational calculus and the domain relational calculus are declarative query languages based on mathematical logic. [end of text]
A relational database consists of a collection of tables, each of which is assigned a unique name. Each table has a structure similar to that presented in Chapter 2, where we represented E-R databases by tables. A row in a table represents a relationship among a set of values. Since a table is a collection of such relationships, there is a close correspondence between the concept of table and the mathematical concept of relation. [end of text]
The account relation is a subset of D1 × D2 × D3. [end of text]
The account relation in Figure 3.1 is a set of tuples with attributes account-number, branch-name, and balance. The order of tuples in a relation is irrelevant, as is the use of sorted or unsorted. The domain of all attributes is atomic. The concept of a relation schema corresponds to the programming-language notion of a variable, and the concept of a relation instance corresponds to the programming-language notion of a value of a variable. The domains of all attributes in a relation schema are atomic. The concept of a relation instance corresponds to the programming-language notion of a value of a variable. The domains of all attributes in a relation schema are atomic. The concept of a relation schema corresponds to the programming-language notion of a variable, and the concept of a relation instance corresponds to the programming-language notion of a value of a variable. The domains of all attributes in a relation schema are atomic. The concept of a relation schema corresponds to the programming-language notion of a variable, and the concept of a relation instance corresponds to the programming-language notion of a value of a variable. The domains of all attributes in a relation schema are atomic. The concept of a relation schema corresponds to the programming-language notion of a variable, and the concept of a relation instance corresponds to the programming-language notion of a value of a variable. The domains of all attributes in a relation schema are atomic. The concept of a relation schema corresponds to the programming-language notion of a variable, and the concept of a relation instance corresponds
In a database, the schema defines the logical design of the database, while instances are snapshots of the data at a specific time. The schema is represented by a relation schema, which consists of attributes and their domains. The value of a variable may change with time, while the contents of a relation instance may change with time as the relation is updated. [end of text]
In a real-world database, the customer-id uniquely identifies a customer. We need a relation to describe the association between customers and accounts. The relation schema to describe this association is Customer-schema = (customer-name, account-number). We include two additional relations to describe data about loans maintained in the various branches in the bank: customer-name, account-number. [end of text]
The banking enterprise described here serves as our primary example in this chapter and in subsequent ones. On occasion, we may need to introduce additional relation schemas to illustrate particular points. [end of text]
The notions of superkey, candidate key, and primary key are applicable to the relational model, and examples include {branch-customer-nameloan-numberAdamsL-16CurryL-93HayesL-15JacksonL-14JonesL-17SmithL-11SmithL-23WilliamsL-17Figure 3.7} and {branch-name, branch-city} are both superkeys. {branch-name, branch-city} is not a candidate key, but serves as a primary key. The attribute branch-city is not a superkey, as two branches in the same city may have different names. Let R be a relation schema. If we say that a subset K is a superkey for R, we restrict consideration to relations r(R) in which no two distinct tuples have the same values on all attributes in K. If t1 and t2 are in r and t1 ≠ t2, then t1[K] ≠ t2[K]. A relational database schema based on tables derived from an E-R schema can determine the primary key from the primary keys of the entity or relationship sets from which the schema is derived: Strong entity set, primary key of the entity; Weak entity set, primary key of the entity; strong entity set, primary key of the entity. [end of text]
The primary key of a relation consists of the union of the primary key of the strong entity set and the discriminator of the weak entity set. Relationship set. The union of the primary keys of the related entity sets becomes a superkey of the relation. If the relationship is many-to-many, this superkey is also the primary key. Section 2.4.2 describes how to determine the primary keys in other cases. Recall from Section 2.9.3 that no table is generated for relationship sets linking a weak entity set to the corresponding strong entity set. Combined tables. Recall from Section 2.9.3 that a binary many-to-one relationship set from A to B can be represented by a table consisting of the attributes of A and attributes (if any exist) of the relationship set. The primary key of the “many” entity set becomes the primary key of the relation (that is, if the relationship set is many to one from A to B, the primary key of A is the primary key of the relation). For one-to-one relationship sets, the relation is constructed like that for a many-to-one relationship set. However, we can choose either entity set’s primary key as the primary key of the relation, since both are candidate keys. Multivalued attributes. Recall from Section 2.9.5 that a multivalued attribute M is represented by a table consisting of the primary key of the entity set or relationship set of which M is an attribute plus
A database schema, along with primary key and foreign key dependencies, can be depicted by schema diagrams. Figure 3.9 shows the schema diagram for our banking enterprise. Each relation appears as a box, with the attributes listed in-side it and the relation name above it. If there are primary key attributes, a horizontalline crosses the box, with the primary key attributes listed above the line. Foreign keys are represented by arrows from the foreign key attributes of the referencing relation to the primary key of the referenced relation. E-R diagrams do not show foreign key attributes explicitly, whereas schema diagrams do. In particular, E-R diagrams do not show foreign key attributes explicitly, whereas schema diagrams show them explicitly. Many database systems provide design tools with a graphical user interface for creating schema diagrams. [end of text]
A query language is a language in which a user requests information from a data base. These languages are on a level higher than standard programming languages and can be categorized as procedural or non-procedural. Most commercial relational database systems offer a query language that includes both procedural and non-procedural approaches. We shall study the very widely used query language SQL in Chapter 4. Chapter 5 covers the query languages QBE and Datalog, the latter a query language that resembles Prolog. In this chapter, we examine "pure" languages: The relational algebra is procedural, whereas the tuple relational calculus and domain relational calculus are non-procedural. These query languages are terse and formal, lacking the "syntactic sugar" of commercial languages, but they illustrate the fundamental techniques for extracting data from the database. Although we shall be concerned with only queries initially, a complete data-manipulation language includes not only a query language, but also a language for database modification. Such languages include commands to insert and delete tuples, Silberschatz-Korth-Sudarshan: Database System Concepts, Fourth Edition. [end of text]
Databases allow modification of existing tuples, which is a crucial step in data management. [end of text]
The relational algebra is a procedural query language consisting of select, project, and rename operations. The fundamental operations include select, project, union, set difference, Cartesian product, and rename. These operations are binary and can be combined using connectives and not. The select operation selects tuples based on a predicate, while project returns a subset of attributes. Composition of relational operations is important, as the result of a relational algebra operation is of the same type as its inputs. [end of text]
The select, project, and rename operations are called unary operations because they operate on one relation. The other three operations operate on pairs of relations and are, therefore, called binary operations.3.2.1.1The Select Operation selects tuples that satisfy a given predicate. We use the lowercase Greek letter sigma (σ) to denote selection. The predicate appears as a subscript to σ. The argument relation is in parentheses after the σ. Thus, to select those tuples of the loan relation where the branch is "Perryridge," we write σbranch-name = "Perryridge" (loan). If the loan relation is as shown in Figure 3.6, then the relation that results from the preceding query is as shown in Figure 3.10. We can ﬁnd all tuples in which the amount lent is more than $1200 by writing σamount>1200 (loan). In general, we allow comparisons using =, ̸=, <, ≤, >, ≥ in the selection predicate. Furthermore, we can combine several predicates into a larger predicate by using connectives and (∧), or (∨), and not (¬). Thus, to ﬁnd those tuples pertaining to loans of more than $1200 made by the Perryridge branch, we write σbranch-name = "Perryridge" ∧amount>1200 (loan) and loan-number branch-name amount L-15 Perry
The select operation selects tuples that satisfy a given predicate. We use the lowercaseGreek letter sigma (σ) to denote selection. The predicate appears as a subscript to σ. The argument relation is in parentheses after the σ. Thus, to select those tuples of theloan relation where the branch is “Perryridge,” we writeσbranch-name = “Perryridge” (loan)If the loan relation is as shown in Figure 3.6, then the relation that results from thepreceding query is as shown in Figure 3.10.We can ﬁnd all tuples in which the amount lent is more than $1200 by writingσamount>1200 (loan)In general, we allow comparisons using =, ̸=, <, ≤, >, ≥ in the selection predicate. Furthermore, we can combine several predicates into a larger predicate by using theconnectives and (∧), or (∨), and not (¬). Thus, to ﬁnd those tuples pertaining to loans of more than $1200 made by the Perryridge branch, we writeσbranch-name = “Perryridge” ∧amount>1200 (loan)loan-numberbranch-nameamountL-15Perryridge1500L-16Perryridge1300Figure 3.10Result of σbranch-name = “Perryridge” (loan).Silberschatz−Korth
The project operation produces a relation with loan numbers and loan amounts, excluding branch names. The query lists these attributes as a subscript to the projection operation. [end of text]
The fact that the result of a relational operation is itself a relation is important. Con-sider the more complicated query “Find those customers who live in Harrison.” Wewrite:Πcustomer-name (σcustomer-city = “Harrison” (customer))Notice that, instead of giving the name of a relation as the argument of the projectionoperation, we give an expression that evaluates to a relation. [end of text]
The Union Operation involves finding the names of all customers who have either an account or a loan, while the Set Difference Operation allows finding customers with an account but not a loan. Both operations are valid for relational-algebra expressions, ensuring compatibility and eliminating duplicates. [end of text]
To find the names of all bank customers who have either an account or a loan, we need the union of the borrower and depositor relations. [end of text]
The set-difference operation allows finding tuples in one relation but not in another. It can be used to find customers with an account but not a loan. Set differences must be taken between compatible relations with the same arity and domains. [end of text]
The Cartesian-product operation combines information from two relations to create a new relation, allowing for the combination of in-formation from any two relations. [end of text]
The relation schema for r = borrower × loan includes customer-name, borrower.loan-number, loan.loan-number, loan.branch-name, loan.amount. The naming convention ensures distinct names for relations that share attributes. The naming schema for r = borrower × loan includes customer-name, borrower.loan-number, loan.loan-number, branch-name, amount. The naming convention avoids ambiguity by using distinct names for relations that are arguments of the Cartesian-product operation. The naming convention also avoids problems when using the result of a relational-algebra expression in a Cartesian product, as it requires a name for the relation. [end of text]
The query "Find the largest accountbalance in the bank" results in the temporary relation consisting of balances not exceeding the largest balance, and the result is the largest account balance. [end of text]
The rename operator ρ allows us to give relational-algebra expressions names, making them easier to refer to. It can be used to rename attributes in a relation and return the same relation under a new name. The rename operation can be applied to a relation to get the same result. [end of text]
The textbook summarizes the relational algebra operations and their applications, with a focus on the fundamental operations and their simplifications. It also mentions the use of positional notation for attributes and the importance of using a relational algebra expression in the context of relational models. [end of text]
The relational algebra allows for the construction of expressions by combining relations and constants, and by using subexpressions and predicates. [end of text]
The fundamental operations of the relational algebra are sufficient to express any relational-algebra query. However, if we restrict ourselves to just the fundamental operations, certain common queries are lengthy to express. Therefore, we deﬁne additional operations that do not add any power to the algebra, but simplify common queries. For each new operation, we give an equivalent expression that uses only the fundamental operations. In Section 3.3, we introduce operations that extend the power of the relational algebra, to handle null and aggregate values. [end of text]
The natural join is a binary operation that combines selections from two relations into one, performing a selection on attributes that appear in both schemas and removing duplicates. It is denoted by the "join" symbol and is used to combine Cartesian products into one operation. [end of text]
The set intersection (∩) operation is used to find customers who have both a loan and an account. It is more convenient to write r ∩s than to use −(r −s). [end of text]
The natural join is a binary operation that combines certain selections and a Cartesian product into one operation. It is denoted by the "join" symbol and forms a Cartesian product of its two arguments, performs a selection forcing equality on attributes that appear in both relation schemas, and removes duplicate attributes. [end of text]
The textbook discusses the use of the natural join and division operations in database queries, including the ability to combine selection and Cartesian products into a single operation. It also explains the division operation, which is suited to queries involving "all" or "for all." The division operation is an extension of the natural join operation, allowing for the combination of a selection and a Cartesian product into a single operation. The textbook provides examples and relations to illustrate these concepts. [end of text]
The division operation is suitable for queries that include the phrase "for all," and it can be used to find customers with an account at all branches located in Brooklyn. The result relation for this expression appears in Figure 3.23. Silberschatz-Korth-Sudarshan: Database System Concepts, Fourth Edition. I. Data Models 3. Relational Model 110 © The McGraw-Hill Companies, 2001 110 Chapter 3 Relational Model branch-name Brighton Downtown Figure 3.23 Result of Πbranch-name(σbranch-city = “Brooklyn” (branch)). Silberschatz-Korth-Sudarshan: Database System Concepts, Fourth Edition. I. Data Models 3. Relational Model 110 © The McGraw-Hill Companies, 2001 110 Chapter 3 Relational Model branch-name Brighton Downtown Figure 3.23 Result of Πbranch-name(σbranch-city = “Brooklyn” (branch)). [end of text]
To see that this expression is true, we observe that ΠR−S (r) gives us all tuples t that satisfy the division condition. The expression on the right side of the set difference operator ΠR−S ((ΠR−S (r) × s) −ΠR−S,S(r))serves to eliminate those tuples that fail to satisfy the division condition. Let us see how it does so. Consider ΠR−S (r) × s. This relation on schema R, and pairs every tuple in ΠR−S (r) with every tuple in s. The expression ΠR−S,S(r) merely reorders the attributes of r. Therefore, (ΠR−S (r) × s) −ΠR−S,S(r) gives us those pairs of tuples from ΠR−S (r) and s that do not appear in r. If a tuple tj is in ΠR−S ((ΠR−S (r) × s) −ΠR−S,S(r))s, then there is some tuple ts in s that does not combine with tuple tj to form a tuple in r. Thus, tj holds a value for attributes R −S that does not appear in r ÷ s. It is these values that we eliminate from ΠR−S (r). [end of text]
It is convenient to write relational-algebra expressions by assigning parts of a temporary relation variable. The assignment operation, denoted by ←, works like assignment in a programming language. To illustrate, consider the division operation in Section 3.2.3.3. We can write r ÷ s as temp1 ←ΠR−S (r)temp2 ←ΠR−S ((temp1 × s) −ΠR−S,S(r))result = temp1 −temp2. The evaluation of an assignment does not result in any relation being displayed to the user. Rather, the result of the expression to the right of the ← is assigned to the relation variable on the left of the ←. This relation variable may be used in subsequent expressions. The assignment operation is a convenient way to express complex queries. Notably, it does not provide any additional power to the algebra. [end of text]
The basic relational-algebra operations have been extended in several ways. A simple extension is to allow arithmetic operations as part of projection. An important extension is to allow aggregate operations such as computing the sum of the elements of a relational model. Another important extension is the outer-join operation, which allows relational-algebra expressions to deal with null values, which model missing information. [end of text]
The generalized-projection operation extends the projection operation by allowing arithmetic functions to be used in the projection list. It has the form ΠF1, F2, ..., Fn(E), where E is any relational-algebra expression, and each F1, F2, ..., Fn is an arithmetic expression involving constants and attributes in the schema of E. As a special case, the arithmetic expression may be simply an attribute or a constant. The rename operation can be combined with generalized projection to give attributes a name. The second attribute of the generalized projection has been given the name credit-available. [end of text]
Aggregate functions return a single value from a collection of values. For example, summing a collection of numbers. [end of text]
The aggregate function Gsum calculates the sum of salaries for each branch, while Gcount-distinct counts the number of employees working in each branch. Both operations return the same result for the pt-works relation, which contains the branch names and salaries. [end of text]
The expression `branch-nameGsum(salary),max(salary)(pt-works)` generates a single relation with all information about full-time employees, including their salaries and positions. The `Gsum` operation partitions the result into groups based on attributes, and the `max` operation finds the maximum salary for each group. The `branch-name` operation is used to identify the branch name, and the `Gsum` operation is used to find the maximum salary for each group. The `max` operation is denoted to avoid losing information about Smith and Gates. The `left outer join`, `right outer join`, and `full outer join` operations are used to compute the join and add extra tuples to the result of the join. [end of text]
The outer-join operation extends the join operation to deal with missing information, allowing for the generation of a single relation with all relevant data about full-time employees. Three forms of the operation—left outer join, right outer join, and full outer join—are available, each computing the join and adding extra tuples to the result. [end of text]
The textbook summarizes the relational model and its operations, including the left and right outer joins, and discusses null values and their handling in relational algebra. It also outlines how different relational operations deal with null values, particularly in natural and join joins. [end of text]
In relational algebra, null values are handled differently depending on the operation. Selection returns true or unknown, join returns true or unknown, and natural join returns true or unknown. Null values can cause ambiguity in comparisons and operations. It is recommended to avoid null values in operations and comparisons. [end of text]
The projection operation treats nulls just like any other value when eliminating duplicates. It treats two tuples with the same values in all fields as duplicates even if some have null values. The union, intersection, and difference operations treat nulls just as in the projection operation. The generalized projection treats nulls as if they were in the projection operation. Aggregate operations treat nulls just as in projection. Outer join operations behave like join operations, except if tuples do not occur in the join result. [end of text]
In database management, we can add, remove, or change information by using the assignment operation. We express database modifications by using the assignment operation. We make assignments to actual database relations by using the same notation as described in Section 3.2.3 for assignment.3.4.1Deletion We express a delete request in much the same way as a query. However, instead of displaying tuples to the user, we remove the selected tuples from the database. We can delete only whole tuples; we cannot delete values on only particular attributes. In relational algebra, a deletion is expressed by r ←r −E where r is a relation and E is a relational-algebra query. We can insert data into a relation by either specifying a tuple to be inserted or writing a query whose result is a set of tuples to be inserted. We express the insertion of a single tuple by letting E be a constant relation containing one tuple. We can insert tuples on the basis of the result of a query. We can provide a gift for all loan customers of the Perryridge branch with a new $200 savings account by writing1 ←(σbranch-name = “Perryridge” (borrower loan))r2 ←Πloan-number, branch-name (r1)account ←account ∪(r2 × {(200)})depositor ←depositor ∪Πcustomer-name, loan-number (r1). [end of text]
In relational algebra, a deletion is represented by r ←r −E, where r is a relation and E is a relational-algebra query. This allows for the deletion of entire tuples from a database, whereas in relational models, values on specific attributes can be deleted. [end of text]
To insert data into a relation, we either specify a tuple to be inserted or write a query whose result is a set of tuples to be inserted. The attribute values for inserted tuples must be members of the attribute's domain. Similarly, tuples inserted must be of the correct arity. The relational algebra expresses an insertion by r ←r ∪E where r is a relation and E is a relational-algebra expression. We express the insertion of a single tuple by letting E be a constant relation containing one tuple. Suppose that we wish to insert the fact that Smith has $1200 in account A-973 at the Perryridge branch. We writeaccount ←account ∪{(A-973, “Perryridge”, 1200)}. Similarly, tuples inserted must be of the correct arity. The relational algebra expresses an insertion by r ←r ∪E where r is a relation and E is a relational-algebra expression. We express the insertion of a single tuple by letting E be a constant relation containing one tuple. Suppose that we wish to insert the fact that Smith has $1200 in account A-973 at the Perryridge branch. We writeaccount ←account ∪{(A-973, “Perryridge”, 1200)}. [end of text]
Updating in certain situations, we can use the generalized-projection operator to change a value in a tuple without changing all values in the tuple. We can update the account number, branch name, and balance of a new account, and update the account number and branch name of an existing account. [end of text]
In certain situations, we can change a value in a tuple without changing all values. We can use the generalized-projection operator to do this task. To select some tuples and update only them, we can use the following expression. [end of text]
In our examples up to this point, we have operated at the logical-model level. That is, we have assumed that the relations in the collection we are given are the actual relations stored in the database. It is not desirable for all users to see the entire logical model. Security considerations may require that certain data be hidden from users. Consider a person who needs to know a customer's loan number and branch name, but has no need to see the loan amount. This person should see a relation described in the relational algebra by Πcustomer-name, loan-number, branch-name (borrower loan). Apart from security concerns, we may wish to create a personalized collection of relations that is better matched to a certain user's intuition than the logical model. Silberschatz-Korth-Sudarshan: Database System Concepts, Fourth Edition.
To define a view, name it, and use it to refer to the virtual relation it generates. Views can be updated without affecting the view itself. [end of text]
Views are stored as stored query expressions rather than the result of evaluation of relational-algebra expressions. Whenever a view relation appears in a query, it is replaced by the stored query expression. Views are maintained up to date whenever the actual relations used in the view deﬁnition change. Materialized views are created to keep view relations up to date. Applications that use views frequently benefit from their use, while those demanding fast response to certain view-based queries may benefit from materialized views. The beneﬁts to queries from the materialization of a view must be weighed against the storage costs and the added overhead for updates. [end of text]
Although views can be useful for queries, updates, and deletions, they present serious problems if we express them. To illustrate, consider a clerk needing to see all loan data except loan-amount, and insert a tuple into the relation loan. The insertion must be represented by an insertion into the relation loan, since it is the actual relation from which the database system constructs the view. Another approach is to reject the insertion and return an error message. [end of text]
In Section 3.5.1, we discussed view relations and their appearance in any place, except for restrictions on the use of views in update operations. [end of text]
View expansion is a technique used to derive the meaning of views by replacing recursive view definitions with their deﬁnitions. The procedure assumes that view deﬁnitions are not recursive and involves repeatedly replacing a view relation by its deﬁnition until no more view relations are present. This loop terminates, resulting in an expression that does not contain any view relations. [end of text]
A relational-algebra expression is a sequence of procedures that generate the answer to a query. The tuple relational calculus is an anonprocedural query language that describes the desired information without giving a specific procedure for obtaining that information. Queries in the tuple relational calculus are expressed as {t | P(t)} where t ∈ loan and t[amount] > 1200. Following earlier notation, we use t[A] to denote the value of tuple t on attribute A, and we use ∈r to denote that tuple t is in relation r. Before we give a formal deﬁnition of the tuple relational calculus, we return to some of the queries for which we wrote relational-algebra expressions in Section 3.2.3.6.1. Example Queries Say that we want to ﬁnd the branch-name, loan-number, and amount for loans of over$1200: {t | t ∈ loan ∧ t[amount] > 1200} Suppose that we want only the loan-number attribute, rather than all attributes of the loan relation. To write this query in the tuple relational calculus, we need to write an expression for a relation on the schema (loan-number). We need those tuples on (loan-number) such that there is a tuple in loan with the amount attribute > 1200. To express this request, we need the construct “there exists” from mathematical logic. The notation ∃t ∈r (Q
The textbook discusses the use of tuple relational calculus to query a database, focusing on finding loans with an amount greater than $1200 and retrieving the loan number for each loan. It explains the syntax for expressing conditions using "there exists" and the use of tuple variables on only the loan-number attribute. The text also covers the complex query "Find the names of all customers who have a loan from the Perryridge branch," which requires two "there exists" clauses connected by the "and" operator. [end of text]
The set of all customer-name tuples for which at least one of the following holds:• The customer-name appears in some tuple of the borrower relation as a borrower from the bank.• The customer-name appears in some tuple of the depositor relation as a depositor of the bank. [end of text]
A tuple-relational-calculus expression is of the form {t | P(t)}, where P is a formula. Tuple variables may appear in a formula, and a tuple variable is a free variable unless it is quantified by a ∃ or ∀. Bound variables are free variables unless they are quantified by a ∃ or ∀. [end of text]
The tuple relational calculus is built up from atoms, with atoms being formulas that contain free tuple variables and relations. Formulae can be built from atoms using rules such as those involving comparison operators and domain constraints. Safety of expressions is addressed by defining the domain of a tuple relational formula, which includes values from the relation and those appearing in a tuple of the relation. The tuple relational calculus is equivalent in expressive power to the basic relational algebra with the operators ∪, −, ×, σ, and ρ, but without extended relational operators such as generalized projection G and outer-join operations. The tuple relational calculus does not have an equivalent of the aggregate operation but can be extended to support aggregation. [end of text]
A tuple-relational-calculus expression may generate an inﬁnite relation, and the domain of a tuple relational formula, P, is the set of all values referenced by P. Safe expressions are those for which all values appearing in the result are from the domain of P, while the expression {t |¬ (t∈loan)} is not safe because it includes values not in loan. [end of text]
The tuple relational calculus restricted to safe expressions is equivalent to the basic relational algebra with the operators ∪, −, ×, σ, and ρ, without extended relational operators such as generalized projection G and outer-join operations. For relational-algebra expressions, there exists an equivalent in the tuple relational calculus, and for tuple-relational-calculus expressions, an equivalent relational algebra expression exists. The proof is not included in the exercises. The tuple relational calculus does not have an equivalent of the aggregate operation, but it can be extended to support aggregation. Extending the tuple relational calculus to handle arithmetic expressions is straightforward. [end of text]
A second form of relational calculus called domain relational calculus uses domain variables that take values from an attributes domain, rather than entire tuples. It is closely related to the tuplerelational calculus, and serves as the theoretical basis for the QBELanguage and SQL language. [end of text]
An expression in the domain relational calculus is of the form {< x1, x2, . . . , xn > | P(x1, x2, . . . , xn)} where x1, x2, . . . , xn represent domain variables. P represents a formula composed of atoms, as was the case in the tuple relational calculus. An atom in the domain relational calculus has one of the following forms: < x1, x2, . . . , xn > ∈r, where r is a relation on n attributes and x1, x2, . . . , xn are domain variables or domain constants. Silberschatz-Korth-Sudarshan: Database System Concepts, Fourth Edition I. Data Models 3. Relational Model 131 © The McGraw-Hill Companies, 2001 [end of text]
In this textbook, we learned about domain-relational-calculus queries and how to build them from atoms using rules such as ∃a, b, c (P(a, b, c)). We also saw examples of expressions and queries involving tuples and branches. Safety is important in tuple-relational-calculus, as it allows values in the result that are not in the domain of the expression. The textbook also covered safety in domain-relational-calculus, as it can generate an infinite relation, and safety is crucial in domain-relational-calculus expressions. [end of text]
Find the loan number, branch name, and amount for loans of over $1200: < l, b, a > | < l, b, a > ∈loan ∧a > 1200>
Find all loan numbers for loans with an amount greater than $1200: < l > | ∃b, a (< l, b, a > ∈loan ∧a > 1200) [end of text]
Safety in tuple relational calculus and domain relational calculus is achieved by ensuring that expressions do not generate an infinite relation. For domain relational calculus, safety also concerns the form of formulae within "there exists" and "for all" clauses. Consider an expression like {< x > | ∃y (< x, y > ∈ r) ∧ ∃z (¬( < x, z > ∈ r) ∧ P(x, z))}. Testing the first part of the formula, ∃y (< x, y > ∈ r), is possible by considering only the values in r. However, testing the second part, ∃z (¬( < x, z > ∈ r) ∧ P(x, z)), requires values not in r. Since all relations are finite, an inﬁnite number of values do not appear in r. Therefore, it is not possible in general to test the second part of the formula. [end of text]
The domain relational calculus is equivalent to the tuple relational calculus with safety, and both are equivalent to the basic relational algebra. [end of text]
The domain relational calculus is equivalent to the tuple relational calculus restricted to safe expressions, and all three are equivalent to the basic relational algebra. [end of text]
The relational data model is based on tables and provides operations like SELECT, INSERT, DELETE, and UPDATE. It uses the relational algebra to express these operations. Databases can be modified by insertion, deletion, or update of tuples. Views are virtual relations defined by query expressions. Views can be materialized to simplify queries. The relational algebra and relational calculus are procedural languages with syntactic sugar. [end of text]
The textbook describes a database system with data about each class, including instructors, students, time and place of meetings, grades, and a relational model. It also explains the concept of an E-R diagram. [end of text]
Illustrate your answer by referring to your solution to Exercise 3.1. [end of text]
In the relational model, primary keys help represent relationships by uniquely identifying each entity in a set. This allows for efficient data management and querying, as each entity can be uniquely identified by its primary key, facilitating the creation of relationships between entities. The primary key ensures that no two entities in the set have the same value, making it possible to establish relationships between them. This is crucial for maintaining data integrity and enabling efficient data management. [end of text]
In the relational algebra, we can express each query as follows:
a. Find the names of all employees who work for First Bank Corporation: <NAME>
b. Find the names and cities of residence of all employees who work for FirstBank Corporation: <NAME>
c. Find the names, street address, and cities of residence of all employees who work for First Bank Corporation and earn more than $10,000 per annum: <NAME>
d. Find the names of all employees in this database who live in the same city as the company for which they work: <NAME>
e. Find the names of all employees who live in the same city and on the same street as do their managers: <NAME>
f. Find the names of all employees in this database who do not work for FirstBank Corporation: <NAME>
g. Find the names of all employees who earn more than every employee of Small Bank Corporation: <NAME>
h. Assume the companies may be located in several cities. Find all companies located in every city in which Small Bank Corporation is located: <NAME> [end of text]
The query is now: SELECT person-name, city FROM employee WHERE person-name = 'Jackson' OR person-name = 'Jackson'
The theta join operation allows for tuples from the left, right, or both relations to be preserved in the result, even if they are not present in the original relations. This is achieved by extending the theta join operation to include tuples from the left, right, or both relations, ensuring that all relevant information is retained in the final result. [end of text]
To modify the database, we need to update the salary of Jones and First Bank employees. For managers, we need to increase their salaries based on their salary level. For Small Bank employees, we need to remove tuples from the works relation. [end of text]
held by more than two customers in the following ways: using an aggregate function, without using any aggregate functions. [end of text]
The textbook summarizes the following queries:
1. Find the company with the most employees.
2. Find the company with the smallest payroll.
3. Find those companies whose employees earn a higher salary, on average,than the average salary at First Bank Corporation. [end of text]
views are the various perspectives or viewpoints that can be taken on a subject or topic. [end of text]
In the tuple relational calculus, the expressions equivalent to the given statements are:
1. ΠA(r)
2. σB = 17 (r)
3. r × s
4. ΠA,F (σC = D(r × s)) [end of text]
In the domain relational calculus, the expressions equivalent to the given relations are:
a. ΠA(r1)
b. σB = 17 (r1)
c. r1 ∪r2
d. r1 ∩r2
e. r1 −r2
f. ΠA,B(r1) ΠB,C(r2) [end of text]
Calculus is the branch of mathematics that focuses on the study of rates of change and accumulation. It includes topics such as limits, derivatives, and integrals, which are essential for understanding how quantities change over time or space. Calculus is widely used in physics, engineering, economics, and other fields to model and analyze complex systems. [end of text]
Relational algebra expressions equivalent to the following domain-relational-calculus expressions:
a. {< a > | ∃b (< a, b > ∈r ∧b = 17)}
b. {< a, b, c > | < a, b > ∈r ∧< a, c > ∈s}
c. {< a > | ∃b (< a, b > ∈r) ∨∀c (∃d (< d, c > ∈s) ⇒< a, c > ∈s)}
d. {< a >| ∃c (< a, c > ∈s ∧∃b1, b2 (< a, b1 > ∈r ∧< c, b2 >∈r ∧b1 > b2))} [end of text]
null = (False, False, False)
null = (False, True, False)
null = (True, False, False)
null = (True, True, False)
END>>> [end of text]
SQL is a user-friendly query language that combines relational algebra and relational calculus constructs. It provides a concise notation for representing queries, but can be more powerful than just querying a database. It deﬁnes the structure of data, modifies it, and specifies security constraints. The fundamental constructs and concepts of SQL are presented in this chapter. Individual implementations may differ in details or support only a subset of the full language. [end of text]
IBM developed the original version of SQL at its San Jose Research Laboratory, implemented it as part of the System R project in the early 1970s, and published an SQL standard in 1986. ANSI and ISO published an extended standard for SQL in 1989, and the next version was SQL:1999. The SQL:1999 standard is a superset of the SQL-92 standard, with more detailed coverage in Chapter 9. Many database systems support some of the new constructs in SQL:1999, although currently no database system supports all the new constructs. [end of text]
In this chapter, hyphens are used for schema, relations, and attributes in SQL, but in actual systems, hyphens are not valid parts of names. A simple translation of these names to valid SQL names is to replace hyphens with underscores. For instance, "branch-name" becomes "branch-name". [end of text]
SQL allows the use of null values to indicate that the value either is unknown or does not exist. It allows a user to specify which attributes cannot be assigned null values, as we shall discuss in Section 4.11. The basic structure of an SQL expression consists of three clauses: select, from, and where. The select clause corresponds to the projection operation of the relational algebra. The from clause corresponds to the Cartesian-product operation of the relational algebra. The where clause corresponds to the selection predicate of the relational algebra. The term select has different meanings in SQL than in the relational algebra. We emphasize the different interpretations here to minimize potential confusion. The Cartesian product of the relations named in the from clause performs a relational-algebra selection using the where clause predicate. The SQL query is equivalent to the relational-algebra expression ΠA1, A2,...,An(σP (r1 × r2 × · · · × rm)). If the where clause is omitted, the predicate P is true. However, unlike the result of the relational-algebra expression, the result of the SQL query may contain multiple copies of some tuples; we shall return to this issue in Section 4.2.8. The select Clause The result of an SQL query is, of course, a relation. Let us consider a simple query using our banking example, “Find the names of all branches in the loan relation”: select branch-namefrom loan The result is a relation consisting of a single attribute with
The result of an SQL query is a relation, and SQL uses sets as the basis for relations. Duplicate tuples are not allowed in relations. SQL allows duplicates in results of SQL expressions, but not in the results of queries. The keyword distinct is used to eliminate duplicates. The keyword all is used to specify that duplicates are not removed. The asterisk symbol “*” can be used to denote “all attributes.” The select clause may contain arithmetic expressions involving constants or attributes of tuples. [end of text]
SQL provides special data types, such as date types, and allows arithmetic operations on these types. It uses the logical connectives and, or, and not—rather than the mathematicalsymbols ∧, ∨, and ¬ —in the where clause. The operands of the logical connectives can be expressions involving the comparison operators <, <=, >, >=, =, and <. SQL allows using comparison operators to compare strings and arithmetic expressions, as well as special types, such as date types. It includes a between comparison operator to simplify where clauses that specify that a value be less than or equal to some value and greater than or equal to some other value. If we wish to find the loan number of those loans with loan amounts between $90,000 and $100,000, we can use the between comparison to write the select statement. [end of text]
SQL uses logical connectives and, or, and not to write queries, allowing comparisons between strings, arithmetic expressions, and special types. It supports between and not between comparisons to find loan numbers with loan amounts between $90,000 and $100,000. [end of text]
The from clause in SQL defines a Cartesian product of relations, allowing selection, projection, and natural join expressions. For the query "For all customers who have a loan from the bank, find their names, loan numbers and loan amount," the SQL expression is Silberschatz−Korth−Sudarshan: Database System Concepts, Fourth Edition II. Relational Databases 4. SQL 146 © The McGraw−Hill Companies, 2001140 Chapter 4 SQL select customer-name, borrower.loan-number, amount from borrower, loan where borrower.loan-number = loan.loan-number Notice that SQL uses the notation relation-name.attribute-name, as does the relationalalgebra, to avoid ambiguity in cases where an attribute appears in the schema of more than one relation. Towrite this query, we need to state two constraints in the where clause, connected by the logical connective and:select customer-name, borrower.loan-number, amount from borrower, loan where borrower.loan-number = loan.loan-number and branch-name = ’Perryridge’ [end of text]
SQL provides a mechanism for renaming both relations and attributes. It uses the as clause, taking the form:old-name as new-name. The as clause can appear in both the select and from clauses. Consider again the query that we used earlier:select customer-name, borrower.loan-number, amountfrom borrower, loanwhere borrower.loan-number = loan. The result of this query is a relation with the following attributes:customer-name, loan-number, amount. The names of the attributes in the result are derived from the names of the attributes in the relations in the from clause. However, if two relations in the from clause have attributes with the same name, an attribute name is duplicated in the result. Also, if we used an arithmetic expression in the select clause, the resultant attribute does not have a name. Lastly, Silberschatz-Korth-Sudarshan: Database System Concepts, Fourth Edition II. Relational Databases, Fourth Edition II. Relational Databases, Fourth Edition II. Relational Databases, Fourth Edition II. Relational Databases, Fourth Edition II. Relational Databases, Fourth Edition II. Relational Databases, Fourth Edition II. Relational Databases, Fourth Edition II. Relational Databases, Fourth Edition II. Relational Databases, Fourth Edition II. Relational Databases, Fourth Edition II. Relational Databases, Fourth Edition II. Relational Databases, Fourth Edition II. Relational Databases, Fourth Edition II
SQL provides a way to rename attributes in results, and it is done using tuple variables. Tuple variables are associated with a particular relation and are defined in the from clause using the as clause. SQL allows for comparison of two tuples in the same relation using the like operation. It also provides functions on character strings, such as concatenation, extraction, length, conversion, and more. [end of text]
The as clause is crucial in SQL for deﬁning tuple variables, which are essential in relational calculus. Tuple variables are associated with relations through the as clause, and they are defined in the from clause by placing them after the relation's name. The syntax is as follows: select customer-name, T.loan-number, S.amountfrom borrower as T, loan as Swhere T.loan-number = S.loan-number. Tuple variables are most useful for comparing two tuples in the same relation. The rename operation in relational algebra can be used to compare tuples, but the notation (v1, v2, . . . , vn) is more suitable for tuples of arbitrary arity. The comparison operators can be used on tuples, and the ordering is deﬁned lexicographically. [end of text]
SQL specifies strings by enclosing them in single quotes, like ’Perryridge’. Patterns are case sensitive, and special characters like % and \ are used to indicate they should be treated as normal characters. SQL supports functions like concatenation, extraction, and length calculation. [end of text]
SQL offers control over the order of tuples in a relation, allowing sorting by customer-name and loan-number in descending order. To list customers with loans at the Perryridge branch, use the SELECT DISTINCT customer-name, loan-number, and branch-name from borrower, loan, and branch where borrower.loan-number = loan.loan-number and branch-name = 'Perryridge' order by customer-name. SQL can also perform sorting on multiple attributes, such as amount. To list loans in descending order of amount, use the SELECT * from loan order by amount desc, loan-number asc. To fulfill an order request, SQL performs sorting only when necessary, using multiset versions of the relational operators. [end of text]
SQL allows controlling the order of tuples in a relation. The order by clause orders tuples in ascending or descending order. To list customers by loan amount in descending order, use the ORDER BY clause with DESC for descending or ASC for ascending. [end of text]
SQL provides a way to determine the number of copies of each tuple in a result by using multiset versions of the relational operators. Given multiset relations r1 and r2, the number of copies of tuple t1 in σθ(r1) is c1, and in ΠA(r1) is c1 ∗c2. The result of an SQL query is equivalent to the relational-algebra expression ΠA1, A2,...,An(σP (r1 × r2 × · · · × rm)) using the multiset versions of the relational operators σ, Π, and ×. [end of text]
The SQL operations union, intersect, and except operate on relations and correspond to the relational-algebra operations ∪, ∩, and −. Like union, intersection, and setdifference in relational algebra, the relations participating in the operations must be compatible; that is, they must have the same set of attributes. Let us demonstrate how several of the example queries that we considered in Chapter 3 can be written in SQL. We shall now construct queries involving the union, intersect, and except operations of two sets: the set of all customers who have an account at the bank, which can be derived byselect customer-namefrom depositor and the set of customers who have a loan at the bank, which can be derived byselect customer-namefrom borrower. The result of the preceding queries is the set of all customers who have a loan, an account, or both at the bank. [end of text]
To find all customers having a loan, an account, or both at the bank, we write (select customer-name from depositor union select customer-name from borrower). SQL151 © The McGraw-Hill Companies, 2001. [end of text]
The union operation eliminates duplicates, while the intersect operation finds customers with both loans and accounts. The except operation eliminates duplicates by finding customers with loans but no accounts. [end of text]
To find all customers who have both a loan and an account at the bank, we write `select distinct customer-namefrom depositor` intersect `select distinct customer-namefrom borrower`. This eliminates duplicates and retains all customers with loans and accounts. If we want to retain all duplicates, we can write `intersect all` in place of `intersect all`. [end of text]
To find all customers who have an account but no loan at the bank, write the SQL query: select distinct customer-namefrom depositor except all(select customer-namefrom borrower). This will eliminate duplicates and return all customers with an account but no loan. [end of text]
Aggregate functions are used to calculate averages, minimums, maximums, totals, and counts of a collection of values. SQL offers five built-in aggregate functions: avg, min, max, sum, and count. These functions operate on collections of numeric values, but other operators can operate on collections of nonnumeric data types. For example, the query "Find the average account balance at the Perryridge branch" can be written as select avg (balance) from account where branch-name = 'Perryridge'. [end of text]
The result of the query is a relation with a single attribute, containing a single tuple with a numerical value corresponding to the average balance at the Perryridgebranch. Optionally, we can give a name to the attribute of the result relation by using the as clause. There are circumstances where we would like to apply the aggregate function not only to a single set of tuples, but also to a group of sets of tuples; we specify this wish in the group by clause. The attribute or attributes given in the group by clause are used to form groups. Tuples with the same value on all attributes in the group by clause are placed in one group.
SQL allows null values to indicate absence of information about an attribute. Null values can be used in predicates to test for null values. NULL values in arithmetic and comparison operations cause complications. SQL handles null values in the relational algebra, but not in arithmetic or comparison operations. [end of text]
SQL treats unknown results of comparisons as null. Boolean operations like and, or, and not extend to unknown values. Null values complicate aggregate operations. Aggregates ignore nulls according to a rule, and count operations return null when empty. [end of text]
SQL provides a mechanism for nesting subqueries. Subqueries are select-from-where expressions nested within another query. Common use: testing set membership, making set comparisons, and determining set cardinality. SQL allows testing set membership using in connective and not in connective. SQL also allows testing set membership in an arbitrary relation. SQL provides a way to write the same query in multiple ways. This flexibility is beneficial for users to think about queries in natural ways. [end of text]
SQL allows testing for membership in an arbitrary relation. It can be used to find customers with both an account and a loan at the Perryridge branch. [end of text]
The textbook summarizes the concepts of the not in construct, set comparison, and test for empty relations in a concise manner. [end of text]
SQL allows < some, <= some, >= some, = some, and <> some comparisons. As an exercise, verify that <> all is identical to not in. The keyword any is synonymous to some in SQL. Early versions of SQL allowed only any. Later versions added the alternative some to avoid the linguistic ambiguity of the word any in English. [end of text]
The textbook explains the SQL feature for testing subqueries and the use of the exists and not exists constructs to simulate set containment. [end of text]
The SQL no duplicate tuples feature allows testing whether a subquery contains duplicate tuples in its result. The unique construct returns true if the argument subquery contains duplicate tuples. [end of text]
The unique construct in SQL returns true if a subquery contains Silberschatz-Korth-Sudarshan: Database System Concepts, Fourth EditionII. Relational Databases4. SQL160© The McGraw-Hill Companies, 2001154Chapter 4SQLno duplicate tuples. Using the unique construct, we can write the query “Find all customers who have at most one account at the Perryridge branch” as follows:select T.customer-namefrom depositor as Twhere unique (select R.customer-namefrom account, depositor as Rwhere T.customer-name = R.customer-name andR.account-number = account.account-number andaccount.branch-name = ’Perryridge’) To test for the existence of duplicate tuples in a subquery, use the notunique construct. [end of text]
A view in SQL is defined by a name and a query that computes the view. The form of the create view command is create view v as <query expression> where <query expression> is any legal query expression. The view name is represented by v. The notation used for view deﬁnition in the relational algebra is based on that of SQL. As an example, consider a view consisting of branch names and the names of customers who have either an account or a loan at that branch, and the view is called all-customer. The view is defined as follows:Silberschatz−Korth−Sudarshan: Database System Concepts, Fourth EditionII. Relational Databases4. SQL161© The McGraw−Hill Companies, 2001 [end of text]
The textbook describes creating a view to aggregate customer information across different branches. The view `branch-total-loan` is created by combining `branch-name` and `total-loan` from two tables: `depositor` and `account`, and `borrower` and `loan` tables. The view gives for each branch the sum of all loans. The view name `all-customer` is used to find customers of the Perryridge branch. [end of text]
Complex queries are often hard or impossible to write as a single SQL block or a union/intersection/difference of SQL blocks. Derived relations and the with clause are two ways of composing multiple SQL blocks to express complex queries. SQL allows a subquery expression to be used in the from clause, but we must give the result relation a name and rename attributes. Derived relations allow subqueries in the from clause, but we can rewrite the query without using the having clause. A with clause provides a temporary view with a defined deﬁnition that remains in the database until a command drop view is executed. [end of text]
SQL allows subqueries in the from clause. Subqueries can be named and attributes can be renamed using the as clause. For example, consider a subquery that calculates the average balance of branches where the average balance is greater than $1200. The subquery result is named branch-avg, with attributes branch-name and avg-balance. The subquery can then be used in a WHERE clause to find the maximum balance across all branches. [end of text]
Breaking complex queries into smaller views and using temporary views for temporary data can make them easier to understand and manage. The with clause provides a way to define a temporary view with access to the query's view definition. [end of text]
The with clause in SQL, introduced in SQL:1999, is currently supported only by some data bases. It makes the query logic clearer and permits a view deﬁnition to be used in multiple places within a query. [end of text]
SQL is used to delete tuples from a database. The delete statement first finds all tuples in a relation for which a predicate is true and then deletes them. The where clause can be omitted, in which case all tuples are deleted. SQL can delete from multiple relations at once, but only one relation can be deleted at a time. Deleting from one relation at a time is important if the average balance changes. The delete statement tests each tuple in a relation to check for a balance below the average, and deletes all tuples that fail the test. Performing all tests before performing any deletion is important if some tuples are deleted before other tuples have been tested. [end of text]
SQL delete request: delete from account where branch-name = 'Perryridge'; delete from loan where amount between 1300 and 1500; delete from account where balance < (select avg (balance)from account); [end of text]
In SQL, we can update values in a tuple without changing all values in the tuple. For example, if annual interest payments are being made, we can update the balance by multiplying it by 1.05. We can choose the tuples to be updated by using a query. [end of text]
To insert data into a relation, specify a tuple or write a query to insert a set of tuples. Attribute values must be members of the attribute's domain. Inserted tuples must be of the correct arity. SQL allows attributes to be specified as part of the insert statement. More complex insert statements involve selecting tuples from a query. Inserting tuples on the basis of a query results in a set of tuples that are inserted into the relation. Each inserted tuple has a loan-number, branch-name, and initial balance. We evaluate the select statement fully before inserting any tuples. If the select statement is evaluated as part of an insert, a request such as inserting a new account might insert an infinite number of tuples. [end of text]
In SQL, update can be used to change values in tuples without altering all values in a tuple. This is achieved by using a query to update specific tuples based on a condition. For example, if annual interest payments are being made and all balances are to be increased by 5%, the update statement can be written to update the balance of accountset to accountset * 1.05. [end of text]
SQL allows updates to multiple relations, but views are not allowed to contain multiple relations. Views are only allowed to reference one relation at a time. [end of text]
The view-update anomaly exists in SQL, where a view name can be inserted into a relation, but the actual relation must have a value for the required column. This constraint prevents the update, insert, and delete operations on the view. [end of text]
A transaction consists of a sequence of query and/or update statements. Commit works commits the current transaction; that is, it makes the updates performed by the transaction permanent in the database. Rollback works causes the current transaction to be rolled back; that is, it un-does all the updates performed by the SQL statements in the transaction. Once a transaction has executed commit work, its effects can no longer be undone by rollback work. The database system guarantees that in the event of some failure, such as an error in one of the SQL statements, a power outage, or a system crash, a transaction’s effects will be rolled back if it has not yet executed commit work. In the case of power outage or other system crash, the rollback occurs when the system restarts. [end of text]
A transaction consists of a sequence of query and/or update statements. Commit works commits the current transaction, making updates permanent. Rollback undoes updates, restoring to before first statement. Transaction rollback is useful for detecting errors. Commit and rollback are similar in editing sessions. Transaction rollback ensures database state is restored. Automatic commit is dependent on implementation. Turn off automatic commit depends on SQL implementation. [end of text]
SQL provides various join mechanisms, including inner, outer, and left outer joins. These operations are used to join relations and retrieve data. The standard does not require unique attribute names in results. The SQL standard does not require attribute names in such results to be unique. An as clause should be used to assign unique names to attributes in query and subquery results. [end of text]
The textbook illustrates various join operations by using the relations loan and borrower in Figure 4.1. Inner joins are computed with loan.loan-number = borrower.loan-number, left outer joins are computed with loan left outer join borrower on loan.loan-number = borrower.loan-number. The attributes of the results consist of the attributes of the left-hand-side relation followed by the attributes of the right-hand-side relation. The SQL standard does not require unique attribute names in results. An as clause should be used to assign unique names to attributes in query and subquery results. [end of text]
The result of loan left outer join borrower on loan number = borrower.loan-number.loan-number.loan-number.loan-number.loan-number.loan-number.loan-number.loan-number.loan-number.loan-number.loan-number.loan-number.loan-number.loan-number.loan-number.loan-number.loan-number.loan-number.loan-number.loan-number.loan-number.loan-number.loan-number.loan-number.loan-number.loan-number.loan-number.loan-number.loan-number.loan-number.loan-number.loan-number.loan-number.loan-number.loan-number.loan-number.loan-number.loan-number.loan-number.loan-number.loan-number.loan-number.loan-number.loan-number.loan-number.loan-number.loan-number.loan-number.loan-number.loan-number.loan-number.loan-number.loan-number.loan-number.loan-number.loan-number.loan-number.loan-number.loan-number.loan-number.loan-number.loan-number.loan-number.loan-number.loan-number.loan-number.loan-number.loan-number.loan-number.loan-number.loan-number.loan-number.loan-number.loan-number.loan-number.loan-number.loan-number.loan-number.loan-number.loan-number.loan-number.loan-number.loan-number.loan-number.loan-number.loan-number.loan-number.loan-number.loan-number.loan-number.loan-number.loan-number.loan-number.loan-number.loan-number.loan
In Section 4.10.1, we saw examples of the join operations permitted in SQL. Join operations take two relations and return another relation as the result. Outer-join expressions are typically used in the from clause, but can be used anywhere a relation can be used. Each variant of the join operations consists of a join type and a join condition. The join condition defines which tuples in the two relations match and what attributes are present in the result of the join. The join type defines how tuples in each relation match. The join condition is mandatory for outer joins, but optional for inner joins (if omitted, a Cartesian product results). The use of a join condition is mandatory for outer joins, but is optional for inner joins (if it is omitted, a Cartesian product results). The meaning of the join condition natural, in terms of which tuples from the two relations match, is straightforward. The ordering of the attributes in the result of an natural join is as follows. The join attributes (that is, the attributes common to both relations) appear ﬁrst, in the order in which they appear in the left-hand-side relation. Next come all nonjoin attributes of the left-hand-side relation, and ﬁnally all nonjoin attributes of the right-hand-side relation. The right outer join is symmetric to the left outer join. Tuples from the right-hand-side relation that do not match any tuple from the left-hand-side relation are padded with nulls and are added to
The SQL-92 join types are cross join and union join, which are equivalent to inner join and full outer join, respectively. These join types are used for relational databases. [end of text]
SQL DDL allows speciﬁcation of schema, domain values, integrity constraints, indices, security, and authorization information for relations, as well as domain types. [end of text]
The SQL standard supports a variety of built-in domain types, including char(n), varchar(n), int, smallint, and numeric(p, d). [end of text]
The textbook covers real, double precision floating-point numbers, date format, and SQL in the context of database systems. [end of text]
SQL allows comparison operations on all the domains listed here, and it allows both arithmetic and comparison operations on the various numeric domains. SQL also provides a data type called interval, and it allows computations based on dates and times and on intervals. For example, if x and y are of type date, then x − y is an interval whose value is the number of days from date x to date y. Similarly, adding or subtracting an interval to a date or time gives back a date or time, respectively. It is often useful to compare values from compatible domains. For example, since every small integer is an integer, a comparison x < y, where x is a small integer and y is an integer (or vice versa), makes sense. We make such a comparison by casting small integer x as an integer. A transformation of this sort is called a type coercion. Type coercion is used routinely in common programming languages, as well as in database systems. [end of text]
An SQL relation is defined by using the create table command, where each attribute has a domain type and a primary key. The primary key is required to be non-null and unique, and can be specified in the create table command. The integrity constraints include primary key, check, and other constraints. The real-world database example does not model the real world. [end of text]
In SQL, the check clause is used to simulate an enumerated type by specifying that attribute values must be nonnegative. This allows for more general and powerful type systems. Relational databases products often use referential integrity constraints to enforce relationships between tables. The drop table command is used to remove a relation from an SQL database, while the alter table command adds attributes to an existing relation. [end of text]
SQL provides a declarative query language, making it easier to write queries in SQL. However, programmers need access to a database from a general-purpose programming language to express queries that cannot be expressed in SQL. Relational databases, SQL, and the McGraw-Hill Company's "Database System Concepts, Fourth Edition" are all mentioned in the text. [end of text]
SQL is a programming language that allows automatic optimization and provides full power to a programming language, making it extremely difficult for applications to write queries. Embedded SQL programs use host languages to access and update database data, extending the programmer's ability to manipulate the database further. The EXEC SQL statement is used to replace embedded SQL requests with host-languagedeclarations and procedure calls, allowing run-time execution of database accesses. The program must be processed by a special preprocessor before compilation, and variables of the host language can be used within embedded SQL statements. The SQL INCLUDE statement is used to identify the place where the preprocessor should insert special variables used between the program and the database system. [end of text]
In embedded SQL, database-modiﬁcation requests are simpler to express, and host-language variables can be used to update database relations. [end of text]
The dynamic SQL component of SQL allows programs to construct and submit SQL queries at run time, while embedded SQL statements must be present at compile time. Using dynamic SQL, programs can create SQL queries as strings at run time and either execute them immediately or prepare them for subsequent use. Preparing a dynamic SQL statement compiles it, and subsequent uses of the prepared statement use the compiled version. The dynamic SQL program contains a ?, which is a place holder for a value provided when the SQL program is executed. The ODBC standard deﬁnes a way for an application program to communicate with a database server, using an application program interface (API) that applications can use to open a connection, send queries and updates, and get back results. [end of text]
The Open Database Connectivity (ODBC) standard defines a way for applications to communicate with a database server. ODBC provides a library for applications to connect to any database server that supports ODBC. The first step is to set up a connection with the server. [end of text]
The textbook summarizes the ODBC example, which establishes a connection to a database, executes SQL commands, and handles the results. It also describes the SQLAllocEnv, SQLAllocConnect, SQLAllocStmt, SQLAllocStmt, SQLAllocStmt, SQLAllocStmt, SQLAllocStmt, SQLAllocStmt, SQLAllocStmt, SQLAllocStmt, SQLAllocStmt, SQLAllocStmt, SQLAllocStmt, SQLAllocStmt, SQLAllocStmt, SQLAllocStmt, SQLAllocStmt, SQLAllocStmt, SQLAllocStmt, SQLAllocStmt, SQLAllocStmt, SQLAllocStmt, SQLAllocStmt, SQLAllocStmt, SQLAllocStmt, SQLAllocStmt, SQLAllocStmt, SQLAllocStmt, SQLAllocStmt, SQLAllocStmt, SQLAllocStmt, SQLAllocStmt, SQLAllocStmt, SQLAllocStmt, SQLAllocStmt, SQLAllocStmt, SQLAllocStmt, SQLAllocStmt, SQLAllocStmt, SQLAllocStmt, SQLAllocStmt, SQLAllocStmt, SQLAllocStmt, SQLAllocStmt, SQLAllocStmt, SQLAllocStmt, SQLAllocStmt, SQLAllocStmt, SQLAllocStmt, SQLAllocStmt, SQLAllocStmt, SQLAllocStmt, SQLAllocStmt, SQLAllocStmt, SQLAllocStmt, SQLAllocStmt, SQLAllocStmt, SQLAllocStmt, SQLAllocStmt, SQLAllocStmt, SQLAllocStmt, SQLAllocStmt, SQLAllocStmt, SQLAllocStmt, SQLAllocStmt, SQLAllocStmt, SQLAllocStmt, SQLAllocStmt
The JDBC standard defines an API for connecting to databases, allowing Java programs to interact with them. The JDBC API loads drivers for databases and opens connections to servers. [end of text]
This Java program uses JDBC to connect to a database, execute SQL statements, and retrieve data. JDBC provides various features such as updatable result sets and allows for updatable SQL statements. [end of text]
Schemas, Catalogs, and Environments introduce the concepts of schemas, catalogs, and environments in SQL. These concepts help in organizing data and managing database operations. [end of text]
Schemas and catalogs are used to organize and manage data in databases. Early systems were flat, with each file stored in a single directory. Current systems have a directory structure, with SQL databases. [end of text]
To name a file uniquely, we must specify the fullpath name of the file. To identify a relation uniquely, a three-part name must be used, for example, catalog5.bank-schema.account. Multiple catalogs and schemas are available, allowing different applications and users to work independently. Procedures in SQL are particularly useful for external database operations without exposing internal details. [end of text]
SQL provides a module language for procedures, allowing them to be defined and stored in a database. Procedures can be executed by calling them, and stored procedures can be used by external applications without exposing internal details. SQL is particularly useful for database operations and procedural extensions. [end of text]
SQL is a formal relational algebra with many syntactic sugar, allowing complex queries. View relations are useful for hiding unnecessary information and collecting information from multiple relations into a single view. SQL provides updates, inserts, and deletes, and transaction atomicity. Modifications lead to null values in tuples. SQL data deﬁnition language creates relations with specified schemas. SQL DDL supports date and time types, and ODBC and JDBC standards define application program interfaces to access SQL databases. [end of text]
To execute the SQL queries, you would need to connect to the database, execute the queries, and then disconnect. The queries are:
a. SELECT COUNT(*) FROM cars WHERE year = 1989 AND type = 'car' AND accident_type = 'accident';
b. SELECT COUNT(*) FROM accidents WHERE car_type = 'car' AND car_brand = 'John Smith';
c. INSERT INTO accidents VALUES('AABB2000', 'AR2197', 'AR2197', 3000);
d. UPDATE accidents SET damage_amount = 3000 WHERE report_number = 'AR2197' AND car_license = 'AABB2000';
e. DELETE FROM cars WHERE license = 'AABB2000'; [end of text]
SELECT employee_name, street, city FROM employee WHERE company_name = 'First Bank Corporation' OR salary > 10000 OR employee_name IN (SELECT employee_name FROM employee WHERE company_name IN (SELECT company_name FROM company WHERE city = 'Small Bank Corporation')) [end of text]
Modify the database so that Jones now lives in Newtown.
Give all employees of First Bank Corporation a 10 percent raise.
Give all managers of First Bank Corporation a 10 percent raise unless the salary becomes greater than $100,000; in such cases, give only a 3 percent raise.
Delete all tuples in the works relation for employees of Small Bank Corporation. [end of text]
In SQL, the equivalent expressions are:
a. ΠA(r)
b. σB = 17 (r)
c. r × sd
ΠA,F (σC = D(r × s)) [end of text]
The textbook states that the equivalent queries in SQL are:
a. r1 ∪ r2
b. r1 ∩ r2
c. r1 − r2
d. ΠAB(r1) ΠBC(r2) [end of text]
SQL queries:
a. SELECT a FROM <a> WHERE ∃b (<a, b> ∈r ∧b = 17)
b. SELECT a, b, c FROM <a, b, c> WHERE <a, b> ∈r AND <a, c> ∈s
c. SELECT a FROM <a> WHERE ∃c (<a, c> ∈s ∧∃b1, b2 (<a, b1> ∈r ∧<c, b2> ∈r ∧b1 >b2)) [end of text]
The database system should not allow updates to be expressed in terms of the view of average salaries. This approach would not provide a meaningful comparison of the manager's salary to the average of all employees' salaries. Instead, the system should use the manager's salary as the key to find the average salary of all employees who work for that manager. This would allow for a more accurate comparison of the manager's salary to the average of all employees' salaries. [end of text]
The query selects values of p.a1 that are either in r1 or in r2. This occurs when either r1 or r2 is empty. [end of text]
The total account deposit is less than the average total account deposit at all branches using a nested query in the from clause. [end of text]
To display the grade for each student based on the score relation:
SELECT student_id, grade FROM grades WHERE score < 40 OR score >= 80
To find the number of students with each grade:
SELECT grade, COUNT(student_id) FROM grades GROUP BY grade [end of text]
The coalesce operation returns the first nonnull element in a list, while the case operation is used to select elements based on a condition. To express the coalesce operation using the case operation, we can use the following code:
```
coalesce(A1, A2, . . . , An) = case when A1 is not null then A1 else null end
``` [end of text]
To express a natural full outer join b using the full outer join operation with an on condition and the coalesce operation, we first need to define the relations a and b. Then, we can use the full outer join operation to combine the attributes name and address from both relations. Finally, we can use the coalesce operation to remove duplicate tuples with null values for name and address. The result relation will not contain two copies of the attributes name and address, and the solution is correct even if some tuples in a and b have null values for attributes name or address. [end of text]
An appropriate domain for each attribute and an appropriate primary key for each relation schema are crucial for database design. The domain defines the set of possible values for each attribute, while the primary key uniquely identifies each record in a relation schema. These elements ensure data integrity and facilitate efficient data retrieval and manipulation. [end of text]
Every employee works for a company located in the same city as the city in which the employee lives, and no employee earns a salary higher than that of his manager. [end of text]
SQL is a commercial relational database language, while QBE and Datalog are graphical languages. QBE is used on personal computers and Datalog is used in research database systems. Forms interfaces and tools for generating reports and analyzing data are also studied. [end of text]
The QBE data-manipulation language, developed at IBM, includes a two-dimensional syntax and is used in IBM's Query Management Facility. Today, many personal computer databases support variants of QBE language. The QBE database system is a data-manipulation language, with distinct features such as two-dimensional syntax and expression. QBE queries are expressed by skeleton tables. [end of text]
This convention distinguishes between constants and variables, which are quoted and appear without qualifiers. Queries on one relation return to a system's knowledge base, where variables are assigned values. To suppress duplicate elimination, insert ALL after the P. command. To display the entire loan relation, create a single row for each field. [end of text]
To find all loan numbers at the Perryridge branch, we bring up the skeleton for the loan relation, and fill it in as follows:loanloan-numberbranch-nameamountP. xPerryridgeThis query tells the system to look for tuples in loan that have “Perryridge” as the value for the branch-name attribute. For each such tuple, the system assigns the value of the loan-number attribute to the variable x. It “prints” (actually, displays) the value of the variable x, because the command P. appears in the loan-number column next to the variable x. Observe that this result is similar to what would be done to answer the domain-relational-calculus query{⟨x⟩| ∃b, a(⟨x, b, a⟩∈loan ∧b = “Perryridge”)}QBE assumes that a blank position in a row contains a unique variable. As a result, if a variable does not appear more than once in a query, it may be omitted. Our previous query could thus be rewritten asloanloan-numberbranch-nameamountP.PerryridgeQBE (unlike SQL) performs duplicate elimination automatically. To suppress du-plicate elimination, we insert the command ALL. after the P. command:loanloan-numberbranch-nameamountP.ALL.PerryridgeTo display the entire loan relation, we can create a single row consisting of P. inevery ﬁeld. Alternatively, we can use a shorthand notation by placing
QBE allows queries that span multiple relations and uses variables to force tuples to have the same values on certain attributes. [end of text]
The system finds tuples in loan with "Perryridge" as the value for the branch-name attribute, then displays the values for the customer-name attribute. The query "Find the names of all customers who have an account and a loan at the bank" involves negation and is written as "Find the names of all customers who have both an account and a loan at the bank". The query "Find the names of all customers who have an account but do not have a loan from the bank" involves negation and is written as "Find the names of all customers who have an account but do not have a loan from the bank". The query "Find the names of all customers who have both an account and a loan at the bank, but who do not have a loan from the bank" involves negation and is written as "Find the names of all customers who have both an account and a loan at the bank, but who do not have a loan from the bank". The query "Find the names of all customers who have an account and a loan at the bank" involves negation and is written as "Find the names of all customers who have an account and a loan at the bank". The query "Find the names of all customers who have an account but do not have a loan from the bank" involves negation and is written as "Find the names of all customers who have an account but do not have a loan from the bank". The query "Find the names of
QBE allows logical expressions to appear in a condition box, enabling general constraints over domain variables. It is possible to express queries without using a condition box, but complex queries with P. in multiple rows are hard to understand and should be avoided. [end of text]
This textbook summarizes the concepts of relational databases, including the use of QBE for ordering and displaying tuples in a relation schema, as well as other relational languages. It also covers the creation of a temporary result relation and the use of QBE for sorting and displaying data in multiple columns. [end of text]
The textbook explains how to construct a single relation schema for a query result in a single table using SQL commands. It provides an example using a SQL query to find customer names, account numbers, and balances for all accounts at the Perryridge branch. [end of text]
QBE allows users to control the order of tuples in a relation. By inserting AO or DO commands, users can sort and display data in ascending or descending order. To list customers at the Perryridge branch in ascending order with their account balances in descending order, QBE uses the command P.AO(1) and P.DO(2). [end of text]
In QBE, we can delete tuples from a relation using the D. command, which allows us to delete wholetuples and values in selected columns. When we delete information in only some of the columns, null values, specified by −, are inserted. [end of text]
The QBE operator is used to aggregate data and the ALL operator ensures that duplicates are not eliminated. The G operator is used to compute functions on groups of tuples, and the conditions are used to filter results based on specific criteria. [end of text]
In QBE.5.1.7.1Deletion, tuples can be deleted from a relation, and null values can be inserted into selected columns. This is done using D. commands, which operate on only one relation at a time. Examples include deleting customer Smith and inserting null values for customer-street. [end of text]
Deletion of tuples from a relation is expressed similarly in SQL, but with D. in place of P. QBE. Deletes information in only some columns, null values, specified by −, are inserted. Deletes from multiple relations using one D. operator per relation. [end of text]
Delete the branch-city value of the branch whose name is "Perryridge".branchbranch-namebranch-cityassetsPerryridgeD. Delete all loans with a loan amount between $1300 and $1500.loanloan-numberbranch-nameamountD.yxborrowercustomer-nameloan-numberD.yconditionsx = (≥ 1300 ≤ 1500)andDelete all accounts at all branches located in Brooklyn.accountaccount-numberbranch-namebalanceD.yxdepositorcustomer-nameaccount-numberD.ybranchbranch-namebranch-cityassetsxBrooklynNote that, in expressing a deletion, we can reference relations other than those from which we are deleting information.5.1.7.2InsertionTo insert data into a relation, we either specify a tuple to be inserted or write a query whose result is a set of tuples to be inserted. We do the insertion by placing the I.operator in the query expression. Obviously, the attribute values for inserted tuples must be members of the attribute's domain. [end of text]
To insert data into a relation, we either specify a tuple to be inserted or write a query whose result is a set of tuples to be inserted. We do the insertion by placing the I.operator in the query expression. We must get the appropriate information from the borrower relation and use that information to insert the appropriate new tuple in the depositor and account relations. [end of text]
The U. operator allows updating a single value in a tuple without changing all values. QBE, however, does not support updating the primary key fields. [end of text]
In Microsoft Access, QBE supports a graphical display environment, where attributes of tables are written one below the other. Access QBE uses a line linking attributes of two tables to specify a join condition, and automatically creates links between tables. Queries involving group by and aggregation can be created in Access as shown in Figure 5.3. [end of text]
In Access, QBE version supports a graphical display environment and uses a line linking attributes of two tables to specify a join condition. It also allows links between tables to create automatic joins and specifies selections on attribute values in the design grid. Group by and aggregation queries can be created in Access. [end of text]
The textbook explains how to design and manipulate tables in a database, including creating queries through a graphical user interface, adding attributes to the design grid, specifying selection conditions and grouping and aggregation, and supporting other features through access queries. [end of text]
Datalog is a nonprocedural query language based on Prolog, with rules that describe views and are written declaratively. Datalog simplifies writing simple queries and makes query optimization easier. Rules can use attributes by position and omit names, resulting in compact Datalog programs compared to SQL. [end of text]
A Datalog program consists of rules that define views. The preceding rule uses the relation account and deﬁnes the view relation v1. The symbol :– is read as “if,” and the comma separating the “account(A, “Perryridge”, B)” from “B > 700” is read as “and.” Intuitively, the rule is understood as follows: for all A, B if (account(A, “Perryridge”, B) ∈ account and B > 700) then (account(A, “Perryridge”, B) ∈ v1). The program speciﬁes the interest rates for accounts and includes two rules deﬁning a view relation interest-rate, whose attributes are account number and interest rate. The rules say that if the balance is less than $10000, then the interest rate is 5 percent, and if the balance is greater than or equal to $10000, the interest rate is 6 percent. Datalog rules can also use negation. The program includes a view relation c that contains the names of all customers who have a deposit, but have no loan, at the bank. [end of text]
The Datalog syntax allows for the definition of relational rules using named attributes, which can be written as literals. These rules can be understood as relational algebra expressions, and their meaning is conceptually equivalent to relational algebra results. The order of rules in a Datalog program does not matter, and the syntax for arithmetic operations is treated as relations. The Datalog program is built from literals and has the form (positive or negative) literal :– L1, L2, . . . , Ln where each Li is a (positive or negative) literal. The head of the rule is referred to as the rule's head, and the rest of the literals constitute the rule's body. Rules are built out of literals and have the form (positive or negative) literal :– L1, L2, . . . , Ln where each Li is a (positive or negative) literal. The head of the rule is referred to as the rule's head, and the rest of the literals constitute the rule's body. Rules are built out of literals and have the form (positive or negative) literal :– L1, L2, . . . , Ln where each Li is a (positive or negative) literal. The head of the rule is referred to as the rule's head, and the rest of the literals constitute the rule's body. Rules are built out of literals and have the form (positive or negative) literal :– L1, L2, . . . , Ln
Literals, relations, attributes, constants, negative literals, positive literals, relational algebra, relational databases, relational languages, Datalog, rules, view relations, Datalog program, relational data model, relational algebra, relational database, relational schema, relational data types, relational data structures, relational data management systems, relational database management systems, relational database management, relational database, relational database design, relational database system, relational database management system, relational database management system, relational database management system, relational database management system, relational database management system, relational database management system, relational database management system, relational database management system, relational database management system, relational database management system, relational database management system, relational database management system, relational database management system, relational database management system, relational database management system, relational database management system, relational database management system, relational database management system, relational database management system, relational database management system, relational database management system, relational database management system, relational database management system, relational database management system, relational database management system, relational database management system, relational database management system, relational database management system, relational database management system, relational database management system, relational database management system, relational database management system, relational database management system, relational database management system, relational database management system, relational database management system, relational database management system, relational database management system, relational database management system, relational database management system, relational database management system, relational database management system, relational database management system, relational database management
The semantics of a program is defined by starting with the semantics of a single rule, and then layering view relations in the following way. [end of text]
The semantics of a rule is defined by starting with the semantics of a single rule. Semantics of a recursive program is somewhat more complicated; it is discussed in Section 5.2.6. The semantics of a nonrecursive program is simpler. The set of facts that can be inferred from a given set of facts using rule R is infer(R, I) = {p(t1, . . . , tni) | there is an instantiation R′ of R, where p(t1, . . . , tni) is the head of R′, and the body of R′ is satisﬁed in I}. [end of text]
A ground instantiation of a rule is the result of replacing each variable in the rule with a constant. Ground instantiations are often referred to as "instantiations" and are simply called instantiations. A rule usually has many possible instantiations, which correspond to different ways of assigning values to each variable. The body of rule instantiation R is satisfied in I if for each positive literal qi(vi,1, . . . , vi,ni) in the body of R, the set of facts I contains the fact q(vi,1, . . . , vi,ni), and for each negative literal not qj(vj,1, . . . , vj,nj) in the body of R, the set of facts I does not contain the fact qj(vj,1, . . . , vj,nj). [end of text]
In a view relation, the set of facts in the first view depends on the set of facts in the second view. The layering of view relations in the program appears in Figure 5.9. The relation account is in the database. Relation interest-rate is Silberschatz-Korth-Sudarshan: Database System Concepts, Fourth Edition II. Relational Databases. [end of text]
The textbook summarizes the concepts of layering view relations, semantics of Datalog programs, and the use of nonrecursive Datalog views. It also discusses safety conditions and relational operations in Datalog. [end of text]
It is possible to write rules that generate an inﬁnite number of answers. Consider a rule that generates a view relation gt(X, Y) :– X > Y. Since the relation deﬁning > is inﬁnite, this rule would generate an inﬁnite number of facts for the relation gt, which calculation would, correspondingly, take an inﬁnite amount of time and space. Negation can also cause similar problems. Consider a rule that generates a view relation not-in-loan(L, B, A) :– not loan(L, B, A). The idea is that a tuple (loan-number, branch-name, amount) is in view relation not-in-loan if the tuple is not present in the loan relation. However, if the set of possible ac-count numbers, branch-names, and balances is inﬁnite, the relation not-in-loan would be inﬁnite as well. Finally, if we have a variable in the head that does not appear in the body, we may get an inﬁnite number of facts where the variable is instantiated to different values. So that these possibilities are avoided, Datalog rules are required to satisfy the following safety conditions:1. Every variable that appears in the head of the rule also appears in a nonarithmetically positive literal in the body of the rule.2. Every variable appearing in a negative literal in the body of the rule also appears in some positive literal in the body of the rule. [end of text]
Datalog expressions without arithmetic operations are equivalent to those using basic relational algebra operations. Examples show how various operations can be expressed in Datalog. [end of text]
In Datalog, projections are performed using only the required attributes in the head of the rule, and Cartesian products are formed by combining two relations in the same way. The relational-algebra operations, such as union and set difference, can be used to express any nonrecursive Datalog query without arithmetic operations. Extensions to Datalog support extended relational update operations like insertion, deletion, and update, and the aggregation operation of extended relational algebra. The view empl-jones is a recursive Datalog view that encodes the set of employees controlled by Jones. The bibliographical employee-namemanager-nameAlonBarinskyBarinskyEstovarCorbinDuarteDuarteJonesEstovarJonesJonesKlingerRensalKlinger illustrates this concept. [end of text]
Several database applications deal with tree-like structures, where employees are managers who manage a set of people reporting to them. Datalog-Fixpoint is a recursive Datalog view that captures the controlled employees by Jones. [end of text]
In recursive Datalog programs, negative literals can lead to problems, and the fixed-point iteration ensures termination by detecting new facts. The transitive closure of the manager relation is used to find direct and indirect subordinates of Jones, and Datalog without recursion cannot express transitive closure. Alternative mechanisms like embedded SQL can implement the fixed-point loop. [end of text]
Datalog with recursion has more expressive power than Datalog without recursion. For example, transitive closure queries cannot be answered without recursion, whereas nonrecursive queries have a fixed number of joins. External mechanisms, such as embedded SQL, can implement nonrecursive queries. [end of text]
Recursive queries can be defined without views, but recursive views are more expressive than other forms of recursive queries. [end of text]
The SQL:1999 standard supports a limited form of recursion, using the with recursive clause. It's possible to deﬁne recursive queries without using views, such as extended relational operations and SQL syntax extensions. However, recursive view deﬁnitions provide more expressive power than the other forms of recursive queries. [end of text]
The textbook section is about the 217th chapter. [end of text]
Forms and graphical user interfaces allow users to enter values that complete predeﬁned queries. Report generators provide a way to generate human-readable summary reports from databases. Data analysis tools allow users to interactively browse and analyze data. Forms, graphical user interfaces, and report generators are used to enter data into databases, extract information, and generate reports. Forms and graphical user interfaces are widely used to enter data into databases, and extract information from databases. Report generators are tools to generate human-readable summary reports from databases. Forms, graphical user interfaces, and report generators are used to enter data into databases, extract information, and generate reports. Forms, graphical user interfaces, and report generators are used to enter data into databases, extract information, and generate reports. Forms, graphical user interfaces, and report generators are used to enter data into databases, extract information, and generate reports. Forms, graphical user interfaces, and report generators are used to enter data into databases, extract information, and generate reports. Forms, graphical user interfaces, and report generators are used to enter data into databases, extract information, and generate reports. Forms, graphical user interfaces, and report generators are used to enter data into databases, extract information, and generate reports. Forms, graphical user interfaces, and report generators are used to enter data into databases, extract information, and generate reports. Forms, graphical user interfaces, and report generators are used to enter data into databases, extract information, and generate reports. Forms, graphical user interfaces,
Forms interfaces are widely used to enter data into databases, and extract information from databases via predeﬁned queries. For example, World Wide Web searchengines provide forms that are used to enter key words. Hitting a "submit" button causes the search engine to execute a query using the entered key words and display the result to the user. As a more database-oriented example, you may connect to a university registration system, where you are asked to fill in your roll number and password into a form. The system uses this information to verify your identity, as well as to extract information, such as your name and the courses you have registered for, from the database and display it. There may be further links on the Web page that let you search for courses and ﬁnd further information about courses such as the syllabus and the instructor. Web browsers supporting HTML constitute the most widely used forms and graphical user interface today. Most database system vendors also provide proprietary forms interfaces that offer facilities beyond those present in HTML forms. Programmers can create forms and graphical user interfaces by using HTML or programming languages such as C or Java. Most database system vendors also pro-vide tools that simplify the creation of graphical user interfaces and forms. Thesetools allow application developers to create forms in an easy declarative fashion, using form-editor programs. Users can deﬁne the type, size, and format of each field in a form by using the form editor. System actions can be associated with user actions
Report generators are tools to generate human-readable summary reports from databases. They integrate querying the database with the creation of formatted text and summary charts. Variables can store parameters such as months and years, and fields can be defined in tables, graphs, or other graphics. Tables, graphs, bar charts, or other graphics can be defined via queries on the database. The query deﬁnitions can make use of parameter values stored in variables. Once a report structure is defined, it can be stored and executed at any time to generate a report. Report-generator systems provide various facilities for structuring tabular output, such as table and column headers, displaying subtotals, splitting long tables into multiple pages, and displaying subtotals at the end of each page. The resulting structure is linked into a text document using OLE technology. [end of text]
The term "form" is less relevant today, as forms and report generators are typically created with graphical tools. [end of text]
In this textbook, we have discussed two query languages: QBE and Datalog. QBE is based on a visual paradigm, while Datalog is derived from Prolog. Both languages are intuitive and easy to use for nonexpert users. Datalog has a declarative semantics, making queries easier to write and optimize. However, there are no accepted standards for important features like grouping and aggregation in Datalog. The textbook also covers relational databases, review terms, and exercises. [end of text]
To summarize the provided section, I will focus on the QBE queries related to the relational database. Here's a concise summary:
1. Find the total number of people who owned cars involved in accidents in 1989.
2. Find the number of accidents involving cars belonging to "John Smith".
3. Add a new accident to the database.
4. Delete the Mazda car belonging to "John Smith".
5. Update the damage amount for the car with license number "AABB2000" in the accident with report number "AR2197" to $3000.
The QBE queries are:
1. SELECT COUNT(*) FROM cars WHERE YEAR(CAR_ID) = 1989;
2. SELECT COUNT(*) FROM accidents WHERE CAR_ID IN (SELECT CAR_ID FROM cars WHERE NAME = 'John Smith');
3. INSERT INTO accidents VALUES (AABB2000, AR2197, 3000);
4. DELETE FROM cars WHERE CAR_ID = 'AABB2000';
5. UPDATE accidents SET DAMAGE_AMOUNT = 3000 WHERE REPORT_NUMBER = 'AR2197' AND CAR_ID = 'AABB2000'; [end of text]
Datalog for each of the following queries:
a. Find the names of all employees who work for First Bank Corporation.
b. Find the names and cities of residence of all employees who work for FirstBank Corporation.
c. Find the names, street addresses, and cities of residence of all employees who work for First Bank Corporation and earn more than $10,000 per year.
d. Find all employees who live in the same city as the company for which they work.
e. Find all employees who live in the same city and on the same street as their managers.
f. Find all employees in the database who do not work for First Bank Corporation.
g. Find all employees who earn more than every employee of Small Bank Corporation.
h. Assume that the companies may be located in several cities. Find all companies located in every city in which Small Bank Corporation is located. [end of text]
Find all employees who earn more than the average salary of all employees in the company.
Find the company that has the most employees.
Find the company that has the smallest payroll.
Find those companies whose employees earn a higher salary, on average, than the average salary at First Bank Corporation. [end of text]
Modifying the database to include Jones in Newtown, giving all employees a 10% raise, and giving all managers a 10% raise unless the salary is greater than $100,000. [end of text]
In QBE, the expressions are:
a. ΠA(r)
b. σB = 17 (r)
c. r × sd
ΠA,F (σC = D(r × s))
In Datalog, the equivalent queries are:
a. ΠA(r)
b. σB = 17 (r)
c. r × sd
ΠA,F (σC = D(r × s)) [end of text]
In QBE, the equivalent queries are:
a. r1 ∪ r2
b. r1 ∩ r2
c. r1 − r2
d. ΠAB(r1) ΠBC(r2) [end of text]
The textbook defines QBE (Quantified Boolean Expression) and Datalog (Datalogic) in terms of existential quantifiers, sets, and relations. It then outlines queries a, b, and c, each involving existential quantifiers and sets. [end of text]
Find all employees who work (directly or indirectly) under the manager "Jones".
Find all cities of residence of all employees who work (directly or indirectly) under the manager "Jones".
Find all pairs of employees who have a (direct or indirect) manager in common.
Find all pairs of employees who have a (direct or indirect) manager in common, and are at the same number of levels of supervision below the com-mon manager. [end of text]
Relational databases are a type of database system that uses tables to organize data. Relational databases use columns and rows to store data. The book discusses the concepts of relations, attributes, and relationships in relation to databases. The book also covers other relational languages and their applications. The McGraw-Hill Companies, 2001. [end of text]
The experimental version of Query-by-Example and the commercial version of IBM DB2 QMF and Borland Paradox implement logic databases, while Microsoft Access and Borland Paradox support Datalog. The XSB system from the State University of New York (SUNY) Stony Brook is a Prolog implementation that supports database querying. [end of text]
A domain is a set of values that a particular attribute can take, and a constraint is a condition that must be satisfied by any value assigned to a variable of that type. The check clause in SQL allows domains to be restricted in powerful ways that most programming language type systems do not permit. [end of text]
The textbook explains the creation of a domain for the HourlyWage and AccountNumber numeric types, and the use of check clauses to enforce domain constraints. It also discusses referential integrity constraints and their use in SQL. [end of text]
Referential integrity constraints arise frequently in relational databases, where we derive schemas by constructing tables from E-R diagrams. [end of text]
In Section 3.3.3, we considered a modified outer join to operate on relations containing dangling tuples. Here, our concern is not with queries but rather with when to permit dangling tuples in the database. If there is a tuple t1 in the account relation with t1[branch-name] = “Lu-nartown,” but no tuple in the branch relation for the Lunartown branch, we expect the branch relation to list all bank branches. Therefore, t1 would refer to an account at a branch that does not exist. We would like to have an integrity constraint that prohibits dangling tuples of this sort. The distinction between these two examples arises from two facts: the attribute branch-name in Account-schema is a foreign key referencing the primary key of Branch-schema, and the attribute branch-name in Branch-schema is not a foreign key. [end of text]
Referential integrity constraints ensure data consistency and security in relational databases. They prevent data inconsistencies and unauthorized access to sensitive information. [end of text]
Referential integrity constraints ensure that data relationships are consistent and secure. SQL allows specifying foreign keys using the foreign key clause, and a version of the references clause allows specifying a list of attributes for referenced relations. If a delete or update action on a referenced relation violates a referential integrity constraint, the system must take steps to change the referenced tuple to restore the constraint. [end of text]
Database modiﬁcations can cause violations of referential integrity. We must ensure that insertions and deletions respect the referential integrity constraint. Updates to referencing and referenced relations should be considered separately. [end of text]
Foreign keys can be specified using the foreign key clause in SQL. They reference the primary key attributes of the referenced table. SQL supports a version with explicit attribute lists for referencing relations. A short form of an attribute definition to declare a foreign key:branch-name char(15) references branch. If a delete or update action violates the constraint, the system must change the tuple in the referenced relation. [end of text]
SQL data definition for part of the bank database. Null values complicate referential integrity constraints in SQL. Transactions may consist of several steps, and integrity constraints may be temporarily violated after one step. [end of text]
SQL does not provide a "for all X, P(X)" construct, so we can't express the constraints in a single statement. We need to use multiple statements to express the conditions. [end of text]
To create an assertion, use the following SQL statements:
1. Balance-constraint check: 
```sql
CREATE TABLE loan AS SELECT * FROM loan WHERE loan.branch-name = branch.branch-name;
CREATE TABLE account AS SELECT * FROM account WHERE account.branch-name = branch.branch-name;
CREATE TABLE borrower AS SELECT * FROM borrower WHERE borrower.customer-name = depositor.customer-name AND depositor.account-number = account.account-number;
CREATE TABLE depositor AS SELECT * FROM depositor WHERE depositor.customer-name = borrower.customer-name AND borrower.account-number = depositor.account-number;
CREATE TABLE account AS SELECT * FROM account WHERE account.branch-name = branch.branch-name;
CREATE TABLE loan AS SELECT * FROM loan WHERE loan.branch-name = branch.branch-name;
CREATE TABLE borrower AS SELECT * FROM borrower WHERE borrower.customer-name = depositor.customer-name AND depositor.account-number = account.account-number;
CREATE TABLE depositor AS SELECT * FROM depositor WHERE depositor.customer-name = borrower.customer-name AND borrower.account-number = depositor.account-number;
CREATE TABLE account AS SELECT * FROM account WHERE account.branch-name = branch.branch-name;
CREATE TABLE loan AS SELECT * FROM loan WHERE loan.branch-name = branch.branch-name;
CREATE TABLE borrower AS SELECT * FROM borrower WHERE borrower.customer-name = depositor.customer-name AND depositor.account-number = account.account-number;
CREATE TABLE depositor AS SELECT * FROM depositor WHERE depositor.customer-name = borrower.customer-name AND borrower.account-number = depositor.account-number;
CREATE TABLE account AS SELECT * FROM account
Triggers are useful mechanisms for alerting humans or for starting certain tasks automatically when certain conditions are met. They are stored as data in the database and can be accessed by all operations. Once entered, triggers are executed automatically whenever the specified event occurs and the corresponding condition is met. [end of text]
Triggers are useful mechanisms for alerting humans or for starting tasks when conditions are met, such as updating account balances or placing orders. They allow for automated actions without requiring manual intervention. [end of text]
Triggers are used extensively in SQL-based database systems, but before SQL:1999, they were not part of the standard. Relational databases, as described in Chapter 6, include integrity and security features. [end of text]
The textbook outlines SQL:1999 syntax for triggers, detailing how they can be initiated after updates, referencing new rows, and creating new tuples to represent new loans. Triggers can be triggered either before or after an event, and can serve as extra constraints to prevent invalid updates. Triggers can be activated before an event to prevent overdrafts and can be triggered before or after an event to perform other actions. [end of text]
Triggers are useful for maintaining summary data but can be unnecessary for replication in most cases. They should be written with great care to prevent runtime errors. [end of text]
Triggers can be used for maintaining summary data, while modern database systems provide built-in facilities for database replication. Triggers should be written with great care, and can be called rules or active rules. [end of text]
The data in databases needs protection against unauthorized access, accidental destruction, and accidental alteration. Relational databases provide a way to store and manage data in a structured manner, ensuring data integrity and security. [end of text]
In this section, we examine ways data may be misused or intentionally made inconsistent. We then present mechanisms to guard against such occurrences. Security at several levels is discussed, including database system, operating system, network, and physical security. Finally, network-level security has gained widespread recognition as the basis for international electronic commerce. [end of text]
Database security refers to protecting the database from malicious access. Absolute protection is not possible, but the cost to the perpetrator can deter most attempts without proper authority. Database systems, operating systems, and physical security are necessary to protect the database. Security at the database system, physical, and human levels is crucial, but operating system security is more important. Network security has gained recognition as an integral part of international commerce. [end of text]
In database systems, users can be granted various types of authorization to access and modify data, including read, insert, update, and delete. Additionally, users can be granted authorization to modify database schema, such as creating indices, relations, and attributes. Index authorization can be unnecessary since it does not alter data in relations, but indices are a performance enhancement structure. However, indices also consume space and require updates to update indices. To regulate the use of system resources, it is necessary to treat index creation as a privilege. [end of text]
The ultimate form of authority is that given to the database administrator. The database administrator may authorize new users, restructure the database, and soon. This form of authorization is analogous to that of a superuser or operator for an operating system.6.5.3Authorization and Views In Chapter 3, we introduced the concept of views as a means of providing a user with a personalized model of the database. A view can hide data that a user does not need to see. The ability of views to hide data serves both to simplify usage of the system and to enhance security. Views simplify system usage because they restrict the user’s attention to the data of interest. Although a user may be denied direct access to a relation, that user may be allowed to access part of that relation through a view. Thus, a combination of relational-level security and view-level security limits a user’s access to precisely the data that the user needs. [end of text]
In Chapter 3, we introduced views as a means to provide a user with a personalized model of the database. Views hide data that a user does not need to see, enhancing security. They simplify usage by restricting access to only the data of interest. In banking, a clerk needing loan information must be denied direct access to the loan relation, but can access the cust-loan view, which contains only names of customers and branches. The system checks authorization before processing queries. Views do not require resource authorization. A user can create a view with read authorization on both relations. [end of text]
In a database system, authorization can be passed among users, but careful handling is necessary to ensure that authorization can be revoked at some future time. The passing of authorization from one user to another can be represented by an authorization graph. The root of the graph is the database administrator. Initially, users U1, U2, and U3 grant update authorization on the loan database. U4 grants authorization from U1. When the database administrator revokes authorization from U2, U2 retains authorization through U3. If U3 eventually revokes authorization from U2, U3 retains authorization through U2. However, when U3 revokes authorization from U2, the edges from U3 to U2 and from U2 to U3 are no longer part of a path starting with the database administrator. [end of text]
The notion of roles captures the scheme where each teller has a set of roles assigned to them, and users are granted roles based on their own userid. This allows for more granular control over authorization and audit trails. [end of text]
A better scheme for assigning authorizations to tellers involves specifying the authorizations that every teller must receive individually and separately identifying database users as tellers. This allows for the use of roles to manage permissions, ensuring that users can only perform actions they are authorized to. The use of roles also reduces the risk of security issues by requiring users to connect to the database with their own userid. [end of text]
Many secure database applications require an audit trail to maintain, which logs all changes, including user actions and timestamps. This aids in detecting and tracking incorrect or fraudulent updates, helping banks manage account balances and prevent fraud. Database systems often provide built-in mechanisms to create audit trails, making them more convenient to use. [end of text]
The SQL language allows for the definition of authorizations, with privileges like delete, insert, select, and update. These privileges are used to control access to data. The select privilege corresponds to read, and references privilege allows users to declare foreign keys in relation creation. The references privilege is useful because it ensures that foreign keys are correctly referenced. The reason for this feature is not fully understood, but it is important for maintaining database integrity. [end of text]
The SQL standard includes privileges for read, delete, insert, and update, as well as references for foreign keys in relational databases. The references privilege is useful for defining foreign keys in relation creation. [end of text]
The SQL data-deﬁnition language includes commands to grant and revoke privileges. The grant statement is used to confer authorization. The basic form of this statement is:grant <privilege list> on <relation name or view name> to <user/role list>. The privilege list allows the granting of several privileges in one command. The following grant statement grants users U1, U2, and U3 select authorization on the account relation:grant select on account to U1, U2, U3 The update authorization may be given either on all attributes of the relation or on only some. If update authorization is included in a grant statement, the list of attributes on which update authorization is to be granted optionally appears in paren-theses immediately after the update keyword. If the list of attributes is omitted, the update privilege will be granted on all attributes of the relation. The SQL references privilege is granted on speciﬁc attributes in a manner likethat for the update privilege. The following grant statement allows user U1 to create relations that reference the key branch-name of the branch relation as a foreign key:grant references (branch-name) on branch to U1 Initially, it may appear that there is no reason ever to prevent users from creating for-eign keys referencing another relation. However, recall from Section 6.2 that foreign-key constraints restrict deletion and update operations on the referenced relation. The privilege all privileges can be used as a short form for all the allowable privileges. Similarly
Roles can be created in SQL, and users can grant privileges to them. Roles can be granted to users, managers, or other roles, and these statements show that grant teller to john, grant teller to manager, and grant manager to mary. The privileges of a user or a role consist of all privileges directly granted to the user/role and all privileges granted to roles that have been granted to the user/role. Roles can inherit privileges from other roles. [end of text]
In Databases, granting a privilege to another user or role requires appending the grant option clause with the user or role name. This allows the recipient to pass the privilege to other users. To revoke a privilege, use the revoke statement with the appropriate form. [end of text]
The SQL standard allows for a primitive authorization mechanism for the database schema, but it is nonstandard. Authorization must be at the level of individual tuples, which is not possible in the current SQL standards for authorization. The benefits of fine-grained authorizations, such as individual tuples, can be implemented by application servers, but the drawbacks include intermixed code and oversight issues. [end of text]
The SQL standard specifies a primitive authorization mechanism for databases, allowing only the owner of a schema to modify it. Database implementations can further enhance authorization with more powerful mechanisms. [end of text]
The current SQL standards for authorization have shortcomings, with individual user identiﬁers on database servers and Web application server access. Fine-grained authorizations can be implemented through application code, but code mixing with application code makes it hard to ensure no loopholes. [end of text]
Encryption is a technique used to protect data by converting it into a coded form that can only be read by authorized users. It relies on a unique encryption key that is difficult for unauthorized users to determine. The Data Encryption Standard (DES) is a well-known encryption technique that uses substitution and rearrangement of characters to create a coded data. However, its security is compromised by the requirement for the encryption key to be transmitted securely. The revalidation of the encryption key in 1983 and 1987 is a major weakness. Relational databases are a type of database system that uses tables to store data and relationships between them. They are designed to provide data integrity and security by ensuring that data is consistent and that only authorized users can access it. [end of text]
Encryption techniques can be weak due to easy breakage by unauthorized users. The substitution of characters can be easily guessed by an intruder. A good encryption technique depends on a key that is difficult to determine. The Data Encryption Standard (DES) is a good example of a good encryption technique. [end of text]
In 1993, weakness in DES was recognized as reaching a point where a new standard needed to be selected, and in 2000, Rijndael was chosen as the AES. The Rijndael algorithm is a symmetric key algorithm with a signiﬁcantly stronger level of security and ease of implementation on current computer systems. It is based on two keys: a public key and a private key. The public key is published, and only authorized users can decrypt it. The private key is known only to the user to whom it belongs. Public-key encryption can be made public without making it easy for people to figure out the scheme for decryption. The details of public-key encryption and the mathematical justiﬁcation of this technique’s properties are referenced in the bibliographic notes. Although public-key encryption is secure, it is also computation-ally expensive. A hybrid scheme used for secure communication is as follows: DES keys are exchanged via a public-key–encryption scheme, and DES encryption is used on the data transmitted subsequently. [end of text]
Authentication involves verifying a user's identity and ensuring data integrity and security in databases. Public-key systems use encryption for challenge-response authentication and digital signatures for verifying data authenticity. [end of text]
Integrity constraints ensure data consistency, while domain constraints specify attribute values. Relational databases, such as SQL, use these concepts to maintain data integrity and security. [end of text]
Referential integrity constraints ensure that values in one relation match those in another relation. Domain constraints and referential integrity constraints are relatively easy to test. Triggers can be used for business rules, audit logging, and logging actions outside the database system. The data stored in the database needs to be protected from unauthorized access, malicious destruction, and accidental loss of consistency. Encryption can be used to protect sensitive data. Roles help assign privileges according to roles in an organization. The various authorization provisions in a database system may not provide sufficient protection for highly sensitive data. In such cases, data can be encrypted. Only a user who knows how to decipher the encrypted data can read them. Encryption forms the basis for secure authentication. [end of text]
The relations loan and borrower represent entities in a database, where a borrower has a loan. [end of text]
CREATE TABLE employee (
    employee_name VARCHAR(255),
    street VARCHAR(255),
    city VARCHAR(255)
);
CREATE TABLE company (
    company_name VARCHAR(255),
    city VARCHAR(255)
);
CREATE TABLE employee_manages (
    employee_name VARCHAR(255),
    manager_name VARCHAR(255)
); [end of text]
In a database, constraints on the relationships between entities must be expressed using syntax. The system must enforce these constraints by checking the presence of names in the addresses. The constraints are defined as "every name in address must appear in either salaried-worker or hourly-worker, but not necessarily in both." The system must also consider the possibility of concurrent access to the database to ensure data integrity and security. [end of text]
When a tuple in the relation `manager` is deleted, it removes the corresponding employee from the `manager-name` key. The `delete cascade` option ensures that the employee is also deleted from the `employee-name` key. This relationship is crucial for maintaining the integrity of the database. [end of text]
The trigger mechanism in SQL can be used to implement the on delete cascade option by creating a trigger that checks the primary key of the parent table and deletes the corresponding tuple from the child table if the child table does not have a matching row. This ensures that any changes made to the child table will be reflected in the parent table, and any changes made to the parent table will be reflected in the child table. The trigger can be defined in the child table's foreign key constraint. [end of text]
The Perryridge branch's total amount is equal to the sum of all the amounts lent. [end of text]
For each owner of an account, check if she has any remaining accounts. If she does not, delete her from the depositor relation. [end of text]
Create a view branch-cust that selects branch-name and customer-name from depositor and account, where depositor.account-number equals account.account-number. The view is materialized and active rules are maintained to keep it up to date on insertions and deletions from depositor or account. No updates are needed. [end of text]
whether this concern relates to physical security, human security, operating-system security, or database security. [end of text]
A view containing account numbers and customer names (but not balances) for all accounts at Deer Park, a view containing customer names and addresses for all customers with accounts at Rock Ridge, and a view containing customer names and average account balances for all customers. [end of text]
would be performed (if they should be allowed at all). Hint: See the discussion of views in Chapter 3. [end of text]
Views are used to hide data from users who do not have access to the database. They allow users to filter data based on specific criteria. However, views can sometimes conflict with other data in the database, as they may not always be accurate or up-to-date. To avoid this conflict, it is important to ensure that the data being filtered is accurate and up-to-date before creating a view. [end of text]
Resource authorization is the process of controlling access to resources in a database system, ensuring that only authorized users can access specific data and functionality. [end of text]
The operating system's security and authorization scheme can be used instead of defining a special scheme, offering an advantage in flexibility and adaptability. However, it may have disadvantages, such as potential security risks if not properly implemented. [end of text]
Schemas for storing passwords allow testing by users attempting to log into the system. Normal forms ensure that the system can test passwords. Schemes allow testing by users. Normal forms ensure that the system can test passwords. Schemes allow testing by users. Normal forms ensure that the system can test passwords. Schemes allow testing by users. Normal forms ensure that the system can test passwords. Schemes allow testing by users. Normal forms ensure that the system can test passwords. Schemes allow testing by users. Normal forms ensure that the system can test passwords. Schemes allow testing by users. Normal forms ensure that the system can test passwords. Schemes allow testing by users. Normal forms ensure that the system can test passwords. Schemes allow testing by users. Normal forms ensure that the system can test passwords. Schemes allow testing by users. Normal forms ensure that the system can test passwords. Schemes allow testing by users. Normal forms ensure that the system can test passwords. Schemes allow testing by users. Normal forms ensure that the system can test passwords. Schemes allow testing by users. Normal forms ensure that the system can test passwords. Schemes allow testing by users. Normal forms ensure that the system can test passwords. Schemes allow testing by users. Normal forms ensure that the system can test passwords. Schemes allow testing by users. Normal forms ensure that the system can test passwords. Schemes allow testing by users. Normal forms ensure that the system can test passwords. Schemes
The first normal form imposes a basic requirement on relations and requires that all attributes have atomic domains. Composite attributes, such as address with components street and city, also have nonatomic domains. Integers are assumed to be atomic, so the set of integers is an atomic domain; the set of all sets of integers is a nonatomic domain. The distinction is that we do not normally consider integers to have subparts, but we consider sets of integers to have subparts—namely, the integers making up the set. The important issue is not what the domain itself is, but rather how we use domain elements in our database. [end of text]
Database System Concepts, Fourth Edition, Silberschatz, Korth, Sudarshan: Database System Concepts, Fourth Edition, Relational Databases, Relational Database Design, 262, McGraw-Hill Companies, 2001. [end of text]
In contrast to the relation schema used in Chapters 3 to 6, we need to modify the database design for our banking example to ensure that we can represent the information concerning loans in a single relation, lending, and add a new loan to the database. We must repeat the asset and city data for the Perryridge branch, and add the tuple(Perryridge, Horseneck, 1700000, Adams, L-31, 1500) to the lending relation. This modification allows us to update the database more efficiently and avoid costly updates. [end of text]
Functional dependencies are constraints on the set of legal relations in database design. They allow us to express facts about the enterprise. SILBERSCHATTZ-KORTH-SUDARSHAN: Database System Concepts, Fourth Edition, 2001. Relational Databases, 7. Relational-Database Design, 264. Copyright McGraw-Hill Companies, 2001. [end of text]
Functional dependencies are constraints on the set of legal relations in databases. They allow us to express facts about the enterprise modeled in a database. 
End of summary. [end of text]
Functional dependencies allow us to express constraints that we cannot express with superkeys. They enable us to test relations for legality and specify constraints on the set of legal relations. [end of text]
In the banking example, the set of functional dependencies includes: branch-name branch-city assets Downtown Brooklyn900000 Redwood Palo Alto2100000 Perryridge Horseneck1700000 Mianus Horseneck400000 Round Hill Horseneck8000000 Pownal Bennington300000 North Town Rye3700000 Brighton Brooklyn7100000 The set of functional dependencies on Customer-schema and Loan-schema are not satisﬁed. Therefore, we do not include customer-street →customer-city in the set of functional dependencies that hold on Customer-schema. In contrast, we do not wish to include assets →branch-name in the set of functional dependencies on Branch-schema. We assume that when designing a relational database, we first list those functional dependencies that must always hold. [end of text]
To prove that certain functional dependencies hold, we need to consider all functional dependencies that hold and prove that others are logically implied by them. This involves checking all functional dependencies on a given relation schema and determining if they are logically implied by the given set. [end of text]
In the textbook, it is shown that whenever a given set of functional dependencies holds on a relation, A →H must also hold on the relation. The closure of a set of functional dependencies, denoted by F +, is the set of all functional dependencies logically implied by F. The Axioms, or rules of inference, provide a simpler technique for reasoning about functional dependencies. In the rules listed, we use Greek letters (α, β, γ, . . . ) for sets of attributes, and uppercase Roman letters from the beginning of the alphabet for individual attributes. We use αβ to denote α ∪β. The closure of F + requires arguments of the type just used to show that A →H is in the closure of our example set of dependencies. [end of text]
To test whether a set α is a superkey, we must devise an algorithm for computing the set of attributes functionally determined by α. One way is to compute F +, then repeat for each functional dependency in F +, adding the resulting functional dependencies to F + until F + does not change. This method can be expensive due to the large size of F +. [end of text]
The algorithm computes the set of attributes functionally determined by α, useful for testing superkeys and other tasks. It works by first testing each functional dependency and adding new attributes to result if necessary. The algorithm is correct and efficient, with a worst-case time complexity of quadratic in the size of F. A faster algorithm with linear time complexity is presented in Exercise 7.14. [end of text]
Whenver a user updates a relation, the database system must ensure that the update does not violate any functional dependencies, and the system can roll back the update if it violates any. The system can reduce the effort by testing a simplified set of functional dependencies that has the same closure as the original set. The simplified set is easier to test since it has the same closure. The system can also check for violations by testing a simplified set of functional dependencies that has the same closure as the original set. [end of text]
In a set of functional dependencies, an attribute is extraneous if it is not included in any of the dependencies that logically imply it. A canonical cover Fc for a set of functional dependencies F is a set of dependencies such that F logically implies all dependencies in Fc, and Fc logically implies all dependencies in F. The algorithm for finding a canonical cover Fc involves combining functional dependencies with the same left side and checking for extraneous attributes. If an extraneous attribute is found, it is deleted from the attribute set. The algorithm ensures that no functional dependency contains an extraneous attribute and that each left side of a functional dependency is unique. The union rule replaces any dependencies in Fc of the form α1 →β1 and α1 →β2 with α1 →β1 β2. The algorithm for testing Fc is equivalent to testing F, but it ensures that no functional dependency contains an extraneous attribute. [end of text]
The textbook explains that deleting B results in the sets {A →C, B →AC, and C →AB}, which is symmetrical to the previous case. For an exercise, you can find another canonical cover for F. [end of text]
The bad design of Section 7.2 suggests that we should decompose a relation schema with many attributes into several schemas with fewer attributes. Careless decomposition may lead to another form of bad design. Consider an alternative design in which we decompose Lending-schema into the following two schemas: Branch-customer-schema = (branch-name, branch-city, assets, customer-name) Customer-loan-schema = (customer-name, loan-number, amount). Figures 7.9 and 7.10 show the resulting branch-customer and customer-loan schemas. When we reconstruct the loan relation, we need to write branch-customer customer-loan branch-name branch-city assets customer-name. If we apply the expression Πbranch-name (σamount < 1000 (branch-customer customer-loan)) to the branch-customer customer-loan relation, we obtain three branch names: Mianus, Round Hill, and Downtown. This shows why the decomposition of Lending-schema into Branch-customer-schema and customer-loan-schema is a lossy-join decomposition. [end of text]
In general, a lossy join decomposition is a bad database design because it results in redundancy and loss of information. The decomposition of Lending-schema into Branch-schema and Loan-info-schema is lossless because the functional dependency branch-name →branch-city assetsholds on Branch-schema. [end of text]
Constraints other than functional dependencies are introduced, and a lossless-join decomposition is defined. This chapter focuses on specifying and obtaining lossless-join decompositions that avoid pitfalls in database design. [end of text]
In Section 7.5, we discussed the desirable properties of a decomposition of a relation schema, which ensures that the decomposition is lossless. We then demonstrated that our Lending-schema decomposition is a lossless-join decomposition by showing a sequence of steps that generate the decomposition. [end of text]
In Section 7.2, we argued that when decomposing a relation into smaller relations, the decomposition must be lossless. We claim that the Silberschatz-Korth-Sudarshan criterion for determining lossiness is essential. To demonstrate this, we first show that a lossless-join decomposition exists by showing a sequence of steps that generate it. [end of text]
Dependency preservation ensures that updates do not create invalid relations in a relational database. [end of text]
In Lending-schema, it was necessary to repeat the city and assets of a branch for each loan. The decomposition separates branch and loan data into distinct relations, thereby eliminating this redundancy. Similar observations apply to customers and borrowers. The attribute closure is with respect to the functional dependencies in F, and the decomposition is dependency preserving if and only if all the dependencies in F are preserved. [end of text]
The decomposition of Lending-schema eliminates redundancy by separating branch and loan data into distinct relations, while maintaining the same amount of information for each customer. [end of text]
The lack of redundancy in our decomposition of the Borrower-schema is desirable, and achieving this lack of redundancy is represented by several normal forms. [end of text]
In BCNF, a relation schema R is in BCNF if for all functional dependencies in F + of the form α →β, where α ⊆R and β ⊆R, at least one of the following holds: α →β is a trivial functional dependency (that is, β ⊆α), or α is a superkey for schema R. A database design is in BCNF if each member of the set of relation schemas that constitutes the design is in BCNF. The schema Loan-info-schema is not in BCNF because it suffers from the problem of repetition of information. [end of text]
A relation schema R is in Boyce–Codd normal form (BCNF) with respect to a set F of functional dependencies if it satisfies the conditions that at least one functional dependency is trivial and at least one functional dependency is superkey for the schema. A database design is in BCNF if each member of the set of relation schemas that constitutes the design is in BCNF. The schema Loan-info-schema is not in BCNF because it violates the trivial functional dependency on loan-number. The schema Branch-schema is in BCNF because it satisfies the nontrivial functional dependency on branch-name. The schema Customer-schema is in BCNF because it is a candidate key for the schema. The schema Loan-schema is not in BCNF because it violates the trivial functional dependency on loan-number. The schema Borrower-schema is in BCNF because it is a candidate key for the schema. The decomposition of Loan-schema into two schemas is a lossless-join decomposition. [end of text]
The BCNF decomposition algorithm is used to decompose the Lending-schema schema into three relation schemas, Branch-schema, Loan-schema, and Borrower-schema, each of which is in BCNF. The algorithm checks if a relation in the decomposition satisfies BCNF and can be used to show that a decomposed relation is not in BCNF. The algorithm takes exponential time in the size of the initial schema. [end of text]
The BCNF decomposition algorithm can decompose a relation schema into BCNF schemas, ensuring lossless-join decompositions. [end of text]
The textbook discusses algorithms for computing BCNF decompositions in polynomial time, with the potential for "overnormalization" that may unnecessarily decompose relations. It also explains that not every BCNF decomposition is dependency preserving, as demonstrated by an example of a relation schema with a superkey that is not a superkey. The textbook concludes by discussing third normal form and its motivation for using it as a small relaxation of BCNF. [end of text]
Not every BCNF decomposition is dependency preserving. The decomposition of Banker-schema into Banker-branch-schema and Customer-banker-schema is not dependency preserving, as it violates the dependency customer-name branch-name →banker-name. [end of text]
BCNF requires that all nontrivial dependencies be of the form α →β, where α is a superkey. 3NF relaxes this constraint slightly by allowing nontrivial functional dependencies whose left side is not a superkey. Relational schemas in third normal form (3NF) with respect to a set F of functional dependencies can be found using a lossless-join, dependency-preserving decomposition that is in 3NF. The choice of alternative depends on the application requirements. [end of text]
BCNF requires that all nontrivial dependencies be of the form α →β, where α is asuperkey. 3NF relaxes this constraint slightly by allowing nontrivial functional dependencies whose left side is not a superkey. Relational databases are in third normal form (3NF) with respect to a set of functional dependencies if, for all functional dependencies in F + of the form α →β, where α ⊆R and β ⊆R, at least one of the following holds: α →β is a trivial functional dependency or α is a superkey for R. [end of text]
The Banker-schema example demonstrates that the relation schema does not have a dependency-preserving, lossless-join decomposition into BCNF. However, it turns out to be in 3NF. The algorithm for finding a dependency-preserving, lossless-join decomposition into 3NF is presented in Figure 7.14, which uses a canonical cover for the given set of dependencies. The algorithm ensures the preservation of dependencies by explicitly building a schema for each dependency in a canonical cover. It guarantees that the decomposition is a lossless-join decomposition by ensuring that at least one schema contains a candidate key for the schema being decomposed. The algorithm is also called the 3NF synthesis algorithm, since it takes a set of dependencies and adds one schema at a time, instead of decomposing the initial schemarepeatedly. The result is not uniquely deﬁned, since a set of functional dependencies can vary. [end of text]
The algorithm for finding a dependency-preserving, lossless-join decomposition into 3NF is shown in Figure 7.14. The set of dependencies Fc used in the algorithm is a canoni-1, and the original deﬁ-nition of 3NF was in terms of transitive dependencies. The algorithm ensures the preservation of dependencies by explicitly building a schema for each dependency in a canonical cover. It guarantees a lossless-join decomposition by guaranteeing that at least one schema contains a candidate key for the schema being decomposed. The algorithm is also called the 3NF synthesis algorithm, since it takes a set of dependencies and adds one schema at a time, instead of decomposing the initial schemarepeatedly. The result is not uniquely deﬁned, since a set of functional dependencies is not uniquely deﬁned. [end of text]
BCNF and 3NF have advantages in obtaining a 3NF design without sacrificing lossless join or dependency preservation. However, there are disadvantages to 3NF, such as the repetition of information and the cost of null values. SQL does not provide a way to specify functional dependencies, except for the special case of superkeys using primary keys or unique constraints. Materialized views can reduce the cost of testing functional dependencies in a BCNF decomposition that is not dependency preserving. [end of text]
In the context of relational databases, 3NF offers advantages over BCNF in terms of possible 3NF designs without sacrificing lossless join or dependency preservation. However, 3NF also has disadvantages, such as the need for null values to represent meaningful relationships and the repetition of information. The repetition of information is illustrated in the Banker-schema, where the information indicating that Johnson is working at the Perryridge branch is repeated. To address this issue, SQL does not provide a way to specify functional dependencies, except for the special case of declaring superkeys by using primary keys or unique constraints. Materialized views can be used to enforce functional dependencies, reducing the cost of testing such dependencies. [end of text]
The textbook section is 289. [end of text]
Some relation schemas, even though they are in BCNF, do not seem to be sufﬁciently normalized, in the sense that they still suffer from the problem of repetition of information. Consider again our banking example. Assume that, in an alternative design for the bank database schema, we have the schema BC-schema = (loan-number, customer-name, customer-street, customer-city). The astute reader will recognize this schema as a non-BCNF schema because of the functional dependency customer-name →customer-street customer-city that we asserted earlier, and because customer-name is not a key for BC-schema. However, assume that our bank is attracting wealthy customers who have several addresses (say, a winter home and a summer home). Then, we no longer wish to enforce the functional dependency customer-name →customer-street customer-city. If we move this functional dependency, we find BC-schema to be in BCNF with respect to our modiﬁed set of functional dependencies. Yet, even though BC-schema is now in BCNF, we still have the problem of repetition of information that we had earlier. To deal with this problem, we must deﬁne a new form of constraint, called a multivalued dependency. As we did for functional dependencies, we shall use multivalued dependencies to deﬁne a normal form for relation schemas. This normal form, called fourth normal form (4NF), is more restrictive than BCNF. We shall see that every 4NF
Multivalued dependencies do not rule out the existence of tuples with the same A value but different B values. They require that other tuples of a certain form be present in the relation. For this reason, functional dependencies sometimes refer to them as equality-generating dependencies, and multivalued dependencies are referred to as tuple-generating dependencies. Relational databases allow for both multivalued and functional dependencies, but multivalued dependencies are more complex and require additional constraints. [end of text]
The textbook summarizes the concepts of 4NF, multivalued dependencies, and decomposition algorithms in a concise manner. It provides a clear understanding of how to convert BC schemas into 4NF using functional and multivalued dependencies. The text also explains how to decompose BC schemas into 4NF using inference rules. [end of text]
The multivalued dependency customer-name →→customer-street customer-city holds, but no nontrivial functional dependencies hold. Decomposing BC-schema into a fourth normal form decomposition improves the database design. [end of text]
The analogy between 4NF and BCNF applies to the algorithm for decomposing schemas into 4NF. Figure 7.19 shows the 4NF decomposition algorithm. It is identical to the BCNF decomposition algorithm of Figure 7.13, except that it uses multivalued, instead of functional, dependencies and uses the restriction of D+ to Ri. Following the algorithm, we decompose Borrower-schema = (customer-name, loan-number) and Customer-schema = (customer-name, customer-street, customer-city) to create Borrower-Loan and Customer-Street-Customer-City schemas, which are in 4NF, eliminating the redundancy of BC-schema. [end of text]
Lossless-join decompositions of relation schemas are preserved by multivalued dependencies. [end of text]
The fourth normal form is by no means the "ultimate" normal form. Multivalued dependencies help understand and tackle some forms of repetition of information that cannot be understood in terms of functional dependencies, and lead to the project-join normal form (PJNF). Second normal form (2NF) is of historical interest only, and is simply deﬁned and left to you to experiment with. [end of text]
In this section, we study how normalization fits into the overall database design process and examine the implications of different approaches to database design, including the universal relation approach. We also discuss practical issues in database design, including denormalization for performance and examples of bad design that are not detected by normalization. [end of text]
When an E-R diagram is carefully deﬁned, the table generated should not need further normalization. However, functional dependencies exist between attributes of entities, which can lead to non-binary relationships. Normalization can be done formally as part of data modeling, or left to the designer's intuition. [end of text]
The second approach to database design starts with a single relation schema and decomposes it, aiming for a lossless-join decomposition. This involves identifying all relevant attributes and computing the natural join of the decomposed database. Tuples that disappear during the join are considered dangling tuples, which are not part of the final database. Silberschatz-Korth-Sudarshan discusses this approach in Chapter 7 of Relational Database Design, 4th Edition. [end of text]
In database design, universal relations are used to store incomplete information, while null values are used to represent incomplete information. Normal forms generate good database designs from the point of view of representation of incomplete information. Returning to the example of Figure 7.20, we would not want to allow storage of the fact “There is a loan (whose number is unknown) to Jones in the amount of $100.” This is because the only way to relate customer-name and amount is through loan-number. If we do not know the loan number, we cannot distinguish this loan from other loans with unknown numbers. The normal forms do not allow us to store undesirable incomplete information. Another consequence of the universal relation approach is that attribute names must be unique in the universal relation. We cannot use name to refer to both customer-name and branch-name. It is generally preferable to use uniquenames, but if we deﬁne our relation schemas directly, we can obtain relations on schemas such as the following for our banking example: branch-loan (name, number) loan-customer (number, name) amt (number, amount) [end of text]
Occasionally, database designers choose a schema with redundant information, leading to performance improvements for specific applications. The penalty for not using a normalized schema is the cost of maintaining redundant data consistency. For example, displaying account holder names along with account numbers and balances requires a join between account and depositor. Denormalizing the schema to make it non-normalized can improve performance for time-critical operations. [end of text]
Normalization is a technique used to reduce data redundancy and improve data integrity. It involves grouping related data into tables and creating a view that combines the results of these tables. Materialized views are a specific type of view that are stored in the database and updated when the data used in the view is updated. However, materialized views have space and time overheads, and they should not be used unless it is necessary. Other design issues include the need for space and time overheads, and the need for a new relation every year. Representations such as company-year are called crosstab and are widely used in spreadsheets and data analysis tools. While they are useful for display, they are not desirable in a database design. [end of text]
Normalization can lead to bad database design, as it introduces functional dependencies that are not necessary. Representations like company-year are also problematic, as they require modifications and more complex queries. Crosstab representations are useful for display but not ideal for database design. [end of text]
In this chapter, we introduced the concept of functional dependencies, and showed how to reason with them. We laid special emphasis on what functional dependencies are logically implied by a set of dependencies, and defined the notion of a canonical cover, which is a minimal set of functional dependencies equivalent to a given set. We also introduced the concept of decomposition and showed that decompositions must be lossless-join decompositions, and preferably be dependency preserving. If the decomposition is dependency preserving, given a database update, all functional dependencies can be veriﬁed from individual relations, without computing a join of relations in the decomposition. We then presented Boyce–Codd Normal Form (BCNF), relations in BCNF are free from the pitfalls outlined earlier. We outlined an algorithm for decomposing relations into BCNF. There are relations for which there is no dependency-preserving BCNF decomposition. We used the canonical covers to decompose a relation into 3NF, which is asmall relaxation of the BCNF condition. Relations in 3NF may have some redundancy, but there is always a dependency-preserving decomposition into 3NF. We presented the notion of multivalued dependencies, which specify constraints that cannot be speciﬁed with functional dependencies alone. We deﬁned fourth normal form (4NF) with multivalued dependencies. Section C.1.1 of the appendix gives details on reasoning about multivalued dependencies. Other normal forms, such as PJNF and DKNF, eliminate
These properties may indicate a bad relational-database design: 
1. Inconsistent data types: If a column has a different data type than another, it may indicate a problem with data type compatibility.
2. Inconsistent data relationships: If a column is a foreign key to another table, but the relationship is not defined, it may indicate a problem with the foreign key definition.
3. Inconsistent data relationships: If a column is a primary key to another table, but the relationship is not defined, it may indicate a problem with the foreign key definition.
4. Inconsistent data relationships: If a column is a foreign key to another table, but the relationship is not defined, it may indicate a problem with the foreign key definition.
5. Inconsistent data relationships: If a column is a primary key to another table, but the relationship is not defined, it may indicate a problem with the foreign key definition.
6. Inconsistent data relationships: If a column is a foreign key to another table, but the relationship is not defined, it may indicate a problem with the foreign key definition.
7. Inconsistent data relationships: If a column is a primary key to another table, but the relationship is not defined, it may indicate a problem with the foreign key definition.
8. Inconsistent data relationships: If a column is a foreign key to another table, but the relationship is not defined, it may indicate a problem with the foreign key definition.
9. Inconsistent data relationships: If
The given set F of functional dependencies is sufficient to decompose the relation R into a lossless-join decomposition. [end of text]
Relational databases are a type of database management system (DBMS) that uses tables to organize and store data. They are designed to be efficient and scalable, allowing for the storage of large amounts of data. Relational databases use a set of rules to determine how data is stored and accessed, and they are commonly used in various fields such as finance, healthcare, and education. The McGraw-Hill Companies' book, Relational Database Design, provides a comprehensive introduction to the concepts and techniques of relational databases. [end of text]
Axioms (reﬂexivity, augmentation, and transitivity) are sound. [end of text]
A one-to-one relationship exists between accounts and customers, while a many-to-one relationship exists between accounts and customers. [end of text]
To prove that the rule γ →β, then α →γ is not sound, we need to show a relation r that satisfies α →β and γ →β but does not satisfy α →γ. This can be done by constructing a relation r that is consistent with the given rules but violates α →γ. For example, consider the relation r = {α, β, γ}. This relation satisfies α →β and γ →β, but it does not satisfy α →γ. Therefore, the rule is not sound. [end of text]
The textbook explains how to use the augmentation rule to show that if α →β, then α →αβ, and then apply the transitivity rule. [end of text]
A, B, C, D, E
Candidate keys for R are: A, B, C, D, E. [end of text]
The section "cover Fc." refers to the first chapter of a book. [end of text]
is more efﬁcient than the one presented in Figure 7.7, which computes α+ correctly. [end of text]
The SQL query to test whether b →c holds on a relation is:
```
SELECT 1 FROM table WHERE b = c;
```
An SQL assertion that enforces the functional dependency is:
```
SELECT 1 FROM table WHERE b = c AND c = d;
```
The assertion checks that b = c and c = d for all rows in the table. [end of text]
The textbook explains the concept of lossless-join decomposition for relational databases, where the result of the join operation is not equal to the original relation. It provides an example of a relation r on schema R with FDs C, D, and E, and shows how to compute the result of the join using the addin procedure. [end of text]
Let \( \text{ri} = \Sigma^* \cup \Sigma^* \cup \Sigma^* \cup \ldots \) where \( \Sigma^* \) is the set of all strings that can be generated by the grammar. Show that \( u \subseteq r_1 r_2 \ldots r_n \). [end of text]
Decomposition is the process of breaking down a complex object or system into its constituent parts or components. [end of text]
A lossless-join decomposition ensures that at least one schema contains a candidate key, preventing tuples from being duplicated during the decomposition process. [end of text]
Desirable qualities are desirable traits that are desirable in a person or a situation. [end of text]
There exist at least three distinct lossless-join decompositions of R′ into BCNF. [end of text]
R of Exercise 7.2 involves finding the sum of the first 100 positive integers. [end of text]
Transitively dependent attributes in a relation schema are not prime, ensuring 3NF. [end of text]
The textbook defines a proper subset γ of α such that γ →β. It also defines partial dependence as β being partially dependent on α. The textbook defines a 3NF schema as one in which each attribute meets one of the criteria of being in a candidate key or not partially dependent on a candidate key. It then shows that every 3NF schema is in 2NF by demonstrating that every partial dependency is a transitive dependency. [end of text]
2NF, but not higher-order normal form. [end of text]
In BCNF, but not in 4NF. [end of text]
The book discusses the development and evolution of relational database design theory, including Codd's paper, Armstrong's axioms, Ullman's proofs, Maier's theory, Graham et al.'s formal aspects, and Ullman's algorithm for lossless join decomposition. It also covers BCNF, Biskup's algorithm, and fundamental results on lossless join property. The book provides a detailed overview of the object-oriented data model, object-relational data model, XML, and SQL. It also discusses the XML language and its applications in data exchange. [end of text]
Oracle provides a variety of tools for database design, querying, report generation, and data analysis, including OLAP. The suite includes tools for forms development, data modeling, reporting, and querying, and supports UML for development modeling. It also supports XML for data exchange with other UML tools. The major database design tool in the suite is Oracle Designer, which translates business logic and data flows into schema definitions and procedural scripts. It supports modeling techniques such as E-R diagrams, information engineering, and object analysis and design. Oracle Designer stores the design in Oracle Repository, which serves as a single point of metadata for the application. The suite also contains application development tools for generating forms, reports, and various aspects of Java and XML-based development. The business intelligence component provides JavaBeans for analytic functionality such as data visualization, querying, and analytic calculations. Oracle also has an application development tool for data warehousing, OracleWarehouse Builder. Warehouse Builder is a tool for design and deployment of all aspects of a data warehouse, including schema design, data mapping and transforma-tions, data load processing, and metadata management. Oracle Warehouse Buildersupports both 3NF and star schemas and can also import designs from Oracle Designer. [end of text]
Oracle's Oracle Internet Development Suite includes Oracle Designer, a database design tool that translates business logic and data flows into schema definitions and procedural scripts for application logic. It supports E-R diagrams, information engineering, and object analysis and design. Oracle Repository stores design information and provides configuration management for database objects, forms applications, Javaclasses, XML ﬁles, and other types of files. The suite also includes application development tools for generating forms, reports, and various aspects of Java and XML-based development. The business intelligence component provides JavaBeans for analytic functionality such as data visualization, querying, and analytic calculations. Oracle also has an application development tool for data warehousing, OracleWarehouse Builder. Warehouse Builder is a tool for design and deployment of all aspects of a data warehouse, including schema design, data mapping and transformation, data load processing, and metadata management. [end of text]
Oracle Discoverer is a Web-based tool for ad-hoc querying, report generation, and data analysis, including OLAP. It allows users to drill up and down on result sets, pivot data, and store calculations as reports. Discoverer has wizards to help users visualize data as graphs. Oracle9i supports a rich set of analytical functions, such as ranking and moving aggregation in SQL. Discoverer's ad hoc query interface can generate SQL that takes advantage of this functionality and can provide end-users with rich analytical functionality. Since the processing takes place in the relational database management system, Discoverer does not require a complex client-side calculation engine and there is a version of Discoverer that is browser-based. Oracle Express Server is a multidimensional database server that supports a wide variety of analytical queries as well as forecasting, modeling, and scenario management. [end of text]
Oracle9i's introduction of OLAP services has led to a model where all data resides in the relational database management system and calculations are done in SQL. This model provides a Java OLAP application programmer interface. Oracle has moved away from a separate multidimensional storage engine and has integrated multidimensional modeling with data warehouse modeling. The model offers fast response times for many calculations and provides a performance challenge. Oracle has added SQL support for analytical functions and extended materialized views to permit analytical functions. [end of text]
Oracle9i supports all core SQL:1999 features fully or partially, with some minor exceptions such as distinct data types. It supports a large number of other language constructs, some of which are Oracle-speciﬁc in syntax or functionality. Oracle provides PL/SQL and Java for procedural languages, and supports XML data types. [end of text]
Oracle supports object-relational constructs, including object types, collection types, object tables, table functions, object views, methods, and XML data types. PL/SQL and Java are supported through a Java virtual machine inside the database engine. [end of text]
Oracle provides SQLJ for Java and JDBC, allowing developers to generate Java class deﬁnitions for database types. Triggers can be written in PL/SQL or Java, and Oracle supports row and statement triggers. Triggers can be executed on DML operations, but view triggers are not supported. Oracle allows creating instead of triggers for views that cannot be DML-affected. Triggers on views can be executed manually or automatically based on view deﬁnitions. Oracle executes triggers instead of DML operations, providing a mechanism to circumvent restrictions on DML operations against views. [end of text]
Oracle provides triggers for various operations, including row and statement triggers. Triggers can be written in PL/SQL or Java, and can be either before or after DML operations. Oracle supports row triggers and statement triggers for DML operations. View triggers are created for views that cannot be subject to DML operations. Oracle allows users to create instead of triggers on views to specify manual operations. Triggers on views execute a DML operation, providing a mechanism to circumvent view restrictions. [end of text]
In Oracle, a database is composed of information stored in files and accessed through an instance, which is a shared memory area and a set of processes that interact with the data in the files. Tables are organized into table spaces, which contain data and storage for triggers and stored procedures. Temporary table spaces are used for sorting data. Oracle allows moving data between databases by copying files and exporting/importing data. Segments are used for data movement between databases, and temporary segments are used during sort operations. [end of text]
A database consists of one or more logical storage units called table spaces, each of which can store data dictionaries and storage for triggers and stored procedures. These structures can be either managed by the operating system or raw devices. Oracle databases typically have the following table spaces: the system table space, which contains data dictionaries and storage for triggers and stored procedures, and table spaces created to store user data, which are separate from the system data. Temporary table spaces are also used for sorting data and moving data between databases. [end of text]
Data segments, index segments, temporary segments, and rollback segments are types of segments in a table space. Data segments store table data, index segments store indexes, temporary segments are used for sort operations, and rollback segments contain undo information. Extent is a level of granularity at which space is allocated at a granularity of database blocks. [end of text]
The percentage of space utilization at which a database block is considered full and at which no more rows will be inserted into that block. Leaving some freespace in a block allows the existing rows to grow in size through updates, without running out of space in the block. Oracle supports nested tables, temporary tables, and hash clusters. Index-organized tables use an index key to store records, requiring a unique key for each row. Secondary indices on nonkey columns are different from indices on a regular heap table. Index-organized tables can improve performance and space utilization. Indexes can be either B-tree or B+-tree. Index entries have a physical row-id corresponding to where the index was created or last rebuilt and a value for the unique key. Index compression can save space. [end of text]
A standard table in Oracle is heap organized, with rows not based on values but fixed when inserted. Oracle supports nested tables, where columns affect partition. Oracle supports temporary tables, where data is stored in a separate table. Cluster organization implies rows belong in a specific place, with hash clustering for efficient access. [end of text]
In an index organized table, records are stored in an Oracle B-tree index instead of a heap. An index-organized table requires a unique key for indexing. While a regular index contains the key and row-id, an index-organized table replaces the row-id with column values for remaining columns. Compared to a heap table, an index-organized table improves performance by reducing the number of probes and space utilization by eliminating the need for a fixed row-id. Secondary indices on nonkey columns of an index-organized table are different from indices on a regular heap table. In a heap table, each row has a fixed row-id. However, a B-tree is reorganized as it grows or shrinks and there is no guarantee that a row will stay in a fixed place. Hence, a secondary index on an index-organized table contains logical row-ids instead. A logical row-id consists of a physical row-id and a key value. The physical row-id is referred to as a "guess" since it could be incorrect if the row has been moved. If so, the key value is used to access the row; however, this access is slower than if the guess had been correct, since it involves a traversal of the B-tree for index-organized table from the root to the leaf nodes, potentially incurring several disk I/Os. If a table is highly volatile and a large percentage of guesses are likely to be wrong, it can be better to create a secondary index with only key
Oracle supports B-tree indices, which are created on columns to optimize storage and performance. Index entries format includes columns, row-id, and preﬁx compression for distinct combinations of values. [end of text]
Bitmap indices use a bitmap representation for index entries, leading to substantial space savings when indexed columns have a moderate number of distinct values, while Oracle uses a B-tree structure to store entries. Bitmap indices allow multiple indices on the same table to be combined in the same access path, with Boolean operations to combine multiple indices. Oracle can convert row-ids to the compressed bitmap representation, allowing Boolean operations to be performed on the bitmap. Join indices are an index where the key columns are not in the referenced table, supported primarily for star schemas. [end of text]
Bitmap indices use a bitmap representation for index entries, leading to substantial space savings when indexed columns have a moderate number of distinct values. Oracle uses a B-tree structure to store the entries, but where a regular index on a column would have entries of the form< col1 >< row-id >, a bitmap index entry has the form< col1 >< startrow-id >< endrow-id >< compressedbitmap>. The compression algorithm is a variation of Byte-Aligned Bitmap Compression (BBC). It stores adjacent zeros in the bitmap, and the compression algorithm deals with such strings of zeros. Bitmap indices allow multiple indices on the same table to be combined in the same access path. For example, Oracle can use Boolean operations to combine multiple indices by putting a row-id-to-bitmap operator on top of the index access in the execution plan. [end of text]
In addition to creating indices on one or multiple columns of a table, Oracle allows indices to be created on expressions involving one or more columns, such as upper(name), which returns the uppercase version of a string. For example, by creating an index on the expression upper(name), where upper is a function that returns the uppercase version of a string, and name is a column, it is possible to do case-insensitive searches on the name column. In order to ﬁnd all rows with name "van Gogh" efficiently, the condition upper(name) = 'VAN GOGH' would be used in the where clause of the query. Oracle then matches the condition with the index deﬁnition and concludes that the index can be used to retrieve all the rows matching "van Gogh" regardless of how the name was capitalized when it was stored in the database. A function-based index can be created as either a bitmap or a B-tree index. [end of text]
A join index is an index where the key columns are not in the table referenced by the row-ids in the index. Oracle supports bitmap join indices primarily for use with star schemas. For example, a bitmap join index on a product dimension table with a product name key column could retrieve rows for a specific product. The rows in the fact and dimension tables correspond based on a join condition. When a query is performed, the join condition is part of the index metadata. [end of text]
The optimizer looks for join conditions in the where clause of a query to determine if a join index is applicable. Oracle allows bitmap join indices with multiple key columns and can combine them with other indices on the same table by using Boolean bitmap operations. Domain indices can be combined with other indices in the same access path by converting between row-id and bitmap representations and using Boolean bitmap operations. Partitioning tables and indices can be used to implement rolling windows of historical data efficiently. [end of text]
Oracle allows tables to be indexed by index structures that are not native to Oracle. This feature enables software vendors to develop domain indices for text, spatial data, and images, with indexing beyond the standard Oracle index types. Domain indices must be registered in the data dictionary, along with the operators they support. The optimizer considers domain indices as one of the possible access paths for a table. Cost functions can be registered with the operators so that the optimizer can compare the cost of using the domain index to those of other access paths. [end of text]
Oracle supports horizontal partitioning, which enables efficient backup and recovery, faster loading, and improved query performance. Range partitioning is particularly suited to date columns in a data warehouse environment. [end of text]
In range partitioning, partitioning criteria are ranges of values, particularly well suited for date columns in data warehouses, where historical data is loaded at regular intervals. Each data load creates a new partition, making the loading process faster and more efficient. The system loads data into a separate table with the same column definition, making the table anew partition of the original partitioned table. This process is nearly instantaneous. [end of text]
In Oracle, materialized views allow the result of an SQL query to be stored in a table and used for later query processing. Oracle supports automatic query rewrites that take advantage of any useful materialized view when resolving a query. The rewrite consists of changing the query to use the materialized view instead of the original tables in the query. In addition, the rewrite may add additional joins or aggregate processing as required. Materialized views are used in data warehousing to speed up query processing but are also used for replication in distributed and mobile environments. [end of text]
In hash partitioning, a hash function maps rows to partitions based on partitioning columns, which helps distribute rows evenly among partitions or optimize query performance for partitionwise joins. [end of text]
In composite partitioning, range partitioning is combined with hash partitioning to achieve a balanced partitioning strategy. [end of text]
In list partitioning, the values associated with a particular partition are stated in an alist. This type of partitioning is useful when the data in the partitioning column have relatively small discrete values, such as a state column in a table. For instance, a table with a state column can be implicitly partitioned by geographical region if each partition list includes states that belong in the same region. [end of text]
Materialized views in Oracle allow storing results of SQL queries in tables, enhancing query performance. They update when referenced tables are updated, aiding replication in distributed and mobile environments. Materialized views are used for data warehousing to speed up query processing, but are also used for replication in distributed and mobile environments. [end of text]
Oracle's query processing engine supports various methods for accessing data, including full table scan and index scan. The full table scan retrieves information about blocks in the table, while the index scan uses a start and stop key to scan relevant parts of the index. [end of text]
Data can be accessed through various methods, including full table scan and index scan. Index scan retrieves columns not part of the index, while full table scan scans the entire table. [end of text]
The summary of the section is shorter than the original section. It retains conceptual information and important definitions while being shorter than the original. [end of text]
In Chapter 14, we discussed the general topic of query optimization. Here, we discussed Oracle's query optimization techniques, including view merging, complex view merging, subquery flattening, materialized view rewrite, and star transformation. These techniques are used to generate cost estimates for both the standard version of the query and optimized versions. [end of text]
Oracle performs query optimization in several stages, including view merging, complex view merging, subquery flattening, materialized view rewrite, and star transformation. These techniques generate a cost estimate and a complete plan for both standard and optimized versions of queries. Oracle uses this information to make an intelligent decision about which query to execute based on cost estimates. [end of text]
Oracle uses subqueries to probe fact table columns, combining bitmaps to access matching rows. The bitmaps are generated from different subqueries and combined by a bitmap. The resultant bitmap can be used to access matching rows. Oracle uses cost estimates based on optimizer decisions and search space issues. The optimizer selects join order, methods, and access paths based on statistics. Oracle uses sampling to speed up statistics gathering and automatic selection of the smallest adequate sample percentage. Oracle uses CPU and disk I/Os in the optimizer cost model. Oracle uses sampling to speed up join order and access paths. Oracle uses column statistics for optimizer statistics and partitioned tables for parallel execution. [end of text]
Oracle's cost-based optimizer determines join order, methods, and access paths by analyzing statistics and optimizing operations. It uses height-balanced and frequency histograms to gather statistics, and monitors modiﬁcation activity to ensure appropriate statistics are updated. Oracle also tracks column usage and creates histograms for where clauses. It uses sampling to speed up statistics gathering and automatically chooses the smallest adequate sample percentage. It also determines whether the distribution of marked columns merits histogram creation. Oracle's optimizer uses CPU cost and disk I/Os in the cost model. To balance these components, it stores measures about CPU speed and disk I/O performance in optimizer statistics. Oracle's package gathers optimizer statistics using sampling. Queries involving nontrivial joins require careful planning to avoid long optimizer runs. Silberschatz-Korth-Sudarshan: Database System Concepts, Fourth Edition III. Object-Based Databases and XML8. Object-Oriented Databases Chapter 25 [end of text]
The optimizer in partitioned tables tries to match conditions with partitioningcriteria to avoid unnecessary access to partitions, improving speed for small subsets. [end of text]
Oracle's parallel execution feature divides work into smaller, independent tasks, enhancing speedup for computationally intensive operations. This method is particularly useful for data-intensive tasks requiring large datasets.
Oracle achieves parallelism by partitioning data among processes by hashing on join columns, each table scanned in parallel by a set of processes, and rows determined by hash functions on join column values. Oracle solves the problem of determining range boundaries by dynamically sampling rows before deciding on range boundaries. [end of text]
The processes involved in the parallel execution of an SQL statement consist of a coordinator process and a number of parallel server processes. The coordinator assigns work to the parallel servers, collects data, and returns to the user process. The degree of parallelism is determined by the optimizer and can be throttled back. The parallel servers operate on a producer/consumer model, with producers performing operations and consumers using the results. Servers communicate through shared-memory hardware and network connections. The cost of accessing data on disk is not uniform among processes. Knowledge about device-to-node and device-to-process affinity is used for parallel execution. [end of text]
Oracle's multiversion concurrency control provides read-consistent snapshots, allowing read-only queries to interfere with other database activity. The Flashback Query feature allows users to set a SCN or wall-clock time in their session. [end of text]
Oracle's multiversion concurrency control differs from the concurrency mechanisms used by most other database vendors. Read-only queries are given a read-consistent snapshot, which is a view of the database as it existed at a specific point in time, containing all updates that were committed by that point in time, and not containing any updates that were not committed at that point in time. Read locks are not used and read-only queries do not interfere with other database activity in terms of locking. (This is basically the multiversion two-phase locking protocol described in Section 16.5.2.)Oracle supports both statement and transaction level read consistency: At the beginning of the execution of either a statement or a transaction (depending on what level of consistency is used), Oracle determines the current system change number (SCN). The SCN essentially acts as a timestamp, where the time is measured in terms of transaction commits instead of wall-clock time. If in the course of a query a data block is found that has a higher SCN than the one being associated with the query, it is evident that the data block has been modified after the time of the original query's SCN by some other transaction that may or may not have committed. Hence, the data in the block cannot be included in a consistent view of the database as it existed at the time of the query's SCN. Instead, an olderversion of the data in the block must be used; speciﬁcally, the one that has the highest
In a database system, once a change is committed, there is no way to get back to the previous state of the data other than performing point-in-time recovery from backups. Oracle supports two ANSI/ISO isolation levels, "read committed" and "serializable". The Flashback Query feature provides a simpler mechanism to deal with user errors. Oracle supports two levels of isolation: statement-level read consistency and transaction-level read consistency. Oracle uses row-level locking and table locks to prevent inconsistencies due to DML and DDL activities. Oracle detects deadlocks automatically and resolves them by rolling back one of the involved transactions. Oracle supports autonomous transactions, independent transactions generated within other transactions. When Oracle invokes an autonomous transaction, it generates a new transaction in a separate context. The new transaction can be either committed or rolled back before control returns to the calling transaction. Oracle supports multiple levels of nesting of autonomous transactions.25.5.2Basic Structures for Recovery In order to understand how Oracle recovers from a failure, such as a disk crash, it is important to understand the basic structures involved. In addition to the data ﬁles that contain tables and indices, there are control ﬁles, redo logs, archived redo logs, and rollback segments. The control ﬁle contains various metadata that are needed to operate the database, including information about backups. Oracle records any transactional modification of a database buffer in the redo log, which consists of two or more ﬁles. It logs the modification as
In order to understand how Oracle recovers from a disk crash, it is important to understand the basic structures involved, including data ﬁles, control ﬁles, redo logs, archived redo logs, and rollback segments. Oracle records any transactional modification of a database buffer in the redo log, which includes two or more ﬁles. It logs the modiﬁcation as part of the operation that causes it, regardless of whether the transaction commits. It logs changes to indices and rollback segments as well as changes to table data. As the redologs fill up, they are archived by one or several background processes (if the database is running in archivelog mode). Oracle supports hot backups, which are performed on an online database subject to transactional activity. During recovery, Oracle performs two steps to reach a consistent state of the database as it existed just before the failure. First, it rolls forward by applying the (archived) redo logs to the backup. Second, it rolls back uncommitted transactions using the rollback segment. Recovery on a database that has been subject to heavy transactional activity can be time-consuming, as Oracle supports parallel recovery in which several processes are used to apply redo information simultaneously. [end of text]
Oracle provides a managed standby database feature, which is the same as remote backups. A standby database is a copy of the regular database installed on a separate system. If a catastrophic failure occurs on the primary system, the standby system is activated and takes over, minimizing the effect on availability. Oracle keeps the standby database up to date by constantly applying archived redo logsthat are shipped from the primary database. The backup database can be brought online in read-only mode and used for reporting and decision support queries. [end of text]
Dedicated server memory is divided into three categories: software code areas, system global areas (SGA), and program global areas (PGA). The SGA is allocated for each process to hold local data and control information. The PGA is shared among multiple processes. The multithreaded server architecture allows multiple processes to execute SQL statements concurrently. [end of text]
The memory used by Oracle falls into three categories: software code areas, system global area (SGA), and program global area (PGA). The system code areas are the memory where the Oracle server coderesides. A PGA is allocated for each process to hold its local data and control information. [end of text]
This area contains stack space for various session data, private memory for SQL statements, and sorting and hashing operations. The SGA is a memory area for structures shared among users. The shared pool is used for structures shared among users and for data structures representing SQL statements. The Oracle SGA is made up of several major structures, including the buffer cache, redo log buffer, shared pool, and dictionary information. The shared pool stores data structures and caches for SQL statements, while the SGA manages the internal representation of SQL statements and procedural code. The Oracle SGA allows sharing of internal representations among users, and the shared pool saves compilation time by minimizing memory usage for each user. The Oracle SGA also includes caches for dictionary information and control structures. The multithreaded server configuration increases the number of users that a given number of server processes can support by sharing server processes among SQL statements. The Oracle SGA is made up of several major structures, including the buffer cache, redo log buffer, shared pool, and dictionary information. The shared pool stores data structures and caches for SQL statements, while the SGA manages the internal representation of SQL statements and procedural code. The Oracle SGA allows sharing of internal representations among users, and the shared pool saves compilation time by minimizing memory usage for each user. The Oracle SGA also includes caches for dictionary information and control structures. The multithreaded server configuration increases the number of users that a given number of server processes can support by sharing server processes among
There are two types of processes that execute Oracle server code: server processes that process SQL statements and background processes that perform various admin-istrative and performance-related tasks. Some of these processes are optional, and in some cases, multiple processes of the same type can be used for performance reasons. Some of the most important types of background processes are: Database writer, Log writer, Checkpoint, System monitor, Process monitor, Recoverer, and Archiver. [end of text]
The multithreaded server configuration increases the number of users that a given number of server processes can support by sharing server processes among state-ments. It differs from the dedicated server architecture in these major aspects: a background dispatch process routes user requests to the next available server process, a request queue and response queue in the SGA, and a session-speciﬁc data store in the SGA. [end of text]
Oracle9i Real Application Clusters allows multiple instances of Ora-cle to run against the same database, enabling scalability and availability in both OLTP and data warehousing environments. [end of text]
Oracle9i Real Application Clusters can achieve high availability by using multiple instances and having them access the same database. This leads to technical issues such as overlaps in data. Oracle supports a distributed lock manager and cache fusion features to overcome these challenges. [end of text]
Oracle provides support for replication and distributed transactions with two-phase commit. It supports multiple master sites for replicated tables. Oracle supports updatable snapshots and multiple master sites for replicated data. External data sources can be used for data warehousing. External tables can be referenced in queries as if they were regular tables. [end of text]
Oracle supports multiple master sites for the same data, where all mastersites act as peers. Replicated tables can be updated at any of the master sites and the update is propagated to the other sites. The updates can be propagated either asynchronously or synchronously. [end of text]
Oracle supports queries and transactions spanning multiple databases on different systems. It uses gateways to include non-Oracle data-bases and transparently supports transactions spanning multiple sites with a two-phase-commit protocol. [end of text]
Oracle's SQL*Loader and External Tables are mechanisms for supporting external data sources, such as ﬂat ﬁles, in data warehousing environments. These tools allow for fast parallel loads and various data filtering operations. External tables provide a convenient way to reference external data in queries, allowing for data transformation and loading operations in a data warehousing environment. [end of text]
Oracle's SQL*Loader is a direct load utility that supports fast parallel loading of large datasets. It supports various data formats and filters, making it suitable for loading data from external files. [end of text]
Oracle allows external data sources, such as flat files, to be referenced in queries as if they were regular tables. An external table is defined by meta-data, mapping external data into Oracle columns. An access driver is needed to access external data. Oracle provides a default driver for flat files. The external table feature is primarily intended for ETL operations in a data warehousing environment. Data can be loaded into the data warehouse using create table table as select ... from external table where ... Transforms and filters can be done as part of the same SQL statement. Scalability can be achieved by parallelizing access to the external table. [end of text]
Oracle provides users with tools for system management and application development. It offers a graphical user interface and various wizards for schema management, security management, instance management, storage management, and job scheduling. The database administrator can control processing power division among users or groups, prevent ad hoc queries, and set limits for parallelism and time limits. Persistent programming languages add database features to existing programming languages, while object-relational databases extend the relational data model by providing a richer type system. Object-relational database systems provide a convenient migration path for users of relational databases who wish to use object-oriented features. [end of text]
Oracle Enterprise Manager is a graphical user interface for managing Oracle database systems. It offers wizards for schema, security, instance, storage, and job management, as well as performance monitoring tools. It suggests the most cost-effective indices under workload conditions. [end of text]
The nested relational model allows for not-first-normal form relations and direct representation of hierarchical structures, while extending SQL to include various object-relational features. [end of text]
In Chapter 7, we deﬁned 1NF, which requires all attributes to have atomic domains. Nested relational models extend the relational model by allowing domains to be either atomic or relation-valued, making it easier to represent complex objects in a single tuple. [end of text]
The textbook explains how to decompose a relation into 4NF using the specified schemas, showing how nested relations can lead to a more complex model. It then proposes a non-nested relational view that eliminates the need for users to include joins in their queries. [end of text]
Nested relations and object-oriented data models have been extensions to the relational model, allowing complex types and features such as inheritance and references. With E-R model concepts, complex types can be represented directly without a translation to the relational model. Object-based databases and XML have been introduced to represent E-R model concepts, such as identity, multivalued attributes, and generalization and specialization. [end of text]
The book defines a table with a set of attributes, allowing multivalued attributes in E-R diagrams. Sets are collections, represented directly in multivalued attributes. [end of text]
Structured types in SQL:1999 allow composite attributes of E-R diagrams to be represented directly, while unnamed row types can be used to deﬁne composite attributes. Tables can be created without creating an intermediate type for the table. Structured types can have methods deﬁned on them. [end of text]
Structured types can be declared and used in SQL, with examples like `Publisher` and `Book`. Nested relations are supported in Oracle 8, but use a different syntax. Structured types allow composite attributes to be represented directly, and named types can be used to define composite attributes. Tables can be created with tuples of type `Book`, but can also be defined as arrays of author names instead. Structured types allow methods to be defined on them, with methods body separate from method declaration. In Oracle PL/SQL, table type `%rowtype` denotes the type of rows, and `%type` denotes the type of attribute a of the table. [end of text]
SQL:1999 constructors are used to create values of structured types, while functions other than constructors support other types of operations. Arrays of values can be created in SQL:1999 using constructor functions, and sets and multisets are part of the standard. Future versions of SQL are likely to support sets and multisets. [end of text]
In SQL:1999, constructors are used to create values of structured types, while functions other than constructors are used to create values of non-structured types. Constructors create values of the type, not objects of the type. Arrays can be created in SQL:1999 using constructor functions, and set-valued attributes can be created using enumerations. Sets and multisets are not part of the SQL:1999 standard. Future versions of SQL are likely to support sets and multisets. [end of text]
Inheritance can be at the level of types or at the level of tables. We can use inheritance to store extra information about students and teachers, and to define subtypes of Person. Methods of a structured type are inherited by subtypes, but subtypes can redefine methods using overriding methods. Multiple inheritance is supported in SQL:1999, but draft versions of the standard provide for it. [end of text]
In SQL:1999, multiple inheritance is supported, allowing students and teachers to inherit attributes and methods. However, the standard does not support multiple inheritance. Draft versions of the SQL:1999 standard provided for multiple inheritance, and the final version included multiple inheritance. [end of text]
SQL:1999 does not support multiple inheritance, which means a type can inherit from only one type. Multiple inheritance is not supported by SQL:1999. The SQL:1999 standard requires an extra field at the end of the type definition, whose value is either final or not final. The keyword ﬁnal says that subtypes may not be created from the given type, while not ﬁnal says that subtypes may be created. For example, a teaching assistant may be a student of one department and a teacher in another department. To avoid a conﬂict between the two occurrences of department, we can rename them by using an as clause, as in this deﬁnition of the type TeachingAssistant:create type TeachingAssistantunder Student with (department as student-dept),Teacher with (department as teacher-dept). [end of text]
Subtables in SQL:1999 correspond to the E-R notion of specialization/generalization. For instance, people can be subtables of students and teachers. SQL:1999 allows multiple inheritance, but not in tables. Teachers can be subtables of students and teachers, but only teachers can be accessed. Multiple inheritance is not supported in SQL:1999. [end of text]
Inheritance of types in database systems should be used with care to avoid redundancy and ensure that each entity has exactly one most-speciﬁc type. Object-relational systems can model this feature by using inheritance at the table level, rather than at the type level, and allow multiple types without having a most-speciﬁc type. [end of text]
Inheritance of types should be used with care. A university database may have many subtypes of Person, such as Student, Teacher, FootballPlayer, ForeignCitizen, and so on. Each category is sometimes called a role. A better approach is to allow an object to have multiple types without having a most-speciﬁc type. Object-relational systems can model this feature by using inheritance at the table level, rather than type level. [end of text]
Object-oriented languages allow referencing objects through attributes, which are references to specific types. In SQL:1999, a type is defined with a name, head, and scope, and a table is created with a department type, department, and a table department. To initialize a reference attribute, a tuple with an empty reference is created first, and then the reference is set separately. This approach is based on Oracle syntax. In SQL:1999, the referenced table must have an attribute that stores the identiﬁer of the tuple. The self-referential attribute is added to the create table statement. [end of text]
In object-relational databases, the primary key is used as the identiﬁer when inserting a tuple, and the ref from clause is included in the type definition to specify the self-referential attribute. [end of text]
In this section, we extend SQL to handle complex types, using dot notation for references, and collection-valued attributes. We can query departments by name, head, and address. References simplify joins and make queries more concise. [end of text]
References in SQL are dereferenced by the −> symbol. In the department table, we can use a query to find the names and addresses of the heads: select head−>name, head−>address from departments. References are used to hide join operations; in the example, without references, the department name would be declared a foreign key of the people table. To find the name and address of a department's head, we would need an explicit join of departments and people. References simplify queries significantly. [end of text]
We now consider how to handle collection-valued attributes. Arrays are the only collection type supported by SQL:1999, but we use the same syntax for relation-valuedattributes. An expression evaluating to a collection can appear anywhere that arelation name may appear, such as in a from clause, as the following paragraphs illustrate. We use the table books which we deﬁned earlier. If we want to ﬁnd all books that have the word “database” as one of their key-words, we can use this query:select titlefrom bookswhere ’database’ in (unnest(keyword-set))Note that we have used unnest(keyword-set) in a position where SQL without nested relations would have required a select-from-where subexpression. Silberschatz−Korth−Sudarshan: Database System Concepts, Fourth EditionIII. Object−Based Databases and XML9. Object−Relational Databases351© The McGraw−Hill Companies, 2001 [end of text]
In SQL, the reverse process of transforming a 1NF relation into a nested relation is called nesting. Nesting can be carried out by an extension of grouping in SQL. In the normal use of grouping in SQL, a temporary multiset relation is created for each group, and an aggregate function is applied on the temporary relation. By returning the multiset instead of applying the aggregate function, we can create a nested relation. Suppose that we are given a 1NF relation ﬂat-books, as in Figure 9.2. The following query nests the relation on the attribute keyword:select title, author, Publisher(pub-name, pub-branch) as publisher,set(keyword) as keyword-setfrom ﬂat-booksgroupby title, publisher The result of the query on the books relation from Figure 9.2 appears in Figure 9.4. If we want to nest the author attribute as well, and thereby to convert the 1NF table to a nested table, we can use the query:select title, set(author) as author-set, Publisher(pub-name, pub-branch) as publisher,( select keywordfrom ﬂat-books as Nwhere N.title = O.title) as keyword-set,from ﬂat-books as O The system executes the nested subqueries in the select clause for each tuple generated by the from and where clauses of the outer query. Observe that the attribute O.title from the outer query is used in the nested queries
The transformation of a nested relation into a single flat relation is called unnesting. The books relation has two attributes, author-array and keyword-set, that are collections, and two attributes, title and publisher, that are not. Suppose we want to convert the relation into a single ﬂat relation, with no nested relations or structured types as attributes. We can use the following query to carry out the task:select title, A as author, publisher.name as pub-name, publisher.branchas pub-branch, K as keywordfrom books as B, unnest(B.author-array) as A, unnest (B.keyword-set) as K The variable B in the from clause is declared to range over books. The variable A is declared to range over the authors in author-array for the book B, and K is declared torange over the keywords in the keyword-set of the book B. Figure 9.1 (in Section 9.1) shows an instance books relation, and Figure 9.2 shows the 1NF relation that is theresult of the preceding query. The reverse process of transforming a 1NF relation into a nested relation is called nesting. Nesting can be carried out by an extension of grouping in SQL. In the normal use of grouping in SQL, a temporary multiset relation is (logically) created for each group, and an aggregate function is applied on the temporary relation. By return the multiset instead of applying the aggregate function, we can
The textbook section 351 discusses the concept of "data types" in databases, which are fundamental to understanding how data is organized and managed in a database system. Data types define the structure and characteristics of data, such as its type, size, and format. Understanding data types is crucial for designing and implementing efficient data management systems. [end of text]
SQL:1999 allows the deﬁnition of functions, procedures, and methods. These can be either by the procedural component of SQL:1999 or by an external program-ming language such as Java, C, or C++. Functions can be defined either by the procedural part of SQL:1999 or by an external language, such as C or C++, but differ in syntax and semantics. Procedures can be written in an external language, as seen in Section 9.6.2. External language routines can be used to deﬁne functions, while methods can be viewed as functions associated with structured types. [end of text]
The author-count function can be used to count the number of authors in a book title. It can be called in a query to return the titles of books with more than one author. Procedures can be written in SQL to perform similar operations, such as checking for overlapping polygons or comparing images for similarity. [end of text]
SQL 1999 allows functions in programming languages like C, enhancing efficiency and allowing complex computations. External procedures and functions can be written in C, handling null values and exceptions. Functions can be loaded and executed with database system code, but may require additional parameters. [end of text]
SQL:1999 is a powerful programming language that supports procedural constructs, including while, repeat, for, and case statements. It also includes signaling exception conditions and deﬁned conditions such as sqlexception, sqlwarning, and not found. The procedure findEmpl computes the set of direct/indirect employees of a manager, storing them in a relation empl. [end of text]
SQL:1999 supports procedural constructs, giving it almost all the power of a general-purpose programming language. The Persistent Storage Module (PSM) deals with compound statements, while while statements and repeat statements are supported by this syntax. For loops, including for loops and while loops, are also supported. The SQL:1999 concept of signaling exception conditions and decaling handlers for handling exceptions is also included. [end of text]
The findEmpl procedure in the database system allows finding all employees who work directly or indirectly for a given manager. It adds these employees to the relation empl, and replaces manager with a sequence of one or more flights from the given city. This ensures that cycles of reachability are eliminated, making the procedure work correctly. [end of text]
Database systems are built around persistent programming languages, offering protection against programming errors and high performance. Persistent-programming-language–based OODBs and object-relational systems provide complex data types and integration with programming languages, while object-relational systems aim for high performance and data modeling. Relational systems use SQL for query and high protection, while persistent-programming-language–based OODBs and object-relational systems use complex data types and integration with programming languages. Object-relational systems provide high performance, while persistent-programming-language–based OODBs and object-relational systems provide complex data types and integration with programming languages. Relational systems use SQL for query and high protection, while persistent-programming-language–based OODBs and object-relational systems use complex data types and integration with programming languages. Object-relational systems provide high performance, while persistent-programming-language–based OODBs and object-relational systems provide complex data types and integration with programming languages. Relational systems use SQL for query and high protection, while persistent-programming-language–based OODBs and object-relational systems use complex data types and integration with programming languages. Object-relational systems provide high performance, while persistent-programming-language–based OODBs and object-relational systems provide complex data types and integration with programming languages. Relational systems use SQL for query and high protection, while persistent-programming-language–based OODBs and object-relational systems use complex data types and integration with programming languages. Object-rel
Many object-relational database systems are built on top of existing relational databases. To do so, complex data types in object-relational systems need to be translated to simpler types in relational databases. The translation involves translating E-R model features into relational tables using ISA hierarchies and techniques. [end of text]
The object-relational data model extends the relational data model by providing a richer type system, object orientation, and collection types. Object orientation includes inheritance with subtypes and subtables, and object (tuple) references. The SQL:1999 standard extends the SQL data definition and query language to deal with new data types and object orientation. We saw a variety of features of the extended data-deﬁnition language, including collection-valued attributes, inheritance, and tuple references. SQL:1999 also extends the query language and provides procedural constructs for object-relational database systems. [end of text]
1999 (with the extensions described in this chapter). Find the names of all employees who have a child who has a birthday in March.
Find those employees who took an examination for the skill type “typing”in the city “Dayton”.
List all skill types in the relation emp. [end of text]
Formal dependencies, referential integrity constraints, and first and fourth normal forms are assumed. [end of text]
In thirdnormal form, the relational schema represents the same information. Constraints on sub-tables include: 1. Primary key constraint; 2. Foreign key constraint; 3. Referential integrity constraint; 4. Candidate key constraint; 5. Primary key-to-reference constraint; 6. Primary key-to-candidate constraint; 7. Foreign key-to-reference constraint; 8. Foreign key-to-candidate constraint; 9. Candidate key-to-reference constraint; 10. Candidate key-to-candidate constraint. Constraints that must be imposed on the relational schema to represent an instance of the schema with inheritance: 1. Primary key constraint; 2. Foreign key constraint; 3. Referential integrity constraint; 4. Candidate key constraint; 5. Primary key-to-reference constraint; 6. Primary key-to-candidate constraint; 7. Foreign key-to-reference constraint; 8. Foreign key-to-candidate constraint; 9. Candidate key-to-reference constraint; 10. Candidate key-to-candidate constraint. [end of text]
CREATE TABLE vehicles (
    vehicle_id INT PRIMARY KEY,
    vehicle_number VARCHAR(255),
    license_number VARCHAR(255),
    manufacturer VARCHAR(255),
    model VARCHAR(255),
    date_of_purchase DATE,
    color VARCHAR(255)
);
CREATE TABLE vehicle_types (
    vehicle_type_id INT PRIMARY KEY,
    vehicle_type VARCHAR(255)
);
CREATE TABLE vehicle_types_vehicle (
    vehicle_type_id INT,
    vehicle_id INT,
    FOREIGN KEY (vehicle_type_id) REFERENCES vehicle_types(vehicle_type_id)
);
CREATE TABLE vehicle_types_vehicle_types (
    vehicle_type_id INT,
    vehicle_type VARCHAR(255),
    FOREIGN KEY (vehicle_type_id) REFERENCES vehicle_types(vehicle_type_id)
);
CREATE TABLE vehicle_types_vehicle_types_vehicle (
    vehicle_type_id INT,
    vehicle_type VARCHAR(255),
    vehicle_id INT,
    FOREIGN KEY (vehicle_type_id) REFERENCES vehicle_types(vehicle_type_id)
);
CREATE TABLE vehicle_types_vehicle_types_vehicle_types (
    vehicle_type_id INT,
    vehicle_type VARCHAR(255),
    vehicle_id INT,
    FOREIGN KEY (vehicle_type_id) REFERENCES vehicle_types(vehicle_type_id)
);
CREATE TABLE vehicle_types_vehicle_types_vehicle_types_vehicle (
    vehicle_type_id INT,
    vehicle_type VARCHAR(255),
    vehicle_id INT,
    FOREIGN KEY (vehicle_type_id) REFERENCES vehicle_types(vehicle_type_id)
);
CREATE TABLE vehicle_types_vehicle_types_vehicle_types_vehicle_types (
Choosing a reference type depends on the specific requirements of your project or application. If you need to work with a large dataset, a reference type might be more efficient. If you need to perform complex calculations or operations, a reference type might be more suitable. Ultimately, the choice depends on the specific needs of your project. [end of text]
In an SQL:1999 schema, an array is used to represent the multivalued attribute. The SQL:1999constructs are used to represent the other attribute types. Constructors for structured types are provided, including arrays, sets, and maps. [end of text]
Specializations refer to distinct areas of expertise or specialization within a field. [end of text]
In SQL, a schema deﬁnition corresponding to the relational schema using references to express foreign-key relationships is:
```sql
CREATE TABLE Employees (
    EmployeeID INT PRIMARY KEY,
    FirstName VARCHAR(50),
    LastName VARCHAR(50),
    HireDate DATE,
    DepartmentID INT,
    FOREIGN KEY (DepartmentID) REFERENCES Departments(DepartmentID)
);
```
Each of the queries given in Exercise 3.10 on the above schema, using SQL:1999, are:
1. SELECT EmployeeID, FirstName, LastName, HireDate FROM Employees WHERE DepartmentID = 1;
2. SELECT FirstName, LastName FROM Employees WHERE DepartmentID = (SELECT DepartmentID FROM Employees WHERE EmployeeID = 1);
3. SELECT DepartmentID FROM Employees WHERE EmployeeID = 1;
4. SELECT FirstName, LastName FROM Employees WHERE DepartmentID = (SELECT DepartmentID FROM Employees WHERE EmployeeID = 1 AND DepartmentID = 2); [end of text]
SELECT T1.company_name, AVG(T2.salary) AS avg_salary
FROM employee AS T1
JOIN works AS T2 ON T1.employee_name = T2.employee_name
WHERE T2.company_name = 'First Bank' AND T1.salary > (SELECT AVG(T3.salary) FROM employee AS T3 WHERE T3.company_name = 'First Bank') [end of text]
more than one author, using the with clause in place of the function. [end of text]
Object-Relational Databases (ORDBs) are a type of database that uses a relational model to store data. They are commonly used in applications that require data to be stored and retrieved in a structured format, such as in a relational database management system (RDBMS). ORDBs are useful when data needs to be stored and retrieved in a structured format, such as in a relational database management system (RDBMS). They are also used in applications that require data to be stored and retrieved in a structured format, such as in a relational database management system (RDBMS). They are commonly used in applications that require data to be stored and retrieved in a structured format, such as in a relational database management system (RDBMS). They are commonly used in applications that require data to be stored and retrieved in a structured format, such as in a relational database management system (RDBMS). They are commonly used in applications that require data to be stored and retrieved in a structured format, such as in a relational database management system (RDBMS). They are commonly used in applications that require data to be stored and retrieved in a structured format, such as in a relational database management system (RDBMS). They are commonly used in applications that require data to be stored and retrieved in a structured format, such as in a relational database management system (RDBMS). They are commonly used in applications that require data to be stored and retrieved in a structured format, such
For the computer-aided design system, we recommend a relational database system. For the system to track contributions to public offices, we recommend a persistent-programming-language-based object relational system. For the information system to support movies, we recommend an object relational system. [end of text]
XML is a markup language that describes how to format content, allowing for uniform formatting in different contexts. It evolved from document formatting and has evolved from specifying instructions for how to format content. XML is used for data representation and exchange, and it is widely accepted as a dominant format for data exchange. [end of text]
XML is a data format used to represent and exchange structured data. It consists of an XML declaration at the beginning, followed by a root element (e.g., bank), containing a series of child elements representing different types of data. Each child element represents a different type of data, such as an account, customer, or depositor. The XML format allows for easy data exchange and manipulation between different systems. [end of text]
An element in XML is a pair of matching start- and end-tags, containing all text between them. Elements must be nested properly, with each start-tag matching a matching end-tag in the same parent element. Text in an XML document appears in the context of its element, and nested representations are used to avoid joins. Attributes are also part of XML, representing types of accounts. [end of text]
XML documents are designed to be exchanged between applications, with unique names for tags and attributes. The concept of a namespace allows organizations to specify globally unique names for elements in documents. The idea of an element with no subelements or text is abbreviated as <element/>; elements with attributes can be abbreviated as <element/>. The root element has an attribute xmlns:FB, which declares that FB is an abbreviation for a Web URL. Elements without an explicit namespace prefix can belong to the default namespace. Values containing tags without being interpreted as XML tags can be stored using <![CDATA[...]]>. [end of text]
367
The document-oriented schema mechanism in XML allows for flexible types and constraints, while XML documents must be processed automatically or in parts. The DTD defines patterns for subelements and attributes, while the XMLSchema specifies more recent types and constraints. [end of text]
The DTD is used to constrain and type information in XML documents, primarily by defining subelement patterns and attributes. It does not constrain types in the sense of basic types like integer or string, but only specifies the appearance of subelements and attributes within an element. The DTD is primarily a list of rules for subelement patterns and attributes, with no explicit type constraints. The DTD defines account, customer, and depositor elements with subelements account-number, branch-name, and balance, and declares them to be of type #PCDATA. It also defines attributes for account-number, branch-name, balance, customer-name, customer-street, and customer-city as CDATA, ID, IDREF, and IDREFS. [end of text]
XML schema provides a more sophisticated way to represent XML documents, allowing for more flexible and accurate type definitions. It improves DTDs by addressing issues like type constraints, unordered sets, and missing types, while still maintaining the flexibility of DTDs. XML schema offers several benefits over DTDs, including user-deﬁned types and text element constraints. [end of text]
XMLSchema provides a more sophisticated schema language by allowing user-deﬁned types and text constraints. It enables the creation of types and text constraints, allowing users to define text that appears in elements. XMLSchema also supports the creation of text constraints, such as numeric types in speciﬁc formats or more complex types like lists or unions. Overall, XMLSchema offers a more flexible and powerful way to represent and control data in databases. [end of text]
XMLSchema is a more complex format than DTDs, allowing types to be restricted, complex types to be extended, and integrating namespaces. It is a superset of DTDs and itself a speciﬁed XML syntax. It is used to create specialized types, enforce uniqueness and foreign key constraints, and integrate with namespaces. XMLSchema is significantly more complex than DTDs. [end of text]
XML is a data format that can be queried and transformed to extract information from large XML data. Several languages provide querying and transformation capabilities, such as XPath, XSLT, XQuery, and Quilt. The text content of an element can be modeled as a text node child of the element. Elements containing text broken up by intervening subelements can have multipletext node children. XML is a tree model of XML data, and an XML document is modeled as a tree with nodes corresponding to elements and attributes. Elements do not contain both text and subelements. [end of text]
XPath is an extension of object-oriented and object-relational databases, providing path expressions for XML documents. XPath evaluates from left to right, testing elements by listing them without comparison operations. It supports selection predicates and functions for testing the position of nodes, counting matches, and handling IDREFs. The | operator allows unioning results, and XPath can skip multiple levels of nodes using “//”. [end of text]
XPath is a language used to navigate and access parts of XML documents by path expressions. It extends object-oriented and object-relational database concepts, viewing as extensions of simple path expressions. XPath expressions evaluate from left to right, with path results being sets of nodes. XPath supports selection predicates, such as “/bank-2/account[balance > 400]” and “/bank-2/account/@account-number”, and testing attributes using “@” symbols. It provides functions for testing existence, counting nodes, and applying IDREFs. XPath can skip multiple levels of nodes using “//” and is useful for navigating XML documents without full knowledge of the schema. [end of text]
A style sheet is a document that specifies formatting options for a document, often stored outside the document, so that formatting is separate from content. For example, a style sheet for HTML specifies the font to be used on all headers. [end of text]
XML is a standard for generating HTML from XML, and XSLT is a powerful extension of HTML. XSLT can be used as a query language, and its syntax and semantics are quite dissimilar from those of SQL. XSLT templates allow selection and content generation in natural and powerful ways, including recursive rules. Structural recursion is a key part of XSLT, and it permits lookup of elements by using values of subelements or attributes. Keys are a feature of XSLT that permit lookup of elements by using values of subelements or attributes, and they can be used in templates as part of any pattern through the key function. [end of text]
XQuery is a query language for XML, derived from an XML query language called Quilt. It includes features from earlier languages such as XPath, discussed in Section 10.4.1, and two other XML query languages, XQL and XML-QL. XQuery does not represent queries in XML, but rather appears more like SQL queries, organized into FLWR expressions. XQuery allows nodes to be sorted and performs additional tests on joined tuples. It provides aggregate functions such as sum and count. XQuery does not provide a group by construct but can be written using nested FLWR constructs. [end of text]
The World Wide Web Consortium (W3C) is developing XQuery, a query language for XML. The main features discussed in this section include the FLWR expression syntax, the let clause for complex expressions, and the return clause for constructing results in XML. [end of text]
customer/account $c customer-name $d/account-name
customer/account $c customer-name $d/customer-name
bank-1/customer/account $c customer-name $d/account-name
bank-1/customer/account $c customer-name $d/customer-name
bank-1/customer/account $c customer-name $d/customer-name
bank-1/customer/account $c customer-name $d/customer-name
bank-1/customer/account $c customer-name $d/customer-name
bank-1/customer/account $c customer-name $d/customer-name
bank-1/customer/account $c customer-name $d/customer-name
bank-1/customer/account $c customer-name $d/customer-name
bank-1/customer/account $c customer-name $d/customer-name
bank-1/customer/account $c customer-name $d/customer-name
bank-1/customer/account $c customer-name $d/customer-name
bank-1/customer/account $c customer-name $d/customer-name
bank-1/customer/account $c customer-name $d/customer-name
bank-1/customer/account $c customer-name $d/customer-name
bank-1/customer/account $c customer-name $d/customer-name
bank-1/customer/account $c customer-name $d/customer-name
bank-1/customer/account $c customer-name $d/customer-name
bank-1/customer/account $c customer-name $d/customer-name
bank-1/customer/account $c customer-name $d/customer-name
bank-1/customer/account $c customer-name $d/customer
Softwares for XML manipulation are widely available, with two models: Object-Based and XML. XML is a widely accepted data format, with various programming languages supporting it. The McGraw-Hill Company published a fourth edition of Database System Concepts in 2001. [end of text]
DOM is a Java API for manipulating XML content as a tree, with each element represented by a node called DOMNode. It provides methods for navigating the DOM tree, starting with the root node. DOM can be used to access XML data stored in databases. The Simple API for XML (SAX) is an event model designed to provide a common interface between parsers and applications. It is built on the notion of event handlers, which consist of user-speciﬁed functions associated with parsing events. Parsing events correspond to the recognition of parts of a document. SAX is not appropriate for database applications. [end of text]
XML data can be stored in relational databases, but converting it to relational form is straightforward if the data were generated from a relational schema. However, there are many applications where the XML data is not generated from a relational schema and translating the data to relational form for storage may not be straightforward. In particular, nested elements and elements that recur (corresponding to set valued attributes) can complicate storage of XML data in relational format. Several alternative approaches are available: store as string, store different types of elements in different relations, and store values of some critical elements as attributes of the relation to enable indexing. [end of text]
XML data can be stored in relational databases, but storing it as a string can be challenging. Several approaches exist, such as storing different types of elements in different relations and storing values of critical elements as attributes. This approach depends on type information about XML data, such as the DTD of the data. [end of text]
XML data is represented in relational form, with all information stored directly in relations and attributes. XML queries can be translated into relational queries and executed in the database system. However, each element is broken down into many pieces, and large joins are required. Nonrelational data stores are used, with XML data stored in flat files. [end of text]
XML data can be stored in various nonrelational data storage systems, including flat files, XML databases, and object-based databases. XML databases use XML as their data model, allowing for data isolation, integrity checks, atomicity, concurrent access, and security. XML is a file format, making it easy to access and query XML data stored in files. XML databases can be built as a layer on top of relational databases. [end of text]
XML is a means of communication, facilitating data exchange and mediation of Web resources. It aims to make data semantics easier to describe, enabling data exchange and mediation in business applications. XML is used for exchanging data and mediation Web information resources, demonstrating how database technologies and interaction are key in supporting exchange-based applications. [end of text]
XML is being developed to represent data for specialized applications in various industries, including business and science. The standard is ChemML for chemical information, shipping records, and online marketplaces. Nested element representations help manage large relational schemas and reduce redundancy. XML is a widely used notation for data representation. [end of text]
XML-based mediation is a solution for extracting and combining information from multiple sources, while maintaining the integrity of the original data. This approach is particularly useful in distributed databases where data is often published in XML format. The use of XML mediation enables efficient data exchange and transformation, while also ensuring the preservation of the original data. [end of text]
Comparison shopping is a mediation application that extracts data from multiple Web sites to provide a more comprehensive view of an item's inventory, pricing, and shipping costs. XML-based mediation involves extracting XML representations of account information from financial institutions and generating data from HTML Web pages. This approach is useful for managing multiple accounts and can be challenging for centralized management. XML queries and XSLT/XQuery are used to transform data between different XML representations. [end of text]
XML is a descendant of Standard General-ized Markup Language (SGML) and is used for data exchange between applications. It contains elements with matching tags, can have nested elements, attributes, and references. XML documents can be represented as tree structures with nodes for elements and attributes. XPath is a language for path expressions, allowing required elements to be specified by a file-system-like path and additional features. XML data can be transformed using XSLT. [end of text]
XML documents, XSLT, XQuery, XML data, relational databases, XML schemas, XML trees, XML data in file systems, XML databases, XML data in relational databases, XML data in nonrelational data stores, XML data in relational databases, XML data in nonrelational data stores, XML data in relational databases, XML data in nonrelational data stores, XML data in relational databases, XML data in nonrelational data stores, XML data in relational databases, XML data in nonrelational data stores, XML data in relational databases, XML data in nonrelational data stores, XML data in relational databases, XML data in nonrelational data stores, XML data in relational databases, XML data in nonrelational data stores, XML data in relational databases, XML data in nonrelational data stores, XML data in relational databases, XML data in nonrelational data stores, XML data in relational databases, XML data in nonrelational data stores, XML data in relational databases, XML data in nonrelational data stores, XML data in relational databases, XML data in nonrelational data stores, XML data in relational databases, XML data in nonrelational data stores, XML data in relational databases, XML data in nonrelational data stores, XML data in relational databases, XML data in nonrelational data stores, XML data in relational databases, XML data in nonrelational data stores, XML data in relational databases, XML data in nonrelational data stores
data represented using attributes instead of subelements. DTD provided. [end of text]
XML is a markup language used for data exchange and is a subset of the W3C XML standard. It is commonly used in web development and data storage. XML is an XML document format that is used to exchange data between different systems and applications. It is a markup language that is used for data exchange and is a subset of the W3C XML standard. It is commonly used in web development and data storage. XML is an XML document format that is used to exchange data between different systems and applications. It is a markup language that is used for data exchange and is a subset of the W3C XML standard. It is commonly used in web development and data storage. XML is an XML document format that is used to exchange data between different systems and applications. It is a markup language that is used for data exchange and is a subset of the W3C XML standard. It is commonly used in web development and data storage. XML is an XML document format that is used to exchange data between different systems and applications. It is a markup language that is used for data exchange and is a subset of the W3C XML standard. It is commonly used in web development and data storage. XML is an XML document format that is used to exchange data between different systems and applications. It is a markup language that is used for data exchange and is a subset of the W3C XML standard. It is commonly used in web development and data storage. XML is an XML document format that is
The schemaEmp represents a family structure, with ename, ChildrenSet, SkillsSet, and Birthday, and Children represents a set of children, with name, Birthday, and Skills. [end of text]
In this exercise, you need to find employees with a child born in March, examine skills types in the city of Dayton, and list all skill types in Emp.Silberschatz−Korth−Sudarshan. [end of text]
Emp is a type of data in databases. [end of text]
The textbook section discusses the total balance across all accounts and branches using SQL group by. [end of text]
The left outer join of customer elements with account elements. (Hint: Use universal quantification.) [end of text]
The outermost level of nesting the output must have elements corresponding to authors, and each such element must have nested within it items corresponding to all the books written by the author. [end of text]
In Databases, each element type is separated into a separate element type to represent relationships, but IDs and IDREF are used to implement primary and foreign keys. [end of text]
The textbook explains nested account elements within customer elements in a bank information representation using ID and IDREFS. [end of text]
The relational schema for XML documents must keep track of the order of author elements. Authors appear as top level elements, and the schema must ensure that the order is maintained. [end of text]
The relational schema needs to be altered to include a new element, such as a new attribute or a new relationship type. [end of text]
In the book "Database System Concepts, Fourth Edition", authors have authored books and articles in the same year. Books are sorted by year, and books with more than one author are also sorted by year. XML is discussed in Chapter 10. [end of text]
The textbook explains how to represent a tree using nodes and child relations in Section 10.6.1. [end of text]
In this chapter, we explore the underlying storage media, such as disk and tape systems. We then define various data structures that allow fast access to data. We consider several alternatives, each best suited to different kinds of access to data. The final choice of data structure needs to be made on the basis of the expected use of the system and the physical characteristics of the specific machine. [end of text]
Data storage media vary in speed, cost per unit of data, reliability, and capacity. Cache is the fastest and most expensive form of storage, while main memory is used for data that can be operated on. Flash memory is a popular replacement for magnetic disks for storing small volumes of data. Compact disk and digital video disk are popular forms of optical storage, with different capacities and record-once and multiple-write versions. [end of text]
The textbook describes different types of storage media, their speeds, costs, and the trade-off between cost and speed. It also explains the differences in storage volatility between primary, secondary, and tertiary storage. [end of text]
Disk capacities are growing rapidly, while storage requirements of large applications are growing very fast.
Disks are flat circular shapes with magnetic material on their surfaces, used for data storage. They are divided into tracks and sectors, with sectors being the smallest unit of information. Read-write heads store information on sectors, which may contain hundreds of concentric tracks. [end of text]
The disk drive uses a thin film of magnetic Silberschatz-Korth-Sudarshan as recording medium. They are much less susceptible to failure by head crashes than the older oxide-coated disks. Fixed-head disks have a separate head for each track, allowing quick switching and accessing of multiple tracks at once. Multiple-arm disks can access more than one track on the same platter. Remapping of bad sectors can be performed by the controller to a different physical location. The AT attachment and small-computer-system interconnect are commonly used to connect disk controllers. [end of text]
The main measures of disk quality include capacity, access time, data transfer rate, and reliability. Access time is the time from issuing a read or write request to when data begins to transfer. The seek time increases with the distance the arm must move. The average seek time is the average of the seek times, measured over a sequence of (uniformly distributed) random requests. The mean time to failure (MTTF) is a measure of disk reliability, with a claimed mean time to failure of 3.4 to 136 years. The mean time to failure of a disk (or any other system) is the amount of time that, on average, we can expect the system to run continuously without any failure. According to vendors' claims, the MTTF of disks today ranges from 30,000 to 1,200,000 hours—about 3.4 to 136 years. In practice, claimed mean time to failure is computed on the probability of failure when the disk is new—on an average one of 1000 relatively new disks, one will fail in 1200 hours. A mean time to failure of 1,200,000 hours does not imply that the disk can be expected to function for 136 years! Most disks have an expected life span of about 5 years, and have significantly higher rates of failure once they become more than a few
The main measures of the qualities of a disk include capacity, access time, data-transferrate, and reliability. Access time is the time from when a read or write request is issued to when datatransfer begins. To access (that is, to read or write) data on a given sector of a disk, the arm must move so that it is positioned over the correct track, and then wait for the sector to appear under it as the disk rotates. The average seek time is the average of the seek times, measured over a sequence of (uniformly distributed) random requests. If all tracks have the same number of sectors, and we disregard the time required for the head to start moving and to stop moving, we can show that the average seek time is one-third the worst case seek time. Taking these factors into account, the average seek time is around one-half of the maximum seek time. Average seek times currently range between 4 milliseconds and 10 milliseconds, depending on the disk model. Once the seek has started, the time spent waiting for the sector to be accessed to appear under the head is called the rotational latency time. Rotational speedsof disks today range from 5400 rotations per minute (90 rotations per second) up to 15,000 rotations per minute (250 rotations per second), or, equivalently, 4 milliseconds to 11.1 milliseconds per rotation. On an average, one
Buffering blocks in memory to satisfy future requests is a technique used by file-system managers to improve disk I/O speed. [end of text]
Disk controllers reorganize data to improve performance by keeping blocks sequentially on adjacent cylinders. File organizations store data on adjacent cylinders to allow sequential access. Nonvolatile write buffers use battery-backed-up RAM to speed up disk writes. Log disk uses a compacted log to minimize fragmentation. Journaling file systems keep data and the log on the same disk, reducing fragmentation. [end of text]
The data storage requirements of applications, especially databases and multimedia data, have grown rapidly, necessitating the use of many disks. The exact arrival rate and rate of service are not needed since disk utilization provides the necessary information. [end of text]
The introduction of redundant arrays of independent disks (RAID) offers improved reliability and performance, while the use of mirroring and striping techniques can further enhance data storage and retrieval capabilities. [end of text]
Mirroring can increase the mean time to data loss in a mirrored disk system, especially when power failures are a concern. [end of text]
Beneﬁt of parallel access to multiple disks, doubling read rates and improving transfer rates through striping. [end of text]
Block-level striping is the most common form of data striping. Other levels of striping, such as bytes of a sector or sectors of a block, are also possible. There are two main goals of parallelism in a disk system: load-balance multiple small accesses (block accesses) to increase throughput, and parallelize large accesses to reduce response time. Various alternative schemes aim to provide redundancy at lower cost by combining disk striping with parity bits. These schemes have different cost-performance trade-offs. RAID levels include RAID 0, RAID 1, RAID 2, RAID 3, RAID 4, and RAID 5. RAID 3, bit-interleaved parity, improves on RAID 2 by detecting sector read errors. [end of text]
Mirroring provides high reliability, but it is expensive. Striping provides high data-transfer rates, but does not improve reliability. Various alternative schemes aim to provide redundancy at lower cost by combining disk striping with "parity" bits. These schemes have different cost–performance trade-offs. RAID levels, as depicted in Figure 11.4, include RAID 0, 1, 2, 3, 4, 5, and 6, with 3 being the most cost-effective. The idea of error-correcting codes is used in disk arrays by striping bytes across disks. For example, the first bit of each byte could be stored in disk 1, the second in disk 2, and so on until the eighth in disk 8, and the error-correction bits are stored in further disks. [end of text]
RAID level 3 is as good as level 2, but is less expensive in the number of extra disks (it has only a one-disk overhead), so level 2 is not used in practice. RAID level 3 has two benefits over level 1: it needs only one parity disk for several regular disks, whereas level 1 needs one mirror disk for every disk. Since reads and writes of a byte are spread over multiple disks, with N-way striping of data, the transfer rate for reading or writing a single block is N times faster than a RAID level 1 or 2. On the other hand, RAID level 3 supports a lower number of I/O operations per second, since every disk has to participate in every I/O request. RAID level 4, block-interleaved parity organization, uses block level striping, like RAID 0, and in addition keeps a parity block on a separate disk for corresponding blocks from N other disks. This scheme is shown pictorially in Figure 11.4e. If one of the disks fails, the parity block can be used with the corresponding blocks from the other disks to restore the blocks of the failed disk. RAID level 5, block-interleaved distributed parity, improves on level 4 by partitioning data and parity among all N + 1 disks, instead of storing data in N disks and parity in one disk. In level 5, all disks can participate in satisfying read requests
The factors to consider when choosing a RAID level include monetary cost, performance requirements, rebuild time, and data recovery performance. RAID levels can affect repair time and data loss. Some products use different RAID levels for mirroring without striping and mirroring with striping. The choice of RAID level depends on the specific requirements of the database system. [end of text]
RAID level 0 is used in high-performance applications where data safety is not critical. Since RAID levels 2 and 4 are subsumed by RAID levels 3 and 5, the choice of RAID levels is restricted to the remaining levels. Bit striping (level 3) is rarely used since block striping (level 5) gives as good data transfer rates for large transfers, while using fewer disks for small transfers. For small transfers, the disk access time dominates anyway, so the beneﬁt of parallel reads diminishes. In fact, level 3 may perform worse than level 5 for a small transfer, since the transfer completes only when corresponding sectors on all disks have been fetched; the average latency for the disk array thus becomes very close to the worst-case latency for a single disk, negating the beneﬁts of higher transfer rates. Level 6 is not supported currently by many RAID implementations, but it offers better reliability than level 5 and can be used in applications where data safety is very important. The choice between RAID level 1 and level 5 is harder to make. RAID level 1 is popular for applications such as storage of log ﬁles in a database system, since it offers the best write performance. RAID level 5 has a lower storage overhead than level 1, but has a higher time overhead for writes. For applications where data areread frequently, and written rarely, level 5 is the preferred choice. RAID
RAID implementations can use nonvolatile RAM to record writes that need to be executed; in case of power failure before a write is completed, when the system comes back up, it retrieves information about incomplete writes from non-volatile RAM and then completes the writes. Without such hardware support, extrawork needs to be done to detect blocks that may have been partially written before power failure (see Exercise 11.4). Some hardware RAID implementations permit hot swapping; that is, faulty disk can be removed and replaced by new ones without turning power off. Hot swapping reduces the mean time to repair, since replacement of a disk does not have to wait until a time when the system can be shut down. Many critical systems today run on a 24 × 7 schedule; that is, they run 24 hours a day, 7 days a week, providing no time for shutting down and replacing a failed disk. Further, many RAID implementations assign a spare disk for each array (or for a set of disk arrays). If a disk fails, the spare disk is immediately used as a replacement. As a result, the mean time to repair is reduced greatly, minimizing the chance of any data loss. The power supply, or the disk controller, or even the system interconnection in a RAID system could become a single point of failure, that could stop functioning of the RAID system. To avoid this possibility, good RAID implementations have multipleredundant power supplies (with battery backups so
The concepts of RAID have been extended to other storage devices, including tapes and wireless systems. When applied to arrays of tapes, RAID structures can recover data even if one tape is damaged. When applied to broadcast of data, a block is split into units, and parity units are broadcast. If a unit is not received, it can be reconstructed from the other units. [end of text]
In a large database system, some data may need to reside on tertiary storage media such as optical disks and magnetic tapes. Compact disks and digital video disks are popular for distributing software and multimedia data. DVDs are replacing compact disks in applications requiring large amounts of data. [end of text]
Compact disks are popular for distributing software, multimedia, and electronically published information. DVDs are replacing compact disks in applications requiring large amounts of data. DVDs use 4.7 gigabytes of storage. [end of text]
The textbook summarizes the storage and file structure of magnetic tapes, discussing their capacity, speed, and limitations. It also covers tape devices and their reliability, emphasizing the importance of seek times for applications requiring large data access. [end of text]
Tapes are slow, limited to sequential access, and are used for backup, infrequently used information, and off-line storage. They are also used for large volumes of data, such as video or image data, that do not need to be accessed quickly or are too voluminous to use magnetic disks. Tapes are kept in spools and wound or rewound past a read-write head. Moving to the correct spot on a tape can take seconds or even minutes, rather than the 10 to 40 gigabytes (with the Digital Linear Tape (DLT) format) of the market. Data transfer rates are of the order of a few to tens of megabytes per second. Tape devices are reliable, but have limits on the number of times they can be read or written reliably. Some tape formats (like the Accelis format) support faster seek times, which is important for applications that need quick access to very large amounts of data. Most other tape formats provide larger capacities, at the cost of slower access; such formats are ideal for data backup, where fast seeks are not important. [end of text]
A database is mapped into multiple files, maintained by the underlying operating system. These files reside permanently on disks and have backups on tapes. Each file is partitioned into fixed-length storage units called blocks, which are used for both storage and data transfer. [end of text]
A block contains several data items, and the exact set is determined by physical data organization. Main memory can hold as many blocks as possible, minimizing disk accesses. Buffer is used to store copies of disk blocks. [end of text]
The buffer manager in a database system allocates space in the buffer when needed, while the buffer manager in an operating system uses a different approach to manage memory. The buffer manager in a database system is transparent to programs that issue disk-block requests, while the buffer manager in an operating system must use more sophisticated techniques to manage memory. The buffer manager in a database system uses a least recently used (LRU) strategy to minimize access to the disk, while the buffer manager in an operating system uses a past pattern of block references as a predictor of future references. The buffer manager in a database system is transparent to programs that issue disk-block requests, while the buffer manager in an operating system must use more sophisticated techniques to manage memory. The buffer manager in a database system uses a least recently used (LRU) strategy to minimize access to the disk, while the buffer manager in an operating system uses a past pattern of block references as a predictor of future references. The buffer manager in a database system is transparent to programs that issue disk-block requests, while the buffer manager in an operating system must use more sophisticated techniques to manage memory. The buffer manager in a database system uses a least recently used (LRU) strategy to minimize access to the disk, while the buffer manager in an operating system uses a past pattern of block references as a predictor of future references. The buffer manager in a database system is transparent to programs that issue disk-block requests, while the buffer manager in an operating system must use more
The buffer manager in a database system requests blocks from disk when needed, allocating space in the buffer for new blocks and writing them back to disk only if necessary. The buffer manager is transparent to programs that issue disk-block requests, using techniques like buffer replacement and pinned blocks to manage memory efficiently. [end of text]
The goal of a replacement strategy for blocks in the buffer is to minimize access to the disk. For general-purpose programs, it is not possible to predict accurately Silberschatz-Korth-Sudarshan: Database System Concepts, Fourth Edition IV. Data Storage and Querying 11. Storage and File Structure 415 © The McGraw-Hill Companies, 2001414 Chapter 11 Storage and File Structure for each tuple b of borrower do for each tuple c of customer do if b[customer-name] = c[customer-name] then begin let x be a tuple deﬁned as follows: x[customer-name] := b[customer-name] x[loan-number] := b[loan-number] x[customer-street] := c[customer-street] x[customer-city] := c[customer-city] include tuple x as part of result of borrower customer end end end Figure 11.5 Procedure for computing join. Therefore, operating systems use the past pattern of block references as a predictor of future references. The assumption generally made is that blocks that have been referenced recently are likely to be referenced again. Therefore, if a block must be replaced, the least recently referenced block is replaced. This approach is called the least recently used (LRU) block-replacement scheme. LRU is an acceptable replacement scheme in operating systems. However, a data-base system is able to predict the pattern of future references more accurately than
The buffer manager uses knowledge about database operations, including those performed and those to be performed in the future, to determine the most appropriate strategy for block replacement. Factors such as concurrency control and crash recovery subsystems influence this decision, while the buffer manager must adapt its strategy to accommodate these changes. [end of text]
A file is organized as a sequence of records, mapped to disk blocks. Files are used in operating systems to store data. Records are mapped to disk blocks, with varying sizes. Fixed-length records are used in a relational database to store distinct relations of different sizes. Multiple lengths can be accommodated in a single file. Fixed-length records are easier to implement than variable-length records. A file of fixed-length records is created for a bank database. [end of text]
As an example, consider a file of account records for a bank database. Each record is 40 bytes long. A simple approach involves using the first 40 bytes for the first record, the next 40 bytes for the second record, and so on. However, deleting a record requires filling space with another record or marking it for deletion. [end of text]
The textbook summarizes the storage and file structure of databases, including the use of pointers and linked lists for storing deleted records. It also describes variable-length records, which can be implemented using different techniques. [end of text]
Variable-length records are used in database systems to store multiple record types in a file, allowing varying lengths for fields and repeating fields. Different techniques include one variable-length record per branch name and account information. [end of text]
The book describes two methods for implementing variable-length records: the byte-string representation and the slotted-page structure. The byte-string representation has advantages such as ease of reuse and space management, but disadvantages such as wasted space and the need for large records to grow longer. The slotted-page structure is commonly used for organizing records within a single block, but it requires that there be no pointers that point directly to records. [end of text]
A simple method for implementing variable-length records is to attach a special end-of-record (⊥) symbol to the end of each record. We can store each record as astring of consecutive bytes. The byte-string representation as described in Figure 11.10 has some disadvantages, such as wasted space and difficulty in growing records longer. However, the slotted-page structure is commonly used for organizing records within a single block. [end of text]
Reserved space is used to represent variable-length records with a fixed length. Unused space is used for records beyond the maximum length. [end of text]
The reserved-space method is useful when most records have a length close to the maximum, as it leads to significant space waste. The linked list method is used to represent the ﬁle by the linked list method, which uses pointers to chain together all records pertaining to the same branch. [end of text]
The textbook discusses various ways of organizing records in a file, including heap and sequential file organization. [end of text]
Hashing is a method of organizing files to store records efficiently. It computes a hash function on each attribute of a record to determine its block position. Chapter 12 describes this organization, closely related to indexing structures. Sequential file organization is used to store records of related relations in the same file, allowing fast retrieval of related records. Chapter 11.7.2 describes this organization. Chapter 11.7.1 describes sequential file organization. Sequential file organization allows records to be read in sorted order, useful for display purposes and certain query-processing algorithms. It is difficult to maintain physical sequential order due to frequent insertions and deletions. Chapter 11.7.2 suggests a clustering file organization. [end of text]
A sequential file is designed for efficient processing of records in sorted order based on a search-key. Records are stored in search-key order using pointers, minimizing block accesses and physical sequential order. Records can be read in sorted order for display and query-processing. Sequential file management is challenging due to insertion and deletion costs, but can be managed with pointer chains. Overﬂow blocks force sequential processing, which may lose correspondence with physical order. Frequent reorganizations are costly and must be done during low system load. [end of text]
Many relational-database systems store each relation in a separate file, allowing full use of file systems.
A simple relational database implementation is suitable for low-cost systems, reducing code size and performance benefits. Many large systems do not manage file management independently. [end of text]
The depositor tuples are stored near the customer tuple for each customer-name, allowing for efﬁcient processing of joins. Clustering files organize related records of two or more relations in each block, enabling faster processing of join queries. Clustering improves query processing but can slow other types of queries. The use of clustering depends on the types of queries the database designer believes to be most frequent. Careful use can produce significant performance gains. [end of text]
The textbook discusses the representation of relational data and the storage and query of relational databases. [end of text]
In database systems, the data dictionary stores information about relations, views, and users. It includes names, attributes, domains, lengths, and views. The data structure is hierarchical, with attributes stored in blocks, and the location of each relation is noted in operating system files. The storage organization is also stored in the database code, facilitating fast access to system data. [end of text]
The heap, sequential, hashing, and clustering organizations can be used for storing objects in an object-oriented database. However, set-valued fields and persistent pointers are needed for object-oriented database features. [end of text]
The mapping of objects to files is similar to the mapping of tuples to files in relational databases. Objects in object-oriented databases may lack uniformity, with fields of records being sets. [end of text]
In first normal form, data are required to be in a set-valued field with a small number of elements, and objects may be extremely large. Set-valued fields can be implemented as relations in the database, and normalization can be used to eliminate objects from storage levels. Persistent pointers can be implemented in a persistent programming language, and in-memory pointers can be used in some implementations. Persistent pointers are at least 8 bytes long, and may be substantially longer. [end of text]
An object's unique identifier (OID) is used to locate an object, while physical OIDs encode the location of the object. Physical OIDs typically have three parts: volume or file identifier, block identifier, and offset within the block. Unique identifiers in an OID and the corresponding object should match. If a unique identifier in a physical OID does not match the unique identifier in the object to which that OID points, the system detects a dangling pointer and signals an error. Physical OIDs may contain a unique identifier, which is an integer that distinguishes the OID from the identiﬁers of other objects that happened to bestored at the same location earlier, and were deleted or moved elsewhere. The unique identifier is stored with the object, and the identiﬁers in an OID and the corresponding object should match. If the unique identifier in a physical OID does not match the unique identifier in the object to which that OID points, the system detects a dangling pointer and signals an error. [end of text]
In persistent programming languages, persistent pointers are used to address physical OIDs, while in-memory pointers are logical OIDs. Persistent pointers address all virtual memory, whereas in-memory pointers are usually 4 bytes long. Persistent pointers need to be at least 8 bytes long to address 4 gigabytes of memory, while in-memory pointers are usually 4 bytes long. Object-oriented databases use unique identifiers in persistent pointers to catch dangling references. Persistent pointers may be longer than in-memory pointers. The action of looking up an object, given its identiﬁer, is called dereferencing. Given an in-memory pointer (as in C++), looking up the object is merely a memory reference. Given a persistent pointer, dereferencing an object has an extra step—finding the actual location in memory by looking up the persistent pointer in a table. If the object is not already in memory, it has to be loaded from disk. We can implement the table lookup fairly efficiently by using a hash table data structure, but the lookup is still slow compared to a pointer dereference, even if the object is already in memory. [end of text]
Hardware swizzling is a way to cut down the cost of locating persistent objects that are already present in memory. It involves storing an in-memory pointer to the object in place of a persistent pointer when the object is dereferenced, and using a small number of bits to distinguish between persistent and in-memory pointers. Hardware swizzling is more complex than software swizzling, but it can be used to deal with in-memory pointers. The term page is used interchangeably in this section. [end of text]
Hardware swizzling allows for efficient storage and conversion between persistent and in-memory pointers, making software written for in-memory pointers compatible with persistent ones. It uses virtual-memory management to address this issue, with a small number of bits needed for the short page identiﬁer. [end of text]
The persistent-pointer representation scheme uses a small number of bits to store a short page identifier, which is then used to map to full database page identifiers. The translation table in the worst case contains only 1024 elements, and the table size is limited by the maximum number of pointers in a page. The short page identifier needs only 10 bits to identify a row in the table, making it suitable for swizzling. The persistent-pointer scheme allows an entire persistent pointer to fit into the same space as an in-memory pointer, facilitating swizzling. [end of text]
The textbook section 4332395679.342784867 refers to a specific chapter or section within a database textbook. Without more context, I cannot provide a more detailed summary. [end of text]
In virtual memory, pointers are swizzled on pages before they are loaded into virtual memory. This process involves allocating free pages in virtual memory to the page if one has not been allocated earlier. After the page is loaded, pointers are updated to reflect the new mapping. Objects in in-memory pages contain only in-memory pointers, and routines using these objects do not need to know about persistent pointers! [end of text]
In a virtual memory database system, database pages are initially allocated virtual memory pages even before they are loaded. When a database page is loaded, the system allocates a free page in virtual memory to it. The system then updates the persistent pointer being considered, replacing pi with vi, and updates the actual space when the database page is actually loaded into virtual memory. The system also converts all persistent pointers in objects in the page to in-memory pointers, allowing routines that use these objects to work without knowing about persistent pointers. [end of text]
The database system refines a segmentation violation by allocating storage for a virtual-memory page, loading the database page into virtual memory, and then loading the database page into memory. [end of text]
Hardware swizzling allows for efficient pointer swizzling out on pages, enabling pointer dereferencing in applications that frequently dereference pointers. It avoids the overhead of translating pointers to objects in memory, making it beneficial for applications that repeatedly dereference pointers. Hardware swizzling works even in larger databases, as long as all pages in the process's memory fit into the virtual memory of the process. It can also be used at the level of sets of pages, instead of for a single page. [end of text]
Hardware swizzling is used to convert in-memory pointers to persistent pointers in databases. It avoids the need for a deswizzling operation by updating translation tables for pages. This allows for more efficient swizzling and reduces the cost. Set-level swizzling is used for sets of pages, where only one page is needed at a time. [end of text]
The format in which objects are stored in memory may differ from the format on disk in a database, and one reason may be software swizzling, where the structures of persistent and in-memory pointers are different. Another reason may be the need for accessibility across different machines, architectures, languages, and compilers. The solution is to make the database's in-memory representation independent of the machine and compiler. The system converts objects from disk to the required format on the specific machine, language, and compiler when brought into memory, making the programmer unaware of the conversion. The deﬁnition of the structure of each class in the database is stored logically in the databases, and the code to translate an object to the representation manipulated with the programming language and vice versa depends on the machine and compiler. The hidden pointers in objects can cause unexpected differences between disk and in-memory representations. [end of text]
Compilers generate and store pointers in objects, which point to tables used to implement methods. These tables are compiled into executable object code, and their locations depend on the executable object code. When a process accesses an object, hidden pointers must be fixed to point to the correct location. Large objects, containing binary data, are called binary large objects (blobs), while those containing character data are called character large objects (clobs). Buffer pages are allocated to manage large objects, and modifications are handled using B-tree structures. Text data, image/graphics data, audio/video data, and other types of data are managed by application programs instead of within the database. [end of text]
Large objects containing binary data are called binary large objects (blobs), while large objects containing character data, are called character large objects (clobs). Most relational databases restrict the size of a record to be no larger than the sizeof a page, to simplify buffer management and free-space management. Large objects and long ﬁelds are often stored in a special ﬁle (or collection of ﬁles) reserved for long-ﬁeld storage. Allocation of buffer pages presents a problem with managing large objects. Large objects may need to be stored in a contiguous sequence of bytes when they are brought into memory; in that case, if an object is bigger than a page, contiguous pages of the buffer pool must be allocated to store it, which makes buffer management more difficult. Text data is usually treated as a byte string manipulated by editors and formatters. Image/Graphical data may be represented as a bitmap or as a set of lines, boxes, and other geometric objects. Although some graphical data often are managed within the database system itself, special application software is used for many cases, such as integrated circuit design. Audio and video data are typically a digitized, compressed representation created and displayed by separate application soft-ware. Data are usually modiﬁed with special-purpose editing software, out-side the database system. [end of text]
Data storage systems use various types, costs, and reliabilities. Media include cache, main memory, and disks. RAID organizations include mirroring, redundant arrays of independent disks (RAIDs), and variable-length records. Data are organized as records mapped to disk blocks. Block accesses are bottleneck in performance. Reducing disk accesses by mapping to blocks can pay performance dividends. [end of text]
The textbook describes the storage and file structure of disk blocks, focusing on tertiary storage, buffer management, and file organization for object-oriented databases. It also covers buffer-replacement policies, file organization, and various file structures for OODBs. The text includes discussions on buffer-replacement policies, file organization, and various file structures for OODBs. It also covers buffer-replacement policies, file organization, and various file structures for OODBs. The text includes discussions on buffer-replacement policies, file organization, and various file structures for OODBs. The text includes discussions on buffer-replacement policies, file organization, and various file structures for OODBs. The text includes discussions on buffer-replacement policies, file organization, and various file structures for OODBs. The text includes discussions on buffer-replacement policies, file organization, and various file structures for OODBs. The text includes discussions on buffer-replacement policies, file organization, and various file structures for OODBs. The text includes discussions on buffer-replacement policies, file organization, and various file structures for OODBs. The text includes discussions on buffer-replacement policies, file organization, and various file structures for OODBs. The text includes discussions on buffer-replacement policies, file organization, and various file structures for OODBs. The text includes discussions on buffer-replacement policies, file organization, and various file structures for OODBs. The text includes discussions on buffer-replacement policies, file organization
The speed of data access varies among different media. Data is typically accessed faster on hard drives than on solid-state drives (SSDs). SSDs offer faster access times due to their use of flash memory, which allows for more frequent writes and reads.
rates are calculated to determine the price of a financial instrument or service. [end of text]
The arrangement of disks and parity blocks in a database system presents a potential problem where data blocks may not be stored in the correct order, leading to issues such as data inconsistency and incorrect data retrieval. [end of text]
Schemes for getting the effect of atomic block writes in RAID levels 1 and 5 involve mirroring and block interleaving, with distributed parity for recovery from failure. [end of text]
RAID 5 is the RAID level that minimizes the amount of interference between rebuild and ongoing disk accesses. [end of text]
In situations where MRU is preferred, use a buffer to store older data. In situations where LRU is preferred, use a buffer to store newer data. [end of text]
The techniques for implementing the deletion are as follows:
a. Move record 6 to the space occupied by record 5, and move record 7 to the space occupied by record 6.
b. Move record 7 to the space occupied by record 5.
c. Mark record 5 as deleted, and move no records. [end of text]
a. Insert (Brighton, A-323, 1600).
b. Delete record 2.
c. Insert (Brighton, A-626, 2000). [end of text]
Variable-length record representation is preferred over pointers for several reasons. Variable-length records allow for efficient storage and retrieval of data, as they can store varying amounts of data in memory. This is particularly useful in applications where data size is not a concern, such as in databases. Additionally, variable-length records can be more memory-efficient than pointers, as they do not require additional memory to store the pointer itself. Finally, variable-length records can be more flexible than pointers, as they can store different data types in memory. [end of text]
Variable-length records are preferred over reserved-space methods because they allow for flexible data storage and retrieval, making them more suitable for large datasets. Variable-length records can accommodate varying lengths of data, making them ideal for applications that require handling of data of varying sizes. Reserved-space methods, on the other hand, require a fixed amount of space to store data, which can be limiting for large datasets. Variable-length records provide a more flexible solution for data storage and retrieval. [end of text]
Insert (Mianus, A-101, 2800). Insert (Brighton, A-323, 1600). Delete (Perryridge, A-102, 400). Silberschatz−Korth−Sudarshan: Database System Concepts, Fourth EditionIV. Data Storage and Querying11. Storage and File Structure443© The McGraw−Hill Companies, 2001442Chapter 11Storage and File Structure [end of text]
Yes, the textbook section is in the file of Figure 11.12. [end of text]
a. Mianus, A-101, 2800
b. Brighton, A-323, 1600
c. Perryridge, A-102, 400
Signiﬁcance is crucial. [end of text]
System running on local computer provides control mechanisms to replace pages, which is useful for database systems implementation. [end of text]
no, at the moment, only one overﬂow record exists. [end of text]
Store each relation in one file. Store multiple relations (perhaps even the entire database) in one file. [end of text]
course (course_id, course_name, room, instructor)enrollment (course_id, student_id, grade) | clustering: students by room, grade by student_id, course_id by room and student_id. [end of text]
Each block in the file is represented by two bits in a bitmap. When the block is between 0 and 30% full, the bits are 00. As the block size increases, the bits are updated accordingly. The bitmap technique can be stored in memory, making it suitable for large files. It helps in searching for free space and updating free space information. [end of text]
The normalized version of a database would generally result in worse performance due to the loss of data redundancy and potential data loss during normalization. [end of text]
Physical storage location is the location where data is stored in a database.
In case an object gets forwarded multiple times, the retrieval speed may decrease. A technique to avoid multiple accesses is to store the object in a cache. This way, the object can be retrieved faster by accessing the cache instead of the original location. [end of text]
Dangling pointers are a common issue in object-oriented databases, where objects are not properly released when they are no longer needed. This can lead to memory leaks, where the object is not freed, and the database consumes more memory than necessary. Detection and handling of dangling pointers are crucial for maintaining database performance and avoiding resource exhaustion. [end of text]
Hardware swizzling is used to change the short identifier of page 679.34278 from 2395 to 5001. Some other pages may have a short identifier of 5001. If they do, they can be handled by changing the identifier to 5001. This is possible because the system can locate the records directly. [end of text]
An index in a database system works similarly to a book index or card catalog in libraries. To find a particular topic, search the index at the back of the book, find pages with the topic, and read the pages to find the information. Indexes are sorted, making it easy to find words. The index is much smaller than the book, reducing search effort. Card catalogs in libraries work similarly, but are rarely used anymore. To find a book by a particular author, search the author catalog. To assist in searching, keep cards in alphabetic order by author, with one card per author per book. Database system indices play the same role as book indices or card catalogs in libraries. [end of text]
An index structure associates a search key with a particular record in a file, allowing fast random access to records using an ordered index. Records may be stored in sorted order within the index, similar to books in a library catalog. [end of text]
In this section, we assume that all ﬁles are ordered sequentially on some search key. Dense and sparse indices are used to represent index-sequential ﬁles. Dense indices store a list of pointers to all records with the same search-key value, while sparse indices store pointers to records with the largest search-key value less than or equal to the search-key value. The trade-off between access time and space overhead is to have a sparse index with one index entry per block. [end of text]
In this section, we assume that all files are ordered sequentially on some search key. Such files, with a primary index on the search key, are called index-sequential files. They represent one of the oldest index schemes used in database systems. They are designed for applications that require both sequential processing of the entire file and random access to individual records. Figure 12.1 shows a sequential file of account records taken from our banking example. In the example of Figure 12.1, the records are stored in search-key order, with branch-name used as the search key.
An index record is a search-key value and pointers to one or more records with that value. It consists of a disk block identifier and an offset within the block to identify the record. Dense indices store all records with the same search-key value, while sparse indices store only some records. Sparse indices require less space but impose less maintenance overhead for insertions and deletions. A trade-off is to have a sparse index with one index entry per block. [end of text]
The textbook discusses the concept of sparse indexing in databases, which involves using a sparse index to efficiently locate and scan records in a database. The index is constructed using a combination of blocks and pointers, allowing for efficient access to records. Multilevel indexing is also discussed, where the outer index is used to locate the largest search-key value less than or equal to the desired record, and the inner index is used to locate the record itself. The process of searching a large index may be costly, as it requires multiple disk block reads. Indices with two or more levels are called multilevel indices, and searching for records with such an index requires significantly fewer I/O operations than searching by binary search. [end of text]
Even if we use a sparse index, the index itself may become too large for efficient processing. It is not unreasonable, in practice, to have a file with 100,000 records, with 10 records per block. If we have one index record per block, the index has 10,000 records. Index records are smaller than data records, so let us assume that 100 index records fit on a block. Thus, our index occupies 100 blocks. Such large indices are stored as sequential files on disk. If an index is sufﬁciently small to be kept in main memory, the search time to ﬁnd an entry is low. However, if the index is so large that it must be kept on disk, a search for an entry requires several disk block reads. Binary search can be used on the index file to locate an entry, but the search still has a large cost. If the index occupies bblocks, binary search requires as many as ⌈log2(b)⌉ blocks to be read. For our 100-block index, binary search requires seven block reads. On a disk system where ablock read takes 30 milliseconds, the search will take 210 milliseconds, which is long. Note that, if overﬂow blocks have been used, binary search will not be possible. In that case, a sequential search is typically used, and that requires b block
Multilevel index: A sparse index on the contents of dictionary pages, used for in-memory indexing. Insertion and deletion algorithms involve updating the lowest-level index. Secondary indices are dense, with pointers to records in the file and may have a different structure from primary indices. [end of text]
Insertion and deletion algorithms for multilevel indices are a simple extension of the scheme just described. On deletion or insertion, the system updates the lowest-level index as described. As far as the second level is concerned, the lowest-level index is merely a file containing records—thus, if there is any change in the lowest-level index, the system updates the second-level index as described. The same technique applies to further levels of the index, if there are any. [end of text]
Secondary indices are dense, with an index entry for every search-key value, and pointers to every record in the file. A primary index may be sparse, storing only some of the search-key values, and sequential access to a part of the file is possible. A secondary index on a candidate key looks like a dense primary index, except that records pointed to by successive values in the index are not stored sequentially. If the search key of a primary index is not a candidate key, it suffices if the index points to the first record with a particular value for the search key, since other records can be fetched by a sequential scan of the file. A secondary index on a candidate key must contain pointers to all the records. Silberschatz-Korth-Sudarshan: Database System Concepts, Fourth Edition IV. Data Storage and Querying 12. Indexing and Hashing 454© The McGraw-Hill Companies, 2001 [end of text]
Secondary indices on searchkeys that are not candidate keys can be implemented using an extra level of indirection. Sequential scans in primary index order are efficient, but storing a file both by search key of the primary index and secondary key order can be challenging. Secondary indices improve query performance by allowing other search keys than the primary index. However, they introduce overhead for modifications. The designer of a database decides which indices are desirable based on query and modification frequencies. [end of text]
The main disadvantage of the index-sequential file organization is that its performance degrades as the file grows, both for index lookups and for sequential scans through the data. Although this degradation can be remedied by reorganization of the file, frequent reorganizations are undesirable. The B+-tree structure, which takes the form of a balanced tree with up to n −1 search-key values and n children, is widely used for index structures that maintain their efﬁciency despite insertion and deletion of data. However, it imposes performance overhead on insertion and deletion and adds space overhead. The overhead is acceptable even for frequently modiﬁed ﬁles, since the cost of ﬁle reorganization is avoided. Furthermore, since nodes may be as much as half empty (if they have the minimum number of children), there is some wasted space. This space overhead is acceptable given the performance benefits of the B+-tree structure. [end of text]
B+-tree index is a multilevel index with a structure different from that of multilevel sequential ﬁle. It has up to n-1 search-key values and pointers to either a file record or a bucket of pointers. Leaf nodes contain up to n-1 values, and pointers are used only if the search key does not form a primary key and the file is not sorted in the search-key value order. Nonleaf nodes hold up to n pointers, and must be pointers to tree nodes. [end of text]
In a B+-tree, the root node can hold fewer than ⌈n/2⌉ pointers, but it must hold at least two pointers unless the tree consists of only one node. The root must have less than ⌈n/2⌉ pointers to ensure balanced performance. Queries on a B+-tree involve traversing a path from the root to a leaf node, with paths no longer than ⌊log⌈n/2⌉(K)⌋. In practice, only a few nodes need to be accessed, typically a disk block size of 4 kilobytes. With a search-key size of 12 bytes and a disk-pointer size of 8 bytes, n is around 100, even with a conservative estimate of 32 bytes for the search-key size. With n = 1 million search-key values, a lookup requires only procedure find(value V ) set C = root nodewhile C is not a leaf node begin. [end of text]
In a B+-tree, we traverse a path from the root to a leaf node, where the path is no longer than⌈log⌈n/2⌉(K)⌉. If there are K search-key values in the file, the path is no longer than⌈log⌈n/2⌉(K)⌉. With a search-key size of 12 bytes, and a disk-pointer size of 8 bytes, n is around 200. Even with a conservative estimate of 32 bytes for the search-key size, n is around 100. With n = 100, if we have 1 million search-key values in the file, a lookup requires only procedure find(value V )set C = root nodewhile C is not a leaf node begin. [end of text]
The B+-tree is a balanced binary tree with a large node size and a small number of pointers. Insertion and deletion are more complicated than lookup, as splitting a node becomes necessary if it becomes too large or too small. The general technique for insertion into a B+-tree is to determine the leaf node into which insertion must occur. If a split results, insert the new node into the parent of the node. If this insertion causes a split, proceed recursively up the tree until either an insertion does not cause a split or a new root is created. Deletions that cause too few pointers to be split are also handled. [end of text]
Insertion and deletion are more complicated than lookup, as they require splitting nodes that become too large or too small, and ensuring balance. When a node is split, we must insert the new leaf node into the B+-tree structure, and ensure that the balance is preserved. Deletion involves inserting a record into the B+-tree, and removing the search-key value from the leaf node if there is no bucket associated with that value or if the bucket becomes empty. The general technique for insertion into a B+-tree is to determine the leaf node into which insertion must occur, and insert the new entry into the parent of that node. Deletions cause tree nodes to contain too few pointers, which requires elimination of nodes along the path to the root. [end of text]
The B+-tree is a data structure used in databases, where each node contains a value and pointers to its children. When inserting a new entry, the procedure `insert` checks if the node has space for the new value and entry. If not, it splits the node into two, creating two new nodes. The procedure `insert entry` inserts the new value into the node and its children. When deleting a node, the procedure `delete` removes the pointer to the node from its parent. In the example, deleting "Downtown" from the B+-tree of Figure 12.14 causes the node to become empty, and the parent node becomes too small, so the B+-tree is restructured to accommodate the new node. The resulting B+-tree appears in Figure 12.15. [end of text]
The B+-tree is a data structure used in database indexing. It stores entries in a hierarchical manner, with each node containing a key and pointers to its children. The B+-tree is optimized for efficient deletion operations. The pseudocode outlines the deletion algorithm for a B+-tree, which involves finding the parent node with the key and deleting the entry from it. The pseudocode also describes how to redistribute entries by borrowing a single entry from an adjacent node or by redistributing entries equally between the two nodes. The B+-tree is used for index-sequential file organization, where the main drawback is the degradation of performance as the file grows. The B+-tree is a frequently used index structure in database implementations. [end of text]
The main drawback of index-sequential file organization is the degradation of performance as the file grows. By using B+-tree indices and leaf levels, we solve the degradation problem for both index lookups and storing actual records. [end of text]
B+-tree is an index and organizer for records in a file, allowing efficient insertion and deletion. It uses a B-tree structure to store leaf nodes and pointers to records, ensuring that leaf nodes are at least half full. Insertion and deletion of records are handled in the same way as in an index. When a record is inserted, the system locates the block containing the largest key in the tree that is ≤v. If the block has enough free space, the record is stored in the block. Otherwise, it splits the block in two and redistributes the records in each block. When a record is deleted, the system removes it from the block containing it. If a block becomes less than half full as a result, it redistributes the records with the adjacent blocks. [end of text]
B-tree indices are similar to B+-tree indices. They eliminate redundant search-key values by storing search keys only once in a B-tree. However, since search keys appear in nonleaf nodes, additional pointers are needed for each key to ensure efficient storage. Nonleaf nodes store pointers Bi for search keys, while leaf nodes store pointers Pi. The discrepancy in keys between nonleaf and leaf nodes arises from the need to include pointers for B+-tree nodes. [end of text]
B-trees offer space advantages for large indices, but their disadvantages outweigh these. Many database system implementers prefer B+-trees. [end of text]
Hashing allows us to avoid accessing an index structure, resulting in fewer I/O operations. It also provides a way to construct indices. In the following sections, we study hash file organization and indexing based on hashing. [end of text]
In a hash file organization, we obtain the address of the disk block containing an adesired record directly by computing a function on the search-key value of the record. A bucket is a unit of storage that can store one or more records. A hash function maps search-key values to bucket addresses. To insert a record, we compute the hash value and store it in the bucket. To perform a lookup, we compute the hash value and search the bucket for the record. To delete a record, we compute the hash value and search the corresponding bucket for the record. The worst possible hash function maps all search-key values to the same bucket, while an ideal hash function distributes the stored keys uniformly across all buckets. [end of text]
The worst possible hash function distributes search-key values uniformly across all buckets, ensuring uniform distribution and random distribution. [end of text]
The textbook discusses the use of hash functions to distribute records uniformly across buckets, with a hash function that divides the search key balance into 10 ranges, and a hash function that requires careful design to avoid bucket overflows. The author also mentions that bucket overflows can occur due to insufficient buckets, skew, and wasted space. The book concludes by discussing how to handle bucket overflows by using overflows buckets, which provide an additional space for records. [end of text]
In database systems, the number of buckets must be chosen such that it exceeds the number of records that fit in a bucket, leading to bucket overﬂow. Skew occurs when some buckets have more records than others, causing overﬂow. To reduce this, the number of buckets is adjusted to (nr/fr) * (1 + d), where d is a fudge factor, typically around 0.2. About 20% of the space in the buckets is wasted, but the benefit is a reduced probability of overﬂow. Despite allocating more buckets, overﬂow can still occur. Overﬂow is handled by using overﬂow buckets. If a record must be inserted into a bucket, and the bucket is full, the system provides an overﬂow bucket. If the overﬂow bucket is also full, another overﬂow bucket is provided. [end of text]
Hashing can be used for file organization and index creation, but it must be chosen dynamically. [end of text]
Hashing can be used for file organization and index structure creation. It organizes search keys into hash files, with pointers stored in buckets. The hash function calculates the sum of digits modulo 7. [end of text]
In database systems, buckets are used to store data, with each bucket containing three keys. The overﬂowbucket is used to handle overflow situations, while the primary key is used to store the unique identifier for each record. Hash indices are used to index records, providing direct access to the data. However, since hash files provide the same access as indexing, a hash file can also be considered a primary index structure. [end of text]
As databases grow larger, extending the hash function to accommodate changes in the database size is a viable option. However, reorganization involves choosing a new hash function and recomputing it on every record, resulting in significant space wastage. Dynamic hashing techniques allow the hash function to be modified dynamically to accommodate growth or shrinkage. In this section, we describe extendable hashing, a form of dynamic hashing that splits and coalesces buckets as the database grows and shrinks. This approach retains space efficiency and allows for efficient reorganization. [end of text]
Extendable hashing copes with database size changes by splitting and coalescing buckets. It ensures space efficiency and reduces performance overhead. Use uniform and random hash functions. Create buckets on demand, using i bits for each entry. The number of entries in the bucket address table changes with the database size. Each entry has a common hash prefix length, but this may be less than i. [end of text]
To locate a bucket containing search-key value Kl, the system uses the first ihigh-order bits of h(Kl) to look up the corresponding table entry, followed by the bucket pointer. If there is room, it inserts the record. [end of text]
The system inserts the record in the bucket, splits the bucket if full, and rehashes each record. If all records have the same hash value, it reuses the bucket. If not, it creates a new bucket and rehashes. The system reinserts the record, and repeats the process. [end of text]
The extendable hash structure allows for efficient storage and retrieval of account records, while maintaining minimal space overhead. This system splits the bucket address table into two entries for each hash value, reducing the number of pointers needed for each record. The system also handles overflows by using an overﬂow bucket, which is a separate bucket for records with the same hash value. The main advantage of extendable hashing is that performance does not degrade as the file grows, while minimal space overhead is minimal compared to other schemes. [end of text]
The main advantage of extendable hashing is that performance does not degrade as the file grows, and it minimizes space overhead. Although the bucket address table incurs additional overhead, it contains one pointer for each hash value for the current pre-hash prefix. [end of text]
The textbook discusses the use of extendable hashing, a technique that avoids the extra level of indirection associated with extendable hashing, at the cost of more overﬂow buckets. It also mentions that extendable hashing is attractive, provided that it is implemented with the added complexity involved. The text provides detailed descriptions of extendable hashing implementation and another form of dynamic hashing called linear hashing. [end of text]
In database systems, ordered indexing and hashing schemes offer distinct advantages. B+-tree organization is suitable for frequent insertions and deletions, while hash structures are preferable for queries that require range-based access. The expected type of query is critical in choosing an index or hash structure, with ordered indexing being preferable for range queries. [end of text]
Let us consider how we process this query using an ordered index. First, we perform a lookup on value c1. Once we have found the bucket for value c1, we follow the pointer chain in the index to read the next bucket in order, and we continue in this manner until we reach c2. If we have a hash structure, we can perform a lookup on c1 and locate the corresponding bucket—but it is not easy, in general, to determine the next bucket that must be examined. The difﬁculty arises because a good hash function assigns values randomly to buckets. Thus, there is no simple notion of "next bucket in sorted order." The reason we cannot chain buckets together in sorted order on Ai is that each bucket is assigned many search-key values. Since values are scattered randomly by the hash function, the values in the specified range are likely to be scattered across many or all of the buckets. Therefore, we have to read all the buckets to ﬁnd the required search keys. Usually the designer will choose ordered indexing unless it is known in advance that range queries will be infrequent, in which case hashing would be chosen. Hash organizations are particularly useful for temporary files created during query processing, if lookups based on a key value are required, but no range queries will be performed. [end of text]
The SQL standard does not provide any way for database users or administrators to control indices, but indices are important for efficient processing of transactions and integrity constraints. Most SQL implementations provide data-deﬁnition-language commands to create and remove indices. The syntax of these commands is widely used and supported by many database systems, but it is not part of the SQL:1999 standard. [end of text]
Assume that the account file has two indices: one for branch-name and one for balance. For certain types of queries, it is advantageous to use multiple indices if they exist. This allows for faster processing of queries that involve multiple records with specific criteria. [end of text]
Assume that the account file has two indices: one for branch-name and one for balance. Consider the following query: "Find all account numbers at the Perryridge branch with balances equal to $1000." We select loan-number from account where branch-name = "Perryridge" and balance = 1000. There are three strategies possible for processing this query: 1. Use the index on branch-name to find all records pertaining to the Perryridge branch. Examine each such record to see whether balance = 1000. 2. Use the index on balance to find all records pertaining to accounts with balances of $1000. Examine each such record to see whether branch-name = "Perryridge." 3. Use the index on branch-name to find pointers to all records pertaining to the Perryridge branch. Also, use the index on balance to find pointers to all records. [end of text]
To record both Perryridge and accounts with a balance of $1000 using an intersection strategy, scan a large number of pointers to obtain a small result. An index structure called a "bitmap index" greatly speeds up the intersection operation used in the third strategy. Bitmap indices are outlined in Section 12.9.4.12.9.2Indices on Multiple KeysAn alternative strategy for this case is to create and use an index on a search key (branch-name, balance)—that is, the search key consisting of the branch name concatenated with the account balance. The structure of the index is the same as any other index, the only difference being that the search key is not a single attribute, but rather a list of attributes. The search key can be represented as a tuple of values, of the form (a1, . . . , an), where the indexed attributes are A1, . . . , An. The ordering of search-key values is the lexicographic ordering. For example, for the case of two attribute search keys, (a1, a2) < (b1, b2) if either a1 < b1 or a1 = b1 and a2 < b2. Lexicographic ordering is basically the same as alphabetic ordering of words. The use of an ordered-index structure on multiple attributes has a few short-comings. As an illustration, consider the query select loan-numberfrom account where branch-name < “
An alternative strategy involves creating an index on a search key consisting of the branch name and account balance, where the search key is a list of attributes. This structure allows for efficient querying and indexing, but may cause issues with I/O operations due to the ordering of indexed attributes. Special structures like the grid file and R-tree can be used to speed up multiple search-key queries involving multiple comparison operations. [end of text]
The grid-ﬁle on keys branch-name and balance of the account ﬁle contains a single grid array with one linear scale for each search-key attribute. To insert a record with search-key value ("Brighton", 500000), we locate the row and column to which the cell belongs using linear scales on branch-name. [end of text]
In this textbook, we summarize the concept of a linear scale on balance, which is used to find the cell in a grid that maps to a search key. We also discuss how to use a grid-ﬁle index to answer queries on multiple keys, and how to optimize the grid-ﬁle approach by expanding the grid array and using expanded linear scales. The textbook also explains the concept of a bitmap index, which is a specialized type of index designed for easy querying on multiple keys. The use of these techniques allows for efficient querying of multiple keys and reduces processing time. [end of text]
Bitmap indices are specialized for easy querying on multiple keys, designed for sequential numbering of records. They are useful for data analysts to simplify analysis of data by breaking values into small ranges. [end of text]
A bitmap is an array of bits used to index attribute values in relation r. Each bitmap contains one bit for each value, with the number of bits equal to the number of records. The ith bit of the bitmap for value vj is set to 1 if the record numbered i has the value vj, and all other bits are set to 0. Bitmap indices are useful for retrieving records with specific values, but they do not significantly speed up queries. [end of text]
In bitmap indices, selecting women with income in the range 10, 000 -19, 999 can be efficiently computed by finding the intersection of the bitmap for gender = f (01101) and the bitmap for income-level = L1 (10100). The intersection of these bitmaps gives the bitmap 00100, which contains only about 1 in 10 records on average. The existence bitmap can be used to count the number of records satisfying the condition. For example, if we want to find out how many women have an income level L2, we compute the intersection of the two bitmaps and count the number of bits that are 1 in the intersection bitmap. [end of text]
The textbook explains how to efficiently compute the intersection and union of bitmaps using bit-wise and instructions, and how to handle null values and deletions. It also discusses counting the number of bits that are 1 in a bitmap and how to handle unknown predicates. [end of text]
Bitmaps are used to represent the list of records for a particular value in a relation, where a few attribute values are extremely common, and other values also occur, but much less frequently. In a B+-tree index leaf, a bitmap is preferred for representing the list of records. [end of text]
Bitmaps can be used as a compressed storage mechanism at the leaf nodes of B+-trees for values that occur very frequently. [end of text]
Index-sequential file organization can reduce overhead in searching for records. B+-tree indices are used for indexing a file and organizing records into a file. B-tree indices eliminate redundant storage of search-key values, while B+-tree indices are similar to B-tree indices. Sequential file organization requires an index structure to locate data, while hashing organization allows direct address computation. Static hashing uses uniform distribution, while dynamic hashing allows changing distribution. Grid file organization provides indexing on multiple attributes, while bitmap index provides a compact representation for indexing attributes with few distinct values. Intersection operations on multiple indices are extremely fast. [end of text]
The textbook section discusses the basics of databases, including tables, data types, and relationships. It covers the fundamental concepts of database design, data management, and data retrieval. The section also covers the use of SQL for database operations and the use of database management systems (DBMS). The textbook emphasizes the importance of data security and privacy in database management. [end of text]
search keys:
- Subject
- Query
- Keyword
- Term
- Phrase
- Boolean operators
- Boolean operators
- Boolean operators
- Boolean operators
- Boolean operators
- Boolean operators
- Boolean operators
- Boolean operators
- Boolean operators
- Boolean operators
- Boolean operators
- Boolean operators
- Boolean operators
- Boolean operators
- Boolean operators
- Boolean operators
- Boolean operators
- Boolean operators
- Boolean operators
- Boolean operators
- Boolean operators
- Boolean operators
- Boolean operators
- Boolean operators
- Boolean operators
- Boolean operators
- Boolean operators
- Boolean operators
- Boolean operators
- Boolean operators
- Boolean operators
- Boolean operators
- Boolean operators
- Boolean operators
- Boolean operators
- Boolean operators
- Boolean operators
- Boolean operators
- Boolean operators
- Boolean operators
- Boolean operators
- Boolean operators
- Boolean operators
- Boolean operators
- Boolean operators
- Boolean operators
- Boolean operators
- Boolean operators
- Boolean operators
- Boolean operators
- Boolean operators
- Boolean operators
- Boolean operators
- Boolean operators
- Boolean operators
- Boolean operators
- Boolean operators
- Boolean operators
- Boolean operators
- Boolean operators
- Boolean operators
- Boolean operators
- Boolean operators
- Boolean operators
- Boolean operators
- Boolean operators
- Boolean operators
- Boolean operators
- Boolean operators
- Boolean operators
- Boolean
Indexes and hash tables are two common data structures used in database management. Indexes store frequently accessed data in a contiguous block of memory, while hash tables use a hash function to map data to a location in memory. Both structures allow for fast access to data, but hash tables are more efficient for large datasets. Indexes are particularly useful for reducing the number of disk accesses, while hash tables are useful for quickly finding data by key. [end of text]
B+-trees are constructed for the given scenarios:
- Four pointers required for one node
- Six pointers required for one node
- Eight pointers required for one node
The B+-tree is a self-balancing tree data structure that ensures efficient insertion, deletion, and search operations. It is particularly useful in scenarios where the number of pointers needed for a node is fixed. [end of text]
Queries:
a. Find records with a search-key value of 11.
b. Find records with a search-key value between 7 and 17, inclusive. [end of text]
The textbook series of operations involve inserting numbers into a database. The sequence of operations is: Insert 9, Insert 10, Insert 8, Delete 23, Delete 19. [end of text]
The expected height of a tree as a function of its number of branches is exponential. [end of text]
Techniques like indexing, partitioning, and normalization offer advantages in database applications, enhancing data management and query performance.
To reduce the occurrence of bucket overflows, implement robust data validation and monitoring strategies, and ensure that data is stored in a secure, encrypted environment. [end of text]
The hash table for the file with the given hash function h(x) = xmod 8 and buckets can hold three records is an extendable hash table. [end of text]
The textbook summarizes the steps of database system concepts, including data storage and querying, and provides a bibliography for further reading. It also mentions the McGraw-Hill Companies, 2001 edition. [end of text]
coalescing buckets is not necessary for reducing the size of the bucket address table. [end of text]
Maintaining a count when buckets are split, coalesced, or deleted involves storing an extra count in the bucket address table. Reducing the size of the bucket address table is expensive and may cause the table to grow again. Therefore, it is best to reduce the size only if the number of index entries becomes small compared to the bucket address table size. [end of text]
Queries are likely to be used in databases to retrieve information from a database. [end of text]
In cases where an overﬂow bucket is needed, we reorganize the grid file to avoid overflows. This algorithm involves restructuring the data to fit within the available space. [end of text]
To construct a bitmap index on the attributes branch-name and balance, we divide balance values into four ranges: below 250, 250 to below 500, 500 to below 750, and 750 and above. For the query, we need to find all accounts with a balance of 500 or more. We outline the steps in answering the query: 
1. Construct a bitmap index on the attributes branch-name and balance.
2. For each account, check if its balance is 500 or more.
3. If the account's balance is 500 or more, add it to the result. 
The intermediate bitmaps are constructed by dividing balance values into the four ranges and marking the corresponding accounts in the bitmap index. The bitmap index is then used to quickly find all accounts with a balance of 500 or more. [end of text]
Your technique works even in the presence of null values by using a bitmap for the value null. [end of text]
B-tree indices, Tries, and other search structures are proposed to allow concurrent accesses and updates on B+-trees. [end of text]
The steps involved in processing a query appear in Figure 13.1. The basic steps are 1. Parsing and translation 2. Optimization 3. Evaluation. Before query processing can begin, the system must translate the query into its internal form. This translation process is similar to the work performed by the parser of a compiler. In generating the internal form of the query, the parser checks the syntax of the user's query, verifies that the relation names appearing in the query are names of the relations in the database, and so on. The system constructs a parse-tree representation of the query, which it then translates into a relational-algebra expression. If the query was expressed in terms of a view, the translation phase also replaces all uses of the view by the relational-algebra expression. [end of text]
The textbook summarizes the concepts of query optimization, query execution, and evaluation plans for a SQL query. It also outlines the cost of each operation and provides a rough estimate for each operation's execution cost. The cost is estimated based on various parameters such as actual memory available to the operation. The cost is measured using the cost of each operation and the cost of each operation is estimated based on various parameters such as actual memory available to the operation. The cost is measured using the cost of each operation and the cost of each operation is estimated based on various parameters such as actual memory available to the operation. The cost is measured using the cost of each operation and the cost of each operation is estimated based on various parameters such as actual memory available to the operation. The cost is measured using the cost of each operation and the cost of each operation is estimated based on various parameters such as actual memory available to the operation. The cost is measured using the cost of each operation and the cost of each operation is estimated based on various parameters such as actual memory available to the operation. The cost is measured using the cost of each operation and the cost of each operation is estimated based on various parameters such as actual memory available to the operation. The cost is measured using the cost of each operation and the cost of each operation is estimated based on various parameters such as actual memory available to the operation. The cost is measured using the cost of each operation and the cost of each operation is estimated based on various parameters such as actual memory
In Section 13.7, we discuss how to coordinate multiple operations in a query evaluation plan, focusing on pipelined operations to avoid intermediate results to disk. [end of text]
The cost of query evaluation can be measured in terms of disk accesses, CPU time, and communication costs. Disk accesses are the most important cost, with CPU speeds improving faster than disk speeds. Estimating disk-access cost is hard compared to estimating CPU time. Most people consider disk-access cost a reasonable measure of query evaluation plan cost. [end of text]
File scans are the lowest-level operators in database query processing. They search for data based on selection conditions. Data Storage and Querying is a chapter in the McGraw-Hill Computer Textbook series. The text covers file scans, data storage, and query processing. [end of text]
Linear search is a basic algorithm for reading a relation's entire contents in cases where the relation is stored in a single, dedicated file. Binary search is a binary search algorithm for locating records that satisfy a selection condition on a key attribute. Index structures allow quick access to records in a sorted order, useful for implementing range queries. Silberschatz-Korth-Sudarshan: Database System Concepts, Fourth Edition, IV. Data Storage and Querying. Primary index, equality on key. For an equality comparison on a key attribute with a primary index, we can use the index to retrieve a single record that satisfies the corresponding equality condition. Silberschatz-Korth-Sudarshan: Database System Concepts, Fourth Edition, IV. Data Storage and Querying. Primary index, equality on nonkey. We can retrieve multiple records by using a primary index when the selection condition specifies an equality comparison on a nonkey attribute. Silberschatz-Korth-Sudarshan: Database System Concepts, Fourth Edition, IV. Data Storage and Querying. Primary index, equality. For an equality comparison on a key attribute with a primary index, we can use the index to retrieve a single record that satisfies the corresponding equality condition. Silberschatz-Korth-Sudarshan: Database System Concepts, Fourth Edition, IV. Data Storage and Querying. Primary index, equality on nonkey. We can retrieve multiple records by using a primary index when the selection condition specifies an equality comparison on
Linear search is a method for selecting records from a relation, with a cost of br/2. Binary search is an alternative for key attribute selections, with a cost of br/2, but requires more blocks to be examined. Both methods can be applied to any ﬁle, regardless of ordering, availability of indices, or nature of selection. [end of text]
Index structures are used to provide a path through data and access records in an order that corresponds to physical order. Search algorithms that use an index are called index scans. Ordered indices like B+-trees allow accessing tuples in a sorted order, useful for range queries. Index scans are guided by the selection predicate, which helps in choosing the right index to use in a query. Search algorithms that use an index include Silberschatz-Korth-Sudarshan's database system concepts. [end of text]
A linear or binary search can be used to implement the selection σA≤v(r) by utilizing a primary index. For comparison conditions of the form A > v or A ≥v, a primary index on A can be used to direct the retrieval of tuples, as described. [end of text]
In database systems, we can use secondary ordered indexes to guide retrieval for comparison conditions involving <, ≤, ≥, or >. The lowest-level index blocks are scanned, either from the smallest value up to v (for < and ≤) or from v up to the maximum value (for > and ≥). The secondary index provides pointers to the records, but to get the actual records we have to fetch them by using the pointers. This step may require an I/O operation for each record fetched, since consecutive records may be on different disk blocks. If the number of retrieved records is large, using the secondary index may be even more expensive than using linear search. [end of text]
In the context of databases, selection predicates allow for more complex conditions, such as conjunctions and disjunctions of simple conditions. These operations can be implemented using various algorithms, including algorithms A8, A9, and A10. The cost of these operations can be reduced by using appropriate indexes and algorithms that minimize the cost of the combined index scans and retrieval of pointers. The implementation of negation conditions is left to the reader as an exercise. [end of text]
501: The textbook summarizes the content of Chapter 501 in a Databases textbook. [end of text]
Sorting of data plays a crucial role in database systems for ensuring efficient query processing and efficient data retrieval. Sorting can be achieved through various techniques, such as building an index on the sort key and reading the relation in sorted order. However, such a process orders the relation logically through an index, rather than physically, leading to disk access for each record. External sorting involves handling relations that do not fit in memory, where standard sorting techniques like quick-sort can be used. The external sort–merge algorithm is a common technique for external sorting, where the relation is first sorted in memory and then merged into a single sorted output. The output of the merge stage is the sorted relation, which is buffered to reduce disk write operations. The initial pass in the external sort–merge algorithm merges the first M −1 runs, reducing the number of runs by a factor of M −1. If the reduced number of runs is still greater than or equal to M, another pass is made, with the runs created by the first pass as input. Each pass reduces the number of runs by a factor of M −1. The passes repeat as many times as required, until the number of runs is less than M; a final pass generates the sorted output. [end of text]
The textbook explains how to compute the number of block transfers required for external sorting in a relation, given the number of records per relation and the merge pass ratio. It calculates the total number of disk accesses by considering the number of runs, merge passes, and the cost of writing out the final result. The equation provides a more accurate count by considering the savings due to the final write operation. [end of text]
The nested-loop join algorithm is expensive due to examining every pair of tuples in two relations. The cost is proportional to the number of pairs, which is \(nr \times ns\), where \(nr\) is the number of records in relation \(r\) and \(ns\) is the number of records in relation \(s\). For each record in \(r\), we need to perform a complete scan on \(s\). In the worst case, the buffer can hold only one block of each relation, and a total of \(nr \times bs + br\) block accesses would be required, where \(br\) and \(bs\) denote the number of blocks containing tuples of \(r\) and \(s\) respectively. In the best case, there is enough space for both relations to fit simultaneously in memory, so each block would have to be read only once. If one relation fits entirely in main memory, our strategy requires only atotal \(br + bs\) accesses—the same cost as the case where both relations fit in memory. If we use customer as the inner relation and depositor as the outer relation, the worst-case cost of our final strategy would be lower, with only \(10000 \times 100 + 400 = 1,000,400\) block accesses. [end of text]
The nested-loop join algorithm is expensive due to examining every pair of tuples in two relations. The number of pairs to be considered is nr ∗ns, where nr denotes the number of tuples in r and ns denotes the number of tuples in s. For each record in r, we have to perform a complete scan on s. In the worst case, the buffer can hold only one block of each relation, and a total of nr ∗bs + br block accesses would be required, where br and bs denote the number of blocks containing tuples of r and s respectively. In the bestcase, there is enough space for both relations to fit simultaneously in memory, so each block would have to be read only once; hence, only br + bs block accesses would be required. If one of the relations fits entirely in main memory, our strategy requires only total br + bs accesses—the same cost as that for the case where both relations fit in memory. Now consider the natural join of depositor and customer. Assume no indices on either relation, and that we are not willing to create any index. We can use the nested loops to compute the join; assume depositoris the outer relation and customer is the inner relation in the join. We will have to examine 5000 ∗10000 = 50 ∗106 pairs of tuples. In the worst case, the number of block accesses is 5000 ∗400 + 
If the buffer is too small, we can process relations on a per-block basis to save block accesses. [end of text]
The block nested-loop join is more efficient than the basic nested-loop join in terms of block accesses, with a total of 40, 100 block accesses in the worst case. The indexed nested-loop join can reduce the number of disk accesses needed by ordering data from the previous scan, while leaving space for the buffers and index. The performance can be further improved by using an indexed nested-loop join with an index on the join attribute. [end of text]
In a nested-loop join, if an index is available on the inner loop's join attribute, it can replace file scans. For each tuple tr in the outer relation r, the index is used to look up tuples in s that satisfy the join condition. This join method is called an indexed nested-loop join, and it can be used with existing indices or temporary indices. Indexing allows faster lookups, but it can increase storage requirements. The cost of an indexed nested-loop join can be computed as the sum of index accesses and record counts. [end of text]
The merge join algorithm is used to compute natural joins and equi-joins. It combines relations R and S to find common attributes, then performs a natural join on these attributes. Silberschatz-Korth-Sudarshan, Database System Concepts, Fourth Edition, IV. Data Storage and Querying, 13. Query Processing, 508 © The McGraw-Hill Companies, 2001. [end of text]
The merge join algorithm requires that all tuples in the main memory for both relations have the same join attribute. The algorithm associates one pointer with each relation, and these pointers point initially to the first tuple of each relation. As the algorithm proceeds, the pointers move through the relations. A group of tuples of one relation with the same value on the join attributes is read into Ss. Silberschatz−Korth−Sudarshan: Database System Concepts, Fourth EditionIV. Data Storage and Querying13. Query Processing510© The McGraw−Hill Companies, 2001508Chapter 13Query ProcessingThe algorithm in Figure 13.6 requires that every set of tuples Ss ﬁt in main memory;we shall look at extensions of the algorithm to avoid this requirement later in this section. Then, the corresponding tuples (if any) of the other relation are read in, and are processed as they are read. [end of text]
This requirement can usually be met, even if the relation s is large. If it cannot be met, a block nested-loop join must be performed between Ss and the tuples in r with the same values for the join attributes. The overall cost of the merge join increases as a result. It is also possible to perform a variation of the merge join operation on unsorted tuples, if secondary indices exist on both join attributes. The algorithm scans the record through the indices, resulting in their being retrieved in sorted order. This variation presents a significant drawback, however, since records may be scattered throughout the file blocks. Hence, each tuple access could involve accessing a disk block, and that is costly. To avoid this cost, we can use a hybrid merge–join technique, which combines indices with merge join. Suppose that one of the relations is sorted; the other is unsorted, but has a secondary B+-tree index on the join attributes. The hybrid merge–join algorithm merges the sorted relation with the leaf entries of the secondary B+-tree index. The result file contains tuples from the sorted relation and addresses for the unsorted relation. The result file is then sorted on the addresses of tuples from the unsorted relation, allowing efficient retrieval of the corresponding tuples in physical storage order. Extensions of the technique to two unsorted relations are left as an exercise for you. [end of text]
The hash join algorithm is a natural join algorithm that partitions tuples of two relations into sets based on their join attributes using a hash function. The algorithm assumes that the hash function has the "goodness" properties of randomness and uniformity. The idea behind the hash join algorithm is to test tuples in one relation only if their join attributes are the same as in the other relation. The hash index on each partition is built in memory and used to retrieve records that match records in the probe input. The build and probe phases require only a single pass through both the build and probe inputs. The value nh must be chosen to be large enough such that, for each i, the tuples in the partition Hsi of the build relation and the hash index on the partition will fit in memory. The size of the probe relation must be less than or equal to M. [end of text]
In database systems, partitioning is used to divide data into smaller, manageable pieces for efficient querying. Recursive partitioning is a technique where relations cannot be partitioned in one pass due to memory constraints. Hash-table overflows occur when the hash index on a partition is larger than the available memory. Overflows can be handled by increasing the number of partitions or using overﬂow resolution or avoidance techniques. The cost of a hash join is estimated to be 3(br + bs) + 4nh, where br and bs are the number of blocks in relations r and s, respectively. [end of text]
Recursive partitioning is used when the number of page frames of memory is greater than the number of partitions. The system repeats this splitting until each partition fits in memory, avoiding recursive partitioning. [end of text]
Hash-table overﬂow occurs in partition i of the build relation s if the hash index on Hsi is larger than main memory. It can occur due to many tuples with the same join attributes or non-random hash function. Skewed partitions can be handled by increasing the number of partitions and using a fudge factor. Overflows can be handled by either overﬂow resolution or overﬂow avoidance. Overﬂow resolution is during build phase, overﬂow avoidance is during partitioning. [end of text]
The cost of a hash join is 3(br + bs) + 4nh, where br and bs denote the number of blocks containing records of relations r and s, respectively. [end of text]
The hybrid hash–join algorithm is useful when memory sizes are relatively large, but not all of the build relation fits in memory. Hybrid hash–join reduces the number of passes required for partitioning, thereby minimizing the number of block transfers. The cost estimate for the join is reduced by 1500 block transfers. The hybrid hash–join can be improved if the main memory size is large. When the entire build input can be kept in main memory, nh can be set to 0, and the hash join algorithm executes quickly without partitioning the relations into temporary files. The cost estimate goes down to br + bs. The hybrid hash–join can save write and read access for each block of both Hr0 and Hs0. [end of text]
The hybrid hash–join algorithm is useful when memory sizes are relatively large, but not all of the build relation fits in memory. [end of text]
Nested-loop and block nested-loop joins can be used regardless of join conditions. Other join techniques are more efficient but can only handle simple join conditions. Complex join conditions can be handled using efﬁcient join techniques if developed in Section 13.3.4. The overall join can be computed by first computing simpler joins and then unioning the results. [end of text]
Other relational operations and extended relational operations can be implemented as outlined in Sections 13.6.1 through 13.6.5. Data Storage and Querying is a textbook by Silberschatz, Korth, and Sudarshan, Fourth Edition, covering database system concepts. [end of text]
Duplicate elimination can be implemented by sorting tuples and removing duplicates during external sort–merge. Projection can be implemented by partitioning and reading tuples, followed by in-memory hash index construction and scanning. Set operations can be implemented by sorting and scanning. Hashing provides another way to implement set operations. Outer join operations can be computed using either left or right outer joins, depending on the schema of the join. [end of text]
We can implement duplicate elimination easily by sorting. Identical tuples will appear adjacent during sorting, and all but one copy can be removed. With external sort-merge, duplicates can be removed before writing to disk, reducing block transfers. Remaining duplicates can be eliminated during merging and the final sorted run will have no duplicates. The worst-case cost estimate for duplicate elimination is the same as sorting the relation. We can also implement duplicate elimination by hashing, as in the hash join algorithm. First, the relation is partitioned on a hash function on the whole tuple. Then, each partition is read and an in-memory hash index is constructed. While constructing the index, a tuple is inserted only if it is not already present. After all tuples in the partition have been processed, the tuples in the index are written to the result. The cost estimate is the same as that for processing (partitioning and reading each partition) of the relation. Because of the relatively high cost of duplicate elimination, SQL requires an explicit request by the user to remove duplicates; otherwise, duplicates are retained. [end of text]
Projection can be easily implemented by removing duplicates from each tuple, and generalized projection eliminates duplicates by the methods described in Section 13.6.1. If attributes include a key, no duplicates exist. Generalized projection can be implemented by removing duplicates in the same way. [end of text]
We can implement union, intersection, and set-difference operations by sorting both relations and scanning once through each sorted relation to produce the result. In r ∪s, when a concurrent scan reveals the same tuple in both relations, only one is retained. The result of r ∩s contains tuples present in both relations. Set difference r −s is implemented similarly by retaining tuples present in r only if they are absent in s. For all operations, only one scan is required, with a cost of br + bs. If relations are not sorted, the cost of sorting must be included. Any sort order can be used in evaluation of set operations, provided both inputs have the same sort order. Hashing provides another way to implement these set operations. The first step is to partition the relations by the same hash function, creating partitions Hr0, Hr1, ..., Hrh and Hs0, Hs1, ..., Hhsnh. Depending on the operation, the system takes these steps on each partition i = 0, 1, ..., nh:Silberschatz-Korth-Sudarshan: Database System Concepts, Fourth Edition IV. Data Storage and Querying 13. Query Processing 517 © The McGraw-Hill Companies, 2001516 Chapter 13 Query Processing r ∪s 1. Build an in-memory hash index on Hri. 2. Add the tuples in Hsi to the
The textbook explains the outer-join operations in Section 3.3.3, including the natural leftouter join. It provides strategies for implementing these operations, including left outer join and right outer join. It also discusses the outer-join operation in a symmetric fashion to the left outer join. The textbook emphasizes the importance of understanding these operations and their applications in database systems. [end of text]
The textbook discusses the implementation of outer joins and aggregation operations, including merge join and hash join algorithms. It also covers the cost estimates for these operations and their differences in size and block transfers. [end of text]
The aggregation operator G in database systems is used to group and compute sums, minima, maxima, counts, and averages of columns in a tuple. The cost of implementing aggregation operations is the same as that of duplicate elimination. For sum, min, and max, when two tuples in the same group are found, the system replaces them with a single tuple containing the sum, min, or max, respectively. For count, the running count is maintained for each group. For avg, the sum and count are computed on the fly and the average is obtained. [end of text]
In this textbook, we have learned how to evaluate an expression containing multiple operations using either the materialization approach or the pipelining approach. The materialization approach involves evaluating operations in sequence, while the pipelining approach uses multiple operations simultaneously. Both approaches have their own advantages and disadvantages, with the pipelining approach being more efficient in some cases. [end of text]
It is easiest to understand intuitively how to evaluate an expression by looking at an operator tree. This visual representation helps understand the flow of operations and relationships between expressions. [end of text]
Materialized evaluation is a method of evaluating expressions by combining intermediate results into temporary relations, then using these to evaluate the next-level operations. Pipelining involves combining operations into a pipeline, reducing the number of temporary files produced, and allowing the system to execute more quickly by performing CPU activity in parallel with I/O activity. [end of text]
Combining operations into a pipeline can reduce temporary file production and improve query-evaluation efficiency. Pipelining involves constructing a single, complex operation that combines the operations that constitute the pipeline. This approach can be implemented by constructing separate processes or threads within the system, which take a stream of tuples from its pipelined inputs and generate a stream of tuples for its output. Pipelining can be executed either demand-driven or producer-driven, with either method requiring the system to switch between operations only when an output buffer is full or an input buffer is empty and more input tuples are needed to generate any more output tuples. [end of text]
Pipelines can be executed in either demand-driven or producer-driven ways. Demand-driven pipelines require repeated requests for tuples, while producer-driven pipelines generate tuples eagerly. [end of text]
Demand-driven pipelining is more commonly used due to its ease of implementation. However, indexed nested-loop join can be used, and hybrid hash–join can be used as a compromise. The cost of writing out r in materialization is approximately 3(br + bs). If nr is substantially more than 4br + 3bs, materialization would be cheaper. [end of text]
Pipelining requires evaluation algorithms that can generate output tuples even as tuples are received for the input operations. Indexed nested-loop join is a natural choice when only one input is pipelined, while both inputs are pipelined leads to indexed nested-loop join, pipelined input tuples sorted on join attributes, and merge join. Hybrid hash–join is useful when both inputs are sorted and the join condition is an equi-join, and nonpipelined input fits in memory. [end of text]
Markers are inserted in the queue after all tuples from r and s have been generated. For efﬁcient evaluation, indices should be built on relations r and s. As tuples are added to r and s, indices must be kept up to date. [end of text]
The first step in processing a query is to translate it into its internal form, which is based on the relational algebra. The parser checks the syntax of the user's query, verifies that relation names are names of relations in the database, and so on. If the query was expressed in terms of a view, the parser replaces all references to the view name with the relational algebra expression to compute the view. Queries involving a natural join may be processed in several ways, depending on the availability of indices and the form of physical storage for the relations. [end of text]
The processing strategy is a method used to determine the most efficient way to execute a query. Users may be aware of the costs of competing query-processing strategies if they consider the potential benefits and drawbacks of each approach. For example, if a user is comparing two search engines, they may be aware of the cost of using Google's search engine and the potential drawbacks of using a competitor's search engine. However, users may not be aware of the costs of competing query-processing strategies if they are not considering the potential benefits and drawbacks of each approach. [end of text]
SELECT T.branch-name FROM branch T, branch S WHERE T.assets > S.assets AND S.branch-city = 'Brooklyn' WHERE T.assets > S.assets AND S.branch-city = 'Brooklyn' [end of text]
Indices can affect query-processing strategies based on the type of index available. For example, a full-text index might be more suitable for searching text-based data, while an index on a database table might be better for querying structured data. The choice of index type can significantly impact query performance and efficiency. [end of text]
The sort-merge algorithm runs 7 times on the first attribute, with the following runs:
1. (kangaroo, 17)
2. (wallaby, 21)
3. (emu, 1)
4. (wombat, 13)
5. (platypus, 3)
6. (lion, 8)
7. (warthog, 4) [end of text]
The number of block accesses required can be estimated using each join strategy as follows:
- Nested-loop join: 25 accesses
- Block nested-loop join: 30 accesses
- Merge join: 25 accesses
- Hash join: 30 accesses [end of text]
Relations are not physically sorted, but both have a sorted secondary index on join attributes. [end of text]
Inefficient, because it uses sorting to reduce the cost of retrieving tuples of the inner relation. This algorithm is more efﬁcient when there are multiple tuples with the same value for the join attributes. [end of text]
cise 13.6 for r1 r2, where r1 and r2 are as deﬁned in Exercise 13.5. Silberschatz−Korth−Sudarshan: Database System Concepts, Fourth Edition, IV. Data Storage and Querying, Chapter 13, Query Processing. [end of text]
The lowest cost way (in terms of I/O operations) to compute r s in an infinite memory is to use a linear scan, requiring O(n) I/O operations. The amount of memory required for this algorithm is O(n). [end of text]
Negation can be handled using various operations. For example:
a. σ¬(branch-city<“Brooklyn”)(branch)
b. σ¬(branch-city=“Brooklyn”)(branch)
c. σ¬(branch-city<“Brooklyn” ∨assets<5000)(branch) [end of text]
To extend the hash join algorithm, we need to create an index on the hash index that includes extra information to detect whether any tuple in the probe relation matches the tuple in the hash index. Then, we can use this index to compute the natural left outer join, right outer join, and full outer join. Finally, we can test our algorithm on the customer and depositor relations. [end of text]
The outer relation is pipelined, and the state information the iterator must maintain between calls is the current state of the pipeline. This ensures that the iterator can maintain a consistent state of the pipeline for each iteration, allowing it to efficiently process data. [end of text]
Query optimization is the process of selecting the most efficient query-evaluation plan from among the many strategies usually possible for processing a given query, especially if the query is complex. The system tries to find an expression equivalent to the given expression but more efﬁcient to execute. Another aspect is selecting a detailed strategy for processing the query, such as choosing the algorithm to use for executing an operation, choosing the specific indices to use, and so on. The cost of a good strategy is often substantial, and may be several orders of magnitude. The system spends a substantial amount of time on the selection of a good strategy for processing a query, even if the query is executed only once. [end of text]
The relational-algebra expression for the query "Find the names of all customers who have an account at any branch located in Brooklyn" is equivalent to the original algebra expression, but generates smaller intermediate relations. The optimizer uses statistical information about relations, such as size and index depth, to estimate the cost of a plan. The optimizer generates alternative plans that produce the same result and chooses the least costly one. [end of text]
The query optimizer generates equivalent expressions, choosing plans based on estimated costs. Materialized views help speed queries. [end of text]
The cost of an operation depends on the size and other statistics of its inputs. Given an expression such as a (b c) to estimate the cost of joining a with (b c), we need to have estimates of statistics such as the size of b c. In this section, we list some statistics about database relations stored in database system catalogs and show how to use them to estimate the results of various relational operations. One thing that will become clear later is that the estimates are not very accurate, since they are based on assumptions that may not hold exactly. A query evaluation plan that has the lowest estimated execution cost may not actually have the lowest actual execution cost. However, real-world experience has shown that even if estimates are not precise, the plans with the lowest estimated costs usually have actual execution costs that are either the lowest actual execution costs or are close to the lowest actual execution costs. [end of text]
The DBMS catalog stores statistical information about database relations, including the number of tuples, blocks, size of tuples, blocking factor, and V (A, r), which is the same as the size of ΠA(r). It can also maintain statistics for sets of attributes if desired. Real-world optimizers often maintain further statistical information to improve the accuracy of their cost estimates. [end of text]
The size estimate of the result of a selection operation depends on the selection predicate, and the assumption of uniform distribution of values is used to estimate the number of tuples. The branch-name attribute in the account relation can be a good example of a predicate that is not valid, and the assumption of uniform distribution can be used to estimate the number of accounts. [end of text]
In databases, the distribution assumption is often not accurate, but it is a reasonable approximation in many cases, and it helps keep presentations simple.
In this section, we estimate the size of the Cartesian product of two relations. If R ∩S = ∅, r × s is the same as r × s. If R ∩S is a key for R, we know that a tuple of s joins with at most one tuple from r. If R ∩S is a key for S, the number of tuples in r s is no greater than the number of tuples in s. If R ∩S is a key for neither R nor S, we assume that each value appears with equal probability. The lower of the two estimates is probably the more accurate one. [end of text]
The estimated size of a theta join rθ s is the sum of the sizes of r and s, with the lower estimate being the same as the earlier estimate from information about foreign keys. For projections, the estimated size is V (A, r), with the lower estimate being the same as the earlier estimate from information about foreign keys. For sets, the estimated size is V (A, σθ(r)), with the lower estimate being the same as the earlier estimate from information about foreign keys. For joins, the estimated size is V (A, r s) with the lower estimate being the same as the earlier estimate from information about foreign keys. [end of text]
Projection estimates the size of a projection of the form ΠA(r), where V(A, r) is the number of records or tuples. Aggregation estimates the size of AGF (r), which is simply V(A, r). Set operations estimate the size of σθ1(r) ∪σθ2(r) as σθ1∨θ2(r). Similarly, σθ1∨θ2(r) can be rewritten as σθ1∨θ2(r). Intersection estimates the size of σθ1 ∪σθ2(r) as the sum of the sizes of σθ1 and σθ2. Disjunctions estimate the size of σθ1∨θ2(r) as the minimum of the sizes of σθ1 and σθ2. Set difference estimates the size of σθ1 ∩σθ2(r) as the size of σθ1 plus the sizes of σθ2 and σθ1. All three estimates may be inaccurate, but provide upper bounds on the sizes. [end of text]
The textbook explains how to estimate the number of distinct values of attributes in a selection or join, using various methods including the number of values from the specified set, the number of values from the selected set, and the number of values in the intersection of the two sets. It also discusses how to estimate the number of distinct values in joins involving attributes from different sets. The textbook provides examples and explanations to illustrate these concepts. [end of text]
The textbook explains that distinct values can be estimated for projections, grouping, results of sum, count, and average, and min(A) and max(A) using probability theory. For min(A) and max(A), distinct values can be estimated as min(V (A, r), V (G, r)), where G denotes the grouping attributes. [end of text]
In this section, we discussed equivalence rules for relational-algebra expressions, which allow us to transform expressions into logically equivalent ones. The discussion was based on the relational algebra, and extensions to the multiset version of the relational algebra are left as exercises. [end of text]
Equivalence rules allow re-arranging relational-algebra expressions to produce logically equivalent ones. They are used in database optimization to transform expressions into other logically equivalent forms. The order of attributes in relational-algebra expressions can affect equivalence, so it's important to consider the order when combining operations. [end of text]
The natural-join operator is associative, and the selection and projection operations distribute over the theta-join operation. The Cartesian product is also associative, and the union and intersection operations are commutative. The selection and projection operations distribute over the theta-join operation under the conditions specified, and the set operations union and intersection are associative. [end of text]
We illustrate the use of the equivalence rules by transforming an algebra expression into a smaller, equivalent query. This process involves using rule 7.a and multiple equivalence rules on a query or part of the query. [end of text]
The book explains how to transform a join branch into a depositor branch using rule 6.a, and then applies rule 7.a to rewrite the query. The selection subexpression within the transformed query is Πcustomer-name ((σbranch-city = “Brooklyn” (branch)) depositor). The book also explains how to optimize the query by using equivalence rules, such as rule 1 and rule 7.b. The book concludes by discussing join ordering and how to choose an appropriate join order. [end of text]
Join operations are crucial for reducing temporary result sizes, and the natural-join is associative, making it a good choice for optimization. The temporary relation size depends on the number of relations and their join types. For example, computing account depositor ﬁrst results in one tuple per account, while σbranch-city = “Brooklyn” (branch) results in one tuple per account held by residents of Brooklyn. Therefore, the temporary relation size is smaller when computed first. [end of text]
Given an expression, if any subexpression matches one side of an equivalence rule, the optimizer generates a new expression where the subexpression is transformed to match the other side of the rule. This process continues until no more new expressions can be generated. The preceding process is costly both in space and in time. If we generate an expression E1 from an expression E2 by using an equivalence rule, then E1 and E2 are similar in structure, and have identical subexpressions. Expression-representation techniques that allow both expressions to point to shared subexpressions can reduce the space requirements significantly, and many query optimizers use them. Additionally, it is not always necessary to generate every expression that can be generated with the equivalence rules. If an optimizer takes cost estimates into account, it may be able to avoid examining some of the expressions, as seen in Section 14.4. We can reduce the time required for optimization by using techniques such as these. [end of text]
Query optimizers use equivalence rules to systematically generate expressions equivalent to a given query, reducing space and time requirements. Space can be reduced by using representation techniques that allow shared subexpressions, and optimization can be reduced by avoiding expensive evaluations. [end of text]
In database query optimization, evaluating expressions involves choosing the most efficient algorithm for each operation, coordinating execution, and deciding on pipelining. Different algorithms can be used for each operation, leading to alternative evaluation plans. Pipelining decisions must be made, and the effectiveness of nested-loop joins with indexing can be evaluated. [end of text]
Choosing the cheapest algorithm for each operation in a query plan can help optimize execution time, but it's not always the best idea. For example, a merge join at a given level may be more expensive than a hash join, but it may provide a sorted output that makes evaluating later operations cheaper. Similarly, a nested-loop join with indexing can offer opportunities for pipelining, but it may not be the cheapest way of sorting the result. [end of text]
To choose the best overall algorithm, we must consider even nonoptimal algorithms for individual operations. We can use rules much like the equivalence rules to deﬁne what algorithms can be used for each operation, and whether its result can be pipelined or must be materialized. We can use these rules to generate all the query-evaluation plans for a given expression. Given an evaluation plan, we can estimate its cost using statistics estimated by the techniques in Section 14.2 coupled with cost estimates for various algorithms and evaluation methods described in Chapter 13. That still leaves the problem of choosing the best evaluation plan for a query. There are two broad approaches: The ﬁrst searches all the plans, and chooses the best plan in a cost-based fashion. The second uses heuristics to choose a plan. Practical query optimizers incorporate elements of both approaches. [end of text]
The cost-based optimizer generates a range of query-evaluation plans from given queries, chooses the least cost plan for complex queries, and calculates join orders for smaller numbers of relations. For joins involving small numbers, the number of join orders is acceptable. However, as the number of relations increases, the number of join orders rises quickly. The dynamic programming algorithm can reduce execution time by storing results of computations and reusing them. [end of text]
In a join operation, the number of interesting sort orders generally does not exceed 2n. Dynamic-programming algorithms can be easily extended to handle sort orders. The cost of the extended algorithm depends on the number of interesting orders for each subset of relations. The storage required is much less than before, since we need to store only one join order for each interesting sort order of each of 1024 subsets of r1, . . . , r10. Although both numbers still increase rapidly with n, commonly occurring joins usually have less than 10 relations, and can be handled easily. Heuristic optimization can reduce the cost of search through a large number of plans. [end of text]
A drawback of cost-based optimization is the cost of optimization itself. Although the cost of query processing can be reduced by clever optimizations, cost-based optimization is still expensive. Heuristics are used to reduce the number of choices in a cost-based fashion, but they may result in increased costs. The projection operation reduces the size of relations, making it advantageous to perform selections early. The heuristic optimizer may not always reduce the cost, and it is recommended to use heuristics instead of cost-based optimization. [end of text]
The textbook discusses various query optimization techniques, including heuristic selection and generation of alternative access plans. It outlines the use of heuristics in Oracle and its successor, Starburst, to push selections and projections down the query tree. The cost estimate for scanning by secondary indices assumes every tuple access results in an I/O operation, while dynamic programming optimizations can find the best join order in time O(n2n). The cost estimate for scanning by secondary indices assumes every tuple access results in an I/O operation, while dynamic programming optimizations can find the best join order in time O(n2n). [end of text]
The textbook describes two approaches to choosing an evaluation plan, as noted, and compares them with dynamic programming optimizations. It also discusses heuristic selection and the generation of alternative access plans, and how these approaches are used in various systems. The cost estimation for scanning by secondary indices assumes that every tuple access results in an I/O operation. The cost is likely to be accurate with small buffers, but with large buffers, the page containing the tuple may already be in the buffer. Some optimizers incorporate a better cost-estimation technique for scans: They take into account the probability that the page con-taining the tuple is in the buffer. [end of text]
The process of replacing a nested query by a query with a join (possibly with a temporary relation) is called decorrelation. Decorrelation is more complicated when the nested subquery uses aggregation, or when the result of the subquery is used to test for equality, or when the condition linking the subquery to the outer query is not exists, and so on. Optimization of complex nested subqueries is a difficult task, as you can infer from the above discussion, and many optimizers do only a limited amount of decorrelation. It is best to avoid using complex nested subqueries, where possible, since we cannot be sure that the query optimizer will succeed in converting them to a form that can be evaluated efficiently. [end of text]
SQL treats nested subqueries as functions that take parameters and return either a single value or a set of values. Correlated evaluation is not efficient, as subqueries are evaluated separately for each tuple. Optimizers transform nested subqueries into joins, avoiding random I/O. Complex nested subqueries are more difficult to optimize. [end of text]
553 is the section number for Chapter 553 in the textbook. [end of text]
Materialized views are redundant data that can be inferred from view deﬁnitions and database contents. They are important for improving performance in some applications. To maintain a materialized view, manually written code is used, while triggers on insert, delete, and update can be used for incremental view maintenance. Incremental view maintenance involves updating the materialized view with the underlying data. Modern database systems provide more direct support for incremental view maintenance. [end of text]
Materialized views can become inconsistent when data changes, requiring manual updates. Triggers can maintain the view, but manual updates are simpler. Modern database systems provide direct view maintenance. [end of text]
To understand how to incrementally maintain materialized views, consider individual operations, differential changes, join operations, selection and projection operations, and solution for materialized view updates. [end of text]
Materialized views are updated by adding or deleting tuples based on the original view. Inserts and deletes are handled symmetrically. [end of text]
The reason for the intuition behind solution is that the same tuple is derived in two ways, and deleting one tuple from r removes only one of the ways of deriving (a); the other is still present. This gives us the intuition for solution: For each tuple in a projection, we will keep a count of how many times it was derived. [end of text]
Aggregation operations proceed somewhat like projections. The aggregate operations in SQL are count, sum, avg, min, and max: count: Consider a materialized view v = AGcount(B)(r), which computes the count of the attribute B, after grouping r by attribute A. sum: Consider a materialized view v = AGsum(B)(r). avg: Consider a materialized view v = AGavg(B)(r). min, max: Consider a materialized view v = AGmin(B)(r). Handling insertions on r is straightforward. Maintaining the aggregate values min and max on deletions may be more expensive. For example, if the tuple corresponding to the minimum value for a group is deleted from r, we have to look at the other tuples of r that are in the same group to ﬁnd the new minimum value. Handling expressions so far we have seen how to update incrementally the result of a single operation. To handle an entire expression, we can derive expressions for computing the incremen-tal change to the result of each subexpression, starting from the smallest subexpression. [end of text]
Aggregation operations in SQL involve count, sum, avg, min, and max. These operations compute the count of attributes, aggregate values, and calculate the average of a group. The materialized view is used to store these aggregates, and their values are updated on insertions and deletions. The sum and count aggregates are maintained to handle cases where the sum for a group is 0. The min and max aggregates are used to handle cases where the minimum or maximum value for a group is deleted. The materialized view is updated on insertions, and the sum and count aggregates are maintained on deletions. The sum and count aggregates are updated on insertions, and the min and max aggregates are used to handle cases where the minimum or maximum value for a group is deleted. The materialized view is updated on insertions, and the sum and count aggregates are maintained on deletions. The sum and count aggregates are updated on insertions, and the min and max aggregates are used to handle cases where the minimum or maximum value for a group is deleted. The materialized view is updated on insertions, and the sum and count aggregates are maintained on deletions. The sum and count aggregates are updated on insertions, and the min and max aggregates are used to handle cases where the minimum or maximum value for a group is deleted. The materialized view is updated on insertions, and the sum and count aggregates are maintained on deletions. The sum and count aggregates are updated on insert
The set operation intersection is a method for determining the common elements between two sets, where a tuple is inserted in one set if it exists in the other, and deleted from the intersection if it no longer exists. The other set operations, union and set difference, are handled similarly. Outer joins involve additional work, while deletions from r require handling tuples in s that no longer match any in r. [end of text]
To incrementally update a materialized view E1E2 when a set of tuples is inserted into relation r, we derive expressions for computing the incremental change to the result of each subexpression, starting from the smallest subexpression. [end of text]
Query optimization can be performed by treating materialized views just like regular relations. Rewriting queries to use materialized views can provide a more efﬁcient query plan. [end of text]
Materialized views can significantly speed up queries by reducing the need for full scans of materialized views, thus improving query performance. Indexes on common attributes can also speed up queries by enabling faster joins and selection. However, materialized views should be selected based on the system workload, which includes the time taken to maintain the materialized views. Index selection is similar to materialized views, but it is simpler to consider the importance of different queries and updates. Database administrators can use tools provided by Microsoft SQL Server 7.5 and Informix RedBrick DataWarehouse to help with index and materialized view selection. [end of text]
Query optimization involves transforming queries into equivalent forms for better efficiency. Statistics estimation helps in choosing the best strategy for processing queries. Materialized views can speed up query processing by reducing the number of alternative expressions and plans. [end of text]
When creating a nonclustering index, it is important to consider the specific requirements of the database and the nature of the data. Nonclustering indexes are useful when the data is sparse or when the data is not well-organized. In such cases, a nonclustering index can provide faster access to the data and improve the performance of the database. However, it is important to note that nonclustering indexes may not provide the same level of performance as clustering indexes, and may also have a higher overhead in terms of storage and processing. Therefore, the decision to create a nonclustering index should be based on the specific requirements of the database and the nature of the data. [end of text]
The size of r1 is 1000, r2 is 1500, and r3 is 750. To compute the join, we can use a hash join strategy. [end of text]
The schema contains 900 tuples in V (C, r1), 1100 in V (C, r2), 50 in V (E, r2), and 100 in V (E, r3). To estimate the size of r1, r2, and r3, we can use the formula: size = (total tuples in V (C, r1)) + (total tuples in V (C, r2)) - (total tuples in V (E, r2)) - (total tuples in V (E, r3)). This gives us a rough estimate of 1000 + 1100 - 50 - 100 = 1040. For computing the join, we can use a hash table or a join strategy based on the number of tuples in each partition. [end of text]
To handle the selections involving negation, we should use the logical operator "¬" to negate the condition. For option a, we should use the logical operator "¬" to negate the condition "branch-city<“Brooklyn”". For option b, we should use the logical operator "¬" to negate the condition "branch". For option c, we should use the logical operator "¬" to negate the condition "branch-city<“Brooklyn” ∨assets<5000". [end of text]
To handle the selection, you should first select the branch with the highest number of assets, then select the branch with the lowest branch name, and finally select the branch that is the best in terms of both assets and branch name. [end of text]
To improve the efficiency of certain queries, we can use the formula E1θ (E2 −E3) = (E1θ E2 −E1θ E3) and σθ( AGF (E)) = AGF (σθ(E)) where θ uses only attributes from A, and σθ(E1 E2) = σθ(E1) E2 where θ uses only attributes from E1. [end of text]
The textbook explains using the equivalence rules in Section 14.3.1.a. and Section 14.3.1.b. to simplify expressions involving multiple variables and attributes. The rules state that if two variables are combined with the same attribute, the result is a new variable with the attribute applied to it. [end of text]
The expressions in part b are not equivalent because the natural left outer join is not associative. The correct expression would be R (S T). The natural left outer join is associative if the schemas of the three relations are R(a, b1), S(a, b2), and T(a, b3), respectively. If the schemas are different, the join is not associative. [end of text]
The textbook defines σ, Π, ×, −, ∪, and ∩ for relations with duplicates, using SQL-like operations. It also checks the equivalence rules 1 through 7 for the multiset version of these operations. [end of text]
A complete binary tree is one where every internal node has exactly two children. The number of different complete binary trees with n leaf nodes is 1n2(n−1)(n−1). The number of binary trees with n nodes is 1n+12nn; this number is known as the Catalan number, and its derivation can be found in any standard textbook on data structures or algorithms. [end of text]
The textbook explains that databases can store and query information about relations in constant time, with a time bound of O(2^2n). [end of text]
The time complexity of finding the most efficient join order in a database with n elements is approximately n^2. Assumption is there is only one interesting sort order. [end of text]
The set of equivalence rules considered in Section 14.3.1 is not complete. [end of text]
To find all accounts with the maximum balance starting with "B" in the account relation, we can use a nested query. To decorrelate the query, we can use a procedure similar to Section 14.4.5. [end of text]
Union and set difference are operations in databases. Left outer join is a common operation. [end of text]
The textbook summarizes the concepts of transactions, including their atomicity, durability, and isolation properties, and their importance in database systems. It also discusses concurrency control techniques and recovery management in detail. [end of text]
The textbook summarizes the properties of transactions in a database system, including atomicity, consistency, isolation, and durability. It also mentions the ACID properties, which are the four properties that ensure the integrity of data. The textbook provides examples of transactions and their ACID requirements. [end of text]
The database system ensures consistency and durability by maintaining a record of old values of data on which transactions write, and restoring these values to ensure the database remains consistent and durable. The transaction management component is responsible for enforcing atomicity and durability, while the recovery management component is responsible for ensuring the database is in a consistent state after a failure. The isolation property ensures that concurrent transactions result in a system state equivalent to one that could have been achieved by executing them one at a time. The concurrency control component is responsible for handling concurrent transactions. [end of text]
In the absence of failures, all transactions complete successfully. However, a transaction may not always complete successfully, termed aborted. Ensuring atomicity requires no effect on the state of the database. [end of text]
A transaction is completed when it enters the committed state, and aborted when it enters the aborted state. Compensating transactions are used to undo the effects of aborted transactions. The responsibility of writing and executing compensating transactions is left to the user, not handled by the database system. The state diagram of a transaction shows it can restart, but only if aborted. [end of text]
Active transactions in databases are restricted to allow observable data, which can be dis-played to users, especially for long-duration transactions. Most current transaction systems ensure atomicity, preventing this form of interaction with users. In Chapter 24, alternative transaction models are discussed that support long-duration, interactive transactions. [end of text]
The shadow copy scheme is a simple but extremely inefﬁcient scheme for atomicity and durability in a database system, based on making copies of the database and using a pointer to point to the current copy. It assumes only one transaction at a time and leaves the original copy untouched. If a transaction aborts, it deletes the new copy. The old copy remains unchanged. Shadow-copying updates the pointer to point to the new copy, and the old copy is deleted. The database state before and after updates is shown in Figure 15.2. The shadow-copy technique ensures atomicity and durability by making updates atomic. The implementation depends on the write to db-pointer being atomic, which ensures that db-pointer lies entirely in a single sector. The atomicity and durability properties are ensured by the shadow-copy implementation of the recovery-management component. [end of text]
573 is the section number for a textbook on databases. [end of text]
Schedules help identify guaranteed concurrent execution that ensures database consistency. [end of text]
The sum of accounts A and B is preserved in both serial and concurrent schedules, and the final values of accounts A and B are $850 and $2150, respectively. The database system ensures that any schedule that executed has the same effect as a schedule that could have occurred without any concurrent execution. [end of text]
The database system must control concurrent execution of transactions to ensure database consistency. Transactions are critical for maintaining data integrity and consistency. [end of text]
In this section, we discuss different forms of schedule equivalence; they lead to the concepts of conﬂict serializability and view serializability. We assume that between a read and write instruction, a transaction may perform an arbitrary sequence of operations on the copy of Q that is residing in the local buffer of the transaction. The only significant operations of a transaction are its read and write instructions. We show only read and write instructions in schedules, as we do in schedule 3 in Figure 15.7. [end of text]
In a schedule S, consecutive instructions Ii and Ij of different data items may swap their order without affecting the results of any instruction, while instructions Ii and Ij of the same data item may have different orders due to conﬂicting instructions. [end of text]
Schedule 3 is equivalent to a serial schedule, and schedule 3 is conﬂict serializable. [end of text]
In this section, we discuss a form of equivalence that is less stringent than conﬂict equivalence, but based on only read and write operations of transactions. [end of text]
The concept of view equivalence leads to view serializability, and schedules 9 and 12 are view serializable. [end of text]
In a system that allows concurrent execution, it is necessary to ensure that any transaction Tj that is dependent on Ti (that is, Tj has read data written by Ti) is also aborted. To achieve this, we need to place restrictions on the type of schedules permitted in the system. [end of text]
Consider schedule 11 in Figure 15.13, where T9 performs only one instruction: read(A). Suppose T9 commits immediately after executing the read(A) instruction. Since T9 has read the value of data item A written by T8, it must abort T9 to ensure transaction atomicity. However, T9 has already committed and cannot be aborted. Therefore, it is impossible to recover correctly from the failure of T8. Schedule 11, with the commit happening immediately after the read(A) instruction, is an example of a nonrecoverable schedule, which should not be allowed. Most database systems require that all schedules be recoverable. A recoverable schedule is one where, for each pair of transactions Ti and Tj such that Tj reads a data item previously written by Ti, the commit operation of Ti appears before the commit operation of Tj. [end of text]
In a recoverable schedule, rolling back several transactions can be necessary if read data is written by a transactor. For example, consider a partial schedule that includes transactions read(A), write(A), read(A), read(B). If read data is written by a transactor, rolling back these transactions would be necessary to recover the schedule. [end of text]
Cascading rollback is undesirable due to its potential to undo significant work. Cascadeless schedules are preferable to prevent cascading rollback. [end of text]
So far, we have seen that schedules must be conﬂict or view serializable and cascadeless to ensure a consistent state and handle transaction failures safely. Various concurrency-control schemes can be used to ensure that multiple transactions are executed concurrently, while only acceptable schedules are generated. These schemes can lead to poor performance due to the requirement to wait for preceding transactions to finish before starting. Silberschatz−Korth−Sudarshan: Database System Concepts, Fourth Edition provides examples of concurrency-control schemes, including a locking policy that provides a poor degree of concurrency. [end of text]
A data-manipulation language must include a construct for specifying the set of actions that constitute a transaction. Transactions are ended by one of these SQL statements: Commit work, Rollback work, or a keyword work. If a program terminates without either, updates are either committed or rolled back, with the system ensuring both serializability and freedom from cascading rollback. The standard also allows a transaction to specify that it may be executed in a manner that causes it to become nonserializable with respect to other transactions. [end of text]
Determining serializability involves constructing a directed graph from a schedule, where each transaction's write or read operation is associated with an edge. This graph helps in identifying conflicts between transactions, ensuring that the schedule is serializable. [end of text]
The precedence graph for schedule 4 in Figure 15.16 contains a cycle, indicating that this schedule is not conﬂict serializable. Testing for view serializability is complicated and NP-complete. Although concurrency-control schemes can use sufﬁcient conditions, there may be view-serializable schedules that do not satisfy the sufﬁcient conditions. [end of text]
A transaction is a unit of program execution that accesses and possibly updates data items. Understanding the concept of a transaction is crucial for understanding and implementing updates of data in a database, in such a way that concurrent executions and failures of various forms do not result in the database becoming inconsistent. Transactions are required to have the ACID properties: atomicity, consistency, isolation, and durability. Atomicity ensures that either all the effects of a transaction are reﬂected in the database, or none are; a failure cannot leave the database in a state where a transaction is partially executed. Consistency ensures that, if the database is initially consistent, the execution of the transaction (by itself) leaves the database in a consistent state. Silberschatz−Korth−Sudarshan: Database System Concepts, Fourth Edition. Transaction Management 15. Transactions 586 © The McGraw−Hill Companies, 2001. [end of text]
Isolation ensures isolation among transactions, while durability ensures that updates are not lost even in the event of system failure. Concurrent execution improves throughput and utilization, and reduces waiting time. When transactions execute concurrently, data consistency may be lost. Ensuring serializability requires controlling interactions among concurrent transactions. Schedules capture key actions affecting concurrent execution, abstracting away internal details. Schedules must be recoverable, ensuring that aborts of one transaction do not cascade to others. Schedules should preferably be cascadeless to prevent cascading aborts. The shadow copy scheme ensures atomicity and durability in text editors, but it has high overheads in databases. Review terms include transaction, ACID properties, transaction state, active, partially committed, failed, aborted, committed, terminated, transaction restart, kill, observable external writes, shadow copy scheme, concurrent executions, serial execution, schedules, conflict of operations, conflict equivalence, conflict serializability, view equivalence, view serializability, blind writes, recoverability, recoverable schedules, cascading rollback, cascadeless schedules, concurrency-control scheme, lock, serializability testing, precedence graph, serializability order. [end of text]
System requirements
The steps involved in creation and deletion of files include opening, writing data, and closing the file. Atomicity ensures data is written without conflicts, while durability ensures data is preserved over time. These concepts are crucial for managing data in databases. [end of text]
File-system implementers are responsible for designing and implementing file systems, which are crucial for managing data and allowing users to access and manipulate files. These implementers often need to understand file systems to ensure that data is organized and accessible efficiently. Therefore, file-system implementers are likely to have a deeper understanding of file systems than other database system developers. [end of text]
Transactions can pass through states like commit and abort. Each state transition may occur based on the transaction's requirements and the database's design. The commit state is typically used when the transaction is complete and all its data is committed to the database. The abort state is used when the transaction is not complete and needs to be retried. The commit state is more common in transactions that require atomicity, while the abort state is more common in transactions that require consistency. [end of text]
Data must be fetched from slow disk or when transactions are long; data in memory and transactions are very short are less important. [end of text]
Every serial execution involving these two transactions pre-serves the consistency of the database. A concurrent execution of T1 and T2 that produces a nonserializable schedule would violate the consistency requirement. There is no concurrent execution of T1 and T2 that produces a serializable schedule. [end of text]
phasize conﬂict serializability rather than view serializability [end of text]
A conflict serializable transaction is one that can be rolled back if any of its parts fail. It ensures that the transaction is atomic and can be rolled back if any part fails. This is useful in scenarios where multiple transactions need to be executed in parallel, and a failure in one transaction can affect the outcome of other transactions. [end of text]
In some cases, allowing non-recoverable schedules might be desirable, such as in scenarios where data consistency is critical or where recovery is not a priority. However, it is important to consider the potential risks and benefits of such an approach and ensure that it aligns with the overall objectives of the database system. [end of text]
Concurrent transactions require serializability to ensure consistency. Schemes to achieve this include serializable transactions, serializable updates, and serializable re-visions. Nonserializable transactions can be handled by other mechanisms, such as nonserializable updates and nonserializable re-visions. The system can manage concurrently executing transactions by controlling theirinteractions and ensuring they are serializable. Nonserializable schedules can be handled by other mechanisms, such as nonserializable updates and nonserializable re-visions. The system can recover from failures by managing nonserializable schedules. [end of text]
To ensure serializability, data items must be accessed in a mutually exclusive manner using shared or exclusive modes. Transactions can only access a data item if they hold a lock on it. The concurrency-control manager grants locks in the required modes, and transactions must wait until all locks are released. Locks are compatible with each other, with shared mode compatible with shared mode but not exclusive mode. [end of text]
In this section, we discussed two modes of locking: shared and exclusive. In a shared mode, a transaction can read but not write. In an exclusive mode, a transaction can read and write. The matrix comp of Figure 16.1 shows the compatibility between the two modes. A transaction can grant a lock in an appropriate mode on data item Q, depending on the types of operations that it will perform on Q. [end of text]
Transaction T2 may unlock a data item immediately after its final access, but serializability may not be ensured. Transaction T3 may unlock a data item only after its final access, but serializability may not be guaranteed. [end of text]
In this textbook, we learned about deadlock, the undesirable situation that occurs when two transactions cannot proceed with their normal execution due to a lack of proper synchronization. We also learned about lock scheduling, which restricts the number of possible schedules and ensures that all legal schedules are conﬂict-serializable. The textbook also covered locking protocols, which are rules that indicate when transactions may lock and unlock data items. The two-phase locking protocol is a locking protocol that ensures serializability and is a way to avoid deadlocks. Deadlocks are deﬁnitely preferable to inconsistent states, since they can be handled by rolling back transactions, whereas inconsistent states may lead to real-world problems that cannot be handled by the database system. Deadlocks are deﬁnitely preferable to inconsistent states, since they can be handled by rolling back transactions, whereas inconsistent states may lead to real-world problems that cannot be handled by the database system. Deadlocks are deﬁnitely preferable to inconsistent states, since they can be handled by rolling back transactions, whereas inconsistent states may lead to real-world problems that cannot be handled by the database system. Deadlocks are deﬁnitely preferable to inconsistent states, since they can be handled by rolling back transactions, whereas inconsistent states may lead to real-world problems that cannot be handled by the database system. Deadlocks are deﬁnitely preferable to inconsistent states, since they can be handled by rolling back transactions, whereas inconsistent states may lead to real-world problems that cannot be handled by
When a transaction requests a lock on a data item in a particular mode, and no other transaction has a lock on the same data item in a conflicting mode, the lock can be granted. However, care must be taken to avoid the scenario where a transaction requests a lock on a shared-mode lock, which would prevent T1 from getting the exclusive-mode lock. [end of text]
Two-phase locking protocol ensures serializability by requiring growing and shrinking phases. [end of text]
Two-phase locking ensures conﬂict serializability, while strict and rigorous two-phase locking protocols ensure freedom from deadlock. Two-phase locking does not ensure freedom from deadlock. Observe that transactions T3 and T4 are two phase, but T8 and T9 are not two phase. Two-phase locking does not ensure freedom from deadlock. Observe that transactions T3 and T4 are two phase, but T8 and T9 are not two phase. [end of text]
Locks are enforced in shared mode, and transactions can be serialized by their lock points. Locks are generated only conﬂict-serializable schedules, and transactions can be serialized by their lock points. Locks are generated automatically for read and write requests, and are unlocked after a transaction commits or aborts. Locks are stored in a linked list, and transactions are granted locks by adding records to the end of the list. Locks are not conﬂict-serializable until a transaction grants them. Locks are stored in a lock table, and transactions are granted locks by adding records to the end of the list. Locks are not conﬂict-serializable until a transaction grants them. Locks are stored in a lock table, and transactions are granted locks by adding records to the end of the list. Locks are not conﬂict-serializable until a transaction grants them. Locks are stored in a lock table, and transactions are granted locks by adding records to the end of the list. Locks are not conﬂict-serializable until a transaction grants them. Locks are stored in a lock table, and transactions are granted locks by adding records to the end of the list. Locks are not conﬂict-serializable until a transaction grants them. Locks are stored in a lock table, and transactions are granted locks by adding records to the end of the list. Locks are not conﬂict-serializable until a transaction
Lock managers manage locks by adding or creating linked lists of data items, with locks granted first. They use a hash table to find the linked list for each data item. When a lock request is made, the manager grants the first lock request on the data item. If a transaction requests a lock on an item already granted, the manager grants the request only if it is compatible with all earlier requests and all earlier requests have been granted. [end of text]
The tree protocol ensures conﬂict serializability and freedom from deadlock, while the alternative protocol improves concurrency and ensures only recoverability. [end of text]
The tree protocol restricts transactions to lock exclusive data items, ensuring conﬂict serializability, while allowing releases until the end of transactions. [end of text]
The tree-locking protocol provides deadlock-free and earlier unlocking, but has an advantage in terms of increased concurrency. However, it may lock data items it does not access, leading to increased locking overhead and potential concurrency issues. Synchronization can be achieved through concurrent transactions, but this requires prior knowledge of data items to be locked. [end of text]
The locking protocols ensure that read and write operations are executed in timestamp order, allowing for serializable execution of transactions. [end of text]
With each transaction Ti in the system, we associate a unique fixed timestamp, de-noted by TS(Ti). This timestamp is assigned by the database system before the trans-action Ti starts execution. If a transaction Ti has been assigned timestamp TS(Ti), anda new transaction Tj enters the system, then TS(Ti) < TS(Tj). There are two simplemethods for implementing this scheme:1. Use the value of the system clock as the timestamp; that is, a transaction’s time-stamp is equal to the value of the clock when the transaction enters the system.2. Use a logical counter that is incremented after a new timestamp has been assigned; that is, a transaction’s timestamp is equal to the value of the counterwhen the transaction enters the system. The timestamps of the transactions determine the serializability order. Thus, ifTS(Ti) < TS(Tj), then the system must ensure that the produced schedule is equiva-lent to a serial schedule in which transaction Ti appears before transaction Tj. [end of text]
The timestamp-ordering protocol ensures that read and write operations are executed in timestamp order, rejecting conflicting reads and rollsbacks when necessary. [end of text]
The timestamp-ordering protocol ensures conﬂict serializability, allowing greater concurrency than the two-phase locking protocol. It generates schedules that are not recoverable, but can be extended to make them recoverable. [end of text]
We modify the timestamp-ordering protocol to allow greater concurrency by rejecting transactions that attempt to read Q before it has been written. [end of text]
The protocol rules for read operations remain unchanged, but the timestamp-ordering protocol, called Thomas' write rule, requires that obsolete write operations be ignored under certain circumstances. The protocol rules for write operations are slightly different, with obsolete write operations being ignored under Thomas' rule. [end of text]
In cases where a majority of transactions are read-only, a concurrency-control scheme may reduce overhead and improve system consistency. To reduce overhead, monitoring the system is necessary. To gain knowledge, timestamps are needed to associate transactions in order. The validation test for concurrent transactions ensures serializability and maintains consistency. [end of text]
The textbook explains the concept of serializability in databases, where transactions must be executed in order to validate their results. It provides an example of a schedule produced by validating transactions T14 and T15, and shows that the serializability order is maintained. The validation scheme automatically guards against cascading rollbacks, but there is a possibility of starvation due to sequence conflicts. The optimistic concurrency control scheme ensures that transactions execute optimistically, but requires temporary blocking of conflicting transactions to avoid starvation. [end of text]
In the concurrency-control schemes, each data item is treated as a unit for synchronization. However, for large data sets, it is better to group data items into multiple levels of granularity. This can be achieved by allowing data items to vary in size and defining a hierarchy of data granularities. The tree protocol is used to represent this hierarchy graphically. Each node in the tree represents the data associated with its descendants. In the tree protocol, each node is an independent data item. [end of text]
The multiple-granularity locking protocol ensures serializability by acquiring locks in top-down (root-to-leaf) order, while releases them in bottom-up (leaf-to-root) order. It enhances concurrency and reduces lock overhead, particularly useful in applications with a mix of short and long transactions. Deadlock is possible in the protocol, but techniques to reduce it and eliminate it are referenced in the bibliographical notes. [end of text]
The textbook discusses concurrency-control schemes that ensure serializability by delaying operations or rejecting transactions. Multiversion concurrency control schemes maintain old versions of data items. [end of text]
The multiversion timestamp-ordering scheme ensures serializability, alleviates the reading advantage of updates, and is extendable to improve recovery and cascadelessness. [end of text]
The most common transaction ordering technique used by multiversion schemes is timestamping. With each transaction, we associate a unique static timestamp, denoted by TS(Ti). The database system assigns this timestamp before the transaction starts execution. Each data item Q has a sequence of versions <Q1, Q2, ..., Qm>. Each version Qk contains three data fields: content, W-timestamp(Qk), and R-timestamp(Qk). A transaction Ti creates a new version Qk of data item Q by issuing a write operation. The system initializes the W-timestamp and R-timestamp to TS(Ti). It updates the R-timestamp value of Qk whenever a transaction Tj reads the content of Qk, and R-timestamp(Qk) < TS(Tj). The multiversion timestamp-ordering scheme ensures serializability. It operates as follows: Suppose that transaction Ti issues a read(Q) or write(Q) operation. Let Qk denote the version of Q whose write timestamp is the largest write timestamp less than or equal to TS(Ti). If transaction Ti issues a read(Q), then the value returned is the content of version Qk. If transaction Ti issues write(Q), and if TS(Ti) < R-timestamp(Qk), then the system rolls back transaction Ti. If TS(Ti) = W-timestamp(Qk), the system overwrites the contents of Qk; otherwise it creates a new version of Q. [end of text]
The multiversion two-phase locking protocol combines the advantages of multiversion concurrency control and two-phase locking, differentiating between read-only and update transactions. Update transactions hold all locks up to the end of the transaction, while read-only transactions start execution with a timestamp incremented by the multiversion protocol. Read-only transactions use a counter for timestamps, while update transactions use exclusive locks. Versions are deleted in a manner like multiversion timestamp ordering, and schedules are recoverable and cascadeless. [end of text]
Multiversion two-phase locking or variations of it is used in some commercial database systems. [end of text]
There are two approaches to deadlock prevention: One ensures no cyclic waits using ordering, while the other uses transaction rollback instead of waiting for a lock. Both methods may result in transaction rollback. Prevention is commonly used if the probability of entering a deadlock is high, while detection and recovery are more efficient. [end of text]
The wait–die scheme is a nonpreemptive technique that requires older transactions to wait for younger ones to release their data items. The wound–wait scheme is a preemptive technique that requires older transactions to never wait for younger ones. Both schemes avoid starvation by always having a transaction with the smallest timestamp. [end of text]
In the wait–die scheme, if a transaction dies and is rolled back due to a request for a data item held by another transaction, it may reissue the same sequence of requests. In contrast, in the wound–wait scheme, a transaction is wounded and rolled back because it requested a data item that is still held by another transaction. Both schemes involve unnecessary rollbacks, with the wound–wait scheme being particularly easy to implement. The timeout-based scheme is particularly suitable for detecting and recovering from deadlocks, but it has limitations due to the difficulty in deciding the appropriate wait time. [end of text]
Another simple approach to deadlock handling is based on lock timeouts. In this scheme, a transaction waits for a specified amount of time if a lock is not granted. If a deadlock occurs, transactions will time out and roll back, allowing others to proceed. This scheme is easy to implement and works well for short transactions. However, it is difficult to decide how long a transaction must wait before timing out. If too long, it can result in wasted resources. Starvation is also a possibility with this scheme. [end of text]
The textbook explains the concept of deadlock detection and recovery in database systems, focusing on the use of wait-for graphs to identify and recover from deadlocks. It discusses the need for maintaining a wait-for graph and periodically invoking an algorithm to detect and recover from deadlocks. The text also illustrates these concepts with a wait-for graph example. [end of text]
Deadlocks are described in terms of a directed graph called a wait-for graph. This graph consists of a pair G = (V, E), where V is a set of vertices and E is a set of edges. Each transaction is waiting for another to release a data item. Deadlocks exist if the wait-for graph contains a cycle. To detect deadlocks, the system needs to maintain the wait-for graph and periodically invoke an algorithm that searches for a cycle. The answer depends on two factors: how often a deadlock occurs and how many transactions will be affected by the deadlock. [end of text]
The textbook discusses deadlock detection, recovery, and concurrency control in databases. Deadlocks occur frequently, and the detection algorithm should be invoked more frequently. Deadlocked transactions will be unavailable until a deadlock can be broken. Data items allocated to deadlocked transactions will be unavailable until a solution is found. The system must re-cover from a deadlock, and the most common solution is to roll back one or more transactions. The system must also maintain information about the state of all running transactions. [end of text]
When a deadlock exists, the system must re-cover from the deadlock. Rollback involves selecting a victim, rolling back transactions to break the deadlock, and maintaining additional information about the state of running transactions. The most effective partial rollback requires maintaining lock requests/grants and update sequences. The number of rollbacks should be limited to a small number of times. [end of text]
To understand how delete instructions affect concurrency control, we must decide when they conflict with other instructions. Instructions Ii and Ij can conflict if Ii comes before Ij, resulting in a logical error. If Ij comes before Ii, Ti can execute the read operation, and vice versa. [end of text]
To understand how delete instructions affect concurrency control, we need to decide when they conflict with read and write instructions. If Ii comes before Ij, Ti will have a logical error. If Ij comes before Ii, Ti can execute the read operation successfully. If Ij comes before Ii, Ti can execute the write operation successfully. If Ij = delete(Q), Ii and Ij conﬂict. If Ii comes before Ij, Tj will have a logical error. If Ij comes before Ii, Tj can execute the read operation successfully. If Ij = write(Q), Ii and Ij conﬂict. If Ii comes before Ij, Tj will have a logical error. If Ij = insert(Q), Ii and Ij conﬂict. Suppose that data item Q did not exist before Ii and Ij. If Ii comes before Ij, a logical error results for Ti. If Ij comes before Ii, no logical error results. Similarly, if Q existed before Ii and Ij, a logical error results for Ti. [end of text]
Under the two-phase locking protocol, an exclusive lock is required on a data item before a delete operation can be performed. Under the timestamp-ordering protocol, a test similar to that for a write must be performed. Suppose that transaction Ti issues delete(Q). If TS(Ti) < R-timestamp(Q), then the value of Q that Ti was to delete has already been read by a transaction Tj with TS(Tj) > TS(Ti). Hence, the delete operation is rejected, and Ti is rolled back. If TS(Ti) < W-timestamp(Q), then a transaction Tj with TS(Tj) > TS(Ti)has written Q. Hence, this delete operation is rejected, and Ti is rolled back. Otherwise, the delete is executed. [end of text]
Insertions and deletions in databases can lead to conflicts. Insertions and reads/writes can also occur concurrently. Under the two-phase locking protocol, insertions are treated as writes, and under the timestamp-ordering protocol, insertions are treated as reads. [end of text]
In a serial schedule equivalent to S, T29 must come before T30 if T29 does not use the newly inserted tuple by T30 in computing sum(balance). To prevent the phantom phenomenon, T29 must prevent other transactions from creating new tuples in the account relation with branch-name = "Perryridge." [end of text]
The index-locking protocol leverages index availability to create conﬂicts on locks for accessing and modifying data, ensuring data consistency and preventing phantom phenomena. It operates by acquiring locks on index leaf nodes and updating them accordingly. The protocol requires exclusive locks on affected nodes for insertion, deletion, or updates, and leaf nodes containing the search-key value for updates. Variants exist for eliminating phantom phenomena under other concurrency-control protocols. [end of text]
Serializability is a useful concept for programmers to ignore issues related to concurrency when coding transactions. If every transaction maintains database consistency if executed alone, then serializability ensures that concurrent executions maintain consistency. However, the protocols required to ensure serializability may allow too little concurrency for certain applications. In these cases, weaker levels of consistency are used. The use of weaker levels of consistency places additional burdens on programmers for ensuring database correctness. [end of text]
Degree-two consistency ensures that transactions can read and write data without causing conflicts, but it may lead to inconsistencies due to concurrent access. This approach is not ideal for applications that require high consistency. [end of text]
Cursor stability is a form of degree-two consistency designed for host languages that iterate over tuples of a relation using cursors. It ensures that the current tuple is locked in shared mode, any modified tuples are locked in exclusive mode until the transaction commits. This guarantees degree-two consistency. Two-phase locking is not required. Serializability is not guaranteed. Cursor stability is used in practice on heavily accessed relations as a means of increasing concurrency and improving system performance. Applications that use cursor stability must be coded in a way that ensures database consistency despite the possibility of nonserializable schedules. Thus, the use of cursor stability is limited to specialized situations with simple consistency constraints. [end of text]
SQL allows transactions to be nonserializable, allowing long transactions with no precise results. [end of text]
Serializable transactions ensure no interference with other transactions, while Repeatable read guarantees only committed records. Read committed allows only committed records, while Read uncommitted allows even uncommitted records. Read committed and Read uncommitted are the lowest levels of consistency allowed by SQL-92. [end of text]
It is possible to treat access to index structures like any other database structure, and to apply the concurrency-control techniques discussed earlier. However, since indices are accessed frequently, they would become a point of great lock contention, leading to a low degree of concurrency. Indices do not have to be treated like other database structures. It is perfectly acceptable for a transaction to perform a lookup on an index twice, and to find that the structure of the index has changed in between, as long as the index lookup returns the correct set of tuples. Thus, it is acceptable to have nonserializable concurrent access to an index, as long as the accuracy of the index is maintained. [end of text]
In the B+-tree, a split operation splits a node, creating a new node according to the algorithm and making it the right sibling of the original node. The right-sibling pointers of both the original node and the new node are set. Following this, the transaction releases the exclusive lock on the original node and requests an exclusive lock on the parent, so that it can insert a pointer to the new node. Splitting a node may lock it, unlock it, and subsequently relock it. A lookup that runs concurrently with a split or coalescence operation may find that the desired search key has been moved to the right-sibling node by the split or coalescence operation. An insertion or deletion may lock a node, unlock it, and subsequently relock it. Coalescence of nodes during deletion can cause inconsistencies, since a lookup may have read a point to a deleted node from its parent, before the parent node was updated, and may then try to access the deleted node. The lookup would then have to restart from the root. Nodes uncoalesced avoid such inconsistencies. This solution results in nodes that contain too few search-key values and that violate some properties of B+-trees. In most databases, however, insertions are more frequent than deletions, so nodes that have too few search-key values will gain additional values relatively quickly. [end of text]
629 is the chapter number for a specific topic in a textbook. [end of text]
In the database, concurrent transactions may no longer be serializable due to locking mechanisms. Various concurrency-control schemes, such as locking protocols, timestamp-ordering schemes, validation techniques, and multiversion schemes, are used to ensure the consistency of data. Locks are acquired in root-to-leaf order, released in leaf-to-root order, and timestamps are used to ensure serializability. Multiversion timestamp ordering ensures serializability by selecting a version for each transaction. Various locking protocols do not guard against deadlocks, while preemption and transaction roll-backs are used to prevent deadlocks. Deadlocks can be dealt with by using a deadlock detection and recovery scheme. [end of text]
Special concurrency-control techniques can be developed for special datastructures. Often, these techniques are applied in B+-trees to allow greater concurrency. They ensure that accesses to the database itself are serializable, but nonserializable access is allowed. Review terms include concurrency control, lock types, lock compatibility, wait, and deadlock. The book discusses various concurrency control techniques, including lock types, lock compatibility, and wait mechanisms. It also covers locking protocols, legal schedules, and two-phase locking protocols. The book also covers graph-based protocols, tree protocols, and commit dependency. It reviews terms like starvation, locking protocol, and two-phase locking protocol. The book also covers locking conversion, upgrade, and downgrade. It covers graph-based protocols, tree protocols, and commit dependency. It discusses concurrency in indices, weak levels of consistency, and degree-two consistency. It covers cursor stability, repeatable read, and read committed, read uncommitted transactions. It covers phantom phenomena, indexing, and weak levels of consistency. It covers the Crabbing B-link tree locking protocol, next-key locking, and weak levels of consistency. It covers the Weak levels of consistency, degree-two consistency, and cursor stability. It covers the Crabbing B-link tree locking protocol, next-key locking, and weak levels of consistency. It covers the Weak levels of consistency, degree-two consistency, and cursor stability. It covers the Crabbing B-link tree locking protocol, next-key locking, and weak levels of consistency. It covers the Weak
Transactions can be serialized to ensure atomicity and consistency by locking points. [end of text]
The execution of transactions T31 and T32 in the two-phase locking protocol does not result in a deadlock. [end of text]
The textbook is about SQL (Structured Query Language). [end of text]
Other forms of two-phase locking involve using two different types of locks to control access to a shared resource. [end of text]
1. It offers the fastest data transfer speeds.
2. It is widely adopted due to its simplicity and ease of implementation.
3. It is suitable for both local and remote data transfers. [end of text]
In the context of database transactions, the authors argue that by inserting a dummy vertex between each pair of existing vertices, we can achieve better concurrency than if we follow the traditional tree protocol. This approach allows for more efficient and concurrent operations on the database. [end of text]
are not possible under the two-phase locking protocol, and vice versa. [end of text]
The protocol ensures serializability by allowing transactions to request shared locks first, ensuring that reads are consistent with updates. Deadlock freedom is ensured by requiring each transaction to follow the rules of the tree protocol, preventing deadlocks. [end of text]
Inclusive lock modes allow transactions to lock any vertex first, ensuring serializability. To lock any other vertex, a transaction must hold a lock on its majority of parents. This protocol ensures deadlock freedom by preventing deadlocks when multiple transactions try to lock the same vertex simultaneously. [end of text]
The protocol ensures serializability by ensuring that each transaction locks a vertex first, and deadlock freedom by preventing any vertex from being locked more than once. [end of text]
The forest protocol does not ensure serializability because data items may be relocked by Ti after it has been unlocked by Ti, violating the first lock rule. [end of text]
The access-protection mechanism in modern operating systems allows setting access protections (no access, read, write) on pages and memory access that violate these protections results in a protection violation. SXIS is true. The access-protection mechanism can be Silberschatz-Korth-Sudarshan, which is used for page-level locking in a persistent programming language. The technique is similar to that used for hardware swizzling in Section 11.9.4. [end of text]
In three-phase locking, transactions lock the data they access in the corresponding mode, ensuring serializability. Increment mode allows for increased concurrency by allowing transactions to check the value of X and clear it if necessary. [end of text]
The wording would likely change, as it would be more precise to describe the timestamp of the most recent transaction to execute write(Q) successfully. [end of text]
Because timestamps are unique identifiers and cannot be reused. [end of text]
Explicit locking is a technique used in databases to ensure that only one thread can access a resource at a time. It involves marking a resource as "locked" when it is accessed by a thread and releasing it when it is no longer needed. This ensures that no other thread can access the resource until it is released, preventing race conditions and data inconsistencies. [end of text]
intend-shared (XIS) mode is of no use because it does not provide a shared view of the data, making it difficult to share information with others. [end of text]
The equivalent system with a single lock granularity allows for a single lock per resource, enabling a single thread to access a resource at a time. This is useful in scenarios where a single thread needs to access a shared resource, such as a database table. Situations where a single lock is not feasible include scenarios where multiple threads need to access a shared resource simultaneously, such as in a web application where multiple users access the same database table concurrently. In these cases, a multi-threaded approach is often used to achieve concurrency. The relative amount of concurrency allowed is dependent on the specific requirements of the application. [end of text]
Show that by choosing Validation(Ti), rather than Start(Ti), as the timestamp of transaction Ti, we can expect better response time provided that conflict rates among transactions are indeed low. Concurrency control is essential for ensuring that transactions do not interfere with each other, thereby improving overall system performance. [end of text]
The timestamp protocol is not possible under the protocol, and vice versa. [end of text]
Two-phase locking, two-phase locking with multiple-granularity locking, the tree protocol, timestamp ordering, validation, multiversion timestamp ordering, and multiversion two-phase locking. [end of text]
A read request must wait if the commit bit is set. This prevents cascading abort. For write requests, the test is unnecessary because the read operation is already committed. [end of text]
In the validation-based techniques, transactions do not perform validation or writes to the database. By rerunning transactions with strict two-phase locking, we can improve performance without the need for validation or writes. [end of text]
deadlocks are a common issue in concurrent systems and are often detected using various techniques such as deadlock detection algorithms and monitoring mechanisms. [end of text]
The textbook is discussing the concept of "sustainability" and its importance in the context of environmental and economic development. Sustainability involves meeting the needs of the present without compromising the ability of future generations to meet their own needs. It is a key concept in environmental policy and business strategy. [end of text]
In a system with two processes, a write operation fails, causing the first transaction to be restarted. This restart triggers a cascading abort of the second transaction. As a result, both transactions are starved, leading to a livelock. [end of text]
No, concurrent execution is not possible with the two-phase locking protocol. The protocol ensures that data is written to the database only when all transactions have completed, preventing data inconsistencies. Therefore, it is not possible to execute multiple transactions simultaneously even with the two-phase locking protocol. [end of text]
Silberschatz, V., Korth, M., & Sudarshan, R. (2001). Database System Concepts, Fourth Edition. McGraw-Hill. Chapter 16: Concurrency Control. [end of text]
A split may occur on an insert that affects the root, preventing an insert from releasing locks until the entire operation is completed. This can occur under certain conditions, such as when a split occurs during an insert operation. [end of text]
Locking protocols, including the two-phase locking protocol, are discussed in various textbooks. The tree-locking protocol is from Silberschatz and Kedem, and other non-two-phase lock-ing protocols are described in Yannakakis et al., Kedem and Silberschatz, and Buckley and Silberschatz. Locking protocols are also explored in general discussions by Lien and Weinberger, Yannakakis et al., and Kedem and Silberschatz. Exercise 16.6 is from Buckley and Silberschatz, Exercise 16.8 is from Kedem Silberschatz, and Exercise 16.9 is from Kedem and Silberschatz. [end of text]
The timestamp-based concurrency-control scheme is from Reed [1983]. An expo-sition of various timestamp-based concurrency-control algorithms is presented by Bernstein and Goodman [1980]. A timestamp algorithm that does not require any rollback to ensure serializability is presented by Buckley and Silberschatz [1983]. The validation concurrency-control scheme is from Kung and Robinson [1981]. The locking protocol for multiple-granularity data items is from Gray et al. [1975]. A detailed description is presented by Gray et al. [1976]. The effects of locking granularity are discussed by Ries and Stonebraker [1977]. Korth [1983] formalizes multiple-granularity locking for an arbitrary collection of lock modes (allowing for more semantics than simply read and write). This approach includes a class of lock modes called update modes to deal with lock conversion. Carey [1983] extends the multiple-granularity idea to timestamp-based concurrency control. An extension of the protocol to ensure deadlock freedom is presented by Korth [1982]. Multiple-granularitylocking for object-oriented database systems is discussed in Lee and Liou [1996]. Discussions concerning multiversion concurrency control are offered by Bernstein et al. [1983]. A multiversion tree-locking algorithm appears in Silberschatz [1982].Silberschatz
In a system, transaction failures can result in loss of information, while system crashes can cause the content of nonvolatile storage to be corrupted. Well-designed systems have internal checks to prevent failures, and recovery algorithms are used to ensure data consistency and transaction atomicity despite failures. [end of text]
Storage media can be classified as volatile or nonvolatile, with volatile media being fast but prone to failure. Nonvolatile media, such as disks and tapes, survive system crashes. [end of text]
In Chapter 11, we distinguished storage media based on speed, capacity, and resilience to failure. Volatile storage is not resilient, while nonvolatile storage is. Stable storage is used for online storage and archival storage. [end of text]
In database systems, nonvolatile storage is slower than volatile storage by several orders of magnitude. Stable storage ensures data integrity, while nonvolatile media like disks and optical media provide high reliability. Flash storage offers even higher reliability than disks, but requires frequent updates. Remote backup systems protect archival backups off-site. Data transfer can be successful with or without failure, but recovery ensures data integrity. [end of text]
To implement stable storage, we need to replicate information in multiple nonvolatile storage media with independent failure modes, update it in controlled manner to ensure data integrity, and store archival backups off-site to guard against disasters. Recovery systems ensure data consistency by detecting and restoring blocks in the correct state during data transfer. Block transfer can result in failures such as fires or floods, and remote backups ensure data is protected. Recovery systems use two physical blocks for each logical block and either local or remote. During recovery, blocks are written to remote sites only after they are completed. The protocol for writing to remote sites is similar to that for writing to mirrored disks, with a small amount of nonvolatile RAM used. This allows using two copies of each block. [end of text]
The database system is permanently stored on nonvolatile storage, consisting of blocks, which contain data and may be partitioned into fixed-length units. [end of text]
In database systems, transactions involve transferring data from disk to main memory and then back to disk. The system uses block-based operations to manage data movement. Transactions read data from disk and update it in the work area. They write data to disk if necessary. The output of a buffer block is not immediately written to disk after writing, but may be later. If the system crashes after the write operation but before the output operation, the new value of data is lost. [end of text]
To achieve atomicity, we must output information describing the transactions' modifications to stable storage without modifying the database. This can be done using two methods: either all or no database modifications made by Ti. [end of text]
Serial execution of transactions, where only one transaction is active at a time. Later, concurrently executing transactions will be described. [end of text]
The most widely used structure for recording database modifications is the log. Logs record all updates in the database, with fields including transaction identifiers, data-item identifiers, old values, and new values. Special log records are used to record important events during transaction processing, such as start, commit, and abort. Logs must reside in stable storage to ensure data volume. The deferred-modiﬁcation technique ensures transaction atomicity by recording all updates in the log, but deferring updates until the transaction partially commits. Logs contain a complete record of database activity, and the volume of data stored may become unreasonably large. The deferred-modiﬁcation technique can be relaxed to reduce overhead by writing log records before updates. [end of text]
The deferred-modiﬁcation technique ensures transaction atomicity by recording all database modifications in the log, but deferring the execution of all write operations until the transaction partially commits. It assumes that transactions are executed serially when a transaction partially commits, and the log records are used for updating the deferred writes. [end of text]
The recovery scheme uses the log to restore the system to a consistent state after a failure, ensuring data integrity and recovery of data items updated by transactions. The log contains both the record <Ti start> and the record <Ti commit>, allowing for the determination of which transactions need to be redone. If a crash occurs, the recovery subsystem uses the log to restore the system to a previous consistent state. [end of text]
In the second crash, the recovery proceeds exactly as in the preceding examples, and redo operations restart the recovery actions from the beginning. The immediate-modiﬁcation technique allows database modiﬁcations to be output to the database while the transaction is still in the active state. Data modiﬁcations written by active transactions are called uncommitted modiﬁcations. In the event of a crash or a transaction failure, the system must use the old-value ﬁeld of the log records described in Section 17.4 to restore the modiﬁed data items to the values they had prior to the start of the transaction. The undo operation, described next, accomplishes this restoration. Before a transaction Ti starts its execution, the system writes the record <Ti start> to the log. During its execution, any write(X) operation by Ti is preceded by the writting of the appropriate new update record to the log. When Ti partially commits, the system writes the record <Ti commit> to the log. The information in the log is used in reconstructing the state of the database, and we cannot allow the actual update to the database to take place before the corresponding log record is written out to stable storage. We therefore require that, before execution of an output(B) operation, the log records corresponding to B be written onto stable storage. [end of text]
The immediate-modiﬁcation technique allows database modiﬁcations to be output to the database while the transaction is still in the active state. Data modiﬁcations written by active transactions are called uncommitted modiﬁcations. In the event of a crash or a transaction failure, the system must use the old-value ﬁeld of the log records described in Section 17.4 to restore the modiﬁed data items to the values they had prior to the start of the transaction. The undo operation, described next, accomplishes this restoration. Before a transaction Ti starts its execution, the system writes the record <Ti start>to the log. During its execution, any write(X) operation by Ti is preceded by the writ-ing of the appropriate new update record to the log. When Ti partially commits, the system writes the record <Ti commit> to the log. The information in the log is used in reconstructing the state of the database, and we cannot allow the actual update to the database to take place before the corresponding log record is written out to stable storage. We therefore require that, before execution of an output(B) operation, the log records corresponding to B be written onto stable storage. We shall return to this issue in Section 17.7. [end of text]
In database systems, checkpoints are used to determine which transactions need to be redone and undone. During execution, the system maintains the log, using two techniques: one where all log records are output to main memory, and another where all modiﬁed buffer blocks are output. Transactions are not allowed to perform update actions, while a checkpoint is in progress. This allows the system to streamline recovery procedures. After a transaction Ti commits prior to a checkpoint, the <Ti commit> record appears in the log before the <checkpoint> record. Any database modifications made by Ti must have been written to the database either prior to the checkpoint or as part of the checkpoint itself. This observation allows us to reﬁne our previous recovery schemes. (We assume transactions are run serially.) After a failure occurs, the recovery scheme examines the log to determine the most recent transaction Ti that started executing before the most recent checkpoint took place. It can find such a transaction by searching backward from the end of the log until it ﬁnds the ﬁrst <checkpoint> record (since we are searching backward, the record found is the ﬁnal <checkpoint> record in the log); then it continues the search backward until it ﬁnds the next <Ti start> record. This record identifies a transaction Ti. The remainder of the log can be ignored, and can be erased whenever desired. The exact recovery operations to be performed depend on the modiﬁcation technique being used. For the immediate-mod
In principle, searching the entire log is time-consuming, but checkpoints reduce overhead by maintaining the log and allowing transactions to proceed without redo. [end of text]
Consider the set of transactions {T0, T1, ..., T100} executed in order. During recovery, only transactions T67, T68, ..., T100 need to be considered, and each needs to be redone if it has committed, otherwise undone. This extension of the checkpoint technique is used for concurrent transaction processing. [end of text]
Shadow paging is an improvement on log-based techniques that requires fewer disk accesses. It allows multiple transactions to execute concurrently by maintaining two page tables during a transaction. [end of text]
Shadow and current page tables for a transaction performing a write to the fourth page of a database consisting of 10 pages. The shadow-page approach to recovery involves storing the shadow page table in nonvolatile storage, so that the state of the database prior to the execution of the transaction can be recovered in the event of a crash or transaction abort. When the transaction commits, the system writes the current page table to nonvolatile storage. The current page table becomes the new shadow page table, and the next transaction is allowed to begin execution. The shadow page table is stored in nonvolatile storage, since it provides the only means of locating database pages. The current page table may be kept in main memory (volatile storage). We don't care whether the current page table is lost in a crash, since the system recovers by using the shadow page table. Successful recovery requires that we ﬁnd the shadow page table on disk after a crash. A simple way of ﬁnding it is to choose one fixed location in stable storage that contains the disk address of the shadow page table. When the system comes back after a crash, it copies the shadow page table into main memory and uses it for subsequent transactions. Because of our deﬁnition of the write operation, we are guaranteed that the shadow page table will point to the database pages corresponding to the state of the database prior to any transaction that was active at the time of the crash. Thus, aborts are automatic. Unlike our log-based
The tree representation offers significant cost savings for large databases, while shadow paging is superior due to its locality and adaptability to concurrent transactions. Garbage collection is a significant overhead for large databases, especially in concurrent systems. The benefits of the tree representation and shadow paging outweigh these drawbacks. [end of text]
In the context of database recovery, strict two-phase locking ensures that data items are restored only after transactions have been committed or rolled back. This prevents data corruption and ensures data consistency. The system scans the log backward to restore data items, and strict two-phase locking is used to prevent conflicts between transactions. [end of text]
The recovery scheme depends on the concurrency-control scheme for rolling back transactions, using log-based recovery to undo updates, and ensuring that no other transaction updates the same data item until the transaction is committed or rolled back. Strict two-phase locking ensures that updates are committed or rolled back only after the transaction is committed or rolled back. [end of text]
We roll back a failed transaction by restoring data items to their old values from logs. Scanning the log backward ensures that only the last update is retained, preventing data corruption. Strict two-phase locking prevents other transactions from updating the same data item. [end of text]
In Section 17.4.3, checkpoints were used to reduce log records during recovery, considering only the transactions that started after the most recent checkpoint or the one active at the time of the most recent checkpoint. When transactions can execute concurrently, the situation becomes more complex, requiring consideration of multiple transactions. [end of text]
In a concurrent transaction-processing system, the checkpoint log record must be of the form <checkpoint L>, where L is a list of transactions active at the time of the checkpoint. Transactions do not perform updates either on the buffer blocks or on the log while the checkpoint is in progress. Fuzzy checkpointing allows updates even while buffer blocks are being written out. Restart recovery constructs undo and redo lists, ensuring correct database state. Redo passes should be performed before redo, to avoid problems. [end of text]
When the system recovers from a crash, it constructs two lists: The undo-list consists of transactions to be undone, and the redo-list consists of transactions to be redone. Initially, they are both empty. The system scans the log backward, examining each record, until it ﬁnds the ﬁrst checkpoint record. For each record found of the form <Ti commit>, it adds Ti to the redo-list. For each record found of the form <Ti start>, if Ti is not in redo-list, then it adds Ti to the undo-list. The system rescans the log from the most recent record backward, and performs an undo for each log record that belongs transaction Ti on the undo-list. It locates the most recent <checkpoint L> record on the log. Noticethat this step may involve scanning the log forward, if the checkpoint recordwas passed in step 1. The system scans the log forward from the most recent <checkpoint L> record, and performs redo for each log record that belongs to a transaction Ti that is on the redo-list. It ignores log records of transactions on the undo-list in this phase. The redo pass is performed ﬁrst, A will be set to 30; then, in the undo pass, A will be set to 10, which is wrong. The final value of Q should be 30, which we can ensure by performing undo before performing redo. [end of text]
In this section, we discuss log-record buffering, which helps in minimizing overhead and ensures data consistency. Log records are output to stable storage in blocks, making their size large. Outputting multiple log records at once involves writing to a log buffer in main memory. This buffer temporarily stores the logs until output to stable storage. The order of logs in stable storage must match the order of their creation. Log buffering can lead to volatile storage, causing log records to be lost if the system crashes. To ensure transaction atomicity, additional recovery techniques are imposed. [end of text]
So far, we assumed logs were output to stable storage at the time of creation. This assumption leads to high overhead for system execution. Writing logs to main memory temporarily allows multiple logs to be output in a single operation. However, volatile storage can cause loss if the system crashes. Recovery techniques must ensure transaction atomicity. [end of text]
The write-ahead logging (WAL) rule ensures that all log records pertaining to a transaction must be output to stable storage before redo information can be written, and all log records pertaining to data must be output before redo information is written. The system must output an entire block of log records if there are enough log records in main memory to fill a block. If there are insufficient log records, all log records in main memory are combined into a partially full block, and are output to stable storage. Writing the buffered log to disk is sometimes referred to as a log force. The three rules state situations in which certain log records must have been output to stable storage. There is no problem resulting from the output of log records earlier than necessary. Thus, when the system finds it necessary to output a log record to stable storage, it outputs an entire block of log records, if there are enough log records in main memory to fill a block. If there are insufﬁcient log records to fill the block, all log records in main memory are combined into a partially full block, and are output to stable storage. [end of text]
In Section 17.2, we described the use of a two-level storage hierarchy. The system stores the database in nonvolatile storage (disk) and brings blocks of data into mainmemory as needed. Main memory is typically much smaller than the entire database, and blocks may be overwritten when another block is brought into memory. If a block has been modified, it must be output prior to the input of a new block. The storage hierarchy is the standard operating system concept of virtual memory. The rules for outputting log records limit the system's freedom to output blocks of data. If a transaction causes a block to be chosen for output, all log records pertaining to that data must be output to stable storage before the block is output. The sequence of actions by the system would be: Output log records to stable storage until all log records pertaining to block B1 have been output. Output block B1 to disk. Input block B2 from disk to main memory. [end of text]
The textbook discusses two approaches to managing the database buffer: one where the database system reserves part of main memory and manages data-block transfer, and another where the database system implements its buffer within the virtual memory provided by the operating system, ensuring write-ahead logging requirements. Both approaches have their trade-offs, with the first limiting flexibility and the second ensuring write-ahead logging requirements. [end of text]
The database system should force-output the buffer blocks to force-output the buffer blocks to the data-base, after writing relevant log records to stable storage. If the operating system decides to output a block, that block is output to the swap space on disk, and the database system cannot control the output. Therefore, if the database buffer is in virtual memory, transfers between database files and the buffer in virtual memory must be managed by the database system, enforcing write-ahead logging requirements. This approach may result in extra output of data to disk. If a block is output by the operating system, it is not output to the database. Instead, it is output to the swap space for the operating system's virtual memory. When the database system needs to output a block, the operating system may need to input it from its swap space. Thus, instead of a single output of a block, there may be two outputs (one by the operating system and one by the database system) and one extra input of a block. Both approaches suffer from some drawbacks, but one or the other must be chosen unless the operating system is designed to support database logging requirements. Only a few current operating systems, such as the Mach operating system, support these requirements. [end of text]
In this section, we discuss the basic scheme of dumping the entire database to stable storage periodically. For nonvolatile storage, we use the most recent dump to restore the database to a consistent state. The system uses log records to bring the database system to the most recent consistent state. No undo operations are needed during the recovery process. A simple dump procedure is costly due to data transfer and wasted CPU cycles. Fuzzy dump schemes allow transactions to be active while the dump is in progress. They are similar to fuzzy checkpointing schemes. [end of text]
The recovery techniques described in Section 17.6 require strict two-phase locking to ensure data consistency. Early lock releases can increase concurrency but may not be applicable to specialized structures like B+-tree index pages. Several advanced recovery schemes, including ARIES, are proposed to support early lock releases. [end of text]
For transactions that release locks early, undo operations cannot be performed by simply reinserting the old value. After releasing locks, other transactions may modify the B+-tree, leading to further changes. [end of text]
In Section 16.9, the B+-tree concurrency-control protocol holds locks on the leaf level until the end of a transaction. When a transaction rolls back, it writes a log record <Ti, Oj, operation-end, U> to indicate the undo information and unique identifier for the operation. This allows the system to recover from conflicts and ensure data integrity. In contrast, physical undo writes out special redo-only log records of the form <Ti, Xj, V> containing the value V being restored to data item Xj during rollback. The system uses these records to perform logical undo operations. When a logical operation begins, it writes a log record <Ti, Oj, operation-begin> to indicate the physical undo information. During rollback, the system skips all log records of the transaction until it finds the log record <Ti, Oj, operation-begin>. When the operation completes, it writes an operation-end log record. In the redo phase, the system replays updates of all transactions by scanning the log forward from the last checkpoint. The log records include log records for transactions that were rolled back before the system was restarted. [end of text]
In our advanced recovery scheme, rollback writes out special redo-only log records containing the value V being restored to data item Xj during the rollback. These log records are called compensation log records. Whenever the system ﬁnds a log record <Ti, Oj, operation-end, U>, it rolls back the operation by using the undo information U in the log record. The system logs physical undo information for the updates performed during the rollback. If the system ﬁnds a record <Ti, Oj, operation-abort>, it skips all preceding log records until it ﬁnds the record <Ti, Oj, operation-begin>. [end of text]
Checkpointing involves temporarily storing log records and modified buffer blocks before updating the database. It outputs these records to stable storage and disk. The system outputs a checkpoint log record <checkpoint L> where L is a list of active transactions. [end of text]
In the redo phase, the system replays updates of all transactions by scanning the log forward from the last checkpoint. The log records re-played include log records for transactions that were rolled back before sys-Silberschatz−Korth−Sudarshan. [end of text]
In the checkpointing technique, updates to the database are temporarily suspended while the checkpoint is in progress. If the buffer is large, a checkpoint may take a long time to complete, resulting in an unacceptable interruption in transaction processing. To avoid such interruptions, the checkpointing technique can be modified to permit updates to start once the checkpoint record is written, but before the modified buffer blocks are written to disk. The checkpoint is generated as a fuzzy checkpoint, and the location of the last completed checkpoint is stored on disk. The system does not update this information when it writes the checkpoint record. Instead, before it writes the checkpoint record, it creates a list of all modiﬁed buffer blocks. The last-checkpoint information is updated only after all buffer blocks in the list of modiﬁed buffer blocks have been output to disk. Even with fuzzy checkpointing, a buffer block must not be updated while it is being output to disk, although other buffer blocks may be updated concurrently. The write-ahead log protocol must be followed so that (undo) log records pertaining to a block are on stable storage before the block is output. [end of text]
The checkpointing technique involves temporarily suspending updates to the database while a checkpoint is in progress. If the number of pages in the buffer is large, a checkpoint may take a long time to complete, resulting in an interruption in processing of transactions. To avoid such interruptions, the checkpointing technique can be modified to permit updates to start once the checkpoint record has been written, but before the modified buffer blocks are written to disk. The location in the log of the checkpoint record of the last completed checkpoint is stored, and the system does not update this information when writing the checkpoint record. Instead, before writing the checkpoint record, it creates a list of all modiﬁed buffer blocks. The last-checkpoint information is updated only after all buffer blocks in the list have been output to disk. Even with fuzzy checkpointing, a buffer block must not be updated while it is being output to disk, although other buffer blocks may be updated concurrently. The write-ahead log protocol must be followed to ensure that undo log records pertaining to a block are on stable storage before the block is output. [end of text]
The state of the art in recovery methods is best illustrated by the ARIES recovery technique, which is modeled after ARIES but simplified to make it easier to understand. ARIES uses a log sequence number to identify operations and reduces overheads, while ARIES also avoids redoing logged operations and reduces the amount of information logged. The price paid is increased complexity, but the benefits are worth it. The major differences are that ARIES uses a log sequence number and supports physiological redo operations. [end of text]
ARIES uses a dirty page table to minimize unnecessary redos during recovery, and uses fuzzy checkpointing to record PageLSNs and avoid even reading many pages for which logged operations are already reﬂected on disk. [end of text]
Each log record in ARIES has a log sequence number (LSN) that uniquely identifies it. The LSN is conceptually a logical identifier greater than the LSN of log records that occur later in the log. In practice, ARIES splits logs into multiple log files, each with a file number. When a log file grows to a limit, ARIES appends new log records to a new file. The LSN consists of a file number and an offset within the file. Each page maintains an identiﬁer called the PageLSN. Whenever an operation (physical or logical) occurs on a page, the LSN of its log record is stored in the PageLSN ﬁeld of the page. During the redo phase of recovery, any log records with LSN less than or equal to the PageLSN of a page should not be executed on the page, since their actions are already reﬂected on the page. In combination with a scheme for recording PageLSNs as part of checkpointing, ARIES avoids even reading many pages for which logged operations are already reﬂected on disk. The PageLSN is essential for ensuring idempotence in the presence of physiologi-cal redo operations, since reapplying a physiological redo that has already been applied to a page could cause incorrect changes to a page. [end of text]
In three passes, the Aries database system recovers from a system crash by analyzing, redoing, and undoing transactions, ensuring a database in a consistent state. [end of text]
The ARIES algorithm is a state-of-the-art recovery algorithm that incorporates a variety of optimizations designed to improve concurrency, reduce logging overhead, and reduce recovery time. It provides recovery independence, savespoints, and fine-grained locking, which are crucial for handling deadlocks and improving concurrency significantly. The algorithm is stateful and can prefetch pages during redo, out-of-order redo, and postpone redo on page fetching. [end of text]
ARIES provides recovery independence, fine-grained locking, and recovery optimizations to improve concurrency, reduce logging overhead, and reduce recovery time. [end of text]
Traditional transaction-processing systems are centralized or client–server systems. Increasingly, remote backup systems are used to ensure high availability. Recovery actions are performed at the remote backup site, using its (perhaps outdated) copy of the primary. [end of text]
The single-site system is more vulnerable to data loss, while remote backup systems offer better availability and performance. Commercial shared-disk systems provide intermediate fault tolerance, offering a balance between centralized and remote backup systems. Distributed databases with data replicated at more than one site provide high availability and reduce data loss. [end of text]
A computer system, like any other mechanical or electrical device, is subject to failure. There are a variety of causes of such failure, including disk crash, power failure, and software errors. In each of these cases, information about the database system is lost. In addition to system failures, transactions may also fail for various reasons, such as violation of integrity constraints or deadlocks. An integral part of a database system is a recovery scheme that is responsible for the detection of failures and for the restoration of the database to a state that existed before the failure. Silberschatz−Korth−Sudarshan: Database System Concepts, Fourth Edition, V. Transaction Management, 17. Recovery System, 673 © The McGraw−Hill Companies, 2001 [end of text]
The various types of storage in a computer are volatile storage, nonvolatile storage, and stable storage. Data in volatile storage, such as in RAM, is lost when the computer crashes. Data in nonvolatile storage, such as disk, are not lost when the computer crashes, but may occasionally be lost because of failures such as disk crashes. Data in stable storage are never lost. Stable storage that must be accessible online is approximated with mirroreddisks, or other forms of RAID, which provide redundant data storage. Ofﬂine, or archival, stable storage may consist of multiple tape copies of data stored in a physically secure location. In case of failure, the state of the database system may no longer be consistent; that is, it may not reﬂect a state of the world that the database is sup-posed to capture. To preserve consistency, we require that each transaction be atomic. It is the responsibility of the recovery scheme to ensure the atomic-ity and durability property. There are basically two different approaches forensuring atomicity: log-based schemes and shadow paging. In log-based schemes, all updates are recorded on a log, which must be kept in stable storage. In the deferred-modiﬁcations scheme, during the execution of a transaction, all the write operations are deferred until the transaction partially commits, at which time the system uses the information on the log asso-ciated with the transaction in executing the deferred writes. In the immediate-modiﬁc
In terms of I/O cost, database recovery systems are crucial for maintaining data integrity and availability. Recovery systems ensure that data can be recovered from a lost or damaged database, allowing users to access and modify data as needed. Recovery systems are essential for maintaining database stability and performance. [end of text]
Database systems deal with this problem by providing a structured way to store and manage data, allowing for efficient retrieval and updating of information.
efficiency of recovery scheme and cost of implementation. [end of text]
An inconsistent database state can arise if log records for a transaction are not output to stable storage prior to data being written to disk. This can lead to data corruption or inconsistencies in the database. [end of text]
The frequency of checkpoints affects system performance, recovery time, and disk recovery time. [end of text]
Log records for transactions on the undo-list must be processed in reverse order, while those for the redo-list in a forward direction. This allows the undo stack to be rebuilt in reverse order, restoring the most recent changes before the most recent error. [end of text]
schemes in terms of ease of implementation and overhead cost. [end of text]
The buffer state is as follows:
- Block 3 is currently being used.
- Block 7 is being used.
- Block 5 is being used.
- Block 3 is being used.
- Block 1 is being used.
- Block 10 is being used.
- Block 5 is being used.
- Block 3 is being used.
- Block 1 is being used.
- Block 5 is being used.
The physical ordering after the updates is:
1. Block 3
2. Block 5
3. Block 1
4. Block 7
5. Block 10
6. Block 3
7. Block 5
8. Block 7
9. Block 10
10. Block 3
The buffer in main memory can hold only three blocks, and a least recently used (LRU) strategy is used for buffer management. The buffer is updated to hold blocks 3, 5, and 10. The buffer is then modified to hold blocks 1, 7, and 5. [end of text]
If log records pertaining to a block are not output to stable storage before the block is output to disk, this can lead to inconsistent data across different storage locations. [end of text]
Logical logging is preferable to physical logging. Physical logging is preferred when logical logging is not feasible. Recovery systems are often used in conjunction with physical logging to ensure data integrity and recovery. [end of text]
The textbook suggests that dealing with batch transactions can be challenging, and an automatic teller machine transaction provides a simple solution by automatically processing cash withdrawals. [end of text]
Using the normal transaction undo mechanism to undo an erroneous transaction could lead to an inconsistent state. Point-in-time recovery involves bringing the database to a state prior to the commit of the erroneous transaction, where all effects are rolled back. This allows later non-erroneous transactions to be reexecuted logically, but not using their log records. [end of text]
Page access protections in modern operating systems allow for pre and post-image creation of updated pages. This is achieved through techniques such as virtual memory management and page table manipulation. By pre-creating a new page, one can then update the original page's content and display the updated version. This process can be repeated multiple times to create multiple images of the same page. [end of text]
Technique: Use a file system that supports both physical and physiological redos. [end of text]
The chapter discusses the architecture of database systems, including central-ized, client–server, and distributed architectures, and the various processes that implement database functionality. It also covers parallel processing within computers, parallel database systems, and distributed database systems. [end of text]
Centralized database systems are those that run on a single computer system and donot interact with other computer systems. Such systems span a range from personal to high-performance server systems. Client-server systems have functionality split between a server and multiple clients. Centralized systems consist of one to a few CPUs and device controllers connected through a shared memory bus. [end of text]
A modern computer system consists of one to a few CPUs and device controllers connected through a shared bus, providing access to shared memory. Computers are used in single-user and multi-user systems, with personal computers and workstations being typical. [end of text]
Database systems designed for single users typically do not provide many of the facilities that multiuser databases offer. They may not support concurrency control, which is not required when only a single user can generate updates. Many such systems do not support SQL, and provide a simpler query language, such as QBE. Database systems designed for multiusers systems support the full transactional features that we have studied earlier. Although general-purpose computer systems today have multiple processors, they have coarse-granularity parallelism, with only a few processors (about two to four, typically), all sharing the main memory. Databases running on such machines usu-ally do not attempt to partition a single query among the processors; instead, they run each query on a single processor, allowing multiple queries to run concurrently. Therefore, such systems support a higher throughput; that is, they allow a greater number of transactions to run per second, although individual transactions do not run any faster. [end of text]
Personal computers replaced terminals, and client-server systems replaced centralized systems. Database functionality is divided into front-end and back-end, with the back-end managing access, query evaluation, concurrency control, and recovery. Standards like ODBC and JDBC interface client-server systems. Application development tools construct user interfaces; they provide graphical tools without programming. Some popular tools include PowerBuilder, Magic, and Borland Delphi; Visual Basic is also used for application development. Transaction-processing systems use remote procedure calls to connect clients with servers. [end of text]
687 is the section number for Chapter 6 in the textbook. [end of text]
Transaction-server systems provide an interface for clients to send requests and receive responses. Data-server systems allow clients to interact with servers by reading and updating data. Shared memory and process structures are used to store and manage data. Lock manager and database writer processes manage locks and log records. Checkpoint and process monitor processes monitor other processes and take recovery actions. [end of text]
A typical transaction server system today consists of multiple processes accessing data in shared memory, with server processes receiving user queries, executing them, and sending results back. The database system includes lock managers, database writers, and checkpoint processes. Shared memory contains all shared data, such as buffer pools and lock tables. [end of text]
Database servers are used in local-area networks, where clients and servers share a high-speed connection, with client machines comparable in processing power to the server. Data is shipped to clients for processing, and then back to the server. Data is cached at the client for transactions, even after they are completed. Locks are usually granted by the server for data items that are shipped to clients. Locks are also cached at the client for transactions that find prefetched items. Locks are exchanged with the server to check validity and acquire locks. Locks are also cached at the client for transactions that find data items in the cache. Locks are exchanged with the server to check validity and acquire locks. Locks are also cached at the client for transactions that find data items in the cache. Locks are exchanged with the server to check validity and acquire locks. Locks are also cached at the client for transactions that find data items in the cache. Locks are exchanged with the server to check validity and acquire locks. Locks are also cached at the client for transactions that find data items in the cache. Locks are exchanged with the server to check validity and acquire locks. Locks are also cached at the client for transactions that find data items in the cache. Locks are exchanged with the server to check validity and acquire locks. Locks are also cached at the client for transactions that find data items in the cache. Locks are exchanged with the server to check validity and acquire locks. Lock
Data-server systems in local-area networks, high-speed connections, client machines comparable in processing power, computationally intensive tasks. Data-server architectures are popular in object-oriented database systems. Locking is handled differently in page shipping versus item shipping. Locking is usually granted by the server for data items. Data caching is used to cache data even after transactions. Locks can be cached at the client machine. Locking is handled differently in page shipping versus item shipping. Locking is usually granted by the server for data items. Data caching is used to cache data even after transactions. Locking is usually granted by the server for data items. Locking is usually granted by the server for data items. Locking is usually granted by the server for data items. Locking is usually granted by the server for data items. Locking is usually granted by the server for data items. Locking is usually granted by the server for data items. Locking is usually granted by the server for data items. Locking is usually granted by the server for data items. Locking is usually granted by the server for data items. Locking is usually granted by the server for data items. Locking is usually granted by the server for data items. Locking is usually granted by the server for data items. Locking is usually granted by the server for data items. Locking is usually granted by the server for data items. Locking is usually granted by the server for data items. Locking is usually granted
The bibliographical references provide more information about client-server data-base systems. [end of text]
Parallel systems improve processing and I/O speeds by using multiple CPUs and disks in parallel. Parallel machines are becoming increasingly common, making the study of parallel database systems correspondingly more important. The drivingforce behind parallel database systems is the demands of applications that have toquery extremely large databases (of the order of terabytes) or that have to process an extremely large number of transactions per second. Centralized and client–server databasesystems are not powerful enough to handle such applications. Parallel processing, many operations are performed simultaneously, as opposed to serial processing. A coarse-grain parallel machine consists of a small number of powerful processors; a massively parallel or ﬁne-grain parallel machine uses thousands of smaller processors. Most high-end machines today offer some degree of coarse-grain parallelism: Two or four processor machines are common. Massively parallel computers can be distinguished from the coarse-grain parallel machines by the much larger degree of parallelism that they support. Parallel computers with hundreds of CPUs and disks are available commercially. [end of text]
The textbook discusses two important issues in studying parallelism: speedup and scaleup. Running a task faster by increasing parallelism is called speedup. Handling larger tasks by increasing parallelism is called scaleup. A database application running on a parallel system with a certain number of processors and disks. The goal is to process the task in time inversely proportional to the number of processors and disks allocated. The execution time of a task on the larger machine is TL, and on the smaller machine is TS. The speedup due to parallelism is deﬁned as TS/TL. The parallel system is said to demonstrate linear speedup if the speedup is N when the larger system has N times the resources. If the speedup is less than N, the system is said to demonstrate sublinear speedup. Figure 18.5 illustrates linear and sublinear speedup. Silberschatz−Korth−Sudarshan: Database System Concepts, Fourth EditionVI. Database System Architecture18. Database System Architecture690© The McGraw−Hill Companies, 2001692Chapter 18Database System Architectureslinear speedupsublinear speedupresourcesspeedFigure 18.5Speedup with increasing resources.Scaleup relates to the ability to process larger tasks in the same amount of time by providing more resources. Let Q be a task, and let QN be a task that is N times bigger than Q. Suppose
The book discusses the challenges and benefits of scaling up database systems as the number of processors increases. It explains that while increasing the capacity of the system by increasing parallelism provides a smoother path for growth, it is important to consider absolute performance numbers when using scaleup measures. Startup costs, interference, and skew are factors that can affect the efficiency of parallel operation. The book provides examples of different interconnection networks and their advantages and disadvantages. [end of text]
Parallel systems use buses, meshes, or hypercubes to connect processors and memory.
Shared memory is a model for parallel machines where all processors share a common memory. It offers extremely efficient communication between processors, but scalability is limited by the bus or network. Shared-disk systems, such as shared nothing or hierarchical models, are hybrid architectures that combine shared memory and shared disk. Shared-disk systems are often used in shared nothing or hierarchical models to speed up transaction processing. Shared-disk architectures are scalable to a larger number of processors but have a slower communication network. DEC's Rdb is one of the early commercial users of shared-disk databases. [end of text]
Shared-memory and shared-disk architectures are two prominent models for parallel machines. Shared-memory architectures use a shared memory, while shared-disk architectures use shared disks. Shared-disk systems are often used in shared-nothing and hierarchical models, but scalability is a challenge. Shared-memory architectures are scalable up to 64 processors, while shared-disk architectures are scalable to a larger number of processors. Shared-disk systems offer fault tolerance but have slower communication between processors. DEC Rdb was one of the early commercial users of shared-disk databases. [end of text]
In a shared-memory architecture, processors and disks access a common memory via a bus or network. This allows for fast data transfer between processors. However, scalability beyond 32 or 64 processors is limited by bus or network bottlenecks. Adding more processors does not improve performance beyond a point, as data remains in the bus. Shared-memory caches help but require coherence to avoid data updates or removals. Current shared-memory machines can support up to 64 processors but are limited by memory and cache coherency overhead. [end of text]
In the shared-disk model, all processors can access all disks directly via an intercon-nection network, but the processors have private memories. This architecture offers a cheap way to provide fault tolerance, but scalability is a problem. DEC clusters running Rdb were one of the early commercial users of the shared-disk database architecture. [end of text]
Shared-nothing systems overcame the disadvantages of shared-memory and shared-disk architectures by using a high-speed interconnection network. They are scalable and can support a large number of processors. The main drawbacks are communication costs and nonlocal disk access, which are higher than in shared-memory or shared-disk architectures. Hierarchical architectures combine shared-memory, shared-disk, and shared-nothing architectures, with a shared-nothing architecture at the top. Distributed virtual-memory architectures reduce complexity by allowing multiple disjoint memories. [end of text]
In a shared-nothing system, each node consists of a processor, memory, and one or more disks. Nodes function as servers for data on disks owned by their respective processors. Shared-nothing systems overcomes the disadvantage of interconnection network scalability and scalability of interconnection networks, enabling large numbers of processors. Costs of communication and nonlocal disk access are higher than in shared-memory or shared-disk architectures. [end of text]
The hierarchical architecture combines shared-memory, shared-disk, and shared-nothing architectures. At the top level, the system consists of nodes connected by an interconnection network, and does not share disks or memory with one another. Each node could be a shared-memory system with a few processors. Alternatively, each node could be a shared-disk system, and each of the systems sharing a set of disks could be a shared-memory system. A system could be built as a hierarchy, with shared-memory architecture with a few processors at the base, and a shared-nothing architecture at the top, with possibly a shared-disk architecture in the mid-dle. Commercial parallel databases systems today run on distributed virtual-memory architectures. [end of text]
In a distributed database system, the database is stored on several computers. The computers in a distributed system communicate with one another through various communication media, such as high-speed networks or telephone lines. They do not share main memory or disks. The computers in a distributed system may vary in size and function, ranging from workstations up to mainframe systems.
In a distributed system, there is a global database administrator responsible for the entire system, and each site has a local database administrator for its own data. The possibility of local autonomy is often a major advantage of distributed databases. Availability is crucial for database systems used for real-time applications, and recovery from failure is more complex in distributed systems than in centralized systems. The ability of most of the system to continue to operate despite the failure of one site results in increased availability. [end of text]
A distributed database system with multiple sites, each with its own database schema and management software, allows for global transactions. In contrast, a single site with a global schema shares a common schema with other sites. [end of text]
Atomicity of transactions is crucial in building a distributed database system. If transactions run across sites, they may commit at one site and abort at another, leading to an inconsistent state. The 2PC protocol ensures this issue. The 2PC protocol divides transactions into the ready and committed states, with a coordinator deciding when to commit. Every site where a transaction executes must follow the coordinator's decision. If a site fails, it should be in a position to either commit or abort the transaction, depending on the decision of the coordinator. The 2PC protocol is detailed in Section 19.4.1. Concurrency control is another issue in distributed databases, requiring coordination among sites to implement locking. The standard transaction models, based on multiple actions, are often inappropriate for cross-database tasks. The 2PC protocol is detailed in Chapter 18. The standard transaction models, based on multiple actions, are often inappropriate for cross-database tasks. The 2PC protocol is detailed in Chapter 18. The 2PC protocol is detailed in Chapter 18. The 2PC protocol is detailed in Chapter 18. The 2PC protocol is detailed in Chapter 18. The 2PC protocol is detailed in Chapter 18. The 2PC protocol is detailed in Chapter 18. The 2PC protocol is detailed in Chapter 18. The 2PC protocol is detailed in Chapter 18. The 2
Distributed databases are used for complex tasks involving multiple databases and/ororm multiple interactions with humans. Workflow management systems are designed to help with coordination and ensure transactions. The advantage of distributed databases is that they reduce complexity. The disadvantage is that it requires more software development cost, potential for bugs, increased processing overhead, and increased potential for subtle bugs. [end of text]
Local-area networks are used in offices, where they offer higher speeds and lower errors compared to wide-area networks. Storage-area networks are specialized for large-scale shared-disk systems, similar to shared-database networks. [end of text]
LANs emerged in the early 1970s to share data and communicate with small computers in an office environment. LANs are used in an office environment, and all sites are close to one another, resulting in higher communication speeds and lower error rates. Storage-area networks connect large banks of storage devices to computers using data, helping build large-scale shared-disk systems. Storage-area networks are built with redundancy, such as multiple paths between nodes, to ensure high availability. [end of text]
Wide-area networks emerged in the late 1960s as a research project to provide efficient communication among sites. The Arpanet was the first WAN designed and developed in 1968. The Arpanet has grown to a worldwide network of networks, the Internet, with hundreds of millions of computers. Typical links on the Internet are fiber-optic lines, sometimes satellite channels, and data rates range from a few megabits per second to hundreds of gigabits per second. WANs can be classified into two types: discontinuous connection WANs and continuous connection WANs. These networks do not allow transactions across sites but may keep local copies of remote data and refresh them periodically. There is a potential for conﬂicting updates at different sites. A mechanism for detecting and resolving conﬂicting updates is described later. [end of text]
Wide-area networks emerged in the late 1960s as a research project to provide efficient communication among sites. Systems connecting remote terminals to a central computer were developed in the early 1960s, but were not true WANs. The Arpanet was the first WAN designed and developed in 1968. The Arpanet has grown from a four-site experimental network to a worldwide network of networks, the Internet, comprising hundreds of millions of computers. Typical links on the Internet are fiber-optic lines and, sometimes, satellite channels. Data rates for wide-area links typically range from a few megabits per second to hundreds of gigabits per second. The last link, to end user sites, is of-ten based on digital subscriber loop (DSL) technology supporting a few megabits per second), or cable modem (supporting 10 megabits per second), or dial-up modem connections over phone lines (supporting up to 56 kilobits per second). Wide-area networks can be classified into two types: discontinuous connection WANs and continuous connection WANs. Networks not continuously connected typically do not allow transactions across sites, but may keep local copies of remote data, and refresh the copies peri-odically. For applications where consistency is not critical, such as sharing of documents, groupware systems such as Lotus Notes allow up-dates of remote data to be made locally, and the updates are then propagated back to
Centralized database systems run entirely on a single computer. With the growth of personal computers and local-area networking, the database front-end functionality has moved increasingly to clients, with server systems providing the back-end functionality. Client–server interface protocols have helped the growth of client–server database systems. Servers can be either transaction servers or data servers, although the use of transaction servers greatly exceeds the use of data servers for providing database services. Transaction servers have multiple processes, possibly running on multiple processors. So that these processes have access to common data, such as Silberschatz-Korth-Sudarshan: Database System Concepts, Fourth Edition VI. Database System Architecture18. Database System Architecture701© The McGraw-Hill Companies, 2001704Chapter 18Database System Architecturesthe database buffer, systems store such data in shared memory. In addition to processes that handle queries, there are system processes that carry out tasks such as lock and log management and checkpointing. Data server systems supply raw data to clients. Such systems strive to minimize communication between clients and servers by caching data and locks at the clients. Parallel database systems use similar optimiza-tions. Parallel database architectures include the shared-memory, shared-disk, shared-nothing, and hierarchical architectures. These architectures have different tradeoffs of scalability versus communication speed. Parallel database architectures include the shared-memory, shared-disk, shared-nothing, and hierarchical architectures. These architectures have different tradeoffs of
A multiprocessor machine allows individual queries to be executed independently, without requiring parallelization. [end of text]
Data servers are popular for object-oriented databases because transactions are expected to be relatively long, making them suitable for client-server systems. However, relational databases are preferred for their simplicity and efficiency, especially in high-volume applications where transactions are expected to be short. [end of text]
The drawback of such an architecture is that it may lead to increased memory usage and potential data corruption. [end of text]
Building a client-server system in a scenario where client and server machines have exactly the same power is not necessarily the best choice. A data-server architecture is more suitable for scenarios where data is stored and accessed by multiple clients, such as in a distributed database system. However, if the system is designed to handle large amounts of data and high concurrency, a client-server system with multiple processors and memory may be more efficient. [end of text]
The speed of interconnection between the client and server affects the choice between object and page shipping. If page shipping is used, the cache can organize data as objects. One beneﬁt of an object cache over a page cache is that it reduces the number of requests needed to retrieve data, improving performance. [end of text]
not required if the unit of data shipping is an item? [end of text]
Speedup is the most relevant measure for a new parallel computer when the company is growing rapidly and has outgrown its current computer system. Transaction scaleup is not as relevant as speedup for a growing company, as it may not improve performance. Batchscaleup may not be feasible for a growing company, as it may not be able to scale up the parallel computer. [end of text]
The textbook states that SQL code is executed at 20% speedup, while C code at 80%. Parallelism is used only for SQL code, and the speedup is 20% for SQL. [end of text]
Shared memory: CPU, shared disk, shared nothing: CPU, shared disk, shared nothing: CPU, shared disk, shared nothing: CPU, shared disk, shared nothing: CPU, shared disk, shared nothing: CPU, shared disk, shared nothing: CPU, shared disk, shared nothing: CPU, shared disk, shared nothing: CPU, shared disk, shared nothing: CPU, shared disk, shared nothing: CPU, shared disk, shared nothing: CPU, shared disk, shared nothing: CPU, shared disk, shared nothing: CPU, shared disk, shared nothing: CPU, shared disk, shared nothing: CPU, shared disk, shared nothing: CPU, shared disk, shared nothing: CPU, shared disk, shared nothing: CPU, shared disk, shared nothing: CPU, shared disk, shared nothing: CPU, shared disk, shared nothing: CPU, shared disk, shared nothing: CPU, shared disk, shared nothing: CPU, shared disk, shared nothing: CPU, shared disk, shared nothing: CPU, shared disk, shared nothing: CPU, shared disk, shared nothing: CPU, shared disk, shared nothing: CPU, shared disk, shared nothing: CPU, shared disk, shared nothing: CPU, shared disk, shared nothing: CPU, shared disk, shared nothing: CPU, shared disk, shared nothing: CPU, shared disk, shared nothing: CPU, shared disk, shared nothing: CPU, shared disk, shared nothing: CPU, shared disk, shared nothing: CPU
A distributed database is not defined by its interaction method, but by its design and architecture. In a distributed database, data is stored across multiple servers, and transactions are executed across multiple nodes. The interaction method used by databases is not relevant to their definition. [end of text]
Distributed databases are characterized as either homogeneous or heterogeneous, and involve storing data in multiple locations. Transaction processing and query processing are common challenges in distributed databases. In this chapter, we address these issues, including model for transaction processing, atomic transactions, concurrency control, replication, and directory systems. [end of text]
In a homogeneous distributed database, all sites have identical database management system software, are aware of one another, and agree to cooperate in processing users' requests. In such a system, local sites surrender a portion of their autonomy.
Data replication is a technique to store a relation in multiple sites to ensure availability and parallelism. It allows for quick access to data even in the event of a site failure. [end of text]
If relation r is replicated, a copy is stored in multiple sites, enhancing parallelism and availability. [end of text]
The system must ensure that all replicas of relation r are consistent, and whenever r is updated, the update must be propagated to all sites containing replicas. Replication enhances read operations and availability for read-only transactions, but it incurs increased overhead. Data fragmentation can be controlled by choosing a primary copy of r. Distributed concurrency control can be simplified by using horizontal or vertical fragmentation. Transparency ensures that data are not required to know where they are physically located or how they can be accessed at the local site. [end of text]
Horizontal fragmentation is used to keep tuples at the sites where they are used the most, minimizing data transfer. Vertical fragmentation involves the deﬁnition of several subsets of attributes R1, R2, ..., Rn of the schema R so that R = R1 ∪R2 ∪· · · ∪Rn. The fragmentation should be done in such a way that the relation r can be reconstructed from the fragments by taking the natural join r = r1 r2 r3 · · · rn. [end of text]
Data transparency in distributed databases allows users to access data at the local site without knowing its location or how it is replicated. This characteristic is achieved through fragmentation and replication transparency. [end of text]
The distributed database system should be able to find any data as long as the data identiﬁer is supplied by the user transaction. Users do not have to be concerned with what data objects have been replicated or where replicas have been placed. Location transparency ensures that two sites do not use the same name for distinct data items. A central name server helps to ensure that the same name does not get used for different data items. The name server can locate a data item given the name of the item. However, it suffers from two major disadvantages: performance bottlenecks and potential name server failures. A more widely used alternative approach requires that each site preﬁxes its own site identiﬁer to any name that it generates. This ensures no two sites generate the same name and allows for site identiﬁers to be stored at each site. The system can use the mapping of aliases to real names to ensure that the user is unaware of the physical location of a data item. [end of text]
The textbook describes the system structure of a distributed database, including its components and how they interact to ensure ACID properties and manage global transactions. It also discusses protocols for atomic commit and concurrency control in distributed databases, as well as how a system can continue functioning even in the presence of various types of failures. [end of text]
Each site has its own local transaction manager, which ensures ACID properties for local transactions and manages global transactions. The overall system architecture includes two subsystems: transaction managers and transaction coordinators. The transaction system consists of two sites, with each site containing two subsystems: transaction managers and transaction coordinators. The overall system architecture includes two subsystems: transaction managers and transaction coordinators. The transaction system consists of two sites, with each site containing two subsystems: transaction managers and transaction coordinators. The overall system architecture includes two subsystems: transaction managers and transaction coordinators. The transaction system consists of two sites, with each site containing two subsystems: transaction managers and transaction coordinators. The overall system architecture includes two subsystems: transaction managers and transaction coordinators. The transaction system consists of two sites, with each site containing two subsystems: transaction managers and transaction coordinators. The overall system architecture includes two subsystems: transaction managers and transaction coordinators. The transaction system consists of two sites, with each site containing two subsystems: transaction managers and transaction coordinators. The overall system architecture includes two subsystems: transaction managers and transaction coordinators. The transaction system consists of two sites, with each site containing two subsystems: transaction managers and transaction coordinators. The overall system architecture includes two subsystems: transaction managers and transaction coordinators. The transaction system consists of two sites, with each site containing two subsystems: transaction managers and transaction coordinators. The overall system architecture includes
The structure of a transaction manager is similar to a centralized system, with a transaction coordinator responsible for maintaining a log and coordinating concurrent transactions. Distributed systems can suffer from loss of messages, network partition, and other failure types, requiring modifications to concurrency control schemes. [end of text]
The textbook discusses the failure modes in a distributed system, including software errors, hardware failures, disk crashes, and network partitioning. It explains that loss of messages and network partitioning are common failures in distributed systems. The text also mentions that information about transmission control protocols, such as TCP/IP, is available in standard textbooks on network systems. It further explains that if two sites are not directly connected, messages must be routed through communication links. If a communication link fails, messages must be rerouted. It also mentions that a system can be partitioned into multiple subsystems, each lacking any connection between them. [end of text]
The two-phase commit protocol ensures atomicity by committing at all sites and aborting at all sites. It uses a two-phase commit protocol (2PC) and three-phase commit protocol (3PC) to manage transactions. The 3PC protocol avoids certain disadvantages of the 2PC protocol but adds complexity and overhead. The 2PC protocol adds the record <prepare T> to the log, forces the log onto stable storage, and sends a prepare T message to all sites. If the answer is no, it adds a record <no T> to the log, and responds with an abort T message. If the answer is yes, it adds a record <ready T> to the log, and forces the log onto stable storage. The transaction manager then replies with a ready T message to the 2PC coordinator. Following this point, the fate of the transaction is sealed. [end of text]
The two-phase commit protocol (2PC) is used during normal operation to ensure that transactions are executed in a consistent manner. It handles failures by adding records to the log and forcing the log onto stable storage. The protocol determines whether a transaction can be committed or aborted based on the responses from participating sites. [end of text]
When T completes its execution, it adds records <prepare T> to the log and forces the log onto stable storage. It then sends a prepare T message to all sites. If the answer is no, it adds a record <no T> to the log, and responds by sending an abort T message. If the answer is yes, it adds a record <ready T> to the log, and forces the log onto stable storage. The transaction manager then replies with a ready T message to the transaction T. [end of text]
The 2PC protocol handles failures by either aborting the transaction or committing it, depending on the site's status. If a site fails before sending a ready T message, it aborts the transaction. If a site fails after receiving a ready T message, it commits the transaction. The coordinator can abort the transaction if at least one site responds with an abort T message. The coordinator writes the verdict to the log and forces it to stable storage. If the coordinator detects a failure, it assumes the site failed and aborts the transaction. The coordinator can decide to commit or abort the transaction at any time before sending the message ready T to the coordinator. The coordinator writes the verdict to the log and forces it to stable storage. If the coordinator detects a failure, it assumes the site failed and aborts the transaction. The coordinator can decide to commit or abort the transaction at any time before sending the message ready T to the coordinator. The coordinator writes the verdict to the log and forces it to stable storage. If the coordinator detects a failure, it assumes the site failed and aborts the transaction. The coordinator can decide to commit or abort the transaction at any time before sending the message ready T to the coordinator. The coordinator writes the verdict to the log and forces it to stable storage. If the coordinator detects a failure, it assumes the site failed and aborts the transaction. The coordinator can decide to commit or abort the transaction at any time before sending the message ready T to the coordinator. The
The 2PC protocol responds in different ways to various types of failures, including coordinator failures, site failures, and network partition failures. When a coordinator fails, the participating sites must decide the fate of T, while a site that contains a <ready T> record in its log must consult C to determine the fate of T. If a site contains an <abort T> record, it must execute undo(T); if a site contains an <commit T> record, it must commit T. In network partition failures, the coordinator and its participants remain in one partition, while the coordinator and its participants belong to several partitions. In the first case, the failure has no effect on the commit protocol; in the second case, the coordinator and its participants belong to several partitions. [end of text]
The 3PC protocol is an extension of the two-phase commit protocol, avoiding blocking under certain assumptions. Persistent messaging is used to avoid network partitioning and ensure atomicity, while workflows are considered in more detail in Section 24.2. [end of text]
When a failed site restarts, recovery involves treating in-doubt transactions, where a <ready T> log record is found but neither a <commit T> nor an <abort T> log record is found. Recovery must determine the commit–abort status by contacting other sites. If normal transaction processing is blocked, recovery may remain unusable for a long period. Recovery algorithms typically provide lock information in the log, supporting concurrent transaction locking. [end of text]
The 3PC protocol is an extension of the two-phase commit protocol that avoids blocking under certain assumptions, introducing an extra third phase where multiple sites are involved in the decision to commit. It ensures that at least k other sites know the intention to commit, and restarts the third phase if a site knows it will commit. The protocol is not widely used due to its overhead. [end of text]
Persistent messaging is a technique to avoid the blocking problem of two-phase commit in distributed applications. It involves transferring funds between banks using messages between the banks. The message ensures atomicity, prevents updates to the total bank balance, and prevents duplicate deposits. Persistent messages are guaranteed to be delivered exactly once, regardless of failures, and not delivered multiple times in some situations. Regular messages may be lost or delivered multiple times. [end of text]
Error handling with persistent messaging is more complex than two-phase commit, requiring both sites to provide error handling code and handle persistent messages. Both sites must be provided with exception handling code, along with code to handle persistent messages. Persistent messaging forms the underlying basis for workflows in a distributed environment. Workﬂows provide a general model of transaction processing involving multiple sites and human processing of certain steps. Persistent messaging can be implemented on top of an unreliable messaging infrastructure, which may lose messages or deliver them multiple times, by these protocols. Exception handling code provided by the application is then invoked to deal with the failure. [end of text]
We show how concurrency-control schemes can be modified for distributed environments, requiring updates on all replicas of data items. Locking protocols can be used in a distributed setting, requiring a shared and exclusive lock mode. [end of text]
The various locking protocols described in Chapter 16 can be used in a distributed environment. The only change that needs to be incorporated is in the way the lock manager deals with replicated data. We present several possible schemes that are applicable to an environment where data can be replicated in several sites. As in Chapter 16, we shall assume the existence of the shared and exclusive lock modes. Silberschatz−Korth−Sudarshan: Database System Concepts, Fourth Edition VI. Database System Architecture 19. Distributed Databases 719 © The McGraw−Hill Companies, 2001 [end of text]
The single lock-manager approach involves maintaining a single lock manager at site Si, where all lock and unlock requests are made. When a transaction needs to lock a data item, it sends a request to Si. The lock manager determines whether the lock can be granted immediately. If the lock can be granted, it sends a message to the site at which the lock request was initiated. Otherwise, it delays the request until it can be granted. The transaction can read the data item from any site where replicas are present. The scheme has advantages such as simplicity and deadlock handling. However, it has disadvantages such as a bottleneck and vulnerability. A compromise between these advantages and disadvantages can be achieved through a distributed lock-manager approach, where the lock manager is distributed over several sites. Each site maintains a local lock manager to administer the lock and unlock requests for data items stored in that site. When a transaction needs to lock data item Q, it requests a lock at the primary site of Q. The response is delayed until it can be granted. The primary site enables concurrency control for replicated data to be handled like that for unreplicated data. This scheme deals with replicated data in a decentralized manner, thus avoiding drawbacks of central control. However, it suffers from disadvantages such as implementation complexity and deadlock handling. [end of text]
The single lock-manager approach in a single site environment involves a single lock manager that handles both lock and unlock requests. When a transaction needs to lock a data item, it sends a request to the site. The lock manager determines if the lock can be granted immediately. If granted, it sends a message to the site. If not, it delays the request until the lock can be granted. The transaction can read the data item from any site involved. The scheme has advantages such as simplicity and deadlock handling, but disadvantages like a bottleneck and vulnerability. The single site environment is suitable for this approach. [end of text]
The distributed lock-manager approach is a compromise between the advantages and disadvantages, allowing for distributed lock management over multiple sites. Each site maintains a local lock manager to administer data items. When a transaction locks data item Q, it sends a message to the lock manager at site Si. If data item Q is locked in an incompatible mode, the request is delayed. Once granted, the lock manager sends a message back indicating the lock has been granted. Deadlock handling is more complex due to intersite deadlocks, requiring modifications to the lock and unlock algorithms discussed in Chapter 16. [end of text]
When a system uses data replication, we can choose one of the replicas as the primary copy. Thus, for each data item Q, the primary copy of Q must reside in precisely one site, which we call the primary site of Q. When a transaction needs to lock a data item Q, it requests a lock at the primary site of Q. As before, the response to the request is delayed until it can be granted. Therefore, the primary copy enables concurrency control for replicated data to be handled like that for unreplicated data. This similarity allows for a simple implementation. However, if the primary site of Q fails, Q is inaccessible, even though other sites containing a replica may be accessible. [end of text]
The majority protocol allows for replicated data to be stored in decentralized manner, avoiding central control. It deals with replicated data in a decentralized manner, thus avoiding the drawbacks of central control. However, it suffers from implementation and deadlock handling issues. [end of text]
The biased protocol is another approach to handling replication, where requests for shared locks are given more favorable treatment than requests for exclusive locks. Shared locks are handled by the lock manager at one site, while exclusive locks are handled at all sites. The advantage of the biased scheme is that it can reduce overhead on read operations, especially in cases where read operations are more frequent than write operations. However, the additional overhead on writes is a disadvantage. The quorum consensus protocol is a generalization of the majority protocol, assigning read and write operations on an item two integers to the read quorum and write quorum, respectively. The quorum consensus approach can permit selective reductions in cost for reads and writes, and it can simulate the majority protocol and the biased protocols. The timestamping scheme in Section 16.2 is used to generate unique timestamps, and the distributed scheme uses a logical counter or local clock to generate unique local timestamps. [end of text]
The biased protocol is another approach to handling replication, where requests for shared locks are given more favorable treatment than requests for exclusive locks. Shared locks are used when a transaction needs to lock data item Q, while exclusive locks are used when a transaction needs to lock data item Q. The biased protocol has the advantage of reducing overhead on read operations, but the additional overhead on writes is a disadvantage. The bias shares the majority protocol's disadvantage of complexity in handling deadlocks. [end of text]
The quorum consensus protocol generalizes the majority protocol by assigning weights to sites, enabling selective reductions in read and write operations. [end of text]
The timestamping scheme in Section 16.2 is used to generate unique timestamps for each transaction, ensuring that the system can order transactions in a specific order. The two primary methods for generating unique timestamps are centralized and distributed. In the centralized scheme, a single site distributes the time-stamps. In the distributed scheme, each site generates a unique local timestamp by using either a logical counter or the local clock. The order of concatenation is important, as the global timestamp is generated by concatenating the unique local timestamp with the site identifier, which must be unique. The order of concatenation is crucial in the distributed scheme. The order of concatenation is important! We use the site identifier in the least significant position to ensure that the global timestamps generated in one site are not always greater than those generated in another site. We may still have a problem if one site generates local timestamps at a rate faster than that of the other sites. In such a case, the fast site's logical counter will be larger than that of other sites. Therefore, all timestamps generated by the fast site will be larger than those generated by other sites. We need a mechanism to ensure that local timestamps are generated fairly across the system. We deﬁne within each site Si a logical clock (LCi), which generates the unique local timestamp. The logical clock can be implemented as a counter that is incremented after a new local time-stamp is generated. To ensure that the various logical clocks are synchronized, we require that
Replication in commercial databases allows updates at a primary site and propagates updates to replicas at other sites. Transactions read replicas but not update them. This ensures consistent data across sites. [end of text]
The database replica should reﬂect a transaction-consistent snapshot of the data at the primary, and updates should be propagated immediately after they occur at the primary. The Oracle database system supports a create snapshot statement to create a transaction-consistent snapshot copy of a relation or set of relations, and snapshot refresh to propagate updates only periodically. Multiserver replication allows updates at any replica of a data item and automatically propagates updates to all replicas. Deadlock prevention and detection algorithms can be used in a distributed system, provided that modifications are made. Deadlock prevention may result in unnecessary waiting and rollback, and certain deadlock-prevention techniques may require more sites to be involved in the execution of a transaction. [end of text]
Deadlock prevention and detection algorithms can be used in distributed systems, with modifications required. Deadlock prevention may lead to unnecessary waiting and rollback. Certain deadlock-prevention techniques may require more sites. To maintain the wait-for graph, each site must keep a local graph. [end of text]
In the centralized deadlock detection approach, the system constructs and maintains a global wait-for graph in a single site, which includes real graphs representing the system's state at any instance in time. Correctness is ensured by generating the constructed graph in such a way that the reported results are correct. Rollbacks are possible under conditions where a cycle exists in the global wait-for graph, but false cycles are unlikely to cause serious performance problems. Deadlock detection can be done in a distributed manner with multiple sites taking on parts of the task. [end of text]
One of the goals in using distributed databases is to ensure high availability and robustness, especially for large systems with various types of failures. The ability to continue functioning even when failures occur is referred to as robustness. Different types of failures are handled in different ways, including message loss and repeated retransmission of messages across links. [end of text]
In the presence of network partitions, a system can detect a failure but may not be able to distinguish between site failures and network partition. The majority-based approach can be used to mitigate failures and facilitate transactions. In the majority-based approach, each data object stores a version number to detect when it was last written. Transactions write objects and update version numbers. Read operations look at replicas and read the highest version number. Writes read replicas and set the version number. The quorum consensus protocol can be used to prevent failures in the presence of site failures. [end of text]
The majority-based approach to distributed concurrency control can be modified to work in spite of failures by using a version number to detect when data is replicated and updating it in read operations. Read operations look at all replicas on which a lock has been obtained, and read the value from the replica with the highest version number. Writes read all replicas just like reads to find the highest version number. If sites are given higher weights, failures can be tolerated, but the system must be re-integrated. [end of text]
In the read one, write all protocol, all replicas can read and write, but a read lock is required. [end of text]
The backup-coordinator approach incurs overhead during normal processing to allow fast recovery from a coordinator failure. [end of text]
Reintegration of a site or link into a system requires careful management to ensure data consistency and prevent disruptions. Techniques like temporary halt and concurrent updates can be used, but they can be disruptive. A solution is to allow failed sites to reintegrate while concurrent updates proceed concurrently, ensuring that all updates are caught up. Sites should be informed promptly of the recovery of links to prevent disruptions. [end of text]
Remote backup systems and replication are two approaches to providing high availability. Remote backup systems perform actions at a single site and replicate data and logs. Replication provides greater availability by having multiple replicas and using the majority protocol. [end of text]
The backup-coordinator approach avoids a substantial amount of delay while the distributed system recovers from a coordinator failure. It requires a unique identification number for each active site and a mechanism for recovering from crashes. [end of text]
The Bully Algorithm is a coordinator election algorithm where a coordinator is elected if a site fails to elect itself, and the algorithm restarts if a site recovers. [end of text]
In Chapter 14, we explored various methods for computing answers to queries. We examined strategies for minimizing disk accesses and trade-offs between data transmission and query performance. For centralized systems, the primary criterion is disk costs, while for distributed systems, network costs must be considered. We found a good balance between these factors. For example, Silberschatz-Korth-Sudarshan: Database System Concepts, Fourth Edition VI. Database System Architecture 19. Distributed Databases 733 © The McGraw-Hill Companies, 2001 [end of text]
In distributed databases, query optimization can be complex due to replication and fragmentation transparency. Techniques like Silberschatz-Korth-Sudarshan can simplify expressions involving replicated and fragmented accounts. The final strategy involves eliminating the Hillside branch to reduce complexity. [end of text]
Silberschatz-Korth-Sudarshan for processing the query, assuming replicated and fragmented relations, is the strategy for site S1. [end of text]
In this approach, we first compute temp1 at S1, then temp2 at S2, and finally temp2 at S1. The resulting relation is the same as r1 r2. Before considering the efficiency of this strategy, we verify that the strategy computes the correct answer. In step 3, temp2 has the result of r2 ΠR1 ∩R2 (r1). In step 5, we compute r1 ΠR1 ∩R2 (r1). Since join is associative and commutative, we can rewrite this expression as (r1 ΠR1 ∩R2 (r1)) r2. Since r1 Π(R1 ∩R2) (r1) = r1, the expression is indeed equal to r1 r2. [end of text]
Semijoin strategy for evaluating r1 r2, where temp2 is computed before shipping r2 to S1. Semijoin operator n is used to select r1 n r2, resulting in temp2 containing fewer tuples than r2. Semijoin strategy is advantageous when r1 is the result of a relational algebra expression involving selection, leading to temp2 having fewer than r2. Semijoin strategy can be extended to multiple semijoin steps. [end of text]
In one strategy, r1 is shipped to S2, and r1 r2 are computed at S2. At S4, r3 is shipped to S4, and r3 r4 are computed at S4. S2 can ship tuples of (r1 r2) to S1 as they are produced, while S4 can ship tuples of (r3 r4) to S1. Once (r1 r2) and (r3 r4) are computed at S1, the pipelined join technique of Section 13.7.2.2 can be used to compute the final join result at S1. [end of text]
Many new database applications require data from a variety of preexisting databases, requiring a multidatabase system for additional software layers and different logical models, data-deﬁnition and data-manipulation languages, and concurrency-control and transaction-management mechanisms. A multi-database system creates the illusion of logical database integration without requiring physical database integration. [end of text]
Multidatabase systems offer significant advantages over homogeneous systems, offering a unified view of data, transaction management, and query processing. However, they face technical and organizational challenges, including the need for a common data model, schema integration, and the provision of a common conceptual schema. These challenges can be addressed by using a relational model with SQL as the common query language, and by providing wrappers for data sources to maintain a global schema. Additionally, mediator systems can integrate multiple heterogeneous data sources and provide an integrated global view of the data, without worrying about transaction processing. [end of text]
In distributed databases, the relational model is commonly used, with SQL as the query language. Schema integration is complex due to semantic heterogeneity and data type differences. The same name may appear in different languages in different systems. Translation functions must be provided, and indices annotated for system-dependent behavior. [end of text]
Query processing in a heterogeneous database can be complicated. Some issues include translating queries between global and local schemas, translating results back to global, and providing wrappers for data sources. Wrappers can be individual sites or separate modules. They can provide relational views of nonrelational data sources, such as Web pages. Global query optimization is difficult since execution systems may not know costs at different sites. Mediator systems provide an integrated global view and query facilities. [end of text]
Virtual databases are systems that provide the appearance of a single database with a global schema, but data exist on multiple sites in local schemas. [end of text]
LDAP is a lightweight directory access protocol that stores entries, similar to objects, with a distinguished name (DN) that uniquely identifies each entry. It defines a data model and access control, and provides many of the X.500 features, but with less complexity. In LDAP, entries can have attributes, and LDAP provides binary, string, and time types, as well as telephone numbers and addresses. Unlike relational databases, LDAP allows for forward queries made at one site to the other site without user intervention. [end of text]
Directory information can be made available through Web interfaces, such as Web browsers, and can be used for storing other types of information. Directory access protocols, like Lightweight Directory Access Protocol (LDAP), provide a standardized way to access directory information. Organizations use directory systems to store and access organizational information online. Directory systems can be set up to automatically forward queries made at one site to the other site, without user intervention. [end of text]
In general, a directory system is implemented as one or more servers, which serve multiple clients. Clients use the application programmer interface defined by directory systems to communicate with directory servers. Directory access protocols also define a data model and access control. The X.500 directory access protocol, deﬁned by the International Organization for Standardization (ISO), is a standard for accessing directory information. However, it is complex and not widely used. The Lightweight DirectoryAccess Protocol (LDAP) provides many of the X.500 features, but with less complexity and is widely used. In this section, we outline the data model and access protocol details of LDAP. [end of text]
LDAP directories store entries, which are similar to objects. Each entry has a distinguished name (DN) made up of relative distinguished names (RDNs). RDNs are ordered to reflect the normal postal address order. Attributes can be added to entries. The schema defines the types of attributes and their types. [end of text]
LDAP is a network protocol for carrying out data definition and manipulation. It allows defining object classes with attribute names and types, and inheritance can be used. Entries can be specified to be of one or more object classes, and entries are organized into a directory information tree (DIT) according to their distinguished names. Entries can have more than one distinguished name, and they are organized into a subtree. Queries can be simple, consisting of selections and projections without joining. The LDAP API contains functions to create, update, and delete entries, as well as other operations on the DIT. [end of text]
LDAP is a network protocol for carrying out data definition and manipulation. Users can use an application programming interface or tools provided by vendors to perform data definition and manipulation. LDAP also defines a file format called LDAP Data Interchange Format (LDIF) for storing and exchanging information. Queries in LDAP are very simple, consisting of just selections and projections, without any join. A query must specify the base, search condition, scope, attributes to return, and limits on number of results and resource consumption. Queries can also automatically dereference aliases, and the last parameter is the search condition. [end of text]
DITs store information about entries, and the suffix identifies what information they store. DITs may be organized geographically and organizationally. Nodes in DITs contain referrals to other nodes. Queries on DITs can be handled by servers. [end of text]
LDAP is a hierarchical naming mechanism used by LDAP to break up control of information across parts of an organization. The referral facility helps integrate all directories into a single virtual directory. Many LDAP implementations support master–slave and multimaster replication of DITs. [end of text]
A distributed database system consists of a collection of sites, each maintaining a local database. Transactions access data only in their own site, requiring communication among sites for global transactions. Distributed databases can be homogeneous or heterogeneous, with schemas and system codes differing. Storage issues include replication and fragmentation. Distributed systems suffer from centralized system failures, requiring recovery schemes. To ensure atomicity, all sites must agree on the final outcome of transactions. To avoid blocking, the three-phase commit protocol can be used. Persistent messaging provides an alternative model for distributed transactions. [end of text]
The various concurrency-control schemes used in a centralized system can be modified for use in a distributed environment. In the case of locking protocols, the only change that needs to be incorporated is in the way that the lock manager is implemented. There are various approaches here, including central coordinators and distributed lock-managers. Protocols for handling replicated data include primary-copy, majority, biased, and quorum-consensus protocols. These protocols have different tradeoffs in terms of cost and ability to work in the presence of failures. Deadlock detection in a distributed lock-manager environment requires cooperation between multiple sites, since there may be global deadlocks even when there are no local deadlocks. To provide high availability, a distributed database must detect failures, recon-ﬁgure itself so that computation may continue, and recover when a processor or a link is repaired. The task is greatly complicated by the fact that it is hard to distinguish between network partitions or site failures. The majority protocol can be extended by using version numbers to permit transaction processing to proceed even in the presence of failures. While the protocol has a signiﬁcant overhead, it works regardless of the type of failure. Less-expensive protocols are available to deal with site failures, but they assume network partitioning does not occur. Some of the distributed algorithms require the use of a coordinator. To provide high availability, the system must maintain a backup copy that is ready to assume responsibility if the coordinator fails. Another approach is to choose the new coordinator after the
Transparency in data sharing and location transparency. [end of text]
one designed for a wide-area network [end of text]
This section is not provided in the textbook. [end of text]
sirable from a human-factors standpoint is the ability to design and implement systems that are user-friendly and efficient. [end of text]
Failures can occur in both distributed and centralized systems. Examples include network failures, hardware failures, and software failures. Centralized systems may have issues with data consistency, redundancy, and scalability. Distributed systems may have issues with fault tolerance, availability, and scalability. [end of text]
2PC ensures transaction atomicity by maintaining a consistent state of the database throughout the transaction, even if one or more nodes fail. Despite the failure, the transaction continues to execute, and the state of the database is updated to reflect the changes made by the failed nodes. This ensures that the transaction is atomic, meaning that the outcome of the transaction is consistent with the state of the database before the failure. [end of text]
The link between A and B is extremely overloaded and response time is 100 times longer than normal. This has implications for recovery in distributed systems. Distributed databases can suffer from high load and slow response times, making recovery more difficult and potentially leading to data loss or system downtime. [end of text]
tamps combined with discarding of received messages if they are too old. Sug-gest an alternative scheme based on sequence numbers instead of timestamps. [end of text]
The textbook section contains an erroneous statement. [end of text]
Only intention-mode locks are allowed on the root, and all transactions are given all possible intention-mode locks on the root automatically. These modifications alleviate the problem of bottlenecking without allowing any nonserializable schedules. [end of text]
The maintenance of a remote backup site involves ensuring its reliability and availability by regularly checking and updating the backup system. [end of text]
The state of a database is determined by the primary (master) copy, and updates get an exclusive lock on this copy. [end of text]
Inconsistent states can be handled using lazy propagation of updates. [end of text]
Generated globally unique timestamps using database systems. [end of text]
The textbook discusses the implementation of a distributed database system, focusing on the detection of conflicts and the construction of wait-for graphs. It outlines the process of inserting requests, handling requests, and constructing graphs to manage concurrent access to resources. The text also mentions the use of synchronization mechanisms and the concept of deadlock in distributed systems. [end of text]
To process the queries, we need to store each employee's information locally at the plant site. For the queries, we can use the following processing strategy:
1. For query a, we can use a join between the New York site and the local site to find all employees at the Boca plant.
2. For query b, we can use a subquery to find the average salary of all employees.
3. For query c, we can use a subquery to find the highest-paid employee at each site.
4. For query d, we can use a subquery to find the lowest-paid employee in the company. [end of text]
To process each query, we need to determine which plants contain the specified machines and then retrieve the corresponding employees or machines. For example, to find all employees at the plant that contains machine number 1130, we would need to look in the plant's database and retrieve the employees who work at that plant. [end of text]
The summary is shorter than the original section. [end of text]
n s for the relations of Figure 19.7. [end of text]
The notation \( r^n \) means \( r \) raised to the power of \( n \). For \( r^n = r^j \) to hold, \( j \) must be a multiple of \( n \). For \( r^n = r^j \) to hold, \( n \) must be a divisor of \( j \). [end of text]
The need for the LDAP standard is to provide a standardized way to store and manage user information in a distributed database environment. It allows for efficient retrieval and modification of user data, while maintaining data integrity and security. The standard is based on the concept of a directory service, which acts as a central repository for user information. The LDAP standard is widely adopted in the database industry, and is used in various applications, including web applications, file systems, and databases. [end of text]
The textbook discusses the implementation of distributed databases, including transaction concepts, 2PC protocol, and distributed concurrency control. It also covers semantic-based transaction-management techniques and the use of distributed recovery in data-base systems with replicated data. The problem of concurrent updates to replicated data has re-emerged in the context of data warehouses. The problem of concurrent updates to replicated data has re-emerged as an important research issue in the context of data warehouses. [end of text]
Fifteen years ago, parallel database systems were largely ignored by their advocates. Today, they are widely used by nearly every database system vendor. The growth of organizations' data requirements, such as those collected on the World Wide Web, has led to extremely large databases at many companies. Single-processor systems are not capable of handling such large volumes of data at the required rates. The set-oriented nature of database queries naturally lends itself to parallelism. As microprocessors become cheaper and more affordable, parallel machines are becoming common and relatively inexpensive. [end of text]
In it simplest form, I/O parallelism refers to reducing the time required to retrieve relations from disk by partitioning them on multiple disks. The most common data partitioning strategy is horizontal partitioning, where tuples are divided among disks. Two partitioning techniques are discussed: round-robin and hash partitioning. Range partitioning distributes contiguous attribute ranges among disks. [end of text]
Round-robin, Hash partitioning, Range partitioning. These three basic data-partitioning strategies ensure an even distribution of tuples across disks. [end of text]
Partitioning techniques can improve I/O efficiency by allowing parallel I/O access to data. However, point and range queries require different levels of parallelism. Hash partitioning is better for point queries based on partitioning attributes, while range partitioning is better for point and range queries. Skew can occur when a relation is partitioned, and it affects joins. In a system with many disks, partitioning can be chosen based on the number of partitions and available disks. [end of text]
Partitioning relations can improve read and write performance by utilizing multiple disks. Hash partitioning is particularly efficient for point queries based on partitioning attributes, while range queries can be answered using a single disk. Hash-based partitioning is also well-suited for point and range queries on partitioning attributes, while range partitioning is preferred for point queries on non-partitioning attributes. The choice of partitioning technique depends on the operations that need to be executed, with hash partitioning or rangepartitioning being preferred for joins and other relational operations. [end of text]
Skew in relation partitioning can occur due to attribute-value skew or partition skew. Skew can lead to skewed partitioning regardless of the partitioning technique used. Skew can be reduced with hash partitioning, if a good hash function is chosen. [end of text]
Skew can result in a significant decrease in performance, especially with higher parallax. Balancing range-partitioning can be achieved by sorting the relation and scanning it in sorted order. After every 1/n of the relation, the value of the partitioning attribute is added to the partition vector. This method can result in a speedup of 25 for a balanced range-partitioning vector. However, it incurs I/O overhead. Virtual processors can be used to minimize skew, particularly with range partitioning. [end of text]
In interquery parallelism, different queries or transactions execute in parallel with one another, increasing transaction throughput but not reducing response times. Interquery parallelism is easier to support in shared-memory architectures but requires coordination in shared-disk or shared-nothing architectures. Various protocols are available to ensure cache coherence. [end of text]
This protocol ensures that when a transaction sets a shared or exclusive lock on a page, it gets the correct copy of the page. It avoids repeated reading and writing to disk by using the buffer pool of some processors. The Oracle 8 and Oracle Rdb systems support interquery parallelism. [end of text]
Intraquery parallelism involves executing a single query in parallel on multiple processors and disks, speeding up long-running queries. Interquery parallelism does not help as each query is executed sequentially. Parallel evaluation of a query can be achieved by sorting each partition in parallel and concatenating the sorted partitions. Interoperation parallelism involves parallelizing different operations in a query expression. Both forms of parallelism are complementary and can be used simultaneously. The choice of algorithms depends on the machine architecture, with shared-nothing architecture models for shared-memory and shared-disk systems. The shared-nothing architecture model allows data to be transferred between processors. [end of text]
Parallel range partitioning is a technique used in database systems to sort relations with large sets of tuples. It involves dividing the relation into smaller subsets (partitions) and sorting each subset separately. This approach reduces the time required for reading the entire relation, making intraoperation parallelism more natural. [end of text]
Range-partitioning sort is a method for sorting a relation on n disks. It involves partitioning the relation into smaller parts and sorting each part separately. This reduces the total time required for reading the entire relation. If the relation is partitioned in any other way, it can be sorted in either of two ways: range-partitioning on the sort attributes and sorting each partition separately, or using a parallel version of the external sort–merge algorithm. [end of text]
Range-partitioning sort works by first partitioning the relation and then sorting each partition separately. This method is efficient when the relation can be partitioned on the same set of processors. [end of text]
In parallel external sort–merge, range partitioning is used to reduce skew, while partitioned join uses hash partitioning to parallelize any join technique. [end of text]
Parallel external sort–merge is an alternative to range partitioning. It works by locally sorting data on each disk and then merging the sorted runs across processors. The system range-partitions data at each processor, then sends tuples in sorted order. Each processor performs a merge on the streams received, and concatenates the sorted runs to get the final result. Some Teradata machines use specialized hardware to merge outputs. [end of text]
The join operation involves testing pairs of tuples to determine if they satisfy a join condition. Parallel join algorithms split pairs across processors to compute joins locally. Partitioned join splits relations into partitions, computes joins locally, and collects results. Partitioned join works correctly only if the join is an equi-join and partitions by the same function. [end of text]
Partitioned join is possible for equi-joins and natural joins, and works correctly only if join is an equi-join and partitioning function is used for both relations. [end of text]
The asymmetric fragment-and-replicate scheme is a special case of general fragment and replicate, where m = 1. It reduces the sizes of the relations at each processor compared to asymmetric fragment and replicate. [end of text]
Partitioning is not applicable to all types of joins. For instance, if the join condition is an inequality, such as rr.a<s.b, it is possible that all tuples in r join with some tuples in s (and vice versa). There may be no easy way of partitioning r and s so that tuples in partition ri join with only tuples in partition si. We can parallelize such joins by using a technique called fragment and replicate. We consider a special case of asymmetric fragment-and-replicate join, which works as follows. If r is already partitioned, no further partitioning is needed. All that's required is to replicate s across all processors. The general case of fragment and replicate join works similarly. [end of text]
In the parallel case, we can use the hybrid hash–join algorithm to cache incoming tuples in memory, avoiding disk writes and reads, and to distribute the larger relation r across m processors by hash function h1, in the same way as before. Each processor Pi executes the build and probe phases of the hash–join algorithm on the local partitions ri and si of r and s to produce a partition of the final result of the hash–join. [end of text]
The partitioned hash–join of Section 13.5.5 can be parallelized, and the hybrid hash–join algorithm can be used to cache incoming tuples in memory and avoid writing and reading them. [end of text]
To illustrate the use of asymmetric fragment and replicate-based parallelization, consider the case where relation s is much smaller than relation r, partitioning the attribute on which it is stored does not matter, and there is an index on a join attribute of relation r at each partition. Each processor reads relation s and replicates it to other processors, then performs an indexed nested-loop join with the ith partition. The join is synchronized to ensure enough memory space on each processor. [end of text]
Selection can be parallelized by partitioning on attributes or ranges, and range partitioning on attributes. Projection can be performed in parallel by partitioning and eliminating duplicates. Aggregation can be parallelized by partitioning on grouping attributes. [end of text]
The parallelization of other operations is covered in several of the exercises.20.5.4Cost of Parallel Evaluation of Operations We achieve parallelism by partitioning the I/O among multiple disks, and partitioning the CPU work among multiple processors. If such a split is achieved without any overhead, and if there is no skew in the splitting of work, a parallel operation using n processors will take 1/n times as long as the same operation on a single processor. We already know how to estimate the cost of an operation such as a join or a selection. The time cost of parallel processing would then be 1/n of the time cost of sequential processing of the operation. We must also account for the following costs:• Startup costs for initiating the operation at multiple processors• Skew in the distribution of work among the processors, with some processors getting a larger number of tuples than others• Contention for resources—such as memory, disk, and the communication network—resulting in delays• Cost of assembling the final result by transmitting partial results from each processor The time taken by a parallel operation can be estimated asTpart + Tasm + max(T0, T1, . . . , Tn−1) where Tpart is the time for partitioning the relations, Tasm is the time for assembling the results and Ti the time taken for the operation at processor Pi. Assuming that the tuples are distributed without any skew, the number of tuples sent to each processor is Sil
We achieve parallelism by partitioning I/O and CPU work among multiple disks and processors, and estimating the cost of parallel operations. Skew in distribution and contention for resources can significantly affect performance. Skew in partitioning is closely related to partition overflow in hash joins, and we can use overflow resolution and avoidance techniques for hash joins to handle skew. [end of text]
Pipelined parallelism is a technique used in database systems to achieve economies of computation by allowing multiple operations to be executed simultaneously. It involves using pipelining to reduce the number of intermediate results that need to be written to disk, making it a source of parallelism in both sequential and pipelined systems. [end of text]
Pipelining forms an important source of economy of computation for database query processing. It allows multiple operations to be executed simultaneously, reducing the need for disk storage. Pipelined parallelism is a source of parallelism in hardware design. [end of text]
Pipelined parallelism is useful with a small number of processors but does not scale well. Pipelined parallelism can avoid writing intermediate results to disk and provides a high degree of parallelism, but it does not provide a high degree of parallelism with a high degree of parallelism. Query optimization accounts for the success of relational technology, and it is more complicated than query optimization for sequential query evaluation. The number of parallel evaluation plans from which to choose is much larger than the number of sequential evaluation plans. Two popular heuristic approaches to reduce the number of parallel execution plans that have to be considered are the exchange-operator model and the Volcano database. [end of text]
In parallelism, operations that do not depend on each other can be executed in parallel. Pipelined joins provide independent parallelism, which is useful in a low degree of parallel system. However, it may not provide a high degree of parallelism. [end of text]
Query optimizers account for the success of relational technology. They take queries and find the cheapest execution plan among many possible plans, with complexity increased by partitioning costs and resource contention. Parallel query evaluation is more complex, with multiple operations to be parallelized, and the choice of pipeline and processor allocation. Optimizing parallel queries by considering all alternatives is much more expensive than sequential queries, and heuristic approaches are used to reduce the number of parallel execution plans considered. The main differences lie in how partitioning is performed and cost estimation formulas. [end of text]
Parallel database systems are essential for handling large volumes of data and processing decision-support queries. Loading data from external sources is a critical requirement. SQL is the language of choice for parallel database systems. [end of text]
Large parallel database systems must address availability issues, including failure of processors or disks. They consider these issues with a large number of processors and disks, where the probability of failure is significantly higher. Large-scale parallel database systems like Compaq Himalaya, Teradata, and Informix XPS (now a division of IBM) are designed to operate even if a processor or disk fails. Data are replicated across at least two processors. If a processor fails, data stored can still be accessed from other processors. The system keeps track of failed processors and distributes work among functioning processors. Requests for data stored at the failed site are automatically routed to backup sites. If all data of a processor A are replicated at a single processor B, B becomes a bottleneck. Online index construction allows operations like insertions, deletions, and updates on relations to be performed while the system is executing other transactions. [end of text]
Parallel databases have gained significant commercial acceptance in the past 15 years. In I/O parallelism, relations are partitioned among disks to improve retrieval speed. Three common techniques are round-robin partitioning, hash partitioning, and range partitioning. Skew is a major issue, especially with increasing parallelism. Balanced partitioning vectors, using histograms, and virtual processor partitioning reduce skew. In interquery parallelism, different queries are executed concurrently to increase throughput. Intraquery parallelism attempts to reduce the cost of a single query. Two types of intraquery parallelism are intraoperation parallelism and interoperation parallelism. Parallel sort, range sort, and parallel external sort–merge are examples of parallel operations. Data parallelism and parallel join are discussed. Query optimization in parallel databases is more complex than in sequential databases. Review terms include decision-support queries, I/O parallelism, horizontal partitioning, partitioning techniques, and partitioning vector. Exercises include range query, skew execution, and range query. [end of text]
Partitioning technique for range queries: Consider a database with a large table of customer orders. To optimize performance, partition the table by order date. When querying for orders within a specific date range, the database can quickly find all orders that fall within that date range, rather than scanning the entire table. This technique allows the database to return results faster than if the table were partitioned by customer ID or product ID. [end of text]
One disk may need to be accessed. Benefits include faster access times and reduced disk usage. Drawbacks include potential data loss if not managed properly. [end of text]
Hash partitioning can reduce skew by distributing data evenly. Range partitioning can reduce skew by grouping data into equal-sized groups. Both methods can be used depending on the specific data distribution. [end of text]
Increasing the throughput of a system with many small queries, when the number of disks and processors is large. Increasing the throughput of a system with a few large queries, when the number of disks and processors is large. [end of text]
In a pipeline on a single processor, even when many processors are available, the shared memory architecture allows for efficient data exchange and parallel execution. However, if the machine has ashared-memory architecture, the shared memory may lead to contention and inefficiencies. In independent parallelism, even if the operations are not pipelined and there are many processors available, it is still a good idea to perform several operations on the same processor to maximize efficiency. [end of text]
use partitioning attributes such as hash function, key, or range. [end of text]
Partitioning can be used to optimize the evaluation of join conditions involving | r.A −s.B |≤k, where k is a small constant. Band joins are a type of join that can be optimized using partitioning. [end of text]
The difference operation, aggregation by the count operation, aggregation by the count distinct operation, and full outer join are concepts in database management. The left outer join is used when the join condition involves only equality, and the right outer join is used when the join condition involves comparisons other than equality. The left outer join is also used with comparisons other than equal-ity. [end of text]
A load-balanced range partitioning function can be used to divide the values into 5 partitions. An algorithm for computing a balanced range partition with p partitions, given a histogram of frequency distributions containing n ranges, can be found in the textbook. [end of text]
Partitioning data items across multiple processors can help avoid data loss in case one processor fails. It is beneficial for optimizing data flow and reducing storage costs. Using RAID storage instead of storing an extra copy of each data item is more efficient and less prone to data loss. It is also advantageous to consider the potential benefits and drawbacks of using RAID storage compared to traditional storage methods. [end of text]
The World Wide Web is a distributed information system based on hypertext, providing a universal front end to information supplied by back ends located anywhere in the world. It allows databases to be linked with the Web, improving performance and presenting static documents on a Web site. The Web interfaces to databases have become very important, with servlets and server-side scripting languages providing techniques for improving performance. [end of text]
The Web has become important as a front end to databases for several reasons: it provides a universal front end to information supplied by back ends anywhere in the world, allows users to access information without downloading special-purpose software, and interfaces databases to the Web by generating dynamic Web documents from databases. These features enable users to present only static documents on a Web site and to tailor the display to the user, while also allowing for updates to company data and multiple Web documents to be replicated and automatically updated. The Web interfaces provide attractive benefits even for database applications that are used only with a single organization. Browsers today can fetch programs along with HTML documents, and run them in safe mode without damaging data on the user's computer. [end of text]
The textbook discusses the construction of sophisticated user interfaces beyond what is possible with justHTML, focusing on Web interfaces that can be used without downloading and installing software. It reviews fundamental technology such as Uniform Resource Locators, HyperText Markup Language, and client-side scripting, emphasizing the versatility and dynamic nature of Web interfaces. [end of text]
The World Wide Web is based on Uniform Resource Locators (URLs) that uniquely identify documents and allow dynamic generation of data. HTML is used to display tables, forms, and other interactive elements on web pages. Client-side scripting and applets enable interactive web pages that can be embedded in web pages to carry out activities like animation. [end of text]
A uniform resource locator (URL) is a globally unique name for each document on the Web. It indicates how to access the document and can include the name of a machine hosting the Web server, arguments to be given to the program, and the path name of the document within that machine. URLs can contain identifiers of programs and arguments to those programs. [end of text]
HTML source text, a uniform look for multiple HTML documents. [end of text]
Embedding program code in documents allows for interactive web pages, enhancing user engagement and speed. [end of text]
The textbook discusses the dangers of supporting Web-based applications, particularly Java programs, which can perform malicious actions on the user's computer. It also explains how to create and maintain sessions in a two-tier architecture, where the application program runs within the Web server, similar to a three-tier architecture. The text emphasizes the importance of using connectionless services and maintaining session information to prevent such issues. [end of text]
A Web server is a program running on the server machine that accepts requests from a browser and sends back results in HTML format. The browser and Web server communicate using the HyperText Transfer Protocol (HTTP), which provides powerful features beyond simple document transfer. A Web server can act as an intermediary to provide access to various information services. Applications can be created by installing an application program that provides services. The common gateway interface (CGI) standard defines how the Web server communicates with application programs. The application program communicates with a database server through ODBC, JDBC, or other protocols to get or store data. A three-tier architecture with a Web server, application server, and database server is common in modern systems. The CGI interface starts a new process for each request, leading to increased overhead. Session information is stored at the client to identify requests as part of a user session. [end of text]
The servlet API provides a convenient method of creating sessions. Invoking the method getSession(true) of the class HttpServletRequest creates a new object of type HttpSession if this is the first request from that client; the argument true says that asession must be created if the request is a new request. The method returns an existing object if it had been created already for that browser session. Internally, cookies are used to recognize that a request is from the same browser session as an earlier request. The servlet code can store and look up (attribute-name, value) pairs in the HttpSession object, to maintain state across multiple requests. For instance, the first request in a session may ask for a user-id and password, and store the user-id in the session object. On subsequent requests from the browser session, the user-id will be found in the session object. Displaying a set of results from a query is a common task for many database applications. It is possible to build a generic function that will take any JDBC ResultSet as argument, and display the tuples in the ResulSet appropriately. [end of text]
In a two-tier Web architecture, the application runs as part of the Web server itself. One way of implementing such an architecture is to load Java programs with the Web server. The Java servlet speciﬁcation deﬁnes an application programming interface for communication between the Web server and the application program. The servlet also refers to a Java program that implements the servlet interface. The program is loaded into the Web server when the server starts up or when the server receives a Web request for executing the servlet application. The servlet code in the example extracts values of the parameter’s type and number by using request.getParameter(), and uses these to run a query against a database. The system returns the results of the query to the requester by printing them out in HTML format to the HttpServlet-Response. The servlet API provides a convenient method of creating sessions. Invoking the method getSession(true) of the class HttpServletRequest creates a new object of type HttpSession if this is the ﬁrst request from that client; the argument true says that asession must be created if the request is a new request. Internally, cookies are used to recognize that a request is from the same browser session as an earlier request. The servlet code can store and look up (attribute-name, value) pairs in the HttpSession object, to maintain state across multiple requests. For instance, the ﬁrst request in a session may ask for a user-id and password, and store the user-id in the session object. On subsequent requests from the browser
Server-side scripting allows for easy creation of many applications, while server-side scripting languages provide embedded scripts that can be executed on the server before delivering a Web page. Scripts can generate text and delete content, and source code is removed from the page. Scripting languages include Java, C, and PHP. Server-side scripting can be used for embedded VBScript and JScript, and for generating HTML reports. Caching techniques include JDBC connections and pooling. Materialized views can be used to cache query results and Web pages. [end of text]
Writing a simple Web application in a programming language like Java or C involves writing many lines of code and familiarity with the language's intricacies. Server-side scripting provides a simpler method for creating applications by embedding scripts within HTML documents. Scripting languages allow for embedded script execution before page delivery, generating text or deleting content. Server-side scripting can execute SQL queries against a database. Multiple scripting languages have appeared in recent years, including Server-Side Javascript, JScript, JavaServer Pages, PHP, ColdFusion, and Zope's DTML. Embedding code from older scripting languages in HTML pages is possible. This allows for embedded VBScript and JScript in ASP. Other approaches extend report writers to generate HTML reports. These support HTML forms for parameter values used in embedded queries. Clear differences exist in programming style and ease of application creation. [end of text]
Caching techniques are used to improve response times for web sites, especially with high transaction rates. Many applications cache JDBC connections, and reuse results of previous queries. Materialized views can be used to reduce communication costs with databases. [end of text]
Improving the performance of a system involves identifying bottlenecks, adjusting parameters, and eliminating bottlenecks by improving component performance. Simple programs have a fixed execution time, while complex systems can be modeled as queueing systems. Each service has a queue, and small transactions spend most of their time waiting. [end of text]
Improving the performance of a bottleneck component can lead to a 20% improvement in overall system speed, whereas improving the rest of the code could result in nearly 80% improvement. Simple programs have a time spent in each region of the code, but database systems are more complex and modeled as queueing systems. Transactions request various services from the database system, starting from entry into a server process, disk reads during execution, CPU cycles, and locks for concurrency control. Each service has a queue associated with it, and small transactions spend most of their time waiting. [end of text]
In databases, bottlenecks often appear in long queues for a particular service or high utilization. Utilization should be kept low to prevent queue length from increasing exponentially. Tuning parameters, such as buffer sizes and checkpointing intervals, can be adjusted to improve performance. The three levels of tuning interact with one another, and tuning at a higher level may result in changes to the hardware or disk subsystem. [end of text]
Database administrators can tune a database system at three levels. The lowest level is at the hardware level. Options for tuning systems at this level include adding disks or using a RAID system if disk I/O is a bottleneck, adding more memory or moving to a faster processor if CPU use is a bottleneck. The second level consists of the database-system parameters, such as buffer size and checkpointing intervals. The exact set of database-system parameters that can be tuned depends on the speciﬁc database system. Well-designed database systems perform as much tuning as possible automatically, freeing the user or database administrator from the burden. For instance, in many database systems the buffer size is ﬁxed but unable. If the system automatically adjusts the buffer size by observing indicatorssuch as page-fault rates, then the user will not have to worry about tuning the buffersize. The third level includes the schema and transactions. Tuning at this level is system independent. [end of text]
In a well-designed transaction processing system, each transaction requires at most 50 I/O operations per second. To support more transactions per second, increasing the number of disks is necessary. If data is striped across n/50 disks, the limiting factor is not the capacity of the disk but the speed at which random data can be accessed. Keeping frequently used data in memory reduces the number of disk I/Os, while keeping very infrequently used data in memory is a waste. The question is, for a given amount of money available for spending on disks or memory, what is the best way to spend the money to achieve maximum number of transactions per second? [end of text]
A reduction of 1 I/O per second saves (price per disk drive) / (access per second per disk). Thus, if a particular page is accessed n times per second, the saving due to keeping it in memory is n times the above value. Storing a page in memory costs (price per MB of memory) / (pages per MB of memory). Thus, the break-even point is ∗price per disk drive access per second per disk = price per MB of memory pages per MB of memory. The 5-minute rule states that if a page is used more frequently than once in 5 minutes, it should be cached in memory. In sequential access, accessing data more frequently can lead to more pages read per second. The formula for finding the break-even point depends on factors like disk prices and memory prices that have changed by factors of 100 or 1000 over the past decade. The 5-minute rule remains the same for data accessed less frequently. The 1-hour rule or 1-second rule have changed to a 5-minute rule. For sequential access, accessing data more frequently can lead to more pages read per second. The formula for finding the break-even point depends on factors like disk prices and memory prices that have changed by factors of 100 or 1000 over the past decade. The 5-minute rule remains the same for data accessed less frequently. The 1-hour rule or 1-second rule have
In the context of BCNF and third normal forms, partitioning relations vertically can lead to faster access to account information, as branch-name is not fetched. Materialized views can provide benefits similar to denormalized relations, but require more storage effort. Clustering records in the join can speed up the join operation. [end of text]
Improving set orientation and reducing lock contention can significantly improve transaction performance. Optimizers on many database systems were not particularly good, so complex queries containing nested subqueries were not optimized well. Today’s advanced optimizers can transform even badly written queries and execute them efficiently, so the need for tuning individual queries is less important than it used to be. However, complex queries containing nested subqueries are not optimized very well by many optimizers. Most systems provide a mechanism to find out the exact execution plan for a query; this information can be used to rewrite the query in a form that the optimizer can deal with better. [end of text]
We can tune indices in a system to improve performance by creating appropriate indices on relations. If queries are bottleneck, indices can be created to speed up updates. If updates are bottleneck, too many indices need to be updated, making them unusable. Removing indices may speed up certain updates. Choosing the type of index is important. Some systems support different kinds of indices, such as hash and B-tree indices. If range queries are common, B-tree indices are preferred. Whether to make an index clustered is another tunable parameter. Only one index on a relation can be made clustered, by storing the relation sorted on the index attributes. Generally, the index that benefits the most number of queries and updates should be made clustered. To help identify what indices to create, and which index (if any) on each relation should be clustered, some database systems provide tuning wizards. These tools use the past history of queries and updates to estimate the effect of various indices on the execution time. Recommendations on what indices to create are based on these estimates. [end of text]
Materialized views can significantly speed up aggregate queries by storing total loan amounts for each branch. Manual selection is time-consuming and requires understanding query costs. A database system can provide support for materialized view selection within the system. [end of text]
In this section, we study two approaches for improving transaction performance: improving set orientation and reducing lock contention. Today’s advanced optimizers can transform even poorly written queries and execute them efficiently, while complex queries containing nested subqueries are not optimized well by many optimizers. Embedded SQL can help combine calls into a set-oriented query that can be better optimized. [end of text]
The costs of communication of SQL queries can be high in client-server systems, while using a single SQL query can significantly reduce the communication overhead. Communication cost can be reduced by combining embedded SQL calls with a single SQL query, fetching its results to the client side, and stepping through the results to find required tuples. Another technique is to use stored procedures for reducing communication and SQL compilation costs. Performance simulation can be used to test the performance of a database system before installation, and various experiments can be run to find the system's behavior under different load conditions and system parameters. [end of text]
To test a database system's performance, we can create a simulation model that captures the time each service takes to process requests, as well as the associated queue. This model can be used to simulate transaction processing and find out how the system behaves under different load conditions and with varying service times. System parameters, such as CPU and disk access times, can also be varied to optimize performance. [end of text]
Performance benchmarks measure software system performance. Variations in vendor implementations lead to differences in performance on different tasks. [end of text]
Software systems vary widely in implementation, affecting performance on different tasks. Examples include DBMSs like SQL Server and Oracle. Performance varies between vendors. [end of text]
The TPC benchmarks define a series of performance standards for database systems, measuring throughput in transactions per second (TPS). They also measure price per TPS, indicating the cost of high throughput. The TPC benchmarks are detailed and measure performance in terms of transactions per second, with a focus on the back-end database server. The TPC benchmarks are used to measure performance in terms of price per TPS, indicating the cost of high throughput. A large system may have a high number of transactions per second, but may be expensive (that is, have a high price per TPS). [end of text]
Online transaction processing and decision support are two classes of applications handled by databases. They require different techniques for high concurrency and query evaluation. Some databases are optimized for transaction processing, while others are tuned for decision support. The choice of database system depends on the application's mix of transaction-processing and decision-support requirements. The harmonic mean of throughput numbers should be used only if the transactions do not interfere with one another. [end of text]
The Transaction Processing Performance Council (TPC) has defined benchmark standards for database systems. These standards define relations, tuples, and transaction rates. The performance metric is throughput, expressed as TPS. The TPC benchmarks measure performance in terms of price per TPS, with a focus on business applications. The TPC-C benchmark is designed to model a more complex system, focusing on order-entry activities. [end of text]
The TPC-D benchmark was designed to test the performance of database systems on decision-support queries. Decision-support systems are becoming increasingly important today. The TPC-A, TPC-B, and TPC-C benchmarks measure performance on transaction-processing workloads, and should not be used as a measure of performance on decision-support queries. The D in TPC-D stands for decision support. The TPC-D benchmark schema models a sales/distribution application, with parts, suppliers, customers, and orders, along with some auxiliary information. The sizes of the relations are deﬁned as a ratio, and database size is the total size of all the relations, expressed in gigabytes. TPC-D at scale factor 1 represents the TPC-D benchmark on a 1-gigabyte database, while scale factor 10 represents a 10-gigabyte database. The benchmark workload consists of a set of 17 SQL queries modeling common tasks executed on decision-support systems. Some of the queries make use of complex SQL features, such as aggregation and nested queries. [end of text]
The nature of applications in an object-oriented database, OODB, is different from typical transaction-processing applications. Therefore, a different set of bench-marks has been proposed for OODBs. The Object Operations benchmark, version 1, popularly known as the OO1 benchmark, was an early proposal. The OO7 benchmark follows a philosophy different from that of the TPC benchmarks. The TPC benchmarks provide one or two numbers (in terms of average transactions per second, and transactions per second per dollar); the OO7 benchmark provides a set of numbers, containing a separate benchmark number for each of several different kinds of operations. The reason for this approach is that it is not yet clear what is the typical OODB transaction. It is clear that such a transaction will carry out certain operations, such as traversing a set of connected objects or retrieving all objects in a class, but it is not clear exactly what mix of these operations will be used. Hence, the benchmark provides separate numbers for each class of operations; the numbers can be combined in an appropriate way, depending on the speciﬁc application. [end of text]
Standards define the interface of a software system, formal standards are developed by organizations through a public process, and formal standards committees are composed of representatives of vendors, user groups, and standards organizations. [end of text]
SQL:1999, ODBC, X/Open XA standards [end of text]
SQL:1999 standard has been standardized to add OLAP features, temporal data, and external data access. Other parts include temporal, management of external data, and object language bindings. Multimedia standards are proposed for text, spatial, and still image data. [end of text]
The ODBC standard is a communication standard between client applications and database systems, based on SQL Call-Level Interface and AccessGroup standards. It defines a CLI, SQL syntax, and conformance levels. ODBC allows simultaneous connections to multiple data sources and transactions, but does not support two-phase commit. X/Open has developed XA standards for interdatabase transactions. [end of text]
Microsoft's OLE-DB is a C++ API with goals similar to ODBC, but for non-database data sources. It provides constructs for connecting to a data source, starting a session, executing commands, and getting results in the form of a rowset. OLE-DB differes from ODBC in several ways. To support limited feature support, features in OLE-DB are divided into interfaces, and a data source may implement only a subset of the interfaces. OLE-DB can negotiate with a data source to find what interfaces are supported. In ODBC commands are always in SQL. In OLE-DB, commands may be in any supported languages. Another major difference is that a rowset is an object that can be shared by multiple applications through shared memory. A rowset object can be updated by one application, and other applications sharing that object would get notiﬁed about the change. The Active Data Objects (ADO) API, also created by Microsoft, provides an easy-to-use interface to the OLE-DB functionality, which can be called from scripting languages, such as VBScript and JScript.21.4.3Object Database StandardsStandards in the area of object-oriented databases have so far been driven primarily by OODB vendors. The Object Database Management Group (ODMG) is a group formed by OODB vendors to standardize the data model and language interfaces to OODBs. The C++ language interface speciﬁed
The Object Management Group (OMG) has standardized the data model and language interfaces for object-oriented databases, while the Object Database Management Group (ODMG) has developed a reference model for distributed software applications based on the object-oriented model. The Object Management Architecture (OMA) and CORBA provide detailed specifications for the ORB and IDL, respectively. [end of text]
XML-based standards have been developed for e-commerce, including RosettaNet, and other frameworks like BizTalk and SOAP. These standards facilitate supply-chain management, online auctions, and electronic marketplaces. XML wrappers form the basis of a unified view of data across participants in these marketplaces, and SOAP is used for remote procedure calls. XQuery is a query language for XML, in development for XML query languages that are in working draft stage. [end of text]
E-commerce involves various activities related to commerce through the internet, including presale, sale, marketplace, auction, and payment. It uses databases to manage and process these activities. [end of text]
In databases, customers support and post-sale service are essential for delivering products or services over the internet. E-commerce sites provide browsing and search facilities, while marketplaces negotiate prices and match buyers with sellers. Database issues include authentication, secure recording, delays, and large performance databases with parallelism. [end of text]
An e-commerce site provides a catalog of products and services, organized into a hierarchy for easy browsing and search. It offers customer information for faster product selection and customization. The site supports personalized offers based on past buying history. It also addresses high transaction rates by caching query results or generated Web pages. [end of text]
Marketplaces help in negotiating prices for products by allowing multiple sellers and buyers. There are different types of marketplaces: reverse auction, closed bidding, and auction. In a reverse auction, buyers specify requirements and sellers bid for supply. In a closed bidding, bids are made public. In an auction, multiple buyers and a single seller bid on items. In an exchange, multiple sellers and multiple buyers bid on items. Bidders need to be authenticated and bids need to be recorded securely in a database. Delays in broadcasting bids can lead to financial losses. Large volumes of trades require high performance databases with parallelism. [end of text]
Settlement involves payment for goods and delivery, with payment through credit card. Credit card fraud and unauthorized billing are problems. Secure payment protocols ensure privacy and prevent unnecessary details. Encryption is used to protect data. [end of text]
Public/private key encryption is widely used for secure payment transactions. Impersonation is prevented by digital certificates, where public keys are signed by a certiﬁcation agency. The Secure Electronic Transaction (SET) protocol requires multiple rounds of communication to ensure transaction safety. Physical cash systems provide greater anonymity, similar to credit card and bank transactions. [end of text]
Legacy systems are older-generation systems that are incompatible with current-generation standards and systems. They may still contain valuable data and support critical applications. Porting legacy applications to a more modern environment is costly due to the large size and complexity of legacy systems. One approach is to build a wrapper layer on top of legacy systems to make them appear as relational databases. This allows for high-level queries and updates. When a new system is built, it must undergo extensive coding to support all the functionality of the legacy system. The process is called re-engineering. When a new system is built and tested, it must be populated with data from the legacy system and all further activities must be carried out on the new system. The big-bang approach carries risks such as users not familiar with the interfaces and new system bugs. The alternative approach, called the chicken-little approach, incrementally replaces legacy functionality. [end of text]
The Web browser is the most widely used user interface for databases, providing hyper-links with forms facilities. HTML allows for richer user interaction. Web servers communicate with servers using the HTTP protocol. Application programs are executed on the server side, reducing overheads. Tuning parameters and higher-level design are important for good performance. [end of text]
Performance benchmarks play an important role in comparisons of databases systems, especially as systems become more standards compliant. Standards are important because of the complexity of database systems and their need for interoperation. E-commerce systems are fast becoming a core part of how commerce is performed. Legacy systems are systems based on older-generation technologies such as nonrelational databases or even directly on ﬁle systems. Interfacing legacysystems with new-generation systems is often important when they run mission-critical systems. Migrating from legacy systems to new-generation systems must be done carefully to avoid disruptions, which can be very expensive. [end of text]
CGI is used for web applications, but Java programs are generally faster. [end of text]
Maintaining connections is crucial for efficient data retrieval and management in databases. [end of text]
Mance is a term used in mathematics to describe a function that is not injective (one-to-one), meaning it can take multiple values for the same input. [end of text]
Tuning can significantly improve performance by adjusting parameters such as the number of threads, cache size, and memory allocation. Two examples of tuning are using a higher number of threads to speed up processing time and adjusting the cache size to reduce memory usage. [end of text]
One of the main problems could arise from the complexity of database systems and the difficulty in maintaining them. This could be addressed by using advanced database management techniques and by implementing efficient data access strategies. Additionally, regular updates and maintenance of the database system are essential to ensure its continued functionality and reliability. [end of text]
The average transaction throughput of the system is 100 transactions per second. Interference between transactions of different types can lead to incorrect throughput calculations. Factors such as transaction overlap and the number of concurrent transactions can affect the accuracy of the average throughput. [end of text]
The 5-minute rule would be affected by a doubling in access rates, while the 1-minute rule would remain unchanged. [end of text]
Dependable measures are reliable and consistent methods used to assess and evaluate performance. [end of text]
The textbook defines "reactionary standard" as a standard that is considered outdated or unworkable, often due to historical or cultural reasons. [end of text]
In the context of databases, impersonated companies can affect things such as purchase orders or programs, and other companies. Projects 21.1 and 21.2 focus on Web-based systems for entering, updating, and viewing data. Project 21.3 is about a shopping cart system, while Project 21.4 is about a system for recording course performance. Project 21.5 is about a system for booking classrooms, and Project 21.6 is about a system for managing online multiple-choice tests. Project 21.7 is about a system for managing e-mail customer service, and Project 21.8 is about a simple electronic marketplace. Project 21.9 is about a Web-based newsgroup system, and Project 21.10 is about a Web-based system for managing sports "laders." [end of text]
Database applications can be broadly classified into transaction processing and decision support, as discussed earlier. Transaction-processing systems are widely used, while companies collect vast amounts of information. These databases can be used for making business decisions, such as stock selection and price adjustments. The storage and retrieval of data for decision support issues include SQL, online analytical processing, and statistical analysis. Knowledge-discovery techniques attempt to discover statistical rules and patterns from data. Large companies use diverse sources of data, such as different schemas. [end of text]
Data warehouses are built to efficiently retrieve data from diverse sources under a unified schema. They provide a single interface for users. Decision support covers all areas mentioned, excluding statistical analysis and data mining. [end of text]
Although complex statistical analysis is best left to statistics packages, databases support simple, commonly used forms of data analysis. OLAP tools support interactive analysis of summary information. SQL extensions have been developed to support OLAP tools. There are many commonly used tasks that cannot be done using basic SQL aggregation and grouping facilities. Examples include finding percentiles, cumulative distributions, and aggregating over sliding windows on sequentially ordered data. Several SQL extensions have been recently proposed to support such tasks, and implemented in productssuch as Oracle and IBM DB2.22.2.1Online Analytical ProcessingStatistical analysis often requires grouping on multiple attributes. Consider an application where a shop wants to find out what kinds of clothes are popular. Let us suppose that clothes are characterized by their item-name, color, and size, and we have a relation sales with the schema sales(item-name, color, size, number). Suppose that item-name can take on the values (skirt, dress, shirt, pant), color can take on the values (dark, pastel, white), and size can take on values (small, medium, large). Given a relation used for data analysis, we can identify some of its attributes as measure attributes, since they measure some value, and can be aggregated upon. For instance, the attribute number of the sales relation is a measure attribute, since it measures the number of units sold. Some (or all) of the other attributes of the relation are identiﬁed as dimension attributes
Statistical analysis often requires grouping on multiple attributes. For instance, a shop uses sales data to identify popular items by item-name, color, and size. The relation `sales` has attributes `item-name`, `color`, and `size`, and the relation `used` has attributes `item-name`, `color`, and `size`. The `item-name` dimension is a measure attribute, while the `color` and `size` dimensions are dimension attributes. Data that can be modeled as dimension attributes and measure attributes are called multidimensional data. Silberschatz-Korth-Sudarshan: Database System Concepts, Fourth Edition VII. Other Topics 22. Advanced Querying and Information Retrieval 813 © The McGraw-Hill Companies, 2001 [end of text]
The data cube in Figure 22.3 visualizes the relationship between item-name, color, and size, where each cell contains a value. Aggregation can be performed with grouping on each of the 2n subsets of the n dimensions. OLAP systems allow users to view data at different levels of granularity, moving from coarse-granularity data to finer-granularity data. Dimension attributes that are not part of the cross-tab are shown above the cross-tab. [end of text]
Hierarchies on dimensions may look at the hour value, while analysts interested in sales by day of the week or aggregates over a month, quarter, or year may map the date to a day-of-the-week and look only at that. Hybrid OLAP systems store some summaries in memory and others in a relational database, allowing data to be viewed on a cross-tab. OLAP implementations precompute and store entire data cubes, allowing OLAP queries to be answered within a few seconds. Hierarchies on attributes increase the number of groupings, making the entire data cube larger and less feasible to store. [end of text]
OLAP systems use multidimensional arrays and relational databases, and hybrid OLAP systems combine these. OLAP facilities are integrated into relational systems, and data is stored in a relational database. OLAP systems are called hybrid OLAP (HOLAP) systems. OLAP systems are implemented as client–server systems. OLAP queries are answered within a few seconds, even on datasets that may contain millions of tuples. However, there are 2n groupings with n dimension attributes; hierarchies on attributes increase the number further. OLAP systems precompute and store entire data cubes, and OLAP queries can be answered within a few seconds. OLAP systems are implemented as client–server systems. OLAP queries are answered within a few seconds, even on datasets that may contain millions of tuples. However, there are 2n groupings with n dimension attributes; hierarchies on attributes increase the number further. OLAP systems precompute and store entire data cubes, and OLAP queries can be answered within a few seconds. OLAP systems are implemented as client–server systems. OLAP queries are answered within a few seconds, even on datasets that may contain millions of tuples. However, there are 2n groupings with n dimension attributes; hierarchies on attributes increase the number further. OLAP systems precompute and store entire data cubes, and OLAP queries can be answered within a few seconds. OLAP systems are implemented as client–server systems. OLAP queries
SQL:1999 and Oracle support most aggregate functions, while IBM DB2 and other databases may support additional ones. Rollup and cube constructs allow multiple levels of aggregation on a column, while multiple rollups and cubes can be used in a single group by clause. The SQL:1999 standard uses the value null to indicate both a null value and all, potentially leading to ambiguity in grouping operations. [end of text]
SQL:1999, Oracle, and IBM DB2 databases support standard deviation and variance, as well as generalizations of the group by construct using the cube and rollup constructs. [end of text]
The output is the same as in the version of the query without grouping, but with three extra columns called item-name-ﬂag, color-ﬂag, and size-ﬂag. In each tuple, the value of a ﬂag ﬁeld is 1 if the corresponding ﬁeld is a null representing all. Instead of using tags to indicate nulls that represent all, we can replace the nullvalue by a value of our choice:decode(grouping(item-name), 1, 'all', item-name) This expression returns the value "all" if the value of item-name is a null corresponding to all, and returns the actual value of item-name otherwise. This expression can be used in place of item-name in the select clause to get "all" in the output of the query, in place of nulls representing all. Neither the rollup nor the cube clause gives complete control on the grouping that are generated. For instance, we cannot use them to specify that we want onlygroupings {(color, size), (size, item-name)}. Such restricted groupings can be generated by using the grouping construct in the having clause; we leave the details as an exercise for you. [end of text]
Finding the position of a value in a larger set is a common operation. SQL-92 queries are difficult to express and inefficient to evaluate. Programmers often resort to writing queries partly in SQL and partly in a programming language. Ranking is done in conjunction with an order by specification. The ranking function gives the same rank to all tuples that are equal on the ordering attribute. For instance, if there are two students with the same marks, both would get rank 1. The next rank given would be 3, not 2, so if three students get the next highest mark, they would all get rank 3. The dense rank function does not create gaps in the ordering. In the above example, the tuples with the second highest value all get rank 2, and tuples with the third highest value get rank 3, and so on. Ranking can be done within partitions of the data. For instance, suppose we have an additional relation student-section that stores for each student the section in which the student studies. The ranking function gives the same rank to all tuples that are equal on the ordering attribute. For instance, if the highest mark is shared by two students, both would get rank 1. The next rank given would be 3, not 2, so if three students get the next highest mark, they would all get rank 3. The dense rank function does not create gaps in the ordering. In the above example, the tuples with the second highest value all
SQL:1999 provides a windowing feature to support moving average queries and cumulative balance queries. It allows specifying where to place null values in the sort order, making the number of tuples in each bucket equal. Windowing can overlap, and different windows can specify ranges of values. [end of text]
SQL:1999 provides a windowing feature to support moving average queries and cumulative balance queries in databases. [end of text]
Data mining is the process of analyzing large databases to discover useful patterns, similar to knowledge discovery in artificial intelligence and statistical analysis. It involves semiautomatically analyzing data stored on disk. [end of text]
The decision tree classiﬁer is a widely used technique for classifying data. It uses a tree with leaf nodes representing classes and internal nodes with predicates. For instance, a decision tree for credit card applications might have a root node representing the current customers and an internal node for each customer's payment history. The process starts at the root and traverses the tree to reach a leaf node, evaluating the predicate on the data instance. [end of text]
The discovery of knowledge has numerous applications, including prediction and association analysis. These techniques are used in various industries to make informed decisions and improve customer experiences. Other applications include predicting credit risk, switchover behavior, and phone calling card usage. Descriptive patterns and clusters are also important in database systems. [end of text]
Classiﬁcation is a method for predicting the class of new items based on past instances and attributes. Decision tree classiﬁers are a popular technique for this purpose. [end of text]
The decision tree classiﬁer is a method for classifying data using a tree structure with leaf nodes representing classes and internal nodes containing functions. It starts at the root and traverses the tree to reach a leaf, evaluating a predicate for each instance. [end of text]
In the context of decision tree classification, the process begins with a root node and recursively builds the tree downward. Initially, there is only one node, the root, and all training instances are associated with that node. If all or "almost all" training instances are associated with the node, it becomes a leaf node. The class at the leaf is "good," so the credit risk is predicted to be good. Building a decision tree classifier involves selecting attributes and conditions for partitioning data, measuring purity quantitatively, and choosing the best splits. The information gain due to a split into fewer sets is then calculated, leading to a preference for simpler and more meaningful decision trees. [end of text]
The best split for an attribute is the one that maximizes information gain, defined as Information-gain(S, {S1, S2, . . . , Sr}) = −ri−1|Si||S| log2|Si||S|, where ri is the number of training instances in the subset of S with value i. The information gain ratio is the ratio of the information content of S to the information content of S1. Finding the best split involves sorting the attribute values and computing the information gain for each split. The best binary split for a continuous-valued attribute is the split that gives the maximum information gain. For categorical attributes, multiway splits are possible, with children for each distinct value. The recursion stops when the purity of a set is 0. For noisy data, the recursion stops when the purity of a set is "sufficiently high." Decision tree construction involves evaluating attributes and partitioning conditions, and finding the best split that maximizes information gain. The algorithm grows a decision tree recursively, stopping when the purity of a set is 0 or the set is too small for further partitioning to be statistically significant. Subtrees are pruned by replacing them with leaves if they have been highly tuned to the training data. Classiﬁcation rules can be generated from a decision tree if desired. [end of text]
Neural network classiﬁers and Bayesian classiﬁers are useful alternatives to decision tree classiﬁers. Neural networks are computationally powerful and can handle complex relationships between attributes. Bayesian classiﬁers estimate the probability of an instance belonging to a class based on the distribution of attribute values. They are useful for predicting values rather than classes. [end of text]
Bayesian classiﬁers use Bayesian theorem to estimate p(cj|d) for each class, where p(d|cj) is the probability of generating instance d given class cj. They assume attributes are independent and use a histogram to approximate the distribution of values for each attribute. Bayesian classiﬁers can handle unknown and null attribute values by ignoring p(d) and focusing on p(cj|d). [end of text]
Regression is used to predict a value, rather than a class. Given values for variables X1, X2, ..., Xn, we aim to predict the value of a variable Y. For instance, we can treat education as a number and income as another number, and use these variables to predict the likelihood of defaulting. Regression aims to find the best possible fit by minimizing the sum of squared errors. There are standard techniques in statistics for finding regression coefficients. [end of text]
Retail shops use association rules to suggest related books to customers. These rules are based on the frequency of purchases. Online shops may suggest bread and milk together to help shoppers find books faster. Grocery stores may place bread close to milk to tempt shoppers to buy other items. Discounts on one associated item may not apply to the other, as customers often buy both. [end of text]
Association rules are used to discover associations among items with high support, such as bread and milk. The number of sets grows exponentially, making the a priori technique infeasible for large numbers of items. Sequence associations (correlations) are another type of data-mining application, where deviations from temporal patterns are interesting. Mining techniques can find deviations based on past temporal/sequential patterns. [end of text]
Using plain association rules has shortcomings, particularly predicting interesting associations. Sequence associations are another important data-mining application. Mining techniques can find deviations from expected patterns. [end of text]
Clustering refers to the problem of grouping points into k sets based on distance metrics. Hierarchical clustering is another type of clustering in biology, where similar species are clustered together. Data visualization systems help users detect patterns visually. [end of text]
Clustering is the problem of grouping points into k sets so that the average distance of points from the centroid of their assigned cluster is minimized. Hierarchical clustering is another type of clustering in biology, where related species are clustered at different levels of the hierarchy. Clustering is useful in clustering documents, and hierarchical clustering algorithms can be classiﬁed as agglomerative clustering algorithms. [end of text]
Text mining uses data mining techniques to analyze textual documents, such as clustering pages based on common words and classifying pages into directories automatically. Data-visualization systems help users quickly identify patterns and make hypotheses about production problems. Visual displays can encode information compactly and provide system support for pattern detection. [end of text]
Large companies store and process large volumes of data from various locations, with complex internal structures and advanced query and information retrieval techniques. [end of text]
Data warehouses provide a solution to managing multiple sources of data, ensuring access to historical data and enabling decision-support queries. They use multidimensional data with attributes and measures, and are typically designed for data analysis using OLAP tools. Data warehouses are structured to be up-to-date with sources, allowing access to historical data and facilitating decision-support queries. [end of text]
The textbook summarizes the architecture of a data warehouse, the gathering, storage, querying, and data-analysis support of a typical data warehouse. It also discusses issues to be addressed in building a warehouse, including data sources, data schemas, data cleansing, data propagation, and summarization. The textbook provides detailed information on how to gather, store, query, and analyze data in a data warehouse. [end of text]
Data warehouses use multidimensional data, with attributes like sales and customer. Fact tables are large, recording sales information. OLAP tools are used for analysis. [end of text]
A facttable is a typical example of a facttable in a retail store, with one tuple for each item sold, and it minimizes storage requirements by using short foreign keys into dimension tables. [end of text]
The field of information retrieval has developed parallel with the field of databases. In traditional models, documents are organized as collections of documents, without schema. Information retrieval involves locating relevant documents based on user input, such as keywords or example documents. The Web provides a convenient way to get to and interact with information sources across the Internet. However, the explosion of stored information poses a problem for the Web, with little guidance to help users locate what is interesting. Information retrieval has played a crucial role in making the Web a productive and useful tool, especially for researchers. Traditional examples of information retrieval systems include online library catalogs and online document-management systems. The data in such systems are organized as a collection of documents, with a set of keywords associated with them. Keyword-based information retrieval can be used for both textual and other types of data, such as video and audio data. The field of information retrieval has dealt with issues such as managing unstructured documents, such as approximate searching by keywords, and ranking documents on estimated degree of relevance to the query. [end of text]
The textbook summarizes the concept of keyword search in information retrieval systems. It explains that query expressions are formed using key-words and logical connectives, and that full text retrieval assumes that keywords are connected. It then discusses relevance ranking, using terms to measure relevance, and how to refine the relevance metric by incorporating other information. The text also mentions the importance of stop words in text documents and how they are removed from indexing when a query contains multiple terms. [end of text]
The information retrieval system estimates relevance of documents to queries by combining the number of occurrences of keywords with the inverse document frequency, which measures the relevance of terms. This approach considers the length of the document and the frequency of terms, providing a more accurate measure of relevance. [end of text]
The set of documents that satisfy a query expression can be very large, with billions of documents on the Web. Full text retrieval makes this problem worse, as each document may contain many terms, and even terms that are only mentioned in passing are treated as equivalent. Relevance ranking is not an exact science, but there are well-accepted approaches. The relevance of a document to a query is estimated by the number of occurrences of the term in the document, with a metric that takes the length of the document into account. Terms are combined using the inverse document frequency, with weights assigned to terms using the inverse document frequency. The proximity of terms in the document can be taken into account when estimating relevance. [end of text]
Information retrieval systems return documents in descending order of relevance to a query. These systems typically return the first few documents with the highest estimated relevance, and allow users to interactively request further documents. Information retrieval systems use hyperlinks to rank documents, which are affected more by hyperlinks pointing to the document than by hyperlinks going out of the document. The popularity of a site is deﬁned by the number of sites containing at least one page with a link to the site, and can be combined with the popularity of the site containing the page to get an overall measure of the relevance of the page. Similarity-based retrieval allows users to retrieve documents that are similar to a given document. [end of text]
The Web search engines used relevance measures similar to those described in Section 22.5.1.1, but researchers soon realized that hyper-links could affect the relevance ranking of documents. The basic idea of site ranking is to find popular sites and rank pages from such sites higher. A site is identified by its internet address part, such as www.bell-labs.com. Sites typically contain multiple Web pages. To find popular sites, ranking pages from popular sites higher is generally a good idea. For instance, the term "google" may appear in vast numbers of pages, but the site google.com is the most popular among sites with pages containing the term "google". Documents from google.com containing the term "google" would therefore be ranked as the most relevant to the term "google". This raises the question of how to deﬁne the popularity of a site. One way would be to find how many times a site is accessed. However, getting such information is impossible without the cooperation of the site, and is infeasible for a Web search engine to implement. A very effective alternative uses hyperlinks; it deﬁnes popularity p(s) as the number of sites that contain at least one page with a link to site s. The popularity of a site s is then ranked as the number of sites containing at least one page with a link to site s. Google.com uses the referring-site popularity index page rank, which is a measure of popularity of a page. This approach of
Certain information-retrieval systems use similarity-based retrieval to find similar documents. Users can give a document A and ask the system to retrieve similar documents. The similarity of a document to another is measured using common terms. The system then presents a few similar documents to the user, allowing him to choose the most relevant ones. The resultant set of documents is likely to be what the user intended to find. [end of text]
The resultant set of documents is likely to be those that contain all of a speciﬁed set of keywords, such as motorcycle and repair, and the system will use synonyms to find the desired document. The index structure should maintain the number of times terms occur in each document and store the document frequency of each term. [end of text]
Keyword-based queries can be solved by replacing words with their synonyms or using disambiguating techniques. However, synonyms can have different meanings, leading to retrieval of unrelated documents. Disambiguation is challenging, and many systems do not implement it. It is advisable to verify synonyms with the user before using them. [end of text]
An effective index structure is crucial for efficient processing of queries in an information retrieval system. It maps each keyword to a set of document identifiers, allowing for efficient location and relevance ranking based on proximity. The index organization minimizes disk I/O operations by storing sets of document identifiers in consecutive pages. To support relevance ranking, the index also includes a list of document locations where keywords appear, making it possible to retrieve documents with the keyword. The intersection and operations find documents containing all keywords, while the or operation finds documents containing at least one keyword. The not operation eliminates documents containing a specific keyword, and the union operation combines sets of documents containing at least one keyword. The index structure should maintain the number of times terms occur in each document and use a compressed representation with few bits to approximate term frequency. [end of text]
In Web indexing, false positives are not desirable, as the actual document may not be quickly accessible for filtering. Precision and recall are also important measures for understanding how well aparticular document ranking strategy performs. [end of text]
False positives may occur because irrelevant documents get higher ranking than relevant ones. This depends on the number of documents examined. One measure is precision as a function of number of documents fetched, and another as a function of recall. These measures can be computed for individual queries and averaged across a query benchmark. However, measuring precision and recall requires understanding of natural language and intent, and may require under-standing of the intent of the query. Researchers have created collections of documents and queries, and have manually tagged documents as relevant or irrelevant to the queries. [end of text]
Web crawlers locate and gather information on the Web, recursively following hyperlinks. They store information in combined indexes, crawl documents, and update indexes periodically. Search engines cover only some portions of the Web, with crawlers taking weeks or months to cover all pages. Indexes are stored on multiple machines, with concurrency control on the index. Pages are updated periodically to keep information up to date. [end of text]
A typical library user uses a catalog to locate books, and she may browse nearby books. Libraries organize books using a classiﬁcation hierarchy, with related books kept close together. This allows users to browse related books physically. The classiﬁcation hierarchy is used in information retrieval systems to organize documents logically. [end of text]
A classiﬁcation DAG for a library information retrieval system. Libraries use a directed, acyclic graph to organize documents, with each node representing a topic and internal nodes containing links to related documents. Users can find information on topics by browsing down the directory, and learn new information by browsing through related classes. Organizing the Web into a directory structure is a daunting task. [end of text]
Decision-support systems analyze online data to help business decisions. OLAP tools help analysts view data in different ways, allowing insights into organization functioning. OLAP tools work on multidimensional data, characterized by dimension attributes and measure attributes. Precomputing the data cube speeds up queries on summaries. OLAP components of SQL:1999 provide new functions for data analysis, including new aggregate functions, cube and rollup operations, ranking functions, windowing functions, which support summa-rization on moving windows, and partitioning, with windowing and ranking applied inside each partition. Data mining is the process of semiautomatically analyzing large databases to find useful patterns. [end of text]
Classiﬁcation is a method used to predict the class of test instances by using attributes of the test instances, based on attributes of training instances, and the actual class of training instances. It can be used for credit-worthiness levels, performance prediction, and new applicant creditworthiness prediction. Decision-tree classifiers are a type of classiﬁer that constructs a tree based on training instances with leaves having class labels. Techniques for constructing decision trees include greedy heuristics. Bayesian classiﬁers are simpler to construct than decision-tree classiﬁers and work better in the case of missing/null attribute values. Association rules identify items that co-occur frequently, for instance, items that tend to be bought by the same customer. Correlations look for deviations from expected levels of association. Other types of data mining include clustering, text mining, and data visualization. Data warehouses help gather and archive important operational data. Warehouses are used for decision support and analysis on historical data, such as predicting trends. Data cleansing from input data sources is often a major task in data warehousing. Warehouses tend to be multidimensional, involving one or a few very large fact tables and several much smaller dimension tables. Information retrieval systems are used to store and query textual data such as documents. They use a simpler data model than database systems but provide more powerful querying capabilities within the restricted model. Queries attempt to locate documents that are of interest by specifying, for example, sets of keywords. The query
To compute the aggregate value on a multiset S1 ∪S2, given the aggregate values on multisets S1 and S2. Based on the above, express expressions to compute aggregate values with grouping on a subset S of the attributes of a relation r(A, B, C, D, E), given aggregate values for grouping on attributes T ⊇S, for the following aggregate functions:
a. sum, count, min and max
b. avg
c. standard deviation [end of text]
have only one group by clause. [end of text]
A single group by clause in a cube and rollup operation combines multiple data sources into a single summary, allowing for the analysis of multiple dimensions simultaneously. [end of text]
Students are categorized by total marks, with rankings applied. [end of text]
A histogram of d versus a, dividing a into 20 equal-sized partitions, where each partition contains 5% of the tuples in r, sorted by a. [end of text]
22.2.5: Windowing constructs are not applicable in SQL queries. [end of text]
To create a histogram of balance values, dividing the range 0 to the maximum account balance into three equal ranges. [end of text]
The cube operation on a relation gives the relation in Figure 22.2. Do not use the with cube construct. [end of text]
The textbook presents a decision tree for the given data set with the following splits and information gain values:
1. C = 2: Split on attribute C, information gain = 1.0
2. C = 5: Split on attribute C, information gain = 0.5
3. C = 6: Split on attribute C, information gain = 0.5
4. C = 3: Split on attribute C, information gain = 0.5
5. C = 7: Split on attribute C, information gain = 0.5
The final tree structure shows the best splits for each attribute along with their information gain values. [end of text]
The rules for credit ratings can be replaced by a single rule if the other rules are consistent with the data. [end of text]
The association rules deduced from the given information are:
1. Every transaction that purchases jeans also purchases T-shirts.
2. Every transaction that purchases T-shirts also purchases jeans.
Support: Both rules have high confidence because they account for a large portion of the transactions in the shop. Support is calculated as the ratio of the number of transactions that satisfy the rule to the total number of transactions. Confidence is calculated as the ratio of the number of transactions that satisfy the rule to the number of transactions that satisfy the rule and the rule. In this case, both rules have a high confidence because they account for a large portion of the transactions in the shop. [end of text]
To find the support of a collection of itemsets by scanning the data, assume itemsets and their counts fit in memory. If an itemset has support less than j, show that no superset of this itemset can have support greater than or equal to j. [end of text]
Data warehouses store and analyze large volumes of data, while destination-driven architectures route data to specific destinations. [end of text]
Marshall sales data by store and date, including hierarchies on store and date. [end of text]
Inverse document frequency measures the frequency of questions in a database. It helps in understanding the relevance of questions to a specific topic. Advanced query techniques and information retrieval methods are discussed in the book. [end of text]
It is acceptable to have either false positives or false drops in an information retrieval query, as long as the system can handle them. False positives occur when the system incorrectly identifies a relevant item as not being relevant, while false drops occur when the system incorrectly identifies a relevant item as not being relevant. To minimize these errors, the system should be trained on a large dataset of relevant items and false positives and false drops should be handled by a sophisticated error detection and correction mechanism. [end of text]
In this chapter, we study several new data types and discuss issues related to mobile computers. [end of text]
Temporal data is crucial for storing and retrieving historical information. Spatial data is essential for storing and querying large amounts of data efficiently. Multimedia data requires specific database features for continuous-media data. Mobile databases require techniques for memory management. [end of text]
A database models the state of some aspect of the real world outside itself, typically storing only one state. It updates when the state changes, but stores information about past states, such as audit trails. Temporal databases store information about past states, while valid time is the set of time intervals during which the fact is true in the real world. Time is measured by the system and observed in the real world. Temporal relations are one where each tuple has an associated time when it is true, with valid time or transaction time depending on the system. [end of text]
Temporal queries and selections, join operations, and functional dependencies are essential for understanding and manipulating temporal data in databases. The SQL standard provides the necessary types and operations to represent and manipulate temporal data. The temporal query language allows for the retrieval of a snapshot of a temporal relation at a specific time, while temporal selections involve time attributes and temporal projections. Join operations are used to combine tuples from different relations based on their time attributes. The concepts of functional dependencies and temporal functional dependencies are crucial for understanding and managing temporal data. [end of text]
SQL defines four-digit years, two-digit months, two-digit dates, and optional fractional digits for seconds. Timestamps include date and time with six fractional digits for seconds. Interval types allow periods of times without specifying a particular time. [end of text]
A snapshot relation is a set of tuples in a relation that reflect the state at a specific point in time, and a temporal selection involves the time attributes. A temporal join combines tuples with the time of a derived tuple, and a functional dependency is used to ensure that the result contains only one interval. [end of text]
Spatial data in databases is crucial for efficient storage, indexing, and querying based on spatial locations. Examples include CAD data used to design objects. Special-purpose index structures like R-trees are needed for complex queries. Temporal databases researchers should have called their type "span" to reflect the lack of specific start or end time. [end of text]
Geographic data such as road maps, land-usage maps, topographic elevation maps, political maps, and more, are stored in specialized databases. Support for geographic data is added to many database systems, including IBM DB2 Spatial, Informix Spatial, and Oracle Spatial. Geometric information can be represented in various ways, such as line segments, polygons, and polylines. Silberschatz-Korth-Sudarshan: Database System Concepts, Fourth Edition, VII. Other Topics.23. Advanced Data Types and New Applications861© The McGraw-Hill Companies, 2001868Chapter 23Advanced Data Types and New Applicationsstude. A polyline (also called a linestring) consists of a connected sequence of line segments, and can be represented by a list containing the coordinates of the endpointsof the segments, in sequence. We can approximately represent an arbitrary curve by polylines, by partitioning the curve into a sequence of segments. This representation is useful for two-dimensional features such as roads; here, the width of the road is small enough relative to the size of the full map that it can be considered two-dimensional. Some systems also support circular arcs as primitives, allowing curves to be represented as sequences of arcs. List-based representations of polylines or polygons are often convenient for query processing. Such non-ﬁrst-normal-form representations are used when supported by the underlying database. So that we can use ﬁ
Geometric constructs can be represented in various ways, including line segments, polylines, and polygons, in a normalized fashion. These representations can be used in two-dimensional features such as roads, and in three-dimensional space for representing points and line segments. [end of text]
Computer-aided-design (CAD) systems store data in memory during editing and write it back at the end of a session. These drawbacks include programming complexity and time cost. Large designs may be impossible to store in memory. Object-oriented databases were motivated by CAD requirements, with closed polygons and polylines being used. [end of text]
Object-oriented databases represent geometric objects as objects, and connections between objects indicate how the design is structured. Simple two-dimensional geometric objects include points, lines, triangles, rectangles, and polygons. Complex two-dimensional objects can be formed from simpler objects by union, intersection, and difference operations. Com-plex three-dimensional objects may be formed from simpler objects such as spheres, cylinders, and cuboids by union, intersection, and difference operations. Three-dimensional surfaces may also be represented by wireframe models. Design databases also store nonspatial information about objects, such as the material from which the objects are constructed. Spatial operations are performed on design databases, such as retrieving a specific region of interest. Spatial-integrity constraints are important in design databases to prevent interference errors. Spatial-integrity constraints help people to avoid design errors, thereby keeping the design consistent. Implementing such integrity checks again depends on the availability of efﬁcient multidimensional index structures. [end of text]
Geographic data are spatial in nature, but differ from design data in their complexity and the types of information they provide. Maps and satellite images are typical examples of geographic data. They may provide location information about boundaries, rivers, and roads, as well as detailed information about elevation, soil type, land usage, and annual rainfall. Geographic data can be categorized into two types: raster data and vector data. Raster data consists of bit maps or pixel maps in two or more dimensions, while vector data is constructed from basic geometric objects in two dimensions and three dimensions. Vector data are often represented in vector format, with polygons representing regions, and with a surface divided into polygons covering regions of equal height. Geographic data are suitable for applications where the data are intrinsically raster based, such as satellite images. However, they are unsuitable for applications where the data are intrinsically vector based, such as in three-dimensional space. [end of text]
Geographical features are represented as complex polygons, with rivers represented either as complex curves or polygons depending on their width. Geographic information related to regions, such as annual rainfall, can be represented as an array in raster form. For space efficiency, the array can be stored in a compressed form. In Section 23.3.5, we study a data structure called a quadtree for representing region information. The vector representation of region information uses polygons, where each polygon represents a region within which the array value is the same. The vector representation is more compact in some applications and more accurate for tasks like road depiction. However, it is unsuitable for raster-based data in satellite images. [end of text]
Geographic databases are used for online map services, vehicle-navigation systems, and distribution-network information for public-service utilities. They offer advanced data types and new applications. [end of text]
Spatial databases are useful for generating online road maps of desired regions, storing information about roads and services, and providing vehicle-navigation systems. Geographic databases for public-utility information are becoming increasingly important as the network of buried cables and pipes grows. Queries involving spatial locations can include nearness, region, and intersection queries. Extensions of SQL have been proposed to store and retrieve spatial information efficiently. [end of text]
Queries involving spatial locations can include nearness queries, region queries, and spatial joins. These queries can request intersections and unions of regions, and may also compute spatial joins on vector data. Extensions of SQL have been proposed to store and retrieve spatial information efficiently and allow queries to mix spatial and nonspatial conditions. [end of text]
Indexing spatial data requires two-dimensional structures such as k-d trees. These structures divide space into smaller parts for efficient access. [end of text]
To understand how to index spatial data consisting of two or more dimensions, we consider indexing points in one-dimensional data. Tree structures, such as binary trees and B-trees, divide space into smaller parts using successive divisions. Other topics include advanced data types and new applications. [end of text]
The k-d-B tree extends the k-d tree to allow multiple child nodes for each internal node, just as a B-tree extends a binary tree, to reduce the height of the tree. k-d-B trees are better suited for secondary storage than k-d trees. [end of text]
A quadtree is an alternative representation for two-dimensional data, dividing space into rectangular regions and associating each node with a rectangular region of space. It is used to store array (raster) information and can be used to store line segments and polygons. The division of space is based on regions, rather than on the actual set of points stored. Indexing of line segments and polygons presents new problems, including inefﬁciencies in storage and querying. [end of text]
An R-tree is a storage structure for indexing rectangles and other polygons, with leaf nodes storing the bounding boxes of polygons and internal nodes storing the bounding boxes of child nodes. The R-tree is balanced and allows for efficient searches, insertions, and deletions, with bounding boxes helping speed up checks for overlaps between rectangle and polygon. The main difference in splitting nodes is that in an R-tree, half of the entries are less than the midpoint and half are greater than the value, while in a B+-tree, it is possible to split entries into two sets with minimum total area. [end of text]
An R-tree is a balanced tree structure for indexing rectangles and other polygons, with rectangular bounding boxes associated with leaf nodes. It stores indexed polygons in leaf nodes, much like a B+-tree. The bounding box of a leaf node is the smallest rectangle parallel to the axes that contains all polygons. Internal nodes store the bounding boxes of child nodes, while leaf nodes store indexed polygons and may store bounding boxes. A bounding box of a polygon is the smallest rectangle parallel to the axes that contains the polygon. Each internal node stores the bounding boxes of the child nodes along with the pointers to the child nodes. Each leaf node stores the indexed polygons, and may optionally store the bounding boxes of the polygons; the bounding boxes help speed up checks for overlaps of the rectangle with the indexed polygons. The R-tree itself is at the right side of Figure 23.6. The bounding boxes are shown with extra space inside them to make them stand out. In reality, the boxes would be smaller and fit tightly on the objects that they contain. The R-tree insertion algorithm ensures that the tree remains balanced and ensures the bounding boxes of leaf nodes, as well as internal nodes, remain consistent. [end of text]
The quadratic split heuristic works by selecting pairs a and b with the largest wasted space in a node, then adding remaining entries to either set S1 or S2. The heuristic iteratively adds entries until all entries are assigned or until one set has enough entries to add all remaining entries. Deletion can be performed by borrowing entries from sibling nodes or merging them if a node becomes underfull, while alternative approaches include redistributing entries to sibling nodes to improve clustering. Spatial joins are simpler with quadtrees, but their storage efficiency is better than k-d trees or R-trees. [end of text]
Multimedia data, such as images, audio, and video, are increasingly stored outside databases in file systems, making database features less important. Issues like transactional updates, querying facilities, and indexing become crucial. Multimedia objects often have descriptive attributes, such as when they were created, who created them, and to what category they belong. To store multimedia data, databases must support large objects, split them into smaller pieces, and store them in the database. Similarity-based retrieval is needed for continuous-media data. For image data, JPEG is the most widely used format. [end of text]
Compressed video and audio data can be stored and transmitted using JPEG and MPEG standards. MPEG-1 encoding introduces some video quality loss, comparable to VHS tape. [end of text]
The MPEG-2 standard is a digital broadcast system and DVD standard, designed for video and audio compression. It introduces minimal video quality loss and uses MP3, RealAudio, and other formats for audio encoding. Continuous-media data includes video and audio data for real-time delivery. Video-on-demand systems use periodic cycles for data fetching and storage. Video-on-demand eventually becomes ubiquitous in offices, hotels, and video-production facilities. Similarity-based retrieval is used in multimedia applications to find data with high similarity. [end of text]
The most important types of continuous-media data are video and audio data. Continuous-media systems are characterized by real-time information-delivery requirements, including data delivery at a rate that does not cause overﬂow of system buffers and synchronization among distinct data streams. Video-on-demand servers supply data predictably at the right time to a large number of consumers, with periodic cycles for fetching data from disk. Video-on-demand service will eventually become ubiquitous, with applications in offices, hotels, and video-production facilities. [end of text]
In many multimedia applications, data are described only approximately in the data-base. Similarity testing is often subjective and user-specific, but it is more successful than speech or handwriting recognition due to limited set of choices available to the system. Several algorithms exist for finding the best matches to a given input by similarity testing. Some systems, including a dial-by-name, voice-activated telephone system, have been deployed commercially. See the bibliographical notes for references. [end of text]
The increasing use of personal computers and laptops has led to the emergence of distributed database applications, where central control and administration are not as necessary. This trend has combined with advancements in advanced data types and new applications to challenge the traditional centralized model. [end of text]
Mobile computing has proven useful in many applications, including delivery services, emergency-response services, and mobile computing applications. Location-dependent queries are a significant class of queries that are motivated by mobile computers, and mobile hosts provide location information either by the user or increasingly by a global positioning system. Mobile computing creates a situation where machines no longer have fixed locations and network addresses, and location-dependent queries are an interesting class of queries. The value of the location parameter is provided either by the user or, increasingly, by a global positioning system. Mobile computing also creates a situation where machines no longer have fixed locations and network addresses, and location-dependent queries are an interesting class of queries. The value of the location parameter is provided either by the user or, increasingly, by a global positioning system. Mobile computing creates a situation where machines no longer have fixed locations and network addresses, and location-dependent queries are an interesting class of queries. The value of the location parameter is provided either by the user or, increasingly, by a global positioning system. Mobile computing creates a situation where machines no longer have fixed locations and network addresses, and location-dependent queries are an interesting class of queries. The value of the location parameter is provided either by the user or, increasingly, by a global positioning system. Mobile computing creates a situation where machines no longer have fixed locations and network addresses, and location-dependent queries are an interesting class of queries. The value of the location parameter is provided either by the user or, increasingly, by a global positioning
The mobile-computing environment consists of mobile computers, referred to as mo-bile hosts, and a wired network of computers. Mobile hosts communicate with thewired network via computers referred to as mobile support stations. Each mobile support station manages those mobile hosts within its cell. Mobile hosts may move between cells, thus necessitating a handoff of control from one mobile support station to another. Since mobile hostsmay, at times, be powered down, a host may leave one cell and rematerialize later atsome distant cell. Therefore, moves between cells are not necessarily between adja-cent cells. Within a small area, such as a building, mobile hosts may be connected by a wireless local-area network (LAN) that provides lower-cost connectivity than a wide-area cellular network, and that reduces the overhead of handoffs. Silberschatz-Korth-Sudarshan: Database System Concepts, Fourth Edition VII. Other Topics 23. Advanced Data Types and New Applications 875 © The McGraw-Hill Companies, 2001882 Chapter 23 Advanced Data Types and New Applications It is possible for mobile hosts to communicate directly without the intervention of a mobile support station. However, such communication can occur between only nearby hosts. Such direct forms of communication are becoming more prevalent with the advent of the Bluetooth standard. Bluetooth uses short-range digital radio to allow wireless connectivity within a 10-meter range at high speed (up to 7
Mobility of hosts affects network routing and database query processing.
The textbook discusses the concept of time-of-day-based charges in cellular and digital systems, the importance of battery power optimization, and the use of broadcast data for efficient data transmission. It also covers the challenges of disconnectivity and consistency in mobile computing, and the use of version-vector and version-numbering schemes for detecting and propagating updates. [end of text]
Broadcast data is used to transmit frequently requested data to mobile hosts, reducing energy consumption and increasing transmission bandwidth. Mobile hosts can cache broadcast data locally, optimizing energy use. Broadcast schedules determine when data is transmitted, with fixed or changeable schedules. Requests are serviced when data is broadcast. [end of text]
In mobile computing, disconnection of mobile hosts can lead to inconsistencies in data, necessitating a solution to propagate updates even when the host is disconnected. This is achieved through version-numbering-based schemes that allow updates of shared files. These schemes do not guarantee consistency but detect conflicts eventually when hosts exchange information. [end of text]
The version-vector scheme is designed to handle failures in distributed file systems and groupware systems, but it fails to address the most important issue of reconciling inconsistent copies of shared data, which is a fundamental problem in distributed systems. Automatic reconciliation can be performed by executing updates during disconnection, but it requires users to resolve inconsistencies manually. [end of text]
Time plays a crucial role in database systems. Databases are models of the real world, whereas most models the state of the real world at a specific time. Facts in temporal relations have associated times when they are valid, which can be represented as a union of intervals. Temporal query languages simplify modeling of time, as well as time-related queries. Spatial databases are finding increasing use today to store computer-aided-design data as well as geographic data. Design data are stored primarily as vector data; geographic data consist of a combination of vector and raster data. Spatial-integrity constraints are important for design data. Vector data can be encoded as first-normal-form data, or can be stored using non-first-normal-form structures. Special-purpose index structures are particularly important for accessing spatial data and for processing spatial queries. Multimedia databases are growing in importance. Issues such as similarity-based retrieval and delivery of data at guaranteed rates are topics of current research. Mobile computing systems have become common, leading to interest in data-base systems that can run on such systems. Query processing in such systems may involve lookups on server databases. The query cost model must include the cost of communication, including monetary cost and battery-power cost, which is relatively high for mobile systems. Broadcast is much cheaper per recipient than point-to-point communication, and broadcast of data such as stock-market data helps mobile systems pick up data inexpensively. Disconnected operation, use of broadcast data, and caching of data are three important issues being addressed in mobile computing
The textbook states that a tuple can contain both types of time, but it does not explicitly state whether it is necessary or optional. [end of text]
Adding a time attribute to a poral relation in an atemporal database involves creating a new attribute that represents the time at which the data is available. This attribute is then used to filter and retrieve data based on the time of availability. The problem is handled by ensuring that the time attribute is consistent and accurate throughout the database. [end of text]
R-trees are preferable for this query because they are efficient for spatial queries, and B-trees are not suitable for this type of query. R-trees store points and their nearest neighbors, while B-trees store points and their distances to other points. In this case, the query is about a point, so a R-tree would be more efficient. [end of text]
It is possible to convert vector data to raster data. However, storing raster data obtained by such conversion instead of the original vector data can have drawbacks, such as loss of information and potential inaccuracies in the data. [end of text]
The algorithm uses multiple region queries to find the nearest neighbor in a multi-neighbor query setting. [end of text]
The bounding box for bounding box queries can be large, containing a large empty area. Techniques to improve performance include dividing segments into smaller pieces. [end of text]
R-tree indices are used to efficiently search for leaf entries under a pair of internal nodes in a tree structure. These indices allow for quick queries to find entries that may intersect with a given pair of nodes. [end of text]
A schema to represent the geographic location of restaurants includes a cuisine column and a level of expensiveness column. A query to find moderately priced Indian restaurants within 5 miles of the user's house would involve a WHERE clause to filter restaurants by cuisine and level of expensiveness. A query to find the distance from each restaurant serving the same cuisine and with the same level of expensiveness would involve a JOIN clause to match the restaurant with the same cuisine and level of expensiveness. [end of text]
either too slowly or too fast? [end of text]
in a broadcast-data environment, where there may occasionally be noise that prevents reception of part of the data being transmitted. [end of text]
Distinct from traditional distributed systems, this approach uses a single server to manage multiple replicas, improving scalability and fault tolerance. [end of text]
Computational complexity that is not typically optimized by traditional query engines. [end of text]
Access time for a virtual disk is longer than for a hard disk, while data-transfer rate is lower.
The version-vector scheme ensures that mobile computers update their copies of documents in the correct order, preventing data conflicts and maintaining the integrity of the database. [end of text]
The textbook discusses the incorporation of time into relational data models, including time management and temporal data management, and provides a glossary of temporal-database concepts. It also covers spatial indexing, spatial operations, and the integration of spatial and non-spatial data. The chapter goes beyond the basic schemes discussed previously and covers advanced transaction-processing concepts, including transaction-processing monitors, transactional workflows, main-memory databases, real-time databases, long-duration transactions, nested transactions, and multidatabase transactions. [end of text]
Transaction-processing monitors (TP monitors) were developed in the 1970s and 1980s, initially in response to a need to support remote terminals. They evolved to provide the core support for distributed transaction processing. Today's TP monitors include CICS, Tuxedo, Top End, Encina, and Transaction Server. The single-server model is used in client-server environments, while many-server, many-router models are used in distributed environments. The server process handles tasks such as user authentication. [end of text]
The textbook discusses the architecture of large-scale transaction processing systems, focusing on a client-server model with a single-server process for each client. The system uses a process-per-client model, where remote clients connect to a single-server process, and remote clients send requests to the server process. The server process handles tasks such as user authentication. The single-server model is used in client-server environments where clients send requests to a single-server process. The server process is multithreaded to handle tasks that would be handled by the operating system. [end of text]
In modern TP monitors, applications can interact with multiple databases and legacy systems, and with users or other applications at remote sites. They also need to communicate with communication subsystems. Coordinating data accesses and implementing ACID properties across such systems is crucial for large applications. TP monitors provide support for the construction and administration of such large applications, built from multiple subsystems such as databases, legacy systems, and communication systems. [end of text]
Modern TP monitors support the construction and management of large applications, including databases, legacy systems, and communication subsystems. They provide a resource manager for each subsystem, defining transaction primitives such as begin, commit, abort, and Silberschatz-Korth-Sudarshan. [end of text]
In distributed systems, TP monitors coordinate transactions, ensure data consistency, and manage complex client-server systems. They can be used to hide database failures in replicated systems and provide a transactional RPC interface to services. [end of text]
A workflow is an activity in which multiple tasks are executed in a coordinated way by different processing entities. Tasks define work to be done and can be speciﬁed in various ways, including textual descriptions in a file or electronic-mail message, a form, a message, or a computer program. Processing entities can be humans or software systems. Workﬂows involve one or more humans and may involve tasks performed by humans or software systems. [end of text]
The workﬂow specification involves detailing tasks and their execution requirements, while workﬂow execution involves coordination and safeguards for traditional database systems. Transactional workﬂows use transactions to extend database concepts to workﬂows, and workﬂow activities may require interactions among multiple systems. Workﬂow systems have been developed in recent years, and properties of workﬂow systems have been studied at a relatively abstract level without going into details. [end of text]
The coordination of tasks can be statically or dynamically specified, with a static speciﬁcation defining tasks and dependencies before execution, and a generalization of this strategy involving preconditions for tasks and dependencies. [end of text]
The workﬂow designer specifies failure-atomicity requirements for a workﬂow, ensuring that every execution terminates in an acceptable state that meets the designer's requirements. [end of text]
In general, a task can commit and release its resources before the workflow reaches an acceptable termination state. However, if a multitask transaction later aborts, its failure atomicity may require undoing compensating tasks by executing compensating tasks. The semantics of compensation requires that a compensating transaction eventually complete its execution successfully, possibly after a number of resubmissions. The workﬂow management system consists of a scheduler, task agents, and a mechanism to query the state of the workflow system. A task agent controls the execution of a task by a processing entity. A scheduler is a program that processes workflows by submitting various tasks for execution, monitoring events, and evaluating conditions related to task dependencies. A scheduler may submit a task for execution (to a task agent), or may request that a previously submitted task be aborted. In the case of multi-database transactions, the tasks are subtransactions, and the processing entities are local database management systems. In accordance with the workﬂow specifications, the scheduler enforces the scheduling dependencies and is responsible for ensuring that tasks reach acceptable termination states. [end of text]
The execution of tasks may be controlled by a human coordinator or a workflow-management system. A workflow-management system consists of a scheduler, task agents, and a mechanism to query the state of the workflow. A task agent controls the execution of a task by a processing entity. A scheduler is a program that processes workflows by submitting various tasks for execution, monitoring events, and evaluating conditions related to inter-task dependencies. A scheduler may submit a task for execution (to a task agent), or may request that a previously submitted task be aborted. Multi-database transactions involve subtransactions and processing entities. Workﬂow specifications define the scheduling dependencies and ensure tasks reach acceptable termination states. Messaging is used in messaging-based systems, with per-sistent messaging mechanisms providing guaranteed delivery. Centralized workﬂow systems use messaging to notify agents, and track task completions. Centralized workﬂow systems are useful in networks with disconnected data. Centralized workﬂow systems may use messaging to ensure safe termination states. Safety checks may be impossible or impractical to implement in schedulers. [end of text]
The objective of workflow recovery is to ensure that failures in work-flow processing components, such as schedulers, do not affect the termination state of work-flows. The recovery procedures must restore the state of the scheduler at the time of failure, including the execution states of tasks. Persistent messaging ensures that tasks are executed only once and avoid lost executions. [end of text]
Workflows are complex and often manual, requiring integration with other systems. Workflows are simplified through commercial systems like FlowMark from IBM, but can also be developed by organizations. Workflows that cross organizational boundaries are becoming common. Standards for interoperation are being developed, using XML. [end of text]
Workflows are often hand-coded in enterprise resource planning systems. Workﬂow management systems aim to simplify the construction and execution of workflows, using high-level specifications. Commercial systems like FlowMark from IBM are general-purpose, while specialized systems like order processing and bug/failure reporting are specific to their domains. Today's interconnected organizations require interworking of workflows across organizational boundaries. The Workflow Management Coalition has standardized communication between workflows using XML. [end of text]
High-performance hardware and parallel processing techniques are essential for achieving low response times. However, disk I/O remains a bottleneck, with long latency of about 10 milliseconds. Increasing the size of the database buffer can reduce disk latency, but many applications require multiple gigabytes of data to fit into main memory. Large main memories allow faster processing of transactions, but data transfer rates are still limited by disk-related limitations. In addition, data structures in main-memory databases can reduce space requirements, but data structures in disk databases have pointers crossing multiple pages, making I/O costs high. Recovery algorithms can be optimized to minimize space overhead while a query is being evaluated. [end of text]
The process of committing a transaction requires writing records to stable storage, with log records not output until several transactions have completed or a certain period has passed since a transaction completes. Group commit ensures nearly full blocks are output, reducing the overhead of logging. It results in fewer output operations per committed transaction, with delays of 10 milliseconds in some cases. Nonvolatile RAM buffers can be used for write operations to eliminate delays. [end of text]
Real-time systems require balancing deadlines with hardware resources. Deadlines are critical in real-time systems, but real-time constraints are more complex than absolute speed. Researchers have focused on concurrency control for real-time databases, including optimistic and optimistic-optimistic protocols. Achieving deadlines while balancing hardware resources remains a significant challenge. [end of text]
The transaction concept was initially developed in data-processing applications, but serious problems arise when applied to databases involving human interaction, where transactions can be of long duration and expose uncommitted data. [end of text]
In interactive transactions, the user may wish to abort a subtask without causing the entire transaction to abort. Recoverability is important to ensure fast response times, while performance is defined as fast response time. Long-duration interactive transactions require fast response times and predictable waiting times to avoid human loss of work. Timestamp-based protocols ensure fast response times, but require transactions to abort under certain circumstances. Validation protocols enforce serializability, but may increase the length of waiting times. The enforcement of serializability leads to long-duration waits, aborts, or both, depending on the properties of operations performed by each transaction. [end of text]
The properties that we discussed make it impractical to enforce the requirement used in earlier chapters that only serializable schedules be permitted. Each of the concurrency-control protocols of Chapter 16 has adverse effects on long-duration transactions. Timestamp-based protocols never require a transaction to wait, but they require transactions to abort under certain circumstances. Validation protocols enforce serializability by means of transaction abort, leading to long-duration waits, abort of long-duration transactions, or both. Recovery issues arise when we consider cascading rollback, which can lead to long-duration waits or cascading rollback. The enforcement of transaction atomicity must either lead to an increased probability of long-duration waits or create a possibility of cascading rollback. [end of text]
The fundamental goal of database concurrency control is to ensure that concurrent execution does not result in database inconsistencies. Serializability can be used to achieve this, but not all schedules that preserve database consistency are serializable. For example, a bank database with two accounts A and B requires the sum of A + B to be preserved, but the schedule of Figure 24.5 is not serializable. Correctness depends on the consistency constraints of the database and properties of operations performed by each transaction. Techniques include using database consistency constraints and simpler techniques like Silberschatz-Korth-Sudarshan. [end of text]
Nested transactions represent a subdivision of transactions into subtasks, subsubtasks, and so on. Multilevel transactions can create higher-level operations that enhance concurrency. Compensating transactions help reduce long-duration waiting by exposing uncommitted data to concurrent transactions. [end of text]
Multilevel transactions can enhance concurrency by allowing subtransactions to release locks on completion, creating higher-level operations that can enhance concurrency. [end of text]
To reduce long-duration waiting, uncommitted up-dates are exposed to concurrent transactions, and compensating transactions help deal with cascading rollbacks. [end of text]
Compensation for the failure of a transaction requires that the semantics of the failed transaction be used. For certain operations, such as incrementation or insertion into a B+-tree, the corresponding compensation is easily deﬁned. For more complex transactions, the application programmers may have to deﬁne the correct form of compensation at the time the transaction is coded. For complex interactive transactions, it may be necessary for the system to interact with the user to determine the proper form of compensation. [end of text]
Long-duration transactions must survive system crashes, and logging of updates is made more complex when certain types of data items exist. Using physical redo logging and logical undo logging, as described in Section 17.9, provides the concurrency benefits of logical logging while avoiding the above pitfalls. [end of text]
Multidatabase systems create the illusion of logical database integration, allowing local systems to use different models, languages, and control mechanisms. They support local and global transactions, with local transactions being executed outside the system's control and global transactions under the system's control. [end of text]
The multidatabase system ensures local serializability among its local transactions, including those that are part of a global transaction. The multidatabase system ensures global serializability by ensuring serializability among the global transactions alone, ignoring the orderings induced by local transactions. Two-level serializability (2LSR) ensures serializability at two levels of the system: local serializability among local transactions, and global serializability among the global transactions. Two-level serializability is simple to enforce, and it can be achieved with local guarantees. However, local systems not designed to be part of a distributed system may not be able to participate in a two-phase commit protocol. In such cases, Silberschatz-Korth-Sudarshan: Database System Concepts, Fourth Edition VII. Other Topics24. Advanced Transaction Processing905© The McGraw-Hill Companies, 2001912Chapter 24Advanced Transaction Processing24.6.1Two-Level Serializability Two-level serializability (2LSR) ensures serializability at two levels of the system:• Each local database system ensures local serializability among its local trans-actions, including those that are part of a global transaction.• The multidatabase system ensures serializability among the global transactions alone—ignoring the orderings induced by local transactions. Each of these serializability levels is simple to enforce. Local systems already offer guarantees of serializability;
Two-level serializability ensures local and global serializability, while the multidatabase system ensures strong correctness. Both are simple to enforce and can be used with standard concurrency-control techniques. [end of text]
The notion of a value dependency, a transaction having a value dependency ensures strong correctness for local-read, global-read, and global-write protocols. However, it imposes both the value dependency condition from the local-read protocol and the consistency constraint between local and global data. Ensuring global serializability involves creating tickets in local databases and controlling the order of global transactions. The problem with schemes that ensure global serializability is that they may restrict concurrency unduly. [end of text]
Schemes to ensure global serializability in an environment where no direct local conﬂicts are generated in each site. [end of text]
Workflows are activities involving multiple processing entities and networks. They provide a convenient way to carry out tasks that involve multiple systems, such as databases. Transaction-processing monitors are used to manage large main memories and ensure high system throughput. Long-duration transactions are complex to manage, requiring alternative techniques to ensure correctness. [end of text]
Advanced Transaction Processing, Nonserializable executions, Nested transactions, Multilevel transactions, Saga, Compensating transactions, Logical logging, Multidatabase systems, Autonomy, Local transactions, Global transactions, Two-level serializability (2LSR), Strong correctness, Local data, Global data, Protocols, Global-read, Local-read, Value dependency, Ensuring global serializability, TicketExercises. [end of text]
effectively than a typical operating system. [end of text]
Simplified version: Servlets are like special servers that have been called TP-lite. [end of text]
The work flow at your organization typically starts with an application, which leads to the hiring process. There are acceptable termination states such as termination due to poor performance or termination due to death. Possible errors include deadlines being missed or human intervention required. The work flow has been automated at your university. [end of text]
Relational databases are not suitable for managing erasure operations because they do not support undo logging or physical undo logging. Physical undo logging is necessary for erasure-correcting codes, which are used in databases to protect data integrity. Physical undo logging is not available in relational databases. [end of text]
To manage data, databases use a combination of technologies and strategies to store, organize, and access information. They store data in a structured format, allowing for efficient retrieval and manipulation. Databases also use indexing, partitioning, and indexing to optimize performance. Additionally, databases use encryption and access controls to protect data. Overall, databases provide a powerful tool for managing and analyzing large amounts of data. [end of text]
Loading data as it is requested by transactions is generally preferred as it minimizes the need for disk I/O and reduces the number of times data needs to be transferred between main memory and disk.
The textbook summary is quite long and complex, so I've condensed it into a single sentence. [end of text]
The textbook section "or why not?" is about the topic of "or why not?" in the context of a database. It explores the concept of "or why not?" in the realm of database design and programming. The textbook defines "or why not?" as a way to express a condition that is not met but can be fulfilled by a different condition. It then discusses the concept of "or why not?" in the context of database design and programming, providing examples and explaining how it can be used to improve database performance. The textbook also explains the concept of "or why not?" in the context of database design and programming, providing examples and explaining how it can be used to improve database performance. The textbook also explains the concept of "or why not?" in the context of database design and programming, providing examples and explaining how it can be used to improve database performance. The textbook also explains the concept of "or why not?" in the context of database design and programming, providing examples and explaining how it can be used to improve database performance. The textbook also explains the concept of "or why not?" in the context of database design and programming, providing examples and explaining how it can be used to improve database performance. The textbook also explains the concept of "or why not?" in the context of database design and programming, providing examples and explaining how it can be used to improve database performance. The textbook also explains the concept of "or why not?" in the context of database design and
The number of disk accesses required to read a data item can vary depending on factors such as the data's size, file structure, and access patterns. This can present a problem to designers of real-time database systems because it can lead to inefficient use of storage and slow performance. Real-time database systems need to be able to handle high volumes of data and transactions quickly, and the number of disk accesses required to read a data item can be a significant bottleneck. [end of text]
A transaction is an action or event that involves the exchange of money or goods between two parties, such as a purchase or sale. Transactions can be recorded in a database and tracked over time. [end of text]
In a distributed system, threads coordinate to deliver messages in a queue, with locks on the queue not required until a message is delivered. [end of text]
Nested transactions allow transactions to be nested within each other, while multilevel transactions allow transactions to be nested within multiple levels of the database. Differences may arise if we allow both types of transactions. [end of text]
Their use is the process of applying knowledge to solve problems or achieve a specific goal. [end of text]
A global transaction is active at any time, and every local site ensures local serializability. Multidatabase systems can ensure at most one active global transaction at any time by using a distributed approach. A nonserializable global schedule can result despite the assumptions if local serializability is maintained. [end of text]
Nonserializable executions may result in a system that is read-only. Nonserializable transactions are read-only because they are not serializable. Ticket schemes ensure global serializability by allowing transactions to be executed in order. A ticket scheme ensures global serializability by allowing transactions to be executed in order. [end of text]
