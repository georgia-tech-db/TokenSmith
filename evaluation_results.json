[
  {
    "config_name": "chars_weighted",
    "config": {
      "name": "chars_weighted",
      "index_prefix": "textbook_index",
      "chunking_strategy": "chars",
      "overlap": 0,
      "fusion": "weighted",
      "bm25_weight": 0.3,
      "tag_weight": 0.2,
      "top_k": 5,
      "embed_model": "sentence-transformers/all-MiniLM-L6-v2"
    },
    "question": "List four significant differences between a file-processing system and a DBMS.",
    "gold_text": "1.2 Database Systems versus File Systems Consider part of a savings-bank enterprise that keeps information about all cus- tomers and savings accounts. One way to keep the information on a computer is to store it in operating system files. To allow users to manipulate the information, the system has a number of application programs that manipulate the files, including • A program to debit or credit an account • A program to add a new account • A program to find the balance of an account • A program to generate monthly statements System programmers wrote these application programs to meet the needs of the bank. New application programs are added to the system as the need arises. For exam- ple, suppose that the savings bank decides to offer checking accounts. As a result, the bank creates new permanent files that contain information about all the checking accounts maintained in the bank, and it may have to write new application programs to deal with situations that do not arise in savings accounts, such as overdrafts. Thus, as time goes by, the system acquires more files and more application programs. This typical file-processing system is supported by a conventional operating sys- tem. The system stores permanent records in various files, and it needs different application programs to extract records from, and add records to, the appropriate files. Before database management systems (DBMSs) came along, organizations usu- ally stored information in such systems. Keeping organizational information in a file-processing system has a number of major disadvantages: • Data redundancy and inconsistency. Since different programmers create the files and application programs over a long period, the various files are likely to have different formats and the programs may be written in several pro- gramming languages. Moreover, the same information may be duplicated in several places (files). For example, the address and telephone number of a par- ticular customer may appear in a file that consists of savings-account records and in a file that consists of checking-account records. This redundancy leads to higher storage and access cost. In addition, it may lead to data inconsis- tency; that is, the various copies of the same data may no longer agree. For example, a changed customer address may be reflected in savings-account records but not elsewhere in the system. • Difficulty in accessing data. Suppose that one of the bank officers needs to find out the names of all customers who live within a particular postal-code area. The officer asks the data-processing department to generate such a list. Because the designers of the original system did not anticipate this request, there is no application program on hand to meet it. There is, however, an ap- plication program to generate the list of all customers. The bank officer hasnow two choices: either obtain the list of all customers and extract the needed information manually or ask a system programmer to write the necessary application program. Both alternatives are obviously unsatisfactory. Suppose that such a program is written, and that, several days later, the same officer needs to trim that list to include only those customers who have an account balance of $10,000 or more. As expected, a program to generate such a list does not exist. Again, the officer has the preceding two options, neither of which is satisfactory. The point here is that conventional file-processing environments do not al- low needed data to be retrieved in a convenient and efficient manner. More responsive data-retrieval systems are required for general use. • Data isolation. Because data are scattered in various files, and files may be in different formats, writing new application programs to retrieve the appropri- ate data is difficult. • Integrity problems. The data values stored in the database must satisfy cer- tain types of consistency constraints. For example, the balance of a bank ac- count may never fall below a prescribed amount (say, $25). Developers enforce these constraints in the system by adding appropriate code in the various ap- plication programs. However, when new constraints are added, it is difficult to change the programs to enforce them. The problem is compounded when constraints involve several data items from different files. • Atomicity problems. A computer system, like any other mechanical or elec- trical device, is subject to failure. In many applications, it is crucial that, if a failure occurs, the data be restored to the consistent state that existed prior to the failure. Consider a program to transfer $50 from account A to account B. If a system failure occurs during the execution of the program, it is possible that the $50 was removed from account A but was not credited to account B, resulting in an inconsistent database state. Clearly, it is essential to database consistency that either both the credit and debit occur, or that neither occur. That is, the funds transfer must be atomic—it must happen in its entirety or not at all. It is difficult to ensure atomicity in a conventional file-processing system. • Concurrent-access anomalies. For the sake of overall performance of the sys- tem and faster response, many systems allow multiple users to update the data simultaneously. In such an environment, interaction of concurrent up- dates may result in inconsistent data. Consider bank account A, containing $500. If two customers withdraw funds (say $50 and $100 respectively) from account A at about the same time, the result of the concurrent executions may leave the account in an incorrect (or inconsistent) state. Suppose that the pro- grams executing on behalf of each withdrawal read the old balance, reduce that value by the amount being withdrawn, and write the result back. If the two programs run concurrently, they may both read the value $500, and write back $450 and $400, respectively. Depending on which one writes the valuelast, the account may contain either $450 or $400, rather than the correct value of $350. To guard against this possibility, the system must maintain some form of supervision. But supervision is difficult to provide because data may be accessed by many different application programs that have not been coordi- nated previously. • Security problems. Not every user of the database system should be able to access all the data. For example, in a banking system, payroll personnel need to see only that part of the database that has information about the various bank employees. They do not need access to information about customer ac- counts. But, since application programs are added to the system in an ad hoc manner, enforcing such security constraints is difficult. These difficulties, among others, prompted the development of database systems. In what follows, we shall see the concepts and algorithms that enable database sys- tems to solve the problems with file-processing systems. In most of this book, we use a bank enterprise as a running example of a typical data-processing application found in a corporation.",
    "retrieved_text": "For Evaluation Only.\ndddddd\nFor Evaluation Only.\nCopyright (c) by Foxit Software Company, 2004\nEdited by Foxit PDF Editor\nFor Evaluation Only.\nCopyright (c) by Foxit Software Company, 2004\nEdited by Foxit PDF Editor\nComputer \nScience\nVolume 1\nSilberschatz−Korth−Sudarshan  •  Database System Concepts, Fourth Edition  \nFront Matter \n1\nPreface \n1\n1. Introduction \n11\nText \n11\nI. Data Models \n35\nIntroduction \n35\n2. Entity−Relationship Model \n36\n3. Relational Model \n87\nII. Relational Databases \n140\nIntroduction \n140\n4. SQL \n141\n5. Other Relational Languages \n194\n6. Integrity and Security \n229\n7. Relational−Database Design \n260\nIII. Object−Based Databases and XML \n307\nIntroduction \n307\n8. Object−Oriented Databases \n308\n9. Object−Relational Databases \n337\n10. XML \n363\nIV. Data Storage and Querying \n393\nIntroduction \n393\n11. Storage and File Structure \n394\n12. Indexing and Hashing \n446\n13. Query Processing \n494\n14. Query Optimization \n529\nV. Transaction Management \n563\nIntroduction \n563\n15. Transactions \n564\n16. Concurrency Control \n590\n17. Recovery System \n637\niii\nVI. Database System Architecture \n679\nIntroduction \n679\n18. Database System Architecture \n680\n19. Distributed Databases \n705\n20. Parallel Databases \n750\nVII. Other Topics \n773\nIntroduction \n773\n21. Application Development and Administration \n774\n22. Advanced Querying and Information Retrieval \n810\n23. Advanced Data Types and New Applications \n856\n24. Advanced Transaction Processing \n884\niv\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nFront Matter\nPreface\n1\n© The McGraw−Hill \nCompanies, 2001\nPreface\nDatabase management has evolved from a specialized computer application to a\ncentral component of a modern computing environment, and, as a result, knowl-\nedge about database systems has become an essential part of an education in com-\nputer science. In this text, we present the fundamental concepts of database manage-\nment. These concepts include aspects of database design, database languages, and\ndatabase-system implementation.\nThis text is intended for a ﬁrst course in databases at the junior or senior under-\ngraduate, or ﬁrst-year graduate, level. In addition to basic material for a ﬁrst course,\nthe text contains advanced material that can be used for course supplements, or as\nintroductory material for an advanced course.\nWe assume only a familiarity with basic data structures, computer organization,\nand a high-level programming language such as Java, C, or Pascal. We present con-\ncepts as intuitive descriptions, many of which are based on our running example of\na bank enterprise. Important theoretical results are covered, but formal proofs are\nomitted. The bibliographical notes contain pointers to research papers in which re-\nsults were ﬁrst presented and proved, as well as references to material for further\nreading. In place of proofs, ﬁgures and examples are used to suggest why a result is\ntrue.\nThe fundamental concepts and algorithms covered in the book are often based\non those used in existing commercial or experimental database systems. Our aim is\nto present these concepts and algorithms in a general setting that is not tied to one\nparticular database system. Details of particular commercial database systems are\ndiscussed in Part 8, “Case Studies.”\nIn this fourth edition of Database System Concepts, we have retained the overall style\nof the ﬁrst three editions, while addressing the evolution of database management.\nSeveral new chapters have been added to cover new technologies. Every chapter has\nbeen edited, and most have been modiﬁed extensively. We shall describe the changes\nin detail shortly.\nxv\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nFront Matter\nPreface\n2\n© The McGraw−Hill \nCompanies, 2001\nxvi\nPreface\nOrganization\nThe text is organized in eight major parts, plus three appendices:\n• Overview (Chapter 1). Chapter 1 provides a general overview of the nature\nand purpose of database systems. We explain how the concept of a database\nsystem has developed, what the common features of database systems are,\nwhat a database system does for the user, and how a database system inter-\nfaces with operating systems. We also introduce an example database applica-\ntion: a banking enterprise consisting of multiple bank branches. This example\nis used as a running example throughout the book. This chapter is motiva-\ntional, historical, and explanatory in nature.\n• Data models (Chapters 2 and 3). Chapter 2 presents the entity-relationship\nmodel. This model provides a high-level view of the issues in database design,\nand of the problems that we encounter in capturing the semantics of realistic\napplications within the constraints of a data model. Chapter 3 focuses on the\nrelational data model, covering the relevant relational algebra and relational\ncalculus.\n• Relational databases (Chapters 4 through 7). Chapter 4 focuses on the most\ninﬂuential of the user-oriented relational languages: SQL. Chapter 5 covers\ntwo other relational languages, QBE and Datalog. These two chapters describe\ndata manipulation: queries, updates, insertions, and deletions. Algorithms\nand design issues are deferred to later chapters. Thus, these chapters are suit-\nable for introductory courses or those individuals who want to learn the basics\nof database systems, without getting into the details of the internal algorithms\nand structure.\nChapter 6 presents constraints from the standpoint of database integrity\nand security; Chapter 7 shows how constraints can be used in the design of\na relational database. Referential integrity; mechanisms for integrity mainte-\nnance, such as triggers and assertions; and authorization mechanisms are pre-\nsented in Chapter 6. The theme of this chapter is the protection of the database\nfrom accidental and intentional damage.\nChapter 7 introduces the theory of relational database design. The theory\nof functional dependencies and normalization is covered, with emphasis on\nthe motivation and intuitive understanding of each normal form. The overall\nprocess of database design is also described in detail.\n• Object-based databases and XML (Chapters 8 through 10). Chapter 8 covers\nobject-oriented databases. It introduces the concepts of object-oriented pro-\ngramming, and shows how these concepts form the basis for a data model.\nNo prior knowledge of object-oriented languages is assumed. Chapter 9 cov-\ners object-relational databases, and shows how the SQL:1999 standard extends\nthe relational data model to include object-oriented features, such as inheri-\ntance, complex types, and object identity.\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nFront Matter\nPreface\n3\n© The McGraw−Hill \nCompanies, 2001\nPreface\nxvii\nChapter 10 covers the XML standard for data representation, which is see-\ning increasing use in data communication and in the storage of complex data\ntypes. The chapter also describes query languages for XML.\n• Data storage and querying (Chapters 11 through 14). Chapter 11 deals with\ndisk, ﬁle, and ﬁle-system structure, and with the mapping of relational and\nobject data to a ﬁle system. A variety of data-access techniques are presented\nin Chapter 12, including hashing, B+-tree indices, and grid ﬁle indices. Chap-\nters 13 and 14 address query-evaluation algorithms, and query optimization\nbased on equivalence-preserving query transformations.\nThese chapters provide an understanding of the internals of the storage and\nretrieval components of a database.\n• Transaction management (Chapters 15 through 17). Chapter 15 focuses on\nthe fundamentals of a transaction-processing system, including transaction\natomicity, consistency, isolation, and durability, as well as the notion of serial-\nizability.\nChapter 16 focuses on concurrency control and presents several techniques\nfor ensuring serializability, including locking, timestamping, and optimistic\n(validation) techniques. The chapter also covers deadlock issues. Chapter 17\ncovers the primary techniques for ensuring correct transaction execution de-\nspite system crashes and disk failures. These techniques include logs, shadow\npages, checkpoints, and database dumps.\n• Database system architecture (Chapters 18 through 20). Chapter 18 covers\ncomputer-system architecture, and describes the inﬂuence of the underlying\ncomputer system on the database system. We discuss centralized systems,\nclient–server systems, parallel and distributed architectures, and network\ntypes in this chapter. Chapter 19 covers distributed database systems, revis-\niting the issues of database design, transaction management, and query eval-\nuation and optimization, in the context of distributed databases. The chap-\nter also covers issues of system availability during failures and describes the\nLDAP directory system.\nChapter 20, on parallel databases explores a variety of parallelization tech-\nniques, including I/O parallelism, interquery and intraquery parallelism, and\ninteroperation and intraoperation parallelism. The chapter also describes\nparallel-system design.\n• Other topics (Chapters 21 through 24). Chapter 21 covers database appli-\ncation development and administration. Topics include database interfaces,\nparticularly Web interfaces, performance tuning, performance benchmarks,\nstandardization, and database issues in e-commerce. Chapter 22 covers query-\ning techniques, including decision support systems, and information retrieval.\nTopics covered in the area of decision support include online analytical pro-\ncessing (OLAP) techniques, SQL:1999 support for OLAP, data mining, and data\nwarehousing. The chapter also describes information retrieval techniques for\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nFront Matter\nPreface\n4\n© The McGraw−Hill \nCompanies, 2001\nxviii\nPreface\nquerying textual data, including hyperlink-based techniques used in Web\nsearch engines.\nChapter 23 covers advanced data types and new applications, including\ntemporal data, spatial and geographic data, multimedia data, and issues in the\nmanagement of mobile and personal databases. Finally, Chapter 24 deals with\nadvanced transaction processing. We discuss transaction-processing monitors,\nhigh-performance transaction systems, real-time transaction systems, and\ntransactional workﬂows.\n• Case studies (Chapters 25 through 27). In this part we present case studies of\nthree leading commercial database systems, including Oracle, IBM DB2, and\nMicrosoft SQL Server. These chapters outline unique features of each of these\nproducts, and describe their internal structure. They provide a wealth of in-\nteresting information about the respective products, and help you see how the\nvarious implementation techniques described in earlier parts are used in real\nsystems. They also cover several interesting practical aspects in the design of\nreal systems.\n• Online appendices. Although most new database applications use either the\nrelational model or the object-oriented model, the network and hierarchical\ndata models are still in use. For the beneﬁt of readers who wish to learn about\nthese data models, we provide appendices describing the network and hier-\narchical data models, in Appendices A and B respectively; the appendices are\navailable only online (http://www.bell-labs.com/topic/books/db-book).\nAppendix C describes advanced relational database design, including the\ntheory of multivalued dependencies, join dependencies, and the project-join\nand domain-key normal forms. This appendix is for the beneﬁt of individuals\nwho wish to cover the theory of relational database design in more detail, and\ninstructors who wish to do so in their courses. This appendix, too, is available\nonly online, on the Web page of the book.\nThe Fourth Edition\nThe production of this fourth edition has been guided by the many comments and\nsuggestions we received concerning the earlier editions, by our own observations\nwhile teaching at IIT Bombay, and by our analysis of the directions in which database\ntechnology is evolving.\nOur basic procedure was to rewrite the material in each chapter, bringing the older\nmaterial up to date, adding discussions on recent developments in database technol-\nogy, and improving descriptions of topics that students found difﬁcult to understand.\nEach chapter now has a list of review terms, which can help you review key topics\ncovered in the chapter. We have also added a tools section at the end of most chap-\nters, which provide information on software tools related to the topic of the chapter.\nWe have also added new exercises, and updated references.\nWe have added a new chapter covering XML, and three case study chapters cov-\nering the leading commercial database systems, including Oracle, IBM DB2, and Mi-\ncrosoft SQL Server.\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nFront Matter\nPreface\n5\n© The McGraw−Hill \nCompanies, 2001\nPreface\nxix\nWe have organized the chapters into several parts, and reorganized the contents\nof several chapters. For the beneﬁt of those readers familiar with the third edition,\nwe explain the main changes here:\n• Entity-relationship model. We have improved our coverage of the entity-\nrelationship (E-R) model. More examples have been added, and some changed,\nto give better intuition to the reader. A summary of alternative E-R notations\nhas been added, along with a new section on UML.\n• Relational databases. Our coverage of SQL in Chapter 4 now references the\nSQL:1999 standard, which was approved after publication of the third edition.\nSQL coverage has been signiﬁcantly expanded to include the with clause, ex-\npanded coverage of embedded SQL, and coverage of ODBC and JDBC whose\nusage has increased greatly in the past few years. Coverage of Quel has been\ndropped from Chapter 5, since it is no longer in wide use. Coverage of QBE\nhas been revised to remove some ambiguities and to add coverage of the QBE\nversion used in the Microsoft Access database.\nChapter 6 now covers integrity constraints and security. Coverage of se-\ncurity has been moved to Chapter 6 from its third-edition position of Chap-\nter 19. Chapter 6 also covers triggers. Chapter 7 covers relational-database\ndesign and normal forms. Discussion of functional dependencies has been\nmoved into Chapter 7 from its third-edition position of Chapter 6. Chapter\n7 has been signiﬁcantly rewritten, providing several short-cut algorithms for\ndealing with functional dependencies and extended coverage of the overall\ndatabase design process. Axioms for multivalued dependency inference, PJNF\nand DKNF, have been moved into an appendix.\n• Object-based databases. Coverage of object orientation in Chapter 8 has been\nimproved, and the discussion of ODMG updated. Object-relational coverage in\nChapter 9 has been updated, and in particular the SQL:1999 standard replaces\nthe extended SQL used in the third edition.\n• XML. Chapter 10, covering XML, is a new chapter in the fourth edition.\n• Storage, indexing, and query processing. Coverage of storage and ﬁle struc-\ntures, in Chapter 11, has been updated; this chapter was Chapter 10 in the\nthird edition. Many characteristics of disk drives and other storage mecha-\nnisms have changed greatly in the past few years, and our coverage has been\ncorrespondingly updated. Coverage of RAID has been updated to reﬂect tech-\nnology trends. Coverage of data dictionaries (catalogs) has been extended.\nChapter 12, on indexing, now includes coverage of bitmap indices; this\nchapter was Chapter 11 in the third edition. The B+-tree insertion algorithm\nhas been simpliﬁed, and pseudocode has been provided for search. Parti-\ntioned hashing has been dropped, since it is not in signiﬁcant use.\nOur treatment of query processing has been reorganized, with the earlier\nchapter (Chapter 12 in the third edition) split into two chapters, one on query\nprocessing (Chapter 13) and another on query optimization (Chapter 14). All\ndetails regarding cost estimation and query optimization have been moved\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nFront Matter\nPreface\n6\n© The McGraw−Hill \nCompanies, 2001\nxx\nPreface\nto Chapter 14, allowing Chapter 13 to concentrate on query processing algo-\nrithms. We have dropped several detailed (and tedious) formulae for calcu-\nlating the exact number of I/O operations for different operations. Chapter 14\nnow has pseudocode for optimization algorithms, and new sections on opti-\nmization of nested subqueries and on materialized views.\n• Transaction processing. Chapter 15, which provides an introduction to trans-\nactions, has been updated; this chapter was numbered Chapter 13 in the third\nedition. Tests for view serializability have been dropped.\nChapter 16, on concurrency control, includes a new section on implemen-\ntation of lock managers, and a section on weak levels of consistency, which\nwas in Chapter 20 of the third edition. Concurrency control of index structures\nhas been expanded, providing details of the crabbing protocol, which is a sim-\npler alternative to the B-link protocol, and next-key locking to avoid the phan-\ntom problem. Chapter 17, on recovery, now includes coverage of the ARIES\nrecovery algorithm. This chapter also covers remote backup systems for pro-\nviding high availability despite failures, an increasingly important feature in\n“24 × 7” applications.\nAs in the third edition, instructors can choose between just introducing\ntransaction-processing concepts (by covering only Chapter 15), or offering de-\ntailed coverage (based on Chapters 15 through 17).\n• Database system architectures. Chapter 18, which provides an overview of\ndatabase system architectures, has been updated to cover current technology;\nthis was Chapter 16 in the third edition. The order of the parallel database\nchapter and the distributed database chapters has been ﬂipped. While the cov-\nerage of parallel database query processing techniques in Chapter 20\n(which was Chapter 16 in the third edition) is mainly of interest to those who\nwish to learn about database internals, distributed databases, now covered in\nChapter 19, is a topic that is more fundamental; it is one that anyone dealing\nwith databases should be familiar with.\nChapter 19 on distributed databases has been signiﬁcantly rewritten, to re-\nduce the emphasis on naming and transparency and to increase coverage of\noperation during failures, including concurrency control techniques to pro-\nvide high availability. Coverage of three-phase commit protocol has been ab-\nbreviated, as has distributed detection of global deadlocks, since neither is\nused much in practice. Coverage of query processing issues in heterogeneous\ndatabases has been moved up from Chapter 20 of the third edition. There is\na new section on directory systems, in particular LDAP, since these are quite\nwidely used as a mechanism for making information available in a distributed\nsetting.\n• Other topics. Although we have modiﬁed and updated the entire text, we\nconcentrated our presentation of material pertaining to ongoing database re-\nsearch and new database applications in four new chapters, from Chapter 21\nto Chapter 24.\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nFront Matter\nPreface\n7\n© The McGraw−Hill \nCompanies, 2001\nPreface\nxxi\nChapter 21 is new in the fourth edition and covers application develop-\nment and administration. The description of how to build Web interfaces to\ndatabases, including servlets and other mechanisms for server-side scripting,\nis new. The section on performance tuning, which was earlier in Chapter 19,\nhas new material on the famous 5-minute rule and the 1-minute rule, as well\nas some new examples. Coverage of materialized view selection is also new.\nCoverage of benchmarks and standards has been updated. There is a new sec-\ntion on e-commerce, focusing on database issues in e-commerce, and a new\nsection on dealing with legacy systems.\nChapter 22, which covers advanced querying and inform\n\nation retrieval,\nincludes new material on OLAP, particulary on SQL:1999 extensions for data\nanalysis. Coverage of data warehousing and data mining has also been ex-\ntended greatly. Coverage of information retrieval has been signiﬁcantly ex-\ntended, particulary in the area of Web searching. Earlier versions of this ma-\nterial were in Chapter 21 of the third edition.\nChapter 23, which covers advanced data types and new applications, has\nmaterial on temporal data, spatial data, multimedia data, and mobile data-\nbases. This material is an updated version of material that was in Chapter 21\nof the third edition. Chapter 24, which covers advanced transaction process-\ning, contains updated versions of sections on TP monitors, workﬂow systems,\nmain-memory and real-time databases, long-duration transactions, and trans-\naction management in multidatabases, which appeared in Chapter 20 of the\nthird edition.\n• Case studies. The case studies covering Oracle, IBM DB2 and Microsoft SQL\nServer are new to the fourth edition. These chapters outline unique features\nof each of these products, and describe their internal structure.\nInstructor’s Note\nThe book contains both basic and advanced material, which might not be covered in\na single semester. We have marked several sections as advanced, using the symbol\n“∗∗”. These sections may be omitted if so desired, without a loss of continuity.\nIt is possible to design courses by using various subsets of the chapters. We outline\nsome of the possibilities here:\n• Chapter 5 can be omitted if students will not be using QBE or Datalog as part\nof the course.\n• If object orientation is to be covered in a separate advanced course, Chapters\n8 and 9, and Section 11.9, can be omitted. Alternatively, they could constitute\nthe foundation of an advanced course in object databases.\n• Chapter 10 (XML) and Chapter 14 (query optimization) can be omitted from\nan introductory course.\n• Both our coverage of transaction processing (Chapters 15 through 17) and our\ncoverage of database-system architecture (Chapters 18 through 20) consist of\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nFront Matter\nPreface\n8\n© The McGraw−Hill \nCompanies, 2001\nxxii\nPreface\nan overview chapter (Chapters 15 and 18, respectively), followed by chap-\nters with details. You might choose to use Chapters 15 and 18, while omitting\nChapters 16, 17, 19, and 20, if you defer these latter chapters to an advanced\ncourse.\n• Chapters 21 through 24 are suitable for an advanced course or for self-study\nby students, although Section 21.1 may be covered in a ﬁrst database course.\nModel course syllabi, based on the text, can be found on the Web home page of the\nbook (see the following section).\nWeb Page and Teaching Supplements\nA Web home page for the book is available at the URL:\nhttp://www.bell-labs.com/topic/books/db-book\nThe Web page contains:\n• Slides covering all the chapters of the book\n• Answers to selected exercises\n• The three appendices\n• An up-to-date errata list\n• Supplementary material contributed by users of the book\nA complete solution manual will be made available only to faculty. For more infor-\nmation about how to get a copy of the solution manual, please send electronic mail to\ncustomer.service@mcgraw-hill.com. In the United States, you may call 800-338-3987.\nThe McGraw-Hill Web page for this book is\nhttp://www.mhhe.com/silberschatz\nContacting Us and Other Users\nWe provide a mailing list through which users of our book can communicate among\nthemselves and with us. If you wish to be on the list, please send a message to\ndb-book@research.bell-labs.com, include your name, afﬁliation, title, and electronic\nmail address.\nWe have endeavored to eliminate typos, bugs, and the like from the text. But, as in\nnew releases of software, bugs probably remain; an up-to-date errata list is accessible\nfrom the book’s home page. We would appreciate it if you would notify us of any\nerrors or omissions in the book that are not on the current list of errata.\nWe would be glad to receive suggestions on improvements to the books. We also\nwelcome any contributions to the book Web page that could be of use to other read-\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nFront Matter\nPreface\n9\n© The McGraw−Hill \nCompanies, 2001\nPreface\nxxiii\ners, such as programming exercises, project suggestions, online labs and tutorials,\nand teaching tips.\nE-mail should be addressed to db-book@research.bell-labs.com. Any other cor-\nrespondence should be sent to Avi Silberschatz, Bell Laboratories, Room 2T-310, 600\nMountain Avenue, Murray Hill, NJ 07974, USA.\nAcknowledgments\nThis edition has beneﬁted from the many useful comments provided to us by the\nnumerous students who have used the third edition. In addition, many people have\nwritten or spoken to us about the book, and have offered suggestions and comments.\nAlthough we cannot mention all these people here, we especially thank the following:\n• Phil Bernhard, Florida Institute of Technology; Eitan M. Gurari, The Ohio State\nUniversity; Irwin Levinstein, Old Dominion University; Ling Liu, Georgia In-\nstitute of Technology; Ami Motro, George Mason University; Bhagirath Nara-\nhari, Meral Ozsoyoglu, Case Western Reserve University; and Odinaldo Ro-\ndriguez, King’s College London; who served as reviewers of the book and\nwhose comments helped us greatly in formulating this fourth edition.\n• Soumen Chakrabarti, Sharad Mehrotra, Krithi Ramamritham, Mike Reiter,\nSunita Sarawagi, N. L. Sarda, and Dilys Thomas, for extensive and invaluable\nfeedback on several chapters of the book.\n• Phil Bohannon, for writing the ﬁrst draft of Chapter 10 describing XML.\n• Hakan Jakobsson (Oracle), Sriram Padmanabhan (IBM), and C´esar Galindo-\nLegaria, Goetz Graefe, Jos´e A. Blakeley, Kalen Delaney, Michael Rys, Michael\nZwilling, Sameet Agarwal, Thomas Casey (all of Microsoft) for writing the\nappendices describing the Oracle, IBM DB2, and Microsoft SQL Server database\nsystems.\n• Yuri Breitbart, for help with the distributed database chapter; Mike Reiter, for\nhelp with the security sections; and Jim Melton, for clariﬁcations on SQL:1999.\n• Marilyn Turnamian and Nandprasad Joshi, whose excellent secretarial assis-\ntance was essential for timely completion of this fourth edition.\nThe publisher was Betsy Jones. The senior developmental editor was Kelley\nButcher. The project manager was Jill Peter. The executive marketing manager was\nJohn Wannemacher. The cover illustrator was Paul Tumbaugh while the cover de-\nsigner was JoAnne Schopler. The freelance copyeditor was George Watson. The free-\nlance proofreader was Marie Zartman. The supplement producer was Jodi Banowetz.\nThe designer was Rick Noel. The freelance indexer was Tobiah Waldron.\nThis edition is based on the three previous editions, so we thank once again the\nmany people who helped us with the ﬁrst three editions, including R. B. Abhyankar,\nDon Batory, Haran Boral, Paul Bourgeois, Robert Brazile, Michael Carey, J. Edwards,\nChristos Faloutsos, Homma Farian, Alan Fekete, Shashi Gadia, Jim Gray, Le Gruen-\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nFront Matter\nPreface\n10\n© The McGraw−Hill \nCompanies, 2001\nxxiv\nPreface\nwald, Ron Hitchens, Yannis Ioannidis, Hyoung-Joo Kim, Won Kim, Henry Korth (fa-\nther of Henry F.), Carol Kroll, Gary Lindstrom, Dave Maier, Keith Marzullo, Fletcher\nMattox, Alberto Mendelzon, Hector Garcia-Molina, Ami Motro, Anil Nigam, Cyril\nOrji, Bruce Porter, Jim Peterson, K. V. Raghavan, Mark Roth, Marek Rusinkiewicz,\nS. Seshadri, Shashi Shekhar, Amit Sheth, Nandit Soparkar, Greg Speegle, and Mari-\nanne Winslett. Lyn Dupr´e copyedited the third edition and Sara Strandtman edited\nthe text of the third edition. Greg Speegle, Dawn Bezviner, and K. V. Raghavan helped\nus to prepare the instructor’s manual for earlier editions. The new cover is an evo-\nlution of the covers of the ﬁrst three editions; Marilyn Turnamian created an early\ndraft of the cover design for this edition. The idea of using ships as part of the cover\nconcept was originally suggested to us by Bruce Stephan.\nFinally, Sudarshan would like to acknowledge his wife, Sita, for her love and sup-\nport, two-year old son Madhur for his love, and mother, Indira, for her support. Hank\nwould like to acknowledge his wife, Joan, and his children, Abby and Joe, for their\nlove and understanding. Avi would like to acknowledge his wife Haya, and his son,\nAaron, for their patience and support during the revision of this book.\nA. S.\nH. F. K.\nS. S.\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\n1. Introduction\nText\n11\n© The McGraw−Hill \nCompanies, 2001\nC\nH\nA\nP\nT\nE\nR\n1\nIntroduction\nA database-management system (DBMS) is a collection of interrelated data and a\nset of programs to access those data. The collection of data, usually referred to as the\ndatabase, contains information relevant to an enterprise. The primary goal of a DBMS\nis to provide a way to store and retrieve database information that is both convenient\nand efﬁcient.\nDatabase systems are designed to manage large bodies of information. Manage-\nment of data involves both deﬁning structures for storage of information and pro-\nviding mechanisms for the manipulation of information. In addition, the database\nsystem must ensure the safety of the information stored, despite system crashes or\nattempts at unauthorized access. If data are to be shared among several users, the\nsystem must avoid possible anomalous results.\nBecause information is so important in most organizations, computer scientists\nhave developed a large body of concepts and techniques for managing data. These\nconcepts and technique form the focus of this book. This chapter brieﬂy introduces\nthe principles of database systems.\n1.1\nDatabase System Applications\nDatabases are widely used. Here are some representative applications:\n• Banking: For customer information, accounts, and loans, and banking transac-\ntions.\n• Airlines: For reservations and schedule information. Airlines were among the\nﬁrst to use databases in a geographically distributed manner—terminals sit-\nuated around the world accessed the central database system through phone\nlines and other data networks.\n• Universities: For student information, course registrations, and grades.\n1\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\n1. Introduction\nText\n12\n© The McGraw−Hill \nCompanies, 2001\n2\nChapter 1\nIntroduction\n• Credit card transactions: For purchases on credit cards and generation of month-\nly statements.\n• Telecommunication: For keeping records of calls made, generating monthly bills,\nmaintaining balances on prepaid calling cards, and storing information about\nthe communication networks.\n• Finance: For storing information about holdings, sales, and purchases of ﬁnan-\ncial instruments such as stocks and bonds.\n• Sales: For customer, product, and purchase information.\n• Manufacturing: For management of supply chain and for tracking production\nof items in factories, inventories of items in warehouses/stores, and orders for\nitems.\n• Human resources: For information about employees, salaries, payroll taxes and\nbeneﬁts, and for generation of paychecks.\nAs the list illustrates, databases form an essential part of almost all enterprises today.\nOver the course of the last four decades of the twentieth century, use of databases\ngrew in all enterprises. In the early days, very few people interacted directly with\ndatabase systems, although without realizing it they interacted with databases in-\ndirectly—through printed reports such as credit card statements, or through agents\nsuch as bank tellers and airline reservation agents. Then automated teller machines\ncame along and let users interact directly with databases. Phone interfaces to com-\nputers (interactive voice response systems) also allowed users to deal directly with\ndatabases—a caller could dial a number, and press phone keys to enter information\nor to select alternative options, to ﬁnd ﬂight arrival/departure times, for example, or\nto register for courses in a university.\nThe internet revolution of the late 1990s sharply increased direct user access to\ndatabases. Organizations converted many of their phone interfaces to databases into\nWeb interfaces, and made a variety of services and information available online. For\ninstance, when you access an online bookstore and browse a book or music collec-\ntion, you are accessing data stored in a database. When you enter an order online,\nyour order is stored in a database. When you access a bank Web site and retrieve\nyour bank balance and transaction information, the information is retrieved from the\nbank’s database system. When you access a Web site, information about you may be\nretrieved from a database, to select which advertisements should be shown to you.\nFurthermore, data about your Web accesses may be stored in a database.\nThus, although user interfaces hide details of access to a database, and most people\nare not even aware they are dealing with a database, accessing databases forms an\nessential part of almost everyone’s life today.\nThe importance of database systems can be judged in another way—today, data-\nbase system vendors like Oracle are among the largest software companies in the\nworld, and database systems form an important part of the product line of more\ndiversiﬁed companies like Microsoft and IBM.\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\n1. Introduction\nText\n13\n© The McGraw−Hill \nCompanies, 2001\n1.2\nDatabase Systems versus File Systems\n3\n1.2\nDatabase Systems versus File Systems\nConsider part of a savings-bank enterprise that keeps information about all cus-\ntomers and savings accounts. One way to keep the information on a computer is\nto store it in operating system ﬁles. To allow users to manipulate the information, the\nsystem has a number of application programs that manipulate the ﬁles, including\n• A program to debit or credit an account\n• A program to add a new account\n• A program to ﬁnd the balance of an account\n• A program to generate monthly statements\nSystem programmers wrote these application programs to meet the needs of the\nbank.\nNew application programs are added to the system as the need arises. For exam-\nple, suppose that the savings bank decides to offer checking accounts. As a result,\nthe bank creates new permanent ﬁles that contain information about all the checking\naccounts maintained in the bank, and it may have to write new application programs\nto deal with situations that do not arise in savings accounts, such as overdrafts. Thus,\nas time goes by, the system acquires more ﬁles and more application programs.\nThis typical ﬁle-processing system is supported by a conventional operating sys-\ntem. The system stores permanent records in various ﬁles, and it needs different\napplication programs to extract records from, and add records to, the appropriate\nﬁles. Before database management systems (DBMSs) came along, organizations usu-\nally stored information in such systems.\nKeeping organizational information in a ﬁle-processing system has a number of\nmajor disadvantages:\n• Data redundancy and inconsistency. Since different programmers create the\nﬁles and application programs over a long period, the various ﬁles are likely\nto have different formats and the programs may be written in several pro-\ngramming languages. Moreover, the same information may be duplicated in\nseveral places (ﬁles). For example, the address and telephone number of a par-\nticular customer may appear in a ﬁle that consists of savings-account records\nand in a ﬁle that consists of checking-account records. This redundancy leads\nto higher storage and access cost. In addition, it may lead to data inconsis-\ntency; that is, the various copies of the same data may no longer agree. For\nexample, a changed customer address may be reﬂected in savings-account\nrecords but not elsewhere in the system.\n• Difﬁculty in accessing data. Suppose that one of the bank ofﬁcers needs to\nﬁnd out the names of all customers who live within a particular postal-code\narea. The ofﬁcer asks the data-processing department to generate such a list.\nBecause the designers of the original system did not anticipate this request,\nthere is no application program on hand to meet it. There is, however, an ap-\nplication program to generate the list of all customers. The bank ofﬁcer has\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\n1. Introduction\nText\n14\n© The McGraw−Hill \nCompanies, 2001\n4\nChapter 1\nIntroduction\nnow two choices: either obtain the list of all customers and extract the needed\ninformation manually or ask a system programmer to write the necessary\napplication program. Both alternatives are obviously unsatisfactory. Suppose\nthat such a program is written, and that, several days later, the same ofﬁcer\nneeds to trim that list to include only those customers who have an account\nbalance of $10,000 or more. As expected, a program to generate such a list does\nnot exist. Again, the ofﬁcer has the preceding two options, neither of which is\nsatisfactory.\nThe point here is that conventional ﬁle-processing environments do not al-\nlow needed data to be retrieved in a convenient and efﬁcient manner. More\nresponsive data-retrieval systems are required for general use.\n• Data isolation. Because data are scattered in various ﬁles, and ﬁles may be in\ndifferent formats, writing new application programs to retrieve the appropri-\nate data is difﬁcult.\n• Integrity problems. The data values stored in the database must satisfy cer-\ntain types of consistency constraints. For example, the balance of a bank ac-\ncount may never fall below a prescribed amount (say, $25). Developers enforce\nthese constraints in the system by adding appropriate code in the various ap-\nplication programs. However, when new constraints are added, it is difﬁcult\nto change the programs to enforce them. The problem is compounded when\nconstraints involve several data items from different ﬁles.\n• Atomicity problems. A computer system, like any other mechanical or elec-\ntrical device, is subject to failure. In many applications, it is crucial that, if a\nfailure occurs, the data be restored to the consistent state that existed prior to\nthe failure. Consider a program to transfer $50 from account A to account B.\nIf a system failure occurs during the execution of the program, it is possible\nthat the $50 was removed from account A but was not credited to account B,\nresulting in an inconsistent database state. Clearly, it is essential to database\nconsistency that either both the credit and debit occur, or that neither occur.\nThat is, the funds transfer must be atomic—it must happen in its entirety or\nnot at all. It is difﬁcult to ensure atomicity in a conventional ﬁle-processing\nsystem.\n• Concurrent-access anomalies. For the sake of overall performance of the sys-\ntem and faster response, many systems allow multiple users to update the\ndata simultaneously. In such an environment, interaction of concurrent up-\ndates may result in inconsistent data. Consider bank account A, containing\n$500. If two customers withdraw funds (say $50 and $100 respectively) from\naccount A at about the same time, the result of the concurrent executions may\nleave the account in an incorrect (or inconsistent) state. Suppose that the pro-\ngrams executing on behalf of each withdrawal read the old balance, reduce\nthat value by the amount being withdrawn, and write the result back. If the\ntwo programs run concurrently, they may both read the value $500, and write\nback $450 and $400, respectively. Depending on which one writes the value\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\n1. Introduction\nText\n15\n© The McGraw−Hill \nCompanies, 2001\n1.3\nView of Data\n5\nlast, the account may contain either $450 or $400, rather than the correct value\nof $350. To guard against this possibili\n\n starting from the student appli-\ncation procedure.\nb. Indicate acceptable termination states, and which steps involve human in-\ntervention.\nc. Indicate possible errors (including deadline expiry) and how they are dealt\nwith.\nd. Study how much of the workﬂow has been automated at your university.\n24.4 Like database systems, workﬂow systems also require concurrency and recov-\nery management. List three reasons why we cannot simply apply a relational\ndatabase system using 2PL, physical undo logging, and 2PC.\n24.5 If the entire database ﬁts in main memory, do we still need a database system\nto manage the data? Explain your answer.\n24.6 Consider a main-memory database system recovering from a system crash.\nExplain the relative merits of\n• Loading the entire database back into main memory before resuming trans-\naction processing\n• Loading data as it is requested by transactions\n24.7 In the group-commit technique, how many transactions should be part of a\ngroup? Explain your answer.\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nVII. Other Topics\n24. Advanced Transaction \nProcessing\n910\n© The McGraw−Hill \nCompanies, 2001\nBibliographical Notes\n917\n24.8 Is a high-performance transaction system necessarily a real-time system? Why\nor why not?\n24.9 In a database system using write-ahead logging, what is the worst-case num-\nber of disk accesses required to read a data item? Explain why this presents a\nproblem to designers of real-time database systems.\n24.10 Explain why it may be impractical to require serializability for long-duration\ntransactions.\n24.11 Consider a multithreaded process that delivers messages from a durable queue\nof persistent messages. Different threads may run concurrently, attempting to\ndeliver different messages. In case of a delivery failure, the message must be\nrestored in the queue. Model the actions that each thread carries out as a mul-\ntilevel transaction, so that locks on the queue need not be held till a message is\ndelivered.\n24.12 Discuss the modiﬁcations that need to be made in each of the recovery schemes\ncovered in Chapter 17 if we allow nested transactions. Also, explain any differ-\nences that result if we allow multilevel transactions.\n24.13 What is the purpose of compensating transactions? Present two examples of\ntheir use.\n24.14 Consider a multidatabase system in which it is guaranteed that at most one\nglobal transaction is active at any time, and every local site ensures local seri-\nalizability.\na. Suggest ways in which the multidatabase system can ensure that there is\nat most one active global transaction at any time.\nb. Show by example that it is possible for a nonserializable global schedule\nto result despite the assumptions.\n24.15 Consider a multidatabase system in which every local site ensures local serial-\nizability, and all global transactions are read only.\na. Show by example that nonserializable executions may result in such a sys-\ntem.\nb. Show how you could use a ticket scheme to ensure global serializability.\nBibliographical Notes\nGray and Edwards [1995] provides an overview of TP monitor architectures; Gray\nand Reuter [1993] provides a detailed (and excellent) textbook description of tran-\nsaction-processing systems, including chapters on TP monitors. Our description of TP\nmonitors is modeled on these two sources. X/Open [1991] deﬁnes the X/Open XA\ninterface. Transaction processing in Tuxedo is described in Huffman [1993]. Wipﬂer\n[1987] is one of several texts on application development using CICS.\nFischer [2001] is a handbook on workﬂow systems. A reference model for work-\nﬂows, proposed by the Workﬂow Management Coalition, is presented in Hollinsworth\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nVII. Other Topics\n24. Advanced Transaction \nProcessing\n911\n© The McGraw−Hill \nCompanies, 2001\n918\nChapter 24\nAdvanced Transaction Processing\n[1994]. The Web site of the coalition is www.wfmc.org. Our description of workﬂows\nfollows the model of Rusinkiewicz and Sheth [1995].\nReuter [1989] presents ConTracts, a method for grouping transactions into multi-\ntransaction activities. Some issues related to workﬂows were addressed in the work\non long-running activities described by Dayal et al. [1990] and Dayal et al. [1991]. The\nauthors propose event–condition–action rules as a technique for specifying work-\nﬂows. Jin et al. [1993] describes workﬂow issues in telecommunication applications.\nGarcia-Molina and Salem [1992] provides an overview of main-memory databases.\nJagadish et al. [1993] describes a recovery algorithm designed for main-memory data-\nbases. A storage manager for main-memory databases is described in Jagadish et al.\n[1994].\nTransaction processing in real-time databases is discussed by Abbott and Garcia-\nMolina [1999] and Dayal et al. [1990]. Barclay et al. [1982] describes a real-time data-\nbase system used in a telecommunications switching system. Complexity and\ncorrectness issues in real-time databases are addressed by Korth et al. [1990b] and\nSoparkar et al. [1995]. Concurrency control and scheduling in real-time databases are\ndiscussed by Haritsa et al. [1990], Hong et al. [1993], and Pang et al. [1995]. Ozsoyoglu\nand Snodgrass [1995] is a survey of research in real-time and temporal databases.\nNested and multilevel transactions are presented by Lynch [1983], Moss [1982],\nMoss [1985], Lynch and Merritt [1986], Fekete et al. [1990b], Fekete et al. [1990a], Ko-\nrth and Speegle [1994], and Pu et al. [1988]. Theoretical aspects of multilevel transac-\ntions are presented in Lynch et al. [1988] and Weihl and Liskov [1990].\nSeveral extended-transaction models have been deﬁned including Sagas (Garcia-\nMolina and Salem [1987]), ACTA (Chrysanthis and Ramamritham [1994]), the Con-\nTract model (Wachter and Reuter [1992]), ARIES (Mohan et al. [1992] and Rothermel\nand Mohan [1989]), and the NT/PV model (Korth and Speegle [1994]).\nSplitting transactions to achieve higher performance is addressed in Shasha et al.\n[1995]. A model for concurrency in nested transactions systems is presented in Beeri\net al. [1989]. Relaxation of serializability is discussed in Garcia-Molina [1983] and\nSha et al. [1988]. Recovery in nested transaction systems is discussed by Moss [1987],\nHaerder and Rothermel [1987], Rothermel and Mohan [1989]. Multilevel transaction\nmanagement is discussed in Weikum [1991].\nGray [1981], Skarra and Zdonik [1989], Korth and Speegle [1988], and Korth and\nSpeegle [1990] discuss long-duration transactions. Transaction processing for\nlong-duration transactions is considered by Weikum and Schek [1984], Haerder and\nRothermel [1987], Weikum et al. [1990], and Korth et al. [1990a]. Salem et al. [1994]\npresents an extension of 2PL for long-duration transactions by allowing the early\nrelease of locks under certain circumstances. Transaction processing in design and\nsoftware-engineering applications is discussed in Korth et al. [1988], Kaiser [1990],\nand Weikum [1991].\nTransaction processing in multidatabase systems is discussed in Breitbart et al.\n[1990], Breitbart et al. [1991], Breitbart et al. [1992], Soparkar et al. [1991], Mehrotra\net al. [1992b] and Mehrotra et al. [1992a]. The ticket scheme is presented in Geor-\ngakopoulos et al. [1994]. 2LSR is introduced in Mehrotra et al. [1991]. An earlier ap-\nproach, called quasi-serializability, is presented in Du and Elmagarmid [1989].\n\n\nty, the system must maintain some form\nof supervision. But supervision is difﬁcult to provide because data may be\naccessed by many different application programs that have not been coordi-\nnated previously.\n• Security problems. Not every user of the database system should be able to\naccess all the data. For example, in a banking system, payroll personnel need\nto see only that part of the database that has information about the various\nbank employees. They do not need access to information about customer ac-\ncounts. But, since application programs are added to the system in an ad hoc\nmanner, enforcing such security constraints is difﬁcult.\nThese difﬁculties, among others, prompted the development of database systems.\nIn what follows, we shall see the concepts and algorithms that enable database sys-\ntems to solve the problems with ﬁle-processing systems. In most of this book, we\nuse a bank enterprise as a running example of a typical data-processing application\nfound in a corporation.\n1.3\nView of Data\nA database system is a collection of interrelated ﬁles and a set of programs that allow\nusers to access and modify these ﬁles. A major purpose of a database system is to\nprovide users with an abstract view of the data. That is, the system hides certain\ndetails of how the data are stored and maintained.\n1.3.1\nData Abstraction\nFor the system to be usable, it must retrieve data efﬁciently. The need for efﬁciency\nhas led designers to use complex data structures to represent data in the database.\nSince many database-systems users are not computer trained, developers hide the\ncomplexity from users through several levels of abstraction, to simplify users’ inter-\nactions with the system:\n• Physical level. The lowest level of abstraction describes how the data are actu-\nally stored. The physical level describes complex low-level data structures in\ndetail.\n• Logical level. The next-higher level of abstraction describes what data are\nstored in the database, and what relationships exist among those data. The\nlogical level thus describes the entire database in terms of a small number\nof relatively simple structures. Although implementation of the simple struc-\ntures at the logical level may involve complex physical-level structures, the\nuser of the logical level does not need to be aware of this complexity. Database\nadministrators, who must decide what information to keep in the database,\nuse the logical level of abstraction.\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\n1. Introduction\nText\n16\n© The McGraw−Hill \nCompanies, 2001\n6\nChapter 1\nIntroduction\n• View level. The highest level of abstraction describes only part of the entire\ndatabase. Even though the logical level uses simpler structures, complexity\nremains because of the variety of information stored in a large database. Many\nusers of the database system do not need all this information; instead, they\nneed to access only a part of the database. The view level of abstraction exists\nto simplify their interaction with the system. The system may provide many\nviews for the same database.\nFigure 1.1 shows the relationship among the three levels of abstraction.\nAn analogy to the concept of data types in programming languages may clarify\nthe distinction among levels of abstraction. Most high-level programming languages\nsupport the notion of a record type. For example, in a Pascal-like language, we may\ndeclare a record as follows:\ntype customer = record\ncustomer-id : string;\ncustomer-name : string;\ncustomer-street : string;\ncustomer-city : string;\nend;\nThis code deﬁnes a new record type called customer with four ﬁelds. Each ﬁeld has\na name and a type associated with it. A banking enterprise may have several such\nrecord types, including\n• account, with ﬁelds account-number and balance\n• employee, with ﬁelds employee-name and salary\nAt the physical level, a customer, account, or employee record can be described as a\nblock of consecutive storage locations (for example, words or bytes). The language\nview 1\nview 2\nlogical\nlevel\nphysical\nlevel\nview n\n…\nview level\nFigure 1.1\nThe three levels of data abstraction.\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\n1. Introduction\nText\n17\n© The McGraw−Hill \nCompanies, 2001\n1.4\nData Models\n7\ncompiler hides this level of detail from programmers. Similarly, the database system\nhides many of the lowest-level storage details from database programmers. Database\nadministrators, on the other hand, may be aware of certain details of the physical\norganization of the data.\nAt the logical level, each such record is described by a type deﬁnition, as in the\nprevious code segment, and the interrelationship of these record types is deﬁned as\nwell. Programmers using a programming language work at this level of abstraction.\nSimilarly, database administrators usually work at this level of abstraction.\nFinally, at the view level, computer users see a set of application programs that\nhide details of the data types. Similarly, at the view level, several views of the database\nare deﬁned, and database users see these views. In addition to hiding details of the\nlogical level of the database, the views also provide a security mechanism to prevent\nusers from accessing certain parts of the database. For example, tellers in a bank see\nonly that part of the database that has information on customer accounts; they cannot\naccess information about salaries of employees.\n1.3.2\nInstances and Schemas\nDatabases change over time as information is inserted and deleted. The collection of\ninformation stored in the database at a particular moment is called an instance of the\ndatabase. The overall design of the database is called the database schema. Schemas\nare changed infrequently, if at all.\nThe concept of database schemas and instances can be understood by analogy to\na program written in a programming language. A database schema corresponds to\nthe variable declarations (along with associated type deﬁnitions) in a program. Each\nvariable has a particular value at a given instant. The values of the variables in a\nprogram at a point in time correspond to an instance of a database schema.\nDatabase systems have several schemas, partitioned according to the levels of ab-\nstraction. The physical schema describes the database design at the physical level,\nwhile the logical schema describes the database design at the logical level. A database\nmay also have several schemas at the view level, sometimes called subschemas, that\ndescribe different views of the database.\nOf these, the logical schema is by far the most important, in terms of its effect on\napplication programs, since programmers construct applications by using the logical\nschema. The physical schema is hidden beneath the logical schema, and can usually\nbe changed easily without affecting application programs. Application programs are\nsaid to exhibit physical data independence if they do not depend on the physical\nschema, and thus need not be rewritten if the physical schema changes.\nWe study languages for describing schemas, after introducing the notion of data\nmodels in the next section.\n1.4\nData Models\nUnderlying the structure of a database is the data model: a collection of conceptual\ntools for describing data, data relationships, data semantics, and consistency con-\nstraints. To illustrate the concept of a data model, we outline two data models in this\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\n1. Introduction\nText\n18\n© The McGraw−Hill \nCompanies, 2001\n8\nChapter 1\nIntroduction\nsection: the entity-relationship model and the relational model. Both provide a way\nto describe the design of a database at the logical level.\n1.4.1\nThe Entity-Relationship Model\nThe entity-relationship (E-R) data model is based on a perception of a real world that\nconsists of a collection of basic objects, called entities, and of relationships among these\nobjects. An entity is a “thing” or “object” in the real world that is distinguishable\nfrom other objects. For example, each person is an entity, and bank accounts can be\nconsidered as entities.\nEntities are described in a database by a set of attributes. For example, the at-\ntributes account-number and balance may describe one particular account in a bank,\nand they form attributes of the account entity set. Similarly, attributes customer-name,\ncustomer-street address and customer-city may describe a customer entity.\nAn extra attribute customer-id is used to uniquely identify customers (since it may\nbe possible to have two customers with the same name, street address, and city).\nA unique customer identiﬁer must be assigned to each customer. In the United States,\nmany enterprises use the social-security number of a person (a unique number the\nU.S. government assigns to every person in the United States) as a customer\nidentiﬁer.\nA relationship is an association among several entities. For example, a depositor\nrelationship associates a customer with each account that she has. The set of all enti-\nties of the same type and the set of all relationships of the same type are termed an\nentity set and relationship set, respectively.\nThe overall logical structure (schema) of a database can be expressed graphically\nby an E-R diagram, which is built up from the following components:\n• Rectangles, which represent entity sets\n• Ellipses, which represent attributes\n• Diamonds, which represent relationships among entity sets\n• Lines, which link attributes to entity sets and entity sets to relationships\nEach component is labeled with the entity or relationship that it represents.\nAs an illustration, consider part of a database banking system consisting of\ncustomers and of the accounts that these customers have. Figure 1.2 shows the cor-\nresponding E-R diagram. The E-R diagram indicates that there are two entity sets,\ncustomer and account, with attributes as outlined earlier. The diagram also shows a\nrelationship depositor between customer and account.\nIn addition to entities and relationships, the E-R model represents certain con-\nstraints to which the contents of a database must conform. One important constraint\nis mapping cardinalities, which express the number of entities to which another en-\ntity can be associated via a relationship set. For example, if each account must belong\nto only one customer, the E-R model can express that constraint.\nThe entity-relationship model is widely used in database design, and Chapter 2\nexplores it in detail.\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\n1. Introduction\nText\n19\n© The McGraw−Hill \nCompanies, 2001\n1.4\nData Models\n9\ncustomer-name\ncustomer-street\ncustomer-id\ncustomer-city\ncustomer\nbalance\naccount\ndepositor\naccount-number\nFigure 1.2\nA sample E-R diagram.\n1.4.2\nRelational Model\nThe relational model uses a collection of tables to represent both data and the rela-\ntionships among those data. Each table has multiple columns, and each column has\na unique name. Figure 1.3 presents a sample relational database comprising three ta-\nbles: One shows details of bank customers, the second shows accounts, and the third\nshows which accounts belong to which customers.\nThe ﬁrst table, the customer table, shows, for example, that the customer identiﬁed\nby customer-id 192-83-7465 is named Johnson and lives at 12 Alma St. in Palo Alto.\nThe second table, account, shows, for example, that account A-101 has a balance of\n$500, and A-201 has a balance of $900.\nThe third table shows which accounts belong to which customers. For example,\naccount number A-101 belongs to the customer whose customer-id is 192-83-7465,\nnamely Johnson, and customers 192-83-7465 (Johnson) and 019-28-3746 (Smith) share\naccount number A-201 (they may share a business venture).\nThe relational model is an example of a record-based model. Record-based mod-\nels are so named because the database is structured in ﬁxed-format records of several\ntypes. Each table contains records of a particular type. Each record type deﬁnes a\nﬁxed number of ﬁelds, or attributes. The columns of the table correspond to the at-\ntributes of the record type.\nIt is not hard to see how tables may be stored in ﬁles. For instance, a special\ncharacter (such as a comma) may be used to delimit the different attributes of a\nrecord, and another special character (such as a newline character) may be used to\ndelimit records. The relational model hides such low-level implementation details\nfrom database developers and users.\nThe relational data model is the most widely used data model, and a vast majority\nof current database systems are based on the relational model. Chapters 3 through 7\ncover the relational model in detail.\nThe relational model is at a lower level of abstraction than the E-R model. Database\ndesigns are often carried out in the E-R model, and then translated to the relational\nmodel; Chapter 2 describes the translation process. For example, it is easy to see that\nthe tables customer and account correspond to the entity sets of the same name, while\nthe table depositor corresponds to the relationship set depositor.\nWe also note that it is possible to create schemas in the relational model that have\nproblems such as unnecessarily duplicated information. For example, suppose we\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\n1. Introduction\nText\n20\n© The McGraw−Hill \nCompanies, 2001\n10\nChapter 1\nIntroduction\ncustomer-id\ncustomer-name\ncustomer-street\ncustomer-city\n192-83-7465\nJohnson\n12 Alma St.\nPalo Alto\n019-28-3746\nSmith\n4 North St.\nRye\n677-89-9011\nHayes\n3 Main St.\nHarrison\n182-73-6091\nTurner\n123 Putnam Ave.\nStamford\n321-12-3123\nJones\n100 Main St.\nHarrison\n336-66-9999\nLindsay\n175 Park Ave.\nPittsfield\n019-28-3746\nSmith\n72 North St.\nRye\n(a) The customer table\naccount-number\nbalance\nA-101\n500\nA-215\n700\nA-102\n400\nA-305\n350\nA-201\n900\nA-217\n750\nA-222\n700\n(b) The account table\ncustomer-id\naccount-number\n192-83-7465\nA-101\n192-83-7465\nA-201\n019-28-3746\nA-215\n677-89-9011\nA-102\n182-73-6091\nA-305\n321-12-3123\nA-217\n336-66-9999\nA-222\n019-28-3746\nA-201\n(c) The depositor table\nFigure 1.3\nA sample relational database.\nstore account-number as an attribute of the customer record. Then, to represent the fact\nthat accounts A-101 and A-201 both belong to customer Johnson (with customer-id\n192-83-7465), we would need to store two rows in the customer table. The values for\ncustomer-name, customer-street, and customer-city for Johnson would get unneces-\nsarily duplicated in the two rows. In Chapter 7, we shall study how to distinguish\ngood schema designs from bad schema designs.\n1.4.3\nOther Data Models\nThe object-oriented data model is another data model that has seen increasing atten-\ntion. The object-oriented model can be seen as extending the E-R model with notions\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\n1. Introduction\nText\n21\n© The McGraw−Hill \nCompanies, 2001\n1.5\nDatabase Languages\n11\nof encapsulation, methods (functions), and object identity. Chapter 8 examines the\nobject-oriented data model.\nThe object-relational data model combines features of the object-oriented data\nmodel and relational data model. Chapter 9 examines it.\nSemistructured data models permit the speciﬁcation of data where individual data\nitems of the same type may have different sets of attributes. This is in contrast with\nthe data models mentioned earlier, where every data item of a particular type must\nhave the same set of attributes. The extensible markup language (XML) is widely\nused to represent semistructured data. Chapter 10 covers it.\nHistorically, two other data models, the network data model and the hierarchical\ndata model, preceded the relational data model. These models were tied closely to\nthe underlying implementation, and complicated the task of modeling data. As a\nresult they are little used now, except in old database code that is still in service in\nsome places. They are outlined in Appendices A and B, for interested readers.\n1.5\nDatabase Languages\nA database system provides a data deﬁnition language to specify the database sche-\nma and a data manipulation language to express database queries and updates. In\npractice, the data deﬁnition and data manipulation languages are not two separate\nlanguages; instead they simply form parts of a single database language, such as the\nwidely used SQL language.\n1.5.1\nData-Deﬁnition Language\nWe specify a database schema by a set of deﬁnitions expressed by a special language\ncalled a data-deﬁnition language (DDL).\nFor instance, the following statement in the SQL language deﬁnes the account table:\ncreate table account\n(account-number char(10),\nbalance integer)\nExecution of the above DDL statement creates the account table. In addition, it up-\ndates a special set of tables called the data dictionary or data directory.\nA data dictionary contains metadata—that is, data about data. The schema of a ta-\nble is an example of metadata. A database system consults the data dictionary before\nreading or modifying actual data.\nWe specify the storage structure and access methods used by the database system\nby a set of statements in a special type of DDL called a data storage and deﬁnition lan-\nguage. These statements deﬁne the implementation details of the database schemas,\nwhich are usually hidden from the users.\nThe data values stored in the database must satisfy certain consistency constraints.\nFor example, suppose the balance on an account should not fall below $100. The DDL\nprovides facilities to specify such constraints. The database systems check these con-\nstraints every time the database is updated.\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\n1. Introduction\nText\n22\n© The McGraw−Hill \nCompanies, 2001\n12\nChapter 1\nIntroduction\n1.5.2\nData-Manipulation Language\nData manipulation is\n• The retrieval of information stored in the database\n• The insertion of new information into the database\n• The deletion of information from the database\n• The modiﬁcation of information stored in the database\nA data-manipulation language (DML) is a language that enables users to access\nor manipulate data as organized by the appropriate data model. There are basically\ntwo types:\n• Procedural DMLs require a user to specify what data are needed and how to\nget those data.\n• Declarative DMLs (also referred to as nonprocedural DMLs) require a user to\nspecify what data are needed without specifying how to get those data.\nDeclarative DMLs are usually easier to learn and use than are procedural DMLs.\nHowever, since a user does not have to specify how to get the data, the database\nsystem has to ﬁgure out an efﬁcient means of accessing data. The DML component of\nthe SQL language is nonprocedural.\nA query is a statement requesting the retrieval of information. The portion of a\nDML that involves information retrieval is called a query language. Although tech-\nnically incorrect, it is common practice to use the terms query language and data-\nmanipulation language synonymously.\nThis query in the SQL language ﬁnds the name of the customer whose customer-id\nis 192-83-7465:\nselect customer.customer-name\nfrom customer\nwhere customer.customer-id = 192-83-7465\nThe query speciﬁes that those rows from the table customer where the customer-id is\n192-83-7465 must be retrieved, and the customer-name attribute of these rows must be\ndisplayed. If the query were run on the table in Figure 1.3, the name Johnson would\nbe displayed.\nQueries may involve information from more than one table. For instance, the fol-\nlowing query ﬁnds the balance of all accounts owned by the customer with customer-\nid 192-83-7465.\nselect account.balance\nfrom depositor, account\nwhere depositor.customer-id = 192-83-7465 and\ndepositor.account-number = account.account-number\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\n1. Introduction\nText\n23\n© The McGraw−Hill \nCompanies, 2001\n1.6\nDatabase Users and Administrators\n1\n\nrs at the base, and a shared-\nnothing architecture at the top, with possibly a shared-disk architecture in the mid-\ndle. Figure 18.8d illustrates a hierarchical architecture with shared-memory nodes\nconnected together in a shared-nothing architecture. Commercial parallel database\nsystems today run on several of these architectures.\nAttempts to reduce the complexity of programming such systems have yielded\ndistributed virtual-memory architectures, where logically there is a single shared\nmemory, but physically there are multiple disjoint memory systems; the virtual-\nmemory-mapping hardware, coupled with system software, allows each processor\nto view the disjoint memories as a single virtual memory. Since access speeds differ,\ndepending on whether the page is available locally or not, such an architecture is also\nreferred to as a nonuniform memory architecture (NUMA).\n18.4\nDistributed Systems\nIn a distributed database system, the database is stored on several computers. The\ncomputers in a distributed system communicate with one another through various\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nVI. Database System \nArchitecture\n18. Database System \nArchitecture\n695\n© The McGraw−Hill \nCompanies, 2001\n698\nChapter 18\nDatabase System Architectures\ncommunication media, such as high-speed networks or telephone lines. They do not\nshare main memory or disks. The computers in a distributed system may vary in size\nand function, ranging from workstations up to mainframe systems.\nThe computers in a distributed system are referred to by a number of different\nnames, such as sites or nodes, depending on the context in which they are mentioned.\nWe mainly use the term site, to emphasize the physical distribution of these systems.\nThe general structure of a distributed system appears in Figure 18.9.\nThe main differences between shared-nothing parallel databases and distributed\ndatabases are that distributed databases are typically geographically separated, are\nseparately administered, and have a slower interconnection. Another major differ-\nence is that, in a distributed database system, we differentiate between local and\nglobal transactions. A local transaction is one that accesses data only from sites\nwhere the transaction was initiated. A global transaction, on the other hand, is one\nthat either accesses data in a site different from the one at which the transaction was\ninitiated, or accesses data in several different sites.\nThere are several reasons for building distributed database systems, including\nsharing of data, autonomy, and availability.\n• Sharing data. The major advantage in building a distributed database system\nis the provision of an environment where users at one site may be able to\naccess the data residing at other sites. For instance, in a distributed banking\nsystem, where each branch stores data related to that branch, it is possible for\na user in one branch to access data in another branch. Without this capability,\na user wishing to transfer funds from one branch to another would have to\nresort to some external mechanism that would couple existing systems.\n• Autonomy. The primary advantage of sharing data by means of data distri-\nbution is that each site is able to retain a degree of control over data that\nsite A\nsite C\nsite B\ncommunication\nvia network\nnetwork\nFigure 18.9\nA distributed system.\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nVI. Database System \nArchitecture\n18. Database System \nArchitecture\n696\n© The McGraw−Hill \nCompanies, 2001\n18.4\nDistributed Systems\n699\nare stored locally. In a centralized system, the database administrator of the\ncentral site controls the database. In a distributed system, there is a global\ndatabase administrator responsible for the entire system. A part of these re-\nsponsibilities is delegated to the local database administrator for each site.\nDepending on the design of the distributed database system, each adminis-\ntrator may have a different degree of local autonomy. The possibility of local\nautonomy is often a major advantage of distributed databases.\n• Availability. If one site fails in a distributed system, the remaining sites may\nbe able to continue operating. In particular, if data items are replicated in sev-\neral sites, a transaction needing a particular data item may ﬁnd that item in\nany of several sites. Thus, the failure of a site does not necessarily imply the\nshutdown of the system.\nThe failure of one site must be detected by the system, and appropriate\naction may be needed to recover from the failure. The system must no longer\nuse the services of the failed site. Finally, when the failed site recovers or is\nrepaired, mechanisms must be available to integrate it smoothly back into the\nsystem.\nAlthough recovery from failure is more complex in distributed systems\nthan in centralized systems, the ability of most of the system to continue to\noperate despite the failure of one site results in increased availability. Avail-\nability is crucial for database systems used for real-time applications. Loss of\naccess to data by, for example, an airline may result in the loss of potential\nticket buyers to competitors.\n18.4.1\nAn Example of a Distributed Database\nConsider a banking system consisting of four branches in four different cities. Each\nbranch has its own computer, with a database of all the accounts maintained at that\nbranch. Each such installation is thus a site. There also exists one single site that\nmaintains information about all the branches of the bank. Each branch maintains\n(among others) a relation account(Account-schema), where\nAccount-schema = (account-number, branch-name, balance)\nThe site containing information about all the branches of the bank maintains the re-\nlation branch(Branch-schema), where\nBranch-schema = (branch-name, branch-city, assets)\nThere are other relations maintained at the various sites; we ignore them for the pur-\npose of our example.\nTo illustrate the difference between the two types of transactions—local and\nglobal—at the sites, consider a transaction to add $50 to account number A-177\nlocated at the Valleyview branch. If the transaction was initiated at the Valleyview\nbranch, then it is considered local; otherwise, it is considered global. A transaction\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nVI. Database System \nArchitecture\n18. Database System \nArchitecture\n697\n© The McGraw−Hill \nCompanies, 2001\n700\nChapter 18\nDatabase System Architectures\nto transfer $50 from account A-177 to account A-305, which is located at the Hillside\nbranch, is a global transaction, since accounts in two different sites are accessed as a\nresult of its execution.\nIn an ideal distributed database system, the sites would share a common global\nschema (although some relations may be stored only at some sites), all sites would\nrun the same distributed database-management software, and the sites would be\naware of each other’s existence. If a distributed database is built from scratch, it\nwould indeed be possible to achieve the above goals. However, in reality a dis-\ntributed database has to be constructed by linking together multiple already-existing\ndatabase systems, each with its own schema and possibly running different database-\nmanagement software. Such systems are sometimes called multidatabase systems\nor heterogeneous distributed database systems. We discuss these systems in Sec-\ntion 19.8, where we show how to achieve a degree of global control despite the het-\nerogeneity of the component systems.\n18.4.2\nImplementation Issues\nAtomicity of transactions is an important issue in building a distributed database sys-\ntem. If a transaction runs across two sites, unless the system designers are careful, it\nmay commit at one site and abort at another, leading to an inconsistent state. Trans-\naction commit protocols ensure such a situation cannot arise. The two-phase commit\nprotocol (2PC) is the most widely used of these protocols.\nThe basic idea behind 2PC is for each site to execute the transaction till just before\ncommit, and then leave the commit decision to a single coordinator site; the trans-\naction is said to be in the ready state at a site at this point. The coordinator decides\nto commit the transaction only if the transaction reaches the ready state at every site\nwhere it executed; otherwise (for example, if the transaction aborts at any site), the\ncoordinator decides to abort the transaction. Every site where the transaction exe-\ncuted must follow the decision of the coordinator. If a site fails when a transaction is\nin ready state, when the site recovers from failure it should be in a position to either\ncommit or abort the transaction, depending on the decision of the coordinator. The\n2PC protocol is described in detail in Section 19.4.1.\nConcurrency control is another issue in a distributed database. Since a transac-\ntion may access data items at several sites, transaction managers at several sites may\nneed to coordinate to implement concurrency control. If locking is used (as is almost\nalways the case in practice), locking can be performed locally at the sites containing\naccessed data items, but there is also a possibility of deadlock involving transactions\noriginating at multiple sites. Therefore deadlock detection needs to be carried out\nacross multiple sites. Failures are more common in distributed systems since not only\nmay computers fail, but communication links may also fail. Replication of data items,\nwhich is the key to the continued functioning of distributed databases when failures\noccur, further complicates concurrency control. Section 19.5 provides detailed cover-\nage of concurrency control in distributed databases.\nThe standard transaction models, based on multiple actions carried out by a single\nprogram unit, are often inappropriate for carrying out tasks that cross the boundaries\nof databases that cannot or will not cooperate to implement protocols such as 2PC.\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nVI. Database System \nArchitecture\n18. Database System \nArchitecture\n698\n© The McGraw−Hill \nCompanies, 2001\n18.5\nNetwork Types\n701\nAlternative approaches, based on persistent messaging for communication, are gener-\nally used for such tasks.\nWhen the tasks to be carried out are complex, involving multiple databases and/or\nmultiple interactions with humans, coordination of the tasks and ensuring transac-\ntion properties for the tasks become more complicated. Workﬂow management systems\nare systems designed to help with carrying out such tasks. Section 19.4.3 describes\npersistent messaging, while Section 24.2 describes workﬂow management systems.\nIn case an organization has to choose between a distributed architecture and a\ncentralized architecture for implementing an application, the system architect must\nbalance the advantages against the disadvantages of distribution of data. We have al-\nready seen the advantages of using distributed databases. The primary disadvantage\nof distributed database systems is the added complexity required to ensure proper\ncoordination among the sites. This increased complexity takes various forms:\n• Software-development cost. It is more difﬁcult to implement a distributed\ndatabase system; thus, it is more costly.\n• Greater potential for bugs. Since the sites that constitute the distributed sys-\ntem operate in parallel, it is harder to ensure the correctness of algorithms,\nespecially operation during failures of part of the system, and recovery from\nfailures. The potential exists for extremely subtle bugs.\n• Increased processing overhead. The exchange of messages and the additional\ncomputation required to achieve intersite coordination are a form of overhead\nthat does not arise in centralized systems.\nThere are several approaches to distributed database design, ranging from fully\ndistributed designs to ones that include a large degree of centralization. We study\nthem in Chapter 19.\n18.5\nNetwork Types\nDistributed databases and client–server systems are built around communication\nnetworks. There are basically two types of networks: local-area networks and wide-\narea networks. The main difference between the two is the way in which they are\ndistributed geographically. In local-area networks, processors are distributed over\nsmall geographical areas, such as a single building or a number of adjacent build-\nings. In wide-area networks, on the other hand, a number of autonomous processors\nare distributed over a large geographical area (such as the United States or the en-\ntire world). These differences imply major variations in the speed and reliability of\nthe communication network, and are reﬂected in the distributed operating-system\ndesign.\n18.5.1\nLocal-Area Networks\nLocal-area networks (LANs) (Figure 18.10) emerged in the early 1970s as a way\nfor computers to communicate and to share data with one another. People recog-\nnized that, for many enterprises, numerous small computers, each with its own self-\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nVI. Database System \nArchitecture\n18. Database System \nArchitecture\n699\n© The McGraw−Hill \nCompanies, 2001\n702\nChapter 18\nDatabase System Architectures\nworkstation\nprinter\nCPU server\nprocessors\nComputer-System Structures\nPC\nworkstation\nfile server\nprocessors\ngateway\nFigure 18.10\nLocal-area network.\ncontained applications, are more economical than a single large system. Because each\nsmall computer is likely to need access to a full complement of peripheral devices\n(such as disks and printers), and because some form of data sharing is likely to oc-\ncur in a single enterprise, it was a natural step to connect these small systems into a\nnetwork.\nLANs are generally used in an ofﬁce environment. All the sites in such systems\nare close to one another, so the communication links tend to have a higher speed and\nlower error rate than do their counterparts in wide-area networks. The most common\nlinks in a local-area network are twisted pair, coaxial cable, ﬁber optics, and, increas-\ningly, wireless connections. Communication speeds range from a few megabits per\nsecond (for wireless local-area networks), to 1 gigabit per second for Gigabit Ether-\nnet. Standard Ethernet runs at 10 megabits per second, while Fast Ethernet run at 100\nmegabits per second.\nA storage-area network (SAN) is a special type of high-speed local-area network\ndesigned to connect large banks of storage devices (disks) to computers that use the\ndata. Thus storage-area networks help build large-scale shared-disk systems. The moti-\nvation for using storage-area networks to connect multiple computers to large banks\nof storage devices is essentially the same as that for shared-disk databases, namely\n• Scalability by adding more computers\n• High availability, since data is still accessible even if a computer fails\nRAID organizations are used in the storage devices to ensure high availability of the\ndata, permitting processing to continue even if individual disks fail. Storage area\nnetworks are usually built with redundancy, such as multiple paths between nodes,\nso if a component such as a link or a connection to the network fails, the network\ncontinues to function.\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nVI. Database System \nArchitecture\n18. Database System \nArchitecture\n700\n© The McGraw−Hill \nCompanies, 2001\n18.6\nSummary\n703\n18.5.2\nWide-Area Networks\nWide-area networks (WANs) emerged in the late 1960s, mainly as an academic re-\nsearch project to provide efﬁcient communication among sites, allowing hardware\nand software to be shared conveniently and economically by a wide community of\nusers. Systems that allowed remote terminals to be connected to a central computer\nvia telephone lines were developed in the early 1960s, but they were not true WANs.\nThe ﬁrst WAN to be designed and developed was the Arpanet. Work on the Arpanet\nbegan in 1968. The Arpanet has grown from a four-site experimental network to a\nworldwide network of networks, the Internet, comprising hundreds of millions of\ncomputer systems. Typical links on the Internet are ﬁber-optic lines and, sometimes,\nsatellite channels. Data rates for wide-area links typically range from a few megabits\nper second to hundreds of gigabits per second. The last link, to end user sites, is of-\nten based on digital subscriber loop (DSL) technology supporting a few megabits per\nsecond), or cable modem (supporting 10 megabits per second), or dial-up modem\nconnections over phone lines (supporting up to 56 kilobits per second).\nWANs can be classiﬁed into two types:\n• In discontinuous connection WANs, such as those based on wireless connec-\ntions, hosts are connected to the network only part of the time.\n• In continuous connection WANs, such as the wired Internet, hosts are con-\nnected to the network at all times.\nNetworks that are not continuously connected typically do not allow transactions\nacross sites, but may keep local copies of remote data, and refresh the copies peri-\nodically (every night, for instance). For applications where consistency is not critical,\nsuch as sharing of documents, groupware systems such as Lotus Notes allow up-\ndates of remote data to be made locally, and the updates are then propagated back\nto the remote site periodically. There is a potential for conﬂicting updates at differ-\nent sites, conﬂicts that have to be detected and resolved. A mechanism for detecting\nconﬂicting updates is described later, in Section 23.5.4; the resolution mechanism for\nconﬂicting updates is, however, application dependent.\n18.6\nSummary\n• Centralized database systems run entirely on a single computer. With the\ngrowth of personal computers and local-area networking, the database front-\nend functionality has moved increasingly to clients, with server systems pro-\nviding the back-end functionality. Client–server interface protocols have\nhelped the growth of client–server database systems.\n• Servers can be either transaction servers or data servers, although the use\nof transaction servers greatly exceeds the use of data servers for providing\ndatabase services.\n\u0000 Transaction servers have multiple processes, possibly running on multiple\nprocessors. So that these processes have access to common data, such as\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nVI. Database System \nArchitecture\n18. Database System \nArchitecture\n701\n© The McGraw−Hill \nCompanies, 2001\n704\nChapter 18\nDatabase System Architectures\nthe database buffer, systems store such data in shared memory. In addition\nto processes that handle queries, there are system processes that carry out\ntasks such as lock and log management and checkpointing.\n\u0000 Data server systems supply raw data to clients. Such systems strive to\nminimize communication between clients and servers by caching data\nand locks at the clients. Parallel database systems use similar optimiza-\ntions.\n• Parallel database systems consist of multiple processors and multiple disks\nconnected by a fast interconnection network. Speedup measures how much\nwe can increase processing speed by increasing parallelism, for a single trans-\naction. Scaleup measures how well we can handle an increased number of\ntransactions by increasing parallelism. Interference, skew, and start–up costs\nact as barriers to getting ideal speedup and scaleup.\n• Parallel database architectures include the shared-memory, shared-disk,\nshared-nothing, and hierarchical architectures. These architectures have dif-\nferent tradeoffs of scalability versus communication speed.\n• A distributed database is a collection of partially independent databases that\n(ideally) share a common schema, and coordinate processing of transactions\nthat access nonlocal data. The processors communicate with one another thro-\nugh a communication network that handles routing and connection strategies.\n• Principally, there ar",
    "precision": 0.15991983967935872,
    "recall": 0.973170731707317,
    "iou": 0.15921787709497207,
    "f1": 0.27469879518072293,
    "gold_tokens_count": 410,
    "retrieved_tokens_count": 2495,
    "intersection_tokens": 399
  },
  {
    "config_name": "chars_weighted",
    "config": {
      "name": "chars_weighted",
      "index_prefix": "textbook_index",
      "chunking_strategy": "chars",
      "overlap": 0,
      "fusion": "weighted",
      "bm25_weight": 0.3,
      "tag_weight": 0.2,
      "top_k": 5,
      "embed_model": "sentence-transformers/all-MiniLM-L6-v2"
    },
    "question": "What are the components of a query processor?",
    "gold_text": "The query processor components include • DDL interpreter, which interprets DDL statements and records the definitions in the data dictionary. • DML compiler, which translates DML statements in a query language into an evaluation plan consisting of low-level instructions that the query evaluation engine understands. A query can usually be translated into any of a number of alternative eval- uation plans that all give the same result. The DML compiler also performs query optimization, that is, it picks the lowest cost evaluation plan from amo- ng the alternatives. • Query evaluation engine, which executes low-level instructions generated by the DML compiler.",
    "retrieved_text": "3\nIf the above query were run on the tables in Figure 1.3, the system would ﬁnd that\nthe two accounts numbered A-101 and A-201 are owned by customer 192-83-7465\nand would print out the balances of the two accounts, namely 500 and 900.\nThere are a number of database query languages in use, either commercially or\nexperimentally. We study the most widely used query language, SQL, in Chapter 4.\nWe also study some other query languages in Chapter 5.\nThe levels of abstraction that we discussed in Section 1.3 apply not only to deﬁning\nor structuring data, but also to manipulating data. At the physical level, we must\ndeﬁne algorithms that allow efﬁcient access to data. At higher levels of abstraction,\nwe emphasize ease of use. The goal is to allow humans to interact efﬁciently with the\nsystem. The query processor component of the database system (which we study in\nChapters 13 and 14) translates DML queries into sequences of actions at the physical\nlevel of the database system.\n1.5.3\nDatabase Access from Application Programs\nApplication programs are programs that are used to interact with the database. Ap-\nplication programs are usually written in a host language, such as Cobol, C, C++, or\nJava. Examples in a banking system are programs that generate payroll checks, debit\naccounts, credit accounts, or transfer funds between accounts.\nTo access the database, DML statements need to be executed from the host lan-\nguage. There are two ways to do this:\n• By providing an application program interface (set of procedures) that can\nbe used to send DML and DDL statements to the database, and retrieve the\nresults.\nThe Open Database Connectivity (ODBC) standard deﬁned by Microsoft\nfor use with the C language is a commonly used application program inter-\nface standard. The Java Database Connectivity (JDBC) standard provides cor-\nresponding features to the Java language.\n• By extending the host language syntax to embed DML calls within the host\nlanguage program. Usually, a special character prefaces DML calls, and a pre-\nprocessor, called the DML precompiler, converts the DML statements to nor-\nmal procedure calls in the host language.\n1.6\nDatabase Users and Administrators\nA primary goal of a database system is to retrieve information from and store new\ninformation in the database. People who work with a database can be categorized as\ndatabase users or database administrators.\n1.6.1\nDatabase Users and User Interfaces\nThere are four different types of database-system users, differentiated by the way\nthey expect to interact with the system. Different types of user interfaces have been\ndesigned for the different types of users.\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\n1. Introduction\nText\n24\n© The McGraw−Hill \nCompanies, 2001\n14\nChapter 1\nIntroduction\n• Naive users are unsophisticated users who interact with the system by invok-\ning one of the application programs that have been written previously. For\nexample, a bank teller who needs to transfer $50 from account A to account B\ninvokes a program called transfer. This program asks the teller for the amount\nof money to be transferred, the account from which the money is to be trans-\nferred, and the account to which the money is to be transferred.\nAs another example, consider a user who wishes to ﬁnd her account bal-\nance over the World Wide Web. Such a user may access a form, where she\nenters her account number. An application program at the Web server then\nretrieves the account balance, using the given account number, and passes\nthis information back to the user.\nThe typical user interface for naive users is a forms interface, where the\nuser can ﬁll in appropriate ﬁelds of the form. Naive users may also simply\nread reports generated from the database.\n• Application programmers are computer professionals who write application\nprograms. Application programmers can choose from many tools to develop\nuser interfaces. Rapid application development (RAD) tools are tools that en-\nable an application programmer to construct forms and reports without writ-\ning a program. There are also special types of programming languages that\ncombine imperative control structures (for example, for loops, while loops\nand if-then-else statements) with statements of the data manipulation lan-\nguage. These languages, sometimes called fourth-generation languages, often\ninclude special features to facilitate the generation of forms and the display of\ndata on the screen. Most major commercial database systems include a fourth-\ngeneration language.\n• Sophisticated users interact with the system without writing programs. In-\nstead, they form their requests in a database query language. They submit\neach such query to a query processor, whose function is to break down DML\nstatements into instructions that the storage manager understands. Analysts\nwho submit queries to explore data in the database fall in this category.\nOnline analytical processing (OLAP) tools simplify analysts’ tasks by let-\nting them view summaries of data in different ways. For instance, an analyst\ncan see total sales by region (for example, North, South, East, and West), or by\nproduct, or by a combination of region and product (that is, total sales of each\nproduct in each region). The tools also permit the analyst to select speciﬁc re-\ngions, look at data in more detail (for example, sales by city within a region)\nor look at the data in less detail (for example, aggregate products together by\ncategory).\nAnother class of tools for analysts is data mining tools, which help them\nﬁnd certain kinds of patterns in data.\nWe study OLAP tools and data mining in Chapter 22.\n• Specialized users are sophisticated users who write specialized database\napplications that do not ﬁt into the traditional data-processing framework.\nAmong these applications are computer-aided design systems, knowledge-\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\n1. Introduction\nText\n25\n© The McGraw−Hill \nCompanies, 2001\n1.7\nTransaction Management\n15\nbase and expert systems, systems that store data with complex data types (for\nexample, graphics data and audio data), and environment-modeling systems.\nChapters 8 and 9 cover several of these applications.\n1.6.2\nDatabase Administrator\nOne of the main reasons for using DBMSs is to have central control of both the data\nand the programs that access those data. A person who has such central control over\nthe system is called a database administrator (DBA). The functions of a DBA include:\n• Schema deﬁnition. The DBA creates the original database schema by execut-\ning a set of data deﬁnition statements in the DDL.\n• Storage structure and access-method deﬁnition.\n• Schema and physical-organization modiﬁcation. The DBA carries out chang-\nes to the schema and physical organization to reﬂect the changing needs of the\norganization, or to alter the physical organization to improve performance.\n• Granting of authorization for data access. By granting different types of\nauthorization, the database administrator can regulate which parts of the data-\nbase various users can access. The authorization information is kept in a\nspecial system structure that the database system consults whenever some-\none attempts to access the data in the system.\n• Routine maintenance. Examples of the database administrator’s routine\nmaintenance activities are:\n\u0000 Periodically backing up the database, either onto tapes or onto remote\nservers, to prevent loss of data in case of disasters such as ﬂooding.\n\u0000 Ensuring that enough free disk space is available for normal operations,\nand upgrading disk space as required.\n\u0000 Monitoring jobs running on the database and ensuring that performance\nis not degraded by very expensive tasks submitted by some users.\n1.7\nTransaction Management\nOften, several operations on the database form a single logical unit of work. An ex-\nample is a funds transfer, as in Section 1.2, in which one account (say A) is debited and\nanother account (say B) is credited. Clearly, it is essential that either both the credit\nand debit occur, or that neither occur. That is, the funds transfer must happen in its\nentirety or not at all. This all-or-none requirement is called atomicity. In addition, it\nis essential that the execution of the funds transfer preserve the consistency of the\ndatabase. That is, the value of the sum A + B must be preserved. This correctness\nrequirement is called consistency. Finally, after the successful execution of a funds\ntransfer, the new values of accounts A and B must persist, despite the possibility of\nsystem failure. This persistence requirement is called durability.\nA transaction is a collection of operations that performs a single logical function\nin a database application. Each transaction is a unit of both atomicity and consis-\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\n1. Introduction\nText\n26\n© The McGraw−Hill \nCompanies, 2001\n16\nChapter 1\nIntroduction\ntency. Thus, we require that transactions do not violate any database-consistency\nconstraints. That is, if the database was consistent when a transaction started, the\ndatabase must be consistent when the transaction successfully terminates. However,\nduring the execution of a transaction, it may be necessary temporarily to allow incon-\nsistency, since either the debit of A or the credit of B must be done before the other.\nThis temporary inconsistency, although necessary, may lead to difﬁculty if a failure\noccurs.\nIt is the programmer’s responsibility to deﬁne properly the various transactions,\nso that each preserves the consistency of the database. For example, the transaction to\ntransfer funds from account A to account B could be deﬁned to be composed of two\nseparate programs: one that debits account A, and another that credits account B. The\nexecution of these two programs one after the other will indeed preserve consistency.\nHowever, each program by itself does not transform the database from a consistent\nstate to a new consistent state. Thus, those programs are not transactions.\nEnsuring the atomicity and durability properties is the responsibility of the data-\nbase system itself—speciﬁcally, of the transaction-management component. In the\nabsence of failures, all transactions complete successfully, and atomicity is achieved\neasily. However, because of various types of failure, a transaction may not always\ncomplete its execution successfully. If we are to ensure the atomicity property, a failed\ntransaction must have no effect on the state of the database. Thus, the database must\nbe restored to the state in which it was before the transaction in question started exe-\ncuting. The database system must therefore perform failure recovery, that is, detect\nsystem failures and restore the database to the state that existed prior to the occur-\nrence of the failure.\nFinally, when several transactions update the database concurrently, the consis-\ntency of data may no longer be preserved, even though each individual transac-\ntion is correct. It is the responsibility of the concurrency-control manager to control\nthe interaction among the concurrent transactions, to ensure the consistency of the\ndatabase.\nDatabase systems designed for use on small personal computers may not have\nall these features. For example, many small systems allow only one user to access\nthe database at a time. Others do not offer backup and recovery, leaving that to the\nuser. These restrictions allow for a smaller data manager, with fewer requirements for\nphysical resources—especially main memory. Although such a low-cost, low-feature\napproach is adequate for small personal databases, it is inadequate for a medium- to\nlarge-scale enterprise.\n1.8\nDatabase System Structure\nA database system is partitioned into modules that deal with each of the responsi-\nbilites of the overall system. The functional components of a database system can be\nbroadly divided into the storage manager and the query processor components.\nThe storage manager is important because databases typically require a large\namount of storage space. Corporate databases range in size from hundreds of gi-\ngabytes to, for the largest databases, terabytes of data. A gigabyte is 1000 megabytes\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\n1. Introduction\nText\n27\n© The McGraw−Hill \nCompanies, 2001\n1.8\nDatabase System Structure\n17\n(1 billion bytes), and a terabyte is 1 million megabytes (1 trillion bytes). Since the\nmain memory of computers cannot store this much information, the information is\nstored on disks. Data are moved between disk storage and main memory as needed.\nSince the movement of data to and from disk is slow relative to the speed of the cen-\ntral processing unit, it is imperative that the database system structure the data so as\nto minimize the need to move data between disk and main memory.\nThe query processor is important because it helps the database system simplify\nand facilitate access to data. High-level views help to achieve this goal; with them,\nusers of the system are not be burdened unnecessarily with the physical details of the\nimplementation of the system. However, quick processing of updates and queries\nis important. It is the job of the database system to translate updates and queries\nwritten in a nonprocedural language, at the logical level, into an efﬁcient sequence of\noperations at the physical level.\n1.8.1\nStorage Manager\nA storage manager is a program module that provides the interface between the low-\nlevel data stored in the database and the application programs and queries submit-\nted to the system. The storage manager is responsible for the interaction with the ﬁle\nmanager. The raw data are stored on the disk using the ﬁle system, which is usu-\nally provided by a conventional operating system. The storage manager translates\nthe various DML statements into low-level ﬁle-system commands. Thus, the storage\nmanager is responsible for storing, retrieving, and updating data in the database.\nThe storage manager components include:\n• Authorization and integrity manager, which tests for the satisfaction of in-\ntegrity constraints and checks the authority of users to access data.\n• Transaction manager, which ensures that the database remains in a consistent\n(correct) state despite system failures, and that concurrent transaction execu-\ntions proceed without conﬂicting.\n• File manager, which manages the allocation of space on disk storage and the\ndata structures used to represent information stored on disk.\n• Buffer manager, which is responsible for fetching data from disk storage into\nmain memory, and deciding what data to cache in main memory. The buffer\nmanager is a critical part of the database system, since it enables the database\nto handle data sizes that are much larger than the size of main memory.\nThe storage manager implements several data structures as part of the physical\nsystem implementation:\n• Data ﬁles, which store the database itself.\n• Data dictionary, which stores metadata about the structure of the database, in\nparticular the schema of the database.\n• Indices, which provide fast access to data items that hold particular values.\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\n1. Introduction\nText\n28\n© The McGraw−Hill \nCompanies, 2001\n18\nChapter 1\nIntroduction\n1.8.2\nThe Query Processor\nThe query processor components include\n• DDL interpreter, which interprets DDL statements and records the deﬁnitions\nin the data dictionary.\n• DML compiler, which translates DML statements in a query language into an\nevaluation plan consisting of low-level instructions that the query evaluation\nengine understands.\nA query can usually be translated into any of a number of alternative eval-\nuation plans that all give the same result. The DML compiler also performs\nquery optimization, that is, it picks the lowest cost evaluation plan from amo-\nng the alternatives.\n• Query evaluation engine, which executes low-level instructions generated by\nthe DML compiler.\nFigure 1.4 shows these components and the connections among them.\n1.9\nApplication Architectures\nMost users of a database system today are not present at the site of the database\nsystem, but connect to it through a network. We can therefore differentiate between\nclient machines, on which remote database users work, and server machines, on\nwhich the database system runs.\nDatabase applications are usually partitioned into two or three parts, as in Fig-\nure 1.5. In a two-tier architecture, the application is partitioned into a component\nthat resides at the client machine, which invokes database system functionality at the\nserver machine through query language statements. Application program interface\nstandards like ODBC and JDBC are used for interaction between the client and the\nserver.\nIn contrast, in a three-tier architecture, the client machine acts as merely a front\nend and does not contain any direct database calls. Instead, the client end communi-\ncates with an application server, usually through a forms interface. The application\nserver in turn communicates with a database system to access data. The business\nlogic of the application, which says what actions to carry out under what conditions,\nis embedded in the application server, instead of being distributed across multiple\nclients. Three-tier applications are more appropriate for large applications, and for\napplications that run on the World Wide Web.\n1.10\nHistory of Database Systems\nData processing drives the growth of computers, as it has from the earliest days of\ncommercial computers. In fact, automation of data processing tasks predates com-\nputers. Punched cards, invented by Hollerith, were used at the very beginning of the\ntwentieth century to record U.S. census data, and mechanical systems were used to\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\n1. Introduction\nText\n29\n© The McGraw−Hill \nCompanies, 2001\n1.10\nHistory of Database Systems\n19\nnaive users\n(tellers, agents, \nweb-users) \nquery processor\nstorage manager\ndisk storage\nindices\nstatistical data\ndata\ndata dictionary\napplication\nprogrammers\napplication\ninterfaces\napplication\nprogram\nobject code\ncompiler and\nlinker\nbuffer manager\nfile manager\nauthorization\nand integrity\n manager\ntransaction\nmanager\nDML compiler \nand organizer\nquery evaluation\nengine\nDML queries\nDDL interpreter\napplication\nprograms\nquery\ntools\nadministration\ntools\nsophisticated\nusers\n(analysts)\ndatabase\nadministrator\nuse\nwrite\nuse\nuse\nFigure 1.4\nSystem structure.\nprocess the cards and tabulate results. Punched cards were later widely used as a\nmeans of entering data into computers.\nTechniques for data storage and processing have evolved over the years:\n• 1950s and early 1960s: Magnetic tapes were developed for data storage. Data\nprocessing tasks such as payroll were automated, with data stored on tapes.\nProcessing of data consisted of reading data from one or more tapes and\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\n1. Introduction\nText\n30\n© The McGraw−Hill \nCompanies, 2001\n20\nChapter 1\nIntroduction\nuser\napplication server\ndatabase system\nuser\ndatabase system\nb.  three-tier architecture\na.  two-tier architecture\nnetwork\nserver\nclient\napplication\nnetwork\napplication client\nFigure 1.5\nTwo-tier and three-tier architectures.\nwriting data to a new tape. Data could also be input from punched card decks,\nand output to printers. For example, salary raises were processed by entering\nthe raises on punched cards and reading the punched card deck in synchro-\nnization with a tape containing the master salary details. The records had to\nbe in the same sorted order. The salary raises would be added to the salary\nread from the master tape, and written to a new tape; the new tape would\nbecome the new master tape.\nTapes (and card decks) could be read only sequentially, and data sizes were\nmuch larger than main memory; thus, data process\n\nies on local schemas at each of the sites where the query has to be exe-\ncuted. The query results have to be translated back into the global schema.\nThe task is simpliﬁed by writing wrappers for each data source, which pro-\nvide a view of the local data in the global schema. Wrappers also translate\nqueries on the global schema into queries on the local schema, and translate\nresults back into the global schema. Wrappers may be provided by individual\nsites, or may be written separately as part of the multidatabase system.\nWrappers can even be used to provide a relational view of nonrelational\ndata sources, such as Web pages (possibly with forms interfaces), ﬂat ﬁles,\nhierarchical and network databases, and directory systems.\n• Some data sources may provide only limited query capabilities; for instance,\nthey may support selections, but not joins. They may even restrict the form\nof selections, allowing selections only on certain ﬁelds; Web data sources with\nform interfaces are an example of such data sources. Queries may therefore\nhave to be broken up, to be partly performed at the data source and partly at\nthe site issuing the query.\n• In general, more than one site may need to be accessed to answer a given\nquery. Answers retrieved from the sites may have to be processed to remove\nduplicates. Suppose one site contains account tuples satisfying the selection\nbalance < 100, while another contains account tuples satisfying balance > 50.\nA query on the entire account relation would require access to both sites and\nremoval of duplicate answers resulting from tuples with balance between 50\nand 100, which are replicated at both sites.\n• Global query optimization in a heterogeneous database is difﬁcult, since the\nquery execution system may not know what the costs are of alternative query\nplans at different sites. The usual solution is to rely on only local-level opti-\nmization, and just use heuristics at the global level.\nMediator systems are systems that integrate multiple heterogeneous data sources,\nproviding an integrated global view of the data and providing query facilities on\nthe global view. Unlike full-ﬂedged multidatabase systems, mediator systems do not\nbother about transaction processing. (The terms mediator and multidatabase are of-\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nVI. Database System \nArchitecture\n19. Distributed Databases\n737\n© The McGraw−Hill \nCompanies, 2001\n19.9\nDirectory Systems\n741\nten used in an interchangeable fashion, and systems that are called mediators may\nsupport limited forms of transactions.) The term virtual database is used to refer\nto multidatabase/mediator systems, since they provide the appearance of a single\ndatabase with a global schema, although data exist on multiple sites in local schemas.\n19.9\nDirectory Systems\nConsider an organization that wishes to make data about its employees available to\na variety of people in the organization; example of the kinds of data would include\nname, designation, employee-id, address, email address, phone number, fax num-\nber, and so on. In the precomputerization days, organizations would create physical\ndirectories of employees and distribute them across the organization. Even today,\ntelephone companies create physical directories of customers.\nIn general, a directory is a listing of information about some class of objects such as\npersons. Directories can be used to ﬁnd information about a speciﬁc object, or in the\nreverse direction to ﬁnd objects that meet a certain requirement. In the world of phys-\nical telephone directories, directories that satisfy lookups in the forward direction are\ncalled white pages, while directories that satisfy lookups in the reverse direction are\ncalled yellow pages.\nIn today’s networked world, the need for directories is still present and, if any-\nthing, even more important. However, directories today need to be available over a\ncomputer network, rather than in a physical (paper) form.\n19.9.1\nDirectory Access Protocols\nDirectory information can be made available through Web interfaces, as many orga-\nnizations, and phone companies in particular do. Such interfaces are good for hu-\nmans. However, programs too, need to access directory information. Directories can\nbe used for storing other types of information, much like ﬁle system directories. For\ninstance, Web browsers can store personal bookmarks and other browser settings in\na directory system. A user can thus access the same settings from multiple locations,\nsuch as at home and at work, without having to share a ﬁle system.\nSeveral directory access protocols have been developed to provide a standardized\nway of accessing data in a directory. The most widely used among them today is the\nLightweight Directory Access Protocol (LDAP).\nObviously all the types of data in our examples can be stored without much trou-\nble in a database system, and accessed through protocols such as JDBC or ODBC. The\nquestion then is, why come up with a specialized protocol for accessing directory\ninformation? There are at least two answers to the question.\n• First, directory access protocols are simpliﬁed protocols that cater to a lim-\nited type of access to data. They evolved in parallel with the database access\nprotocols.\n• Second, and more important, directory systems provide a simple mechanism\nto name objects in a hierarchical fashion, similar to ﬁle system directory names,\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nVI. Database System \nArchitecture\n19. Distributed Databases\n738\n© The McGraw−Hill \nCompanies, 2001\n742\nChapter 19\nDistributed Databases\nwhich can be used in a distributed directory system to specify what informa-\ntion is stored in each of the directory servers. For example, a particular direc-\ntory server may store information for Bell Laboratories employees in Murray\nHill, while another may store information for Bell Laboratories employees in\nBangalore, giving both sites autonomy in controlling their local data. The di-\nrectory access protocol can be used to obtain data from both directories, across\na network. More importantly, the directory system can be set up to automati-\ncally forward queries made at one site to the other site, without user interven-\ntion.\nFor these reasons, several organizations have directory systems to make organiza-\ntional information available online. As may be expected, several directory implemen-\ntations ﬁnd it beneﬁcial to use relational databases to store data, instead of creating\nspecial-purpose storage systems.\n19.9.2\nLDAP: Lightweight Directory Access Protocol\nIn general a directory system is implemented as one or more servers, which service\nmultiple clients. Clients use the application programmer interface deﬁned by direc-\ntory system to communicate with the directory servers. Directory access protocols\nalso deﬁne a data model and access control.\nThe X.500 directory access protocol, deﬁned by the International Organization for\nStandardization (ISO), is a standard for accessing directory information. However,\nthe protocol is rather complex, and is not widely used. The Lightweight Directory\nAccess Protocol (LDAP) provides many of the X.500 features, but with less complex-\nity, and is widely used. In the rest of this section, we shall outline the data model and\naccess protocol details of LDAP.\n19.9.2.1\nLDAP Data Model\nIn LDAP directories store entries, which are similar to objects. Each entry must have a\ndistinguished name (DN), which uniquely identiﬁes the entry. A DN is in turn made\nup of a sequence of relative distinguished names (RDNs). For example, an entry may\nhave the following distinguished name.\ncn=Silberschatz, ou=Bell Labs, o=Lucent, c=USA\nAs you can see, the distinguished name in this example is a combination of a name\nand (organizational) address, starting with a person’s name, then giving the orga-\nnizational unit (ou), the organization (o), and country (c). The order of the compo-\nnents of a distinguished name reﬂects the normal postal address order, rather than\nthe reverse order used in specifying path names for ﬁles. The set of RDNs for a DN is\ndeﬁned by the schema of the directory system.\nEntries can also have attributes. LDAP provides binary, string, and time types, and\nadditionally the types tel for telephone numbers, and PostalAddress for addresses\n(lines separated by a “$” character). Unlike those in the relational model, attributes\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nVI. Database System \nArchitecture\n19. Distributed Databases\n739\n© The McGraw−Hill \nCompanies, 2001\n19.9\nDirectory Systems\n743\nare multivalued by default, so it is possible to store multiple telephone numbers or\naddresses for an entry.\nLDAP allows the deﬁnition of object classes with attribute names and types. In-\nheritance can be used in deﬁning object classes. Moreover, entries can be speciﬁed to\nbe of one or more object classes. It is not necessary that there be a single most-speciﬁc\nobject class to which an entry belongs.\nEntries are organized into a directory information tree (DIT), according to their\ndistinguished names. Entries at the leaf level of the tree usually represent speciﬁc\nobjects. Entries that are internal nodes represent objects such as organizational units,\norganizations, or countries. The children of a node have a DN containing all the RDNs\nof the parent, and one or more additional RDNs. For instance, an internal node may\nhave a DN c=USA, and all entries below it have the value USA for the RDN c.\nThe entire distinguished name need not be stored in an entry; The system can\ngenerate the distinguished name of an entry by traversing up the DIT from the entry,\ncollecting the RDN=value components to create the full distinguished name.\nEntries may have more than one distinguished name—for example, an entry for a\nperson in more than one organization. To deal with such cases, the leaf level of a DIT\ncan be an alias, which points to an entry in another branch of the tree.\n19.9.2.2\nData Manipulation\nUnlike SQL, LDAP does not deﬁne either a data-deﬁnition language or a data manip-\nulation language. However, LDAP deﬁnes a network protocol for carrying out data\ndeﬁnition and manipulation. Users of LDAP can either use an application program-\nming interface, or use tools provided by various vendors to perform data deﬁnition\nand manipulation. LDAP also deﬁnes a ﬁle format called LDAP Data Interchange\nFormat (LDIF) that can be used for storing and exchanging information.\nThe querying mechanism in LDAP is very simple, consisting of just selections and\nprojections, without any join. A query must specify the following:\n• A base—that is, a node within a DIT—by giving its distinguished name (the\npath from the root to the node).\n• A search condition, which can be a Boolean combination of conditions on in-\ndividual attributes. Equality, matching by wild-card characters, and approxi-\nmate equality (the exact deﬁnition of approximate equality is system depen-\ndent) are supported.\n• A scope, which can be just the base, the base and its children, or the entire\nsubtree beneath the base.\n• Attributes to return.\n• Limits on number of results and resource consumption.\nThe query can also specify whether to automatically dereference aliases; if alias deref-\nerences are turned off, alias entries can be returned as answers.\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nVI. Database System \nArchitecture\n19. Distributed Databases\n740\n© The McGraw−Hill \nCompanies, 2001\n744\nChapter 19\nDistributed Databases\nOne way of querying an LDAP data source is by using LDAP URLs. Examples of\nLDAP URLs are:\nldap:://aura.research.bell-labs.com/o=Lucent,c=USA\nldap:://aura.research.bell-labs.com/o=Lucent,c=USA??sub?cn=Korth\nThe ﬁrst URL returns all attributes of all entries at the server with organization being\nLucent, and country being USA. The second URL executes a search query (selection)\ncn=Korth on the subtree of the node with distinguished name o=Lucent, c=USA. The\nquestion marks in the URL separate different ﬁelds. The ﬁrst ﬁeld is the distinguished\nname, here o=Lucent,c=USA. The second ﬁeld, the list of attributes to return, is left\nempty, meaning return all attributes. The third attribute, sub, indicates that the entire\nsubtree is to be searched. The last parameter is the search condition.\nA second way of querying an LDAP directory is by using an application program-\nming interface. Figure 19.6 shows a piece of C code used to connect to an LDAP server\nand run a query against the server. The code ﬁrst opens a connection to an LDAP\nserver by ldap open and ldap bind. It then executes a query by ldap search s. The\narguments to ldap search s are the LDAP connection handle, the DN of the base from\nwhich the search should be done, the scope of the search, the search condition, the\nlist of attributes to be returned, and an attribute called attrsonly, which, if set to 1,\nwould result in only the schema of the result being returned, without any actual tu-\nples. The last argument is an output argument that returns the result of the search as\nan LDAPMessage structure.\nThe ﬁrst for loop iterates over and prints each entry in the result. Note that an\nentry may have multiple attributes, and the second for loop prints each attribute.\nSince attributes in LDAP may be multivalued, the third for loop prints each value of\nan attribute. The calls ldap msgfree and ldap value free free memory that is allocated\nby the LDAP libraries. Figure 19.6 does not show code for handling error conditions.\nThe LDAP API also contains functions to create, update, and delete entries, as well\nas other operations on the DIT. Each function call behaves like a separate transaction;\nLDAP does not support atomicity of multiple updates.\n19.9.2.3\nDistributed Directory Trees\nInformation about an organization may be split into multiple DITs, each of which\nstores information about some entries. The sufﬁx of a DIT is a sequence of RDN=value\npairs that identify what information the DIT stores; the pairs are concatenated to the\nrest of the distinguished name generated by traversing from the entry to the root.\nFor instance, the sufﬁx of a DIT may be o=Lucent, c=USA, while another may have\nthe sufﬁx o=Lucent, c=India. The DITs may be organizationally and geographically\nseparated.\nA node in a DIT may contain a referral to another node in another DIT; for in-\nstance, the organizational unit Bell Labs under o=Lucent, c=USA may have its own\nDIT, in which case the DIT for o=Lucent, c=USA would have a node ou=Bell Labs\nrepresenting a referral to the DIT for Bell Labs.\nReferrals are the key component that help organize a distributed collection of di-\nrectories into an integrated system. When a server gets a query on a DIT, it may\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nVI. Database System \nArchitecture\n19. Distributed Databases\n741\n© The McGraw−Hill \nCompanies, 2001\n19.9\nDirectory Systems\n745\n#include <stdio.h>\n#include <ldap.h>\nmain() {\nLDAP *ld;\nLDAPMessage *res, *entry;\nchar *dn, *attr, *attrList[] = {“telephoneNumber”, NULL};\nBerElement *ptr;\nint vals, i;\nld = ldap open(“aura.research.bell-labs.com”, LDAP PORT);\nldap simple bind(ld, “avi”, “avi-passwd”) ;\nldap search s(ld, “o=Lucent, c=USA”, LDAP SCOPE SUBTREE, “cn=Korth”,\nattrList, /*attrsonly*/ 0, &res);\nprintf(“found %d entries”, ldap count entries(ld, res));\nfor (entry=ldap ﬁrst entry(ld, res); entry != NULL;\nentry = ldap next entry(ld, entry)\n{\ndn = ldap get dn(ld, entry);\nprintf(“dn: %s”, dn);\nldap memfree(dn);\nfor (attr = ldap ﬁrst attribute(ld, entry, &ptr);\nattr ! NULL;\nattr = ldap next attribute(ld, entry, ptr))\n{\nprintf(“%s: ”, attr);\nvals = ldap get values(ld, entry, attr);\nfor (i=0; vals[i] != NULL; i++)\nprintf(“%s, ”, vals[i]);\nldap value free(vals);\n}\n}\nldap msgfree(res);\nldap unbind(ld);\n}\nFigure 19.6\nExample of LDAP code in C.\nreturn a referral to the client, which then issues a query on the referenced DIT. Ac-\ncess to the referenced DIT is transparent, proceeding without the user’s knowledge.\nAlternatively, the server itself may issue the query to the referred DIT and return the\nresults along with locally computed results.\nThe hierarchical naming mechanism used by LDAP helps break up control of in-\nformation across parts of an organization. The referral facility then helps integrate all\nthe directories in an organization into a single virtual directory.\nAlthough it is not an LDAP requirement, organizations often choose to break up\ninformation either by geography (for instance, an organization may maintain a direc-\ntory for each site where the organization has a large presence) or by organizational\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nVI. Database System \nArchitecture\n19. Distributed Databases\n742\n© The McGraw−Hill \nCompanies, 2001\n746\nChapter 19\nDistributed Databases\nstructure (for instance, each organizational unit, such as department, maintains its\nown directory).\nMany LDAP implementations support master–slave and multimaster replication\nof DITs, although replication is not part of the current LDAP version 3 standard. Work\non standardizing replication in LDAP is in progress.\n19.10\nSummary\n• A distributed database system consists of a collection of sites, each of which\nmaintains a local database system. Each site is able to process local transac-\ntions: those transactions that access data in only that single site. In addition, a\nsite may participate in the execution of global transactions; those transactions\nthat access data in several sites. The execution of global transactions requires\ncommunication among the sites.\n• Distributed databases may be homogeneous, where all sites have a common\nschema and database system code, or heterogeneous, where the schemas and\nsystem codes may differ.\n• There are several issues involved in storing a relation in the distributed data-\nbase, including replication and fragmentation. It is essential that the system\nminimize the degree to which a user needs to be aware of how a relation is\nstored.\n• A distributed system may suffer from the same types of failure that can afﬂict\na centralized system. There are, however, additional failures with which we\nneed to deal in a distributed environment, including the failure of a site, the\nfailure of a link, loss of a message, and network partition. Each of these prob-\nlems needs to be considered in the design of a distributed recovery scheme.\n• To ensure atomicity, all the sites in which a transaction T executed must agree\non the ﬁnal outcome of the execution. T either commits at all sites or aborts at\nall sites. To ensure this property, the transaction coordinator of T must execute\na commit protocol. The most widely used commit protocol is the two-phase\ncommit protocol.\n• The two-phase commit protocol may lead to blocking, the situation in which\nthe fate of a transaction cannot be determined until a failed site (the coordi-\nnator) recovers. We can use the three-phase commit protocol to reduce the\nprobability of blocking.\n• Persistent messaging provides an alternative model for handling distributed\ntransactions. The model breaks a single transaction into parts that are exe-\ncuted at different databases. Persistent messages (which are guaranteed to be\ndelivered exactly once, regardless of failures), are sent to remote sites to re-\nquest actions to be taken there. While persistent messaging avoids the block-\ning problem, application developers have to write code to handle various\ntypes of failures.\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nVI. Database System \nArchitecture\n19. Distributed Databases\n743\n© The McGraw−Hill \nCompanies, 2001\n19.10\nSummary\n747\n• The various concurrency-control schemes used in a centralized system can be\nmodiﬁed for use in a distributed environment.\n\u0000 In the case of locking protocols, the only change that needs to be incor-\nporated is in the way that the lo\n\nfound at the end of that chapter.\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nII. Relational Databases\nIntroduction\n140\n© The McGraw−Hill \nCompanies, 2001\nP\nA\nR\nT\n2\nRelational Databases\nA relational database is a shared repository of data. To make data from a relational\ndatabase available to users, we have to address several issues. One is how users spec-\nify requests for data: Which of the various query languages do they use? Chapter 4\ncovers the SQL language, which is the most widely used query language today. Chap-\nter 5 covers two other query languages, QBE and Datalog, which offer alternative\napproaches to querying relational data.\nAnother issue is data integrity and security; databases need to protect data from\ndamage by user actions, whether unintentional or intentional. The integrity main-\ntenance component of a database ensures that updates do not violate integrity con-\nstraints that have been speciﬁed on the data. The security component of a database\nincludes authentication of users, and access control, to restrict the permissible actions\nfor each user. Chapter 6 covers integrity and security issues. Security and integrity\nissues are present regardless of the data model, but for concreteness we study them\nin the context of the relational model. Integrity constraints form the basis of relational\ndatabase design, which we study in Chapter 7.\nRelational database design—the design of the relational schema—is the ﬁrst step\nin building a database application. Schema design was covered informally in ear-\nlier chapters. There are, however, principles that can be used to distinguish good\ndatabase designs from bad ones. These are formalized by means of several “normal\nforms,” which offer different tradeoffs between the possibility of inconsistencies and\nthe efﬁciency of certain queries. Chapter 7 describes the formal design of relational\nschemas.\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nII. Relational Databases\n4. SQL\n141\n© The McGraw−Hill \nCompanies, 2001\nC\nH\nA\nP\nT\nE\nR\n4\nSQL\nThe formal languages described in Chapter 3 provide a concise notation for repre-\nsenting queries. However, commercial database systems require a query language\nthat is more user friendly. In this chapter, we study SQL, the most inﬂuential commer-\ncially marketed query language, SQL. SQL uses a combination of relational-algebra\nand relational-calculus constructs.\nAlthough we refer to the SQL language as a “query language,” it can do much\nmore than just query a database. It can deﬁne the structure of the data, modify data\nin the database, and specify security constraints.\nIt is not our intention to provide a complete users’ guide for SQL. Rather, we\npresent SQL’s fundamental constructs and concepts. Individual implementations of\nSQL may differ in details, or may support only a subset of the full language.\n4.1\nBackground\nIBM developed the original version of SQL at its San Jose Research Laboratory (now\nthe Almaden Research Center). IBM implemented the language, originally called Se-\nquel, as part of the System R project in the early 1970s. The Sequel language has\nevolved since then, and its name has changed to SQL (Structured Query Language).\nMany products now support the SQL language. SQL has clearly established itself as\nthe standard relational-database language.\nIn 1986, the American National Standards Institute (ANSI) and the International\nOrganization for Standardization (ISO) published an SQL standard, called SQL-86.\nIBM published its own corporate SQL standard, the Systems Application Architec-\nture Database Interface (SAA-SQL) in 1987. ANSI published an extended standard for\nSQL, SQL-89, in 1989. The next version of the standard was SQL-92 standard, and the\nmost recent version is SQL:1999. The bibliographic notes provide references to these\nstandards.\n135\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nII. Relational Databases\n4. SQL\n142\n© The McGraw−Hill \nCompanies, 2001\n136\nChapter 4\nSQL\nIn this chapter, we present a survey of SQL, based mainly on the widely imple-\nmented SQL-92 standard. The SQL:1999 standard is a superset of the SQL-92 standard;\nwe cover some features of SQL:1999 in this chapter, and provide more detailed cov-\nerage in Chapter 9. Many database systems support some of the new constructs in\nSQL:1999, although currently no database system supports all the new constructs. You\nshould also be aware that some database systems do not even support all the fea-\ntures of SQL-92, and that many databases provide nonstandard features that we do\nnot cover here.\nThe SQL language has several parts:\n• Data-deﬁnition language (DDL). The SQL DDL provides commands for deﬁn-\ning relation schemas, deleting relations, and modifying relation schemas.\n• Interactive data-manipulation language (DML). The SQL DML includes a\nquery language based on both the relational algebra and the tuple relational\ncalculus. It includes also commands to insert tuples into, delete tuples from,\nand modify tuples in the database.\n• View deﬁnition. The SQL DDL includes commands for deﬁning views.\n• Transaction control. SQL includes commands for specifying the beginning\nand ending of transactions.\n• Embedded SQL and dynamic SQL. Embedded and dynamic SQL deﬁne how\nSQL statements can be embedded within general-purpose programming lan-\nguages, such as C, C++, Java, PL/I, Cobol, Pascal, and Fortran.\n• Integrity. The SQL DDL includes commands for specifying integrity constraints\nthat the data stored in the database must satisfy. Updates that violate integrity\nconstraints are disallowed.\n• Authorization. The SQL DDL includes commands for specifying access rights\nto relations and views.\nIn this chapter, we cover the DML and the basic DDL features of SQL. We also\nbrieﬂy outline embedded and dynamic SQL, including the ODBC and JDBC standards\nfor interacting with a database from programs written in the C and Java languages.\nSQL features supporting integrity and authorization are described in Chapter 6, while\nChapter 9 outlines object-oriented extensions to SQL.\nThe enterprise that we use in the examples in this chapter, and later chapters, is a\nbanking enterprise with the following relation schemas:\nBranch-schema = (branch-name, branch-city, assets)\nCustomer-schema = (customer-name, customer-street, customer-city)\nLoan-schema = (loan-number, branch-name, amount)\nBorrower-schema = (customer-name, loan-number)\nAccount-schema = (account-number, branch-name, balance)\nDepositor-schema = (customer-name, account-number)\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nII. Relational Databases\n4. SQL\n143\n© The McGraw−Hill \nCompanies, 2001\n4.2\nBasic Structure\n137\nNote that in this chapter, as elsewhere in the text, we use hyphenated names for\nschema, relations, and attributes for ease of reading. In actual SQL systems, however,\nhyphens are not valid parts of a name (they are treated as the minus operator). A\nsimple way of translating the names we use to valid SQL names is to replace all hy-\nphens by the underscore symbol (“ ”). For example, we use branch name in place of\nbranch-name.\n4.2\nBasic Structure\nA relational database consists of a collection of relations, each of which is assigned\na unique name. Each relation has a structure similar to that presented in Chapter 3.\nSQL allows the use of null values to indicate that the value either is unknown or does\nnot exist. It allows a user to specify which attributes cannot be assigned null values,\nas we shall discuss in Section 4.11.\nThe basic structure of an SQL expression consists of three clauses: select, from, and\nwhere.\n• The select clause corresponds to the projection operation of the relational al-\ngebra. It is used to list the attributes desired in the result of a query.\n• The from clause corresponds to the Cartesian-product operation of the rela-\ntional algebra. It lists the relations to be scanned in the evaluation of the ex-\npression.\n• The where clause corresponds to the selection predicate of the relational alge-\nbra. It consists of a predicate involving attributes of the relations that appear\nin the from clause.\nThat the term select has different meaning in SQL than in the relational algebra is an\nunfortunate historical fact. We emphasize the different interpretations here to mini-\nmize potential confusion.\nA typical SQL query has the form\nselect A1, A2, . . . , An\nfrom r1, r2, . . . , rm\nwhere P\nEach Ai represents an attribute, and each ri a relation. P is a predicate. The query is\nequivalent to the relational-algebra expression\nΠA1, A2,...,An(σP (r1 × r2 × · · · × rm))\nIf the where clause is omitted, the predicate P is true. However, unlike the result of a\nrelational-algebra expression, the result of the SQL query may contain multiple copies\nof some tuples; we shall return to this issue in Section 4.2.8.\nSQL forms the Cartesian product of the relations named in the from clause,\nperforms a relational-algebra selection using the where clause predicate, and then\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nII. Relational Databases\n4. SQL\n144\n© The McGraw−Hill \nCompanies, 2001\n138\nChapter 4\nSQL\nprojects the result onto the attributes of the select clause. In practice, SQL may con-\nvert the expression into an equivalent form that can be processed more efﬁciently.\nHowever, we shall defer concerns about efﬁciency to Chapters 13 and 14.\n4.2.1\nThe select Clause\nThe result of an SQL query is, of course, a relation. Let us consider a simple query\nusing our banking example, “Find the names of all branches in the loan relation”:\nselect branch-name\nfrom loan\nThe result is a relation consisting of a single attribute with the heading branch-name.\nFormal query languages are based on the mathematical notion of a relation being\na set. Thus, duplicate tuples never appear in relations. In practice, duplicate elimina-\ntion is time-consuming. Therefore, SQL (like most other commercial query languages)\nallows duplicates in relations as well as in the results of SQL expressions. Thus, the\npreceding query will list each branch-name once for every tuple in which it appears in\nthe loan relation.\nIn those cases where we want to force the elimination of duplicates, we insert the\nkeyword distinct after select. We can rewrite the preceding query as\nselect distinct branch-name\nfrom loan\nif we want duplicates removed.\nSQL allows us to use the keyword all to specify explicitly that duplicates are not\nremoved:\nselect all branch-name\nfrom loan\nSince duplicate retention is the default, we will not use all in our examples. To ensure\nthe elimination of duplicates in the results of our example queries, we will use dis-\ntinct whenever it is necessary. In most queries where distinct is not used, the exact\nnumber of duplicate copies of each tuple present in the query result is not important.\nHowever, the number is important in certain applications; we return to this issue in\nSection 4.2.8.\nThe asterisk symbol “ * ” can be used to denote “all attributes.” Thus, the use of\nloan.* in the preceding select clause would indicate that all attributes of loan are to be\nselected. A select clause of the form select * indicates that all attributes of all relations\nappearing in the from clause are selected.\nThe select clause may also contain arithmetic expressions involving the operators\n+, −, ∗, and / operating on constants or attributes of tuples. For example, the query\nselect loan-number, branch-name, amount * 100\nfrom loan\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nII. Relational Databases\n4. SQL\n145\n© The McGraw−Hill \nCompanies, 2001\n4.2\nBasic Structure\n139\nwill return a relation that is the same as the loan relation, except that the attribute\namount is multiplied by 100.\nSQL also provides special data types, such as various forms of the date type, and\nallows several arithmetic functions to operate on these types.\n4.2.2\nThe where Clause\nLet us illustrate the use of the where clause in SQL. Consider the query “Find all loan\nnumbers for loans made at the Perryridge branch with loan amounts greater that\n$1200.” This query can be written in SQL as:\nselect loan-number\nfrom loan\nwhere branch-name = ’Perryridge’ and amount > 1200\nSQL uses the logical connectives and, or, and not—rather than the mathematical\nsymbols ∧, ∨, and ¬ —in the where clause. The operands of the logical connectives\ncan be expressions involving the comparison operators <, <=, >, >=, =, and <>.\nSQL allows us to use the comparison operators to compare strings and arithmetic\nexpressions, as well as special types, such as date types.\nSQL includes a between comparison operator to simplify where clauses that spec-\nify that a value be less than or equal to some value and greater than or equal to some\nother value. If we wish to ﬁnd the loan number of those loans with loan amounts\nbetween $90,000 and $100,000, we can use the between comparison to write\nselect loan-number\nfrom loan\nwhere amount between 90000 and 100000\ninstead of\nselect loan-number\nfrom loan\nwhere amount <= 100000 and amount >= 90000\nSimilarly, we can use the not between comparison operator.\n4.2.3\nThe from Clause\nFinally, let us discuss the use of the from clause. The from clause by itself deﬁnes a\nCartesian product of the relations in the clause. Since the natural join is deﬁned in\nterms of a Cartesian product, a selection, and a projection, it is a relatively simple\nmatter to write an SQL expression for the natural join.\nWe write the relational-algebra expression\nΠcustomer-name, loan-number, amount (borrower\n\u0001 loan)\nfor the query “For all customers who have a loan from the bank, ﬁnd their names,\nloan numbers and loan amount.” In SQL, this query can be written as\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nII. Relational Databases\n4. SQL\n146\n© The McGraw−Hill \nCompanies, 2001\n140\nChapter 4\nSQL\nselect customer-name, borrower.loan-number, amount\nfrom borrower, loan\nwhere borrower.loan-number = loan.loan-number\nNotice that SQL uses the notation relation-name.attribute-name, as does the relational\nalgebra, to avoid ambiguity in cases where an attribute appears in the schema of more\nthan one relation. We could have written borrower.customer-name instead of customer-\nname in the select clause. However, since the attribute customer-name appears in only\none of the relations named in the from clause, there is no ambiguity when we write\ncustomer-name.\nWe can extend the preceding query and consider a more complicated case in which\nwe require also that the loan be from the Perryridge branch: “Find the customer\nnames, loan numbers, and loan amounts for all loans at the Perryridge branch.” To\nwrite this query, we need to state two constraints in the where clause, connected by\nthe logical connective and:\nselect customer-name, borrower.loan-number, amount\nfrom borrower, loan\nwhere borrower.loan-number = loan.loan-number and\nbranch-name = ’Perryridge’\nSQL includes extensions to perform natural joins and outer joins in the from clause.\nWe discuss these extensions in Section 4.10.\n4.2.4\nThe Rename Operation\nSQL provides a mechanism for renaming both relations and attributes. It uses the as\nclause, taking the form:\nold-name as new-name\nThe as clause can appear in both the select and from clauses.\nConsider again the query that we used earlier:\nselect customer-name, borrower.loan-number, amount\nfrom borrower, loan\nwhere borrower.loan-number = loan.loan-number\nThe result of this query is a relation with the following attributes:\ncustomer-name, loan-number, amount.\nThe names of the attributes in the result are derived from the names of the attributes\nin the relations in the from clause.\nWe cannot, however, always derive names in this way, for several reasons: First,\ntwo relations in the from clause may have attributes with the same name, in which\ncase an attribute name is duplicated in the result. Second, if we used an arithmetic\nexpression in the select clause, the resultant attribute does not have a name. Third,\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nII. Relational Databases\n4. SQL\n147\n© The McGraw−Hill \nCompanies, 2001\n4.2\nBasic Structure\n141\neven if an attribute name can be derived from the base relations as in the preced-\ning example, we may want to change the attribute name in the result. Hence, SQL\nprovides a way of renaming the attributes of a result relation.\nFor example, if we want the attribute name loan-number to be replaced with the\nname loan-id, we can rewrite the preceding query as\nselect customer-name, borrower.loan-number as loan-id, amount\nfrom borrower, loan\nwhere borrower.loan-number = loan.loan-number\n4.2.5\nTuple Variables\nThe as clause is particularly useful in deﬁning the notion of tuple variables, as is\ndone in the tuple relational calculus. A tuple variable in SQL must be associated with\na particular relation. Tuple variables are deﬁned in the from clause by way of the as\nclause. To illustrate, we rewrite the query “For all customers who have a loan from\nthe bank, ﬁnd their names, loan numbers, and loan amount” as\nselect customer-name, T.loan-number, S.amount\nfrom borrower as T, loan as S\nwhere T.loan-number = S.loan-number\nNote that we deﬁne a tuple variable in the from clause by placing it after the name of\nthe relation with which it is associated, with the keyword as in between (the keyword\nas is optional). When we write expressions of the form relation-name.attribute-name,\nthe relation name is, in effect, an implicitly deﬁned tuple variable.\nTuple variables are most useful for comparing two tuples in the same relation.\nRecall that, in such cases, we could use the rename operation in the relational algebra.\nSuppose that we want the query “Find the names of all branches that have assets\ngreater than at least one branch located in Brooklyn.” We can write the SQL expression\nselect distinct T.branch-name\nfrom branch as T, branch as S\nwhere T.assets > S.assets and S.branch-city = ’Brooklyn’\nObserve that we could not use the notation branch.asset, since it would not be clear\nwhich reference to branch is intended.\nSQL permits us to use the notation (v1, v2, . . . , vn) to denote a tuple of arity n con-\ntaining values v1, v2, . . . , vn. The comparison operators can be used on tuples, and\nthe ordering is deﬁned lexicographically. For example, (a1, a2) <= (b1, b2) is true if\na1 < b1, or (a1 = b1) ∧(a2 <= b2); similarly, the two tuples are equal if all their\nattributes are equal.\n4.2.6\nString Operations\nSQL speciﬁes strings by enclosing them in single quotes, for example, ’Perryridge’,\nas we saw earlier. A single quote character that is part of a string can be speciﬁed by\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nII. Relational Databases\n4. SQL\n148\n© The McGraw−Hill \nCompanies, 2001\n142\nChapter 4\nSQL\nusing two single quote characters; for example the string “It’s right” can be speciﬁed\nby ’It”s right’.\nThe most commonly used operation on strings is pattern matching using the op-\nerator like. We describe patterns by using two special characters:\n• Percent (%): The % character matches any substring.\n• Underscore ( ): The\ncharacter matches any character.\nPatterns are case sensitive; that is, uppercase characters do not match lowercase char-\nacters, or vice versa. To illustrate pattern matching, we consider the following exam-\nples:\n• ’Perry%’ matches any string beginning with “Perry”.\n• ’%idge%’ matches any string containing “idge” as a substring, for example,\n’Perryridge’, ’Rock Ridge’, ’Mianus Bridge’, and ’Ridgeway’.\n• ’\n’ matches any string of exactly three characters.\n• ’\n%’ matches any string of at least three characters.\nSQL expresses patterns by using the like comparison operator. Consider the query\n“Find the names of all customers whose street address includes the substring ‘Main’.”\nThis query can be written as\nselect customer-name\nfrom customer\nwhere customer-street like ’%Main%’\nFor pattern\n\nery frequently.\n12.10\nSummary\n• Many queries reference only a small proportion of the records in a ﬁle. To\nreduce the overhead in searching for these records, we can construct indices\nfor the ﬁles that store the database.\n• Index-sequential ﬁles are one of the oldest index schemes used in database\nsystems. To permit fast retrieval of records in search-key order, records are\nstored sequentially, and out-of-order records are chained together. To allow\nfast random access, we use an index structure.\n• There are two types of indices that we can use: dense indices and sparse\nindices. Dense indices contain entries for every search-key value, whereas\nsparse indices contain entries only for some search-key values.\n• If the sort order of a search key matches the sort order of a relation, an index\non the search key is called a primary index. The other indices are called sec-\nondary indices. Secondary indices improve the performance of queries that use\nsearch keys other than the primary one. However, they impose an overhead\non modiﬁcation of the database.\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nIV. Data Storage and \nQuerying\n12. Indexing and Hashing\n489\n© The McGraw−Hill \nCompanies, 2001\n488\nChapter 12\nIndexing and Hashing\n• The primary disadvantage of the index-sequential ﬁle organization is that per-\nformance degrades as the ﬁle grows. To overcome this deﬁciency, we can use\na B+-tree index.\n• A B+-tree index takes the form of a balanced tree, in which every path from the\nroot of the tree to a leaf of the tree is of the same length. The height of a B+-\ntree is proportional to the logarithm to the base N of the number of records\nin the relation, where each nonleaf node stores N pointers; the value of N is\noften around 50 or 100. B+-trees are much shorter than other balanced binary-\ntree structures such as AVL trees, and therefore require fewer disk accesses to\nlocate records.\n• Lookup on B+-trees is straightforward and efﬁcient. Insertion and deletion,\nhowever, are somewhat more complicated, but still efﬁcient. The number of\noperations required for lookup, insertion, and deletion on B+-trees is propor-\ntional to the logarithm to the base N of the number of records in the relation,\nwhere each nonleaf node stores N pointers.\n• We can use B+-trees for indexing a ﬁle containing records, as well as to orga-\nnize records into a ﬁle.\n• B-tree indices are similar to B+-tree indices. The primary advantage of a B-tree\nis that the B-tree eliminates the redundant storage of search-key values. The\nmajor disadvantages are overall complexity and reduced fanout for a given\nnode size. System designers almost universally prefer B+-tree indices over B-\ntree indices in practice.\n• Sequential ﬁle organizations require an index structure to locate data. File or-\nganizations based on hashing, by contrast, allow us to ﬁnd the address of a\ndata item directly by computing a function on the search-key value of the de-\nsired record. Since we do not know at design time precisely which search-key\nvalues will be stored in the ﬁle, a good hash function to choose is one that as-\nsigns search-key values to buckets such that the distribution is both uniform\nand random.\n• Static hashing uses hash functions in which the set of bucket addresses is ﬁxed.\nSuch hash functions cannot easily accommodate databases that grow signiﬁ-\ncantly larger over time. There are several dynamic hashing techniques that allow\nthe hash function to be modiﬁed. One example is extendable hashing, which\ncopes with changes in database size by splitting and coalescing buckets as the\ndatabase grows and shrinks.\n• We can also use hashing to create secondary indices; such indices are called\nhash indices. For notational convenience, we assume hash ﬁle organizations\nhave an implicit hash index on the search key used for hashing.\n• Ordered indices such as B+-trees and hash indices can be used for selections\nbased on equality conditions involving single attributes. When multiple\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nIV. Data Storage and \nQuerying\n12. Indexing and Hashing\n490\n© The McGraw−Hill \nCompanies, 2001\nExercises\n489\nattributes are involved in a selection condition, we can intersect record iden-\ntiﬁers retrieved from multiple indices.\n• Grid ﬁles provide a general means of indexing on multiple attributes.\n• Bitmap indices provide a very compact representation for indexing attributes\nwith very few distinct values. Intersection operations are extremely fast on\nbitmaps, making them ideal for supporting queries on multiple attributes.\nReview Terms\n• Access types\n• Access time\n• Insertion time\n• Deletion time\n• Space overhead\n• Ordered index\n• Primary index\n• Clustering index\n• Secondary index\n• Nonclustering index\n• Index-sequential ﬁle\n• Index record/entry\n• Dense index\n• Sparse index\n• Multilevel index\n• Sequential scan\n• B+-Tree index\n• Balanced tree\n• B+-Tree ﬁle organization\n• B-Tree index\n• Static hashing\n• Hash ﬁle organization\n• Hash index\n• Bucket\n• Hash function\n• Bucket overﬂow\n• Skew\n• Closed hashing\n• Dynamic hashing\n• Extendable hashing\n• Multiple-key access\n• Indices on multiple keys\n• Grid ﬁles\n• Bitmap index\n• Bitmap operations\n\u0000 Intersection\n\u0000 Union\n\u0000 Complement\n\u0000 Existence bitmap\nExercises\n12.1 When is it preferable to use a dense index rather than a sparse index? Explain\nyour answer.\n12.2 Since indices speed query processing, why might they not be kept on several\nsearch keys? List as many reasons as possible.\n12.3 What is the difference between a primary index and a secondary index?\n12.4 Is it possible in general to have two primary indices on the same relation for\ndifferent search keys? Explain your answer.\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nIV. Data Storage and \nQuerying\n12. Indexing and Hashing\n491\n© The McGraw−Hill \nCompanies, 2001\n490\nChapter 12\nIndexing and Hashing\n12.5 Construct a B+-tree for the following set of key values:\n(2, 3, 5, 7, 11, 17, 19, 23, 29, 31)\nAssume that the tree is initially empty and values are added in ascending or-\nder. Construct B+-trees for the cases where the number of pointers that will ﬁt\nin one node is as follows:\na. Four\nb. Six\nc. Eight\n12.6 For each B+-tree of Exercise 12.5, show the steps involved in the following\nqueries:\na. Find records with a search-key value of 11.\nb. Find records with a search-key value between 7 and 17, inclusive.\n12.7 For each B+-tree of Exercise 12.5, show the form of the tree after each of the\nfollowing series of operations:\na. Insert 9.\nb. Insert 10.\nc. Insert 8.\nd. Delete 23.\ne. Delete 19.\n12.8 Consider the modiﬁed redistribution scheme for B+-trees described in page\n463. What is the expected height of the tree as a function of n?\n12.9 Repeat Exercise 12.5 for a B-tree.\n12.10 Explain the distinction between closed and open hashing. Discuss the relative\nmerits of each technique in database applications.\n12.11 What are the causes of bucket overﬂow in a hash ﬁle organization? What can\nbe done to reduce the occurrence of bucket overﬂows?\n12.12 Suppose that we are using extendable hashing on a ﬁle that contains records\nwith the following search-key values:\n2, 3, 5, 7, 11, 17, 19, 23, 29, 31\nShow the extendable hash structure for this ﬁle if the hash function is h(x) = x\nmod 8 and buckets can hold three records.\n12.13 Show how the extendable hash structure of Exercise 12.12 changes as the result\nof each of the following steps:\na. Delete 11.\nb. Delete 31.\nc. Insert 1.\nd. Insert 15.\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nIV. Data Storage and \nQuerying\n12. Indexing and Hashing\n492\n© The McGraw−Hill \nCompanies, 2001\nBibliographical Notes\n491\n12.14 Give pseudocode for deletion of entries from an extendable hash structure,\nincluding details of when and how to coalesce buckets. Do not bother about\nreducing the size of the bucket address table.\n12.15 Suggest an efﬁcient way to test if the bucket address table in extendable hash-\ning can be reduced in size, by storing an extra count with the bucket address\ntable. Give details of how the count should be maintained when buckets are\nsplit, coalesced or deleted.\n(Note: Reducing the size of the bucket address table is an expensive oper-\nation, and subsequent inserts may cause the table to grow again. Therefore, it\nis best not to reduce the size as soon as it is possible to do so, but instead do\nit only if the number of index entries becomes small compared to the bucket\naddress table size.)\n12.16 Why is a hash structure not the best choice for a search key on which range\nqueries are likely?\n12.17 Consider a grid ﬁle in which we wish to avoid overﬂow buckets for perfor-\nmance reasons. In cases where an overﬂow bucket would be needed, we in-\nstead reorganize the grid ﬁle. Present an algorithm for such a reorganization.\n12.18 Consider the account relation shown in Figure 12.25.\na. Construct a bitmap index on the attributes branch-name and balance, divid-\ning balance values into 4 ranges: below 250, 250 to below 500, 500 to below\n750, and 750 and above.\nb. Consider a query that requests all accounts in Downtown with a balance of\n500 or more. Outline the steps in answering the query, and show the ﬁnal\nand intermediate bitmaps constructed to answer the query.\n12.19 Show how to compute existence bitmaps from other bitmaps. Make sure that\nyour technique works even in the presence of null values, by using a bitmap\nfor the value null.\n12.20 How does data encryption affect index schemes? In particular, how might it\naffect schemes that attempt to store data in sorted order?\nBibliographical Notes\nDiscussions of the basic data structures in indexing and hashing can be found in\nCormen et al. [1990]. B-tree indices were ﬁrst introduced in Bayer [1972] and Bayer\nand McCreight [1972]. B+-trees are discussed in Comer [1979], Bayer and Unterauer\n[1977] and Knuth [1973]. The bibliographic notes in Chapter 16 provides references to\nresearch on allowing concurrent accesses and updates on B+-trees. Gray and Reuter\n[1993] provide a good description of issues in the implementation of B+-trees.\nSeveral alternative tree and treelike search structures have been proposed. Tries\nare trees whose structure is based on the “digits” of keys (for example, a dictionary\nthumb index, which has one entry for each letter). Such trees may not be balanced\nin the sense that B+-trees are. Tries are discussed by Ramesh et al. [1989], Orenstein\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nIV. Data Storage and \nQuerying\n12. Indexing and Hashing\n493\n© The McGraw−Hill \nCompanies, 2001\n492\nChapter 12\nIndexing and Hashing\n[1982], Litwin [1981] and Fredkin [1960]. Related work includes the digital B-trees of\nLomet [1981].\nKnuth [1973] analyzes a large number of different hashing techniques. Several dy-\nnamic hashing schemes exist. Extendable hashing was introduced by Fagin et al.\n[1979]. Linear hashing was introduced by Litwin [1978] and Litwin [1980]; Larson\n[1982] presents a performance analysis of linear hashing. Ellis [1987] examined con-\ncurrency with linear hashing. Larson [1988] presents a variant of linear hashing. An-\nother scheme, called dynamic hashing, was proposed by Larson [1978]. An alterna-\ntive given by Ramakrishna and Larson [1989] allows retrieval in a single disk access\nat the price of a high overhead for a small fraction of database modiﬁcations. Par-\ntitioned hashing is an extension of hashing to multiple attributes, and is covered in\nRivest [1976], Burkhard [1976] and Burkhard [1979].\nThe grid ﬁle structure appears in Nievergelt et al. [1984] and Hinrichs [1985].\nBitmap indices, and variants called bit-sliced indices and projection indices are de-\nscribed in O’Neil and Quass [1997]. They were ﬁrst introduced in the IBM Model\n204 ﬁle manager on the AS 400 platform. They provide very large speedups on cer-\ntain types of queries, and are today implemented on most database systems. Recent\nresearch on bitmap indices includes Wu and Buchmann [1998], Chan and Ioannidis\n[1998], Chan and Ioannidis [1999], and Johnson [1999a].\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nIV. Data Storage and \nQuerying\n13. Query Processing\n494\n© The McGraw−Hill \nCompanies, 2001\nC\nH\nA\nP\nT\nE\nR\n1\n3\nQuery Processing\nQuery processing refers to the range of activities involved in extracting data from\na database. The activities include translation of queries in high-level database lan-\nguages into expressions that can be used at the physical level of the ﬁle system, a\nvariety of query-optimizing transformations, and actual evaluation of queries.\n13.1\nOverview\nThe steps involved in processing a query appear in Figure 13.1. The basic steps are\n1. Parsing and translation\n2. Optimization\n3. Evaluation\nBefore query processing can begin, the system must translate the query into a us-\nable form. A language such as SQL is suitable for human use, but is ill-suited to be\nthe system’s internal representation of a query. A more useful internal representation\nis one based on the extended relational algebra.\nThus, the ﬁrst action the system must take in query processing is to translate a\ngiven query into its internal form. This translation process is similar to the work\nperformed by the parser of a compiler. In generating the internal form of the query,\nthe parser checks the syntax of the user’s query, veriﬁes that the relation names ap-\npearing in the query are names of the relations in the database, and so on. The sys-\ntem constructs a parse-tree representation of the query, which it then translates into\na relational-algebra expression. If the query was expressed in terms of a view, the\ntranslation phase also replaces all uses of the view by the relational-algebra expres-\n493\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nIV. Data Storage and \nQuerying\n13. Query Processing\n495\n© The McGraw−Hill \nCompanies, 2001\n494\nChapter 13\nQuery Processing\nquery\noutput\nquery\nparser and\ntranslator\nevaluation engine\nrelational algebra\nexpression\nexecution plan\noptimizer\ndata\nstatistics\nabout data\nFigure 13.1\nSteps in query processing.\nsion that deﬁnes the view.1 Most compiler texts cover parsing (see the bibliographical\nnotes).\nGiven a query, there are generally a variety of methods for computing the answer.\nFor example, we have seen that, in SQL, a query could be expressed in several differ-\nent ways. Each SQL query can itself be translated into a relational-algebra expression\nin one of several ways. Furthermore, the relational-algebra representation of a query\nspeciﬁes only partially how to evaluate a query; there are usually several ways to\nevaluate relational-algebra expressions. As an illustration, consider the query\nselect balance\nfrom account\nwhere balance < 2500\nThis query can be translated into either of the following relational-algebra expres-\nsions:\n• σbalance<2500 (Πbalance (account))\n• Πbalance (σbalance<2500 (account))\nFurther, we can execute each relational-algebra operation by one of several dif-\nferent algorithms. For example, to implement the preceding selection, we can search\nevery tuple in account to ﬁnd tuples with balance less than 2500. If a B+-tree index is\navailable on the attribute balance, we can use the index instead to locate the tuples.\nTo specify fully how to evaluate a query, we need not only to provide the relational-\nalgebra expression, but also to annotate it with instructions specifying how to eval-\n1.\nFor materialized views, the expression deﬁning the view has already been evaluated and stored. There-\nfore, the stored relation can be used, instead of uses of the view being replaced by the expression deﬁning\nthe view. Recursive views are handled differently, via a ﬁxed-point procedure, as discussed in Section 5.2.6.\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nIV. Data Storage and \nQuerying\n13. Query Processing\n496\n© The McGraw−Hill \nCompanies, 2001\n13.2\nMeasures of Query Cost\n495\nΠ balance\nσ balance < 2500; use index 1\naccount\nFigure 13.2\nA query-evaluation plan.\nuate each operation. Annotations may state the algorithm to be used for a speciﬁc\noperation, or the particular index or indices to use. A relational-algebra operation\nannotated with instructions on how to evaluate it is called an evaluation primitive.\nA sequence of primitive operations that can be used to evaluate a query is a query-\nexecution plan or query-evaluation plan. Figure 13.2 illustrates an evaluation plan\nfor our example query, in which a particular index (denoted in the ﬁgure as “in-\ndex 1”) is speciﬁed for the selection operation. The query-execution engine takes a\nquery-evaluation plan, executes that plan, and returns the answers to the query.\nThe different evaluation plans for a given query can have different costs. We do not\nexpect users to write their queries in a way that suggests the most efﬁcient evaluation\nplan. Rather, it is the responsibility of the system to construct a query-evaluation plan\nthat minimizes the cost of query evaluation. Chapter 14 describes query optimization\nin detail.\nOnce the query plan is chosen, the query is evaluated with that plan, and the result\nof the query is output.\nThe sequence of steps already described for processing a query is representa-\ntive; not all databases exactly follow those steps. For instance, instead of using the\nrelational-algebra representation, several databases use an annotated parse-tree rep-\nresentation based on the structure of the given SQL query. However, the concepts that\nwe describe here form the basis of query processing in databases.\nIn order to optimize a query, a query optimizer must know the cost of each oper-\nation. Although the exact cost is hard to compute, since it depends on many param-\neters such as actual memory available to the operation, it is possible to get a rough\nestimate of execution cost for each operation.\nSection 13.2 outlines how we measure the cost of a query. Sections 13.3 through\n13.6 cover the evaluation of individual relational-algebra operations. Several opera-\ntions may be grouped together into a pipeline, in which each of the operations starts\nworking on its input tuples even as they are being generated by another operation.\nIn Section 13.7, we examine how to coordinate the execution of multiple operations\nin a query evaluation plan, in particular, how to use pipelined operations to avoid\nwriting intermediate results to disk.\n13.2\nMeasures of Query Cost\nThe cost of query evaluation can be measured in terms of a number of different re-\nsources, including disk accesses, CPU time to execute a query, and, in a distributed\nor parallel database system, the cost of communication (which we discuss later, in\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nIV. Data Storage and \nQuerying\n13. Query Processing\n497\n© The McGraw−Hill \nCompanies, 2001\n496\nChapter 13\nQuery Processing\nChapters 19 and 20). The response time for a query-evaluation plan (that is, the clock\ntime required to execute the plan), assuming no other activity is going on on the com-\nputer, would account for all these costs, and could be used as a good measure of the\ncost of the plan.\nIn large database systems, however, disk accesses (which we measure as the num-\nber of transfers of blocks from disk) are usually the most important cost, since disk ac-\ncesses are slow compared to in-memory operations. Moreover, CPU speeds have been\nimproving much faster than have disk speeds. Thus, it is likely that the time spent in\ndisk activity will continue to dominate the total time to execute a query. Finally, esti-\nmating the CPU time is relatively hard, compared to estimating the disk-access cost.\nTherefore, most people consider the disk-access cost a reasonable measure of the cost\nof a query-evaluation plan.\nWe use the number of block transfers from disk as a measure of the actual cost. To\nsimplify our computation of disk-access cost, we assume that all t\n\nit requires under-\nstanding of natural language, and understanding of the intent of the query, to decide\nif a document is relevant or not. Researchers therefore have created collections of doc-\numents and queries, and have manually tagged documents as relevant or irrelevant\nto the queries. Different ranking systems can be run on these collections to measure\ntheir average precision and recall across multiple queries.\n22.5.4\nWeb Search Engines\nWeb crawlers are programs that locate and gather information on the Web. They\nrecursively follow hyperlinks present in known documents to ﬁnd other documents.\nA crawler retrieves the documents and adds information found in the documents to a\ncombined index; the document is generally not stored, although some search engines\ndo cache a copy of the document to give clients faster access to the documents.\nSince the number of documents on the Web is very large, it is not possible to crawl\nthe whole Web in a short period of time; and in fact, all search engines cover only\nsome portions of the Web, not all of it, and their crawlers may take weeks or months\nto perform a single crawl of all the pages they cover. There are usually many pro-\ncesses, running on multiple machines, involved in crawling. A database stores a set\nof links (or sites) to be crawled; it assigns links from this set to each crawler process.\nNew links found during a crawl are added to the database, and may be crawled later\nif they are not crawled immediately. Pages found during a crawl are also handed over\nto an indexing system, which may be running on a different machine. Pages have to\nbe refetched (that is, links recrawled) periodically to obtain updated information, and\nto discard sites that no longer exist, so that the information in the search index is kept\nreasonably up to date.\nThe indexing system itself runs on multiple machines in parallel. It is not a good\nidea to add pages to the same index that is being used for queries, since doing so\nwould require concurrency control on the index, and affect query and update perfor-\nmance. Instead, one copy of the index is used to answer queries while another copy\nis updated with newly crawled pages. At periodic intervals the copies switch over,\nwith the old one being updated while the new copy is being used for queries.\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nVII. Other Topics\n22. Advanced Querying and \nInformation Retrieval\n847\n© The McGraw−Hill \nCompanies, 2001\n854\nChapter 22\nAdvanced Querying and Information Retrieval\nTo support very high query rates, the indices may be kept in main memory, and\nthere are multiple machines; the system selectively routes queries to the machines to\nbalance the load among them.\n22.5.5\nDirectories\nA typical library user may use a catalog to locate a book for which she is looking.\nWhen she retrieves the book from the shelf, however, she is likely to browse through\nother books that are located nearby. Libraries organize books in such a way that re-\nlated books are kept close together. Hence, a book that is physically near the desired\nbook may be of interest as well, making it worthwhile for users to browse through\nsuch books.\nTo keep related books close together, libraries use a classiﬁcation hierarchy. Books\non science are classiﬁed together. Within this set of books, there is a ﬁner classiﬁca-\ntion, with computer-science books organized together, mathematics books organized\ntogether, and so on. Since there is a relation between mathematics and computer sci-\nence, relevant sets of books are stored close to each other physically. At yet another\nlevel in the classiﬁcation hierarchy, computer-science books are broken down into\nsubareas, such as operating systems, languages, and algorithms. Figure 22.10 illus-\ntrates a classiﬁcation hierarchy that may be used by a library. Because books can be\nkept at only one place, each book in a library is classiﬁed into exactly one spot in the\nclassiﬁcation hierarchy.\nIn an information retrieval system, there is no need to store related documents\nclose together. However, such systems need to organize documents logically so as to\npermit browsing. Thus, such a system could use a classiﬁcation hierarchy similar to\nbooks\nalgorithms\ngraph algorithms\nmath\nscience\nfiction\nengineering\ncomputer science\n…\n…\n…\n…\n…\n…\n…\n…\nFigure 22.10\nA classiﬁcation hierarchy for a library system.\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nVII. Other Topics\n22. Advanced Querying and \nInformation Retrieval\n848\n© The McGraw−Hill \nCompanies, 2001\n22.5\nInformation-Retrieval Systems\n855\none that libraries use, and, when it displays a particular document, it can also display\na brief description of documents that are close in the hierarchy.\nIn an information retrieval system, there is no need to keep a document in a single\nspot in the hierarchy. A document that talks of mathematics for computer scientists\ncould be classiﬁed under mathematics as well as under computer science. All that is\nstored at each spot is an identiﬁer of the document (that is, a pointer to the document),\nand it is easy to fetch the contents of the document by using the identiﬁer.\nAs a result of this ﬂexibility, not only can a document be classiﬁed under two lo-\ncations, but also a subarea in the classiﬁcation hierarchy can itself occur under two\nareas. The class of “graph algorithm” document can appear both under mathemat-\nics and under computer science. Thus, the classiﬁcation hierarchy is now a directed\nacyclic graph (DAG), as shown in Figure 22.11. A graph-algorithm document may\nappear in a single location in the DAG, but can be reached via multiple paths.\nA directory is simply a classiﬁcation DAG structure. Each leaf of the directory\nstores links to documents on the topic represented by the leaf. Internal nodes may\nalso contain links, for example to documents that cannot be classiﬁed under any of\nthe child nodes.\nTo ﬁnd information on a topic, a user would start at the root of the directory and\nfollow paths down the DAG until reaching a node representing the desired topic.\nWhile browsing down the directory, the user can ﬁnd not only documents on the\ntopic he is interested in, but also ﬁnd related documents and related classes in the\nclassiﬁcation hierarchy. The user may learn new information by browsing through\ndocuments (or subclasses) within the related classes.\nOrganizing the enormous amount of information available on the Web into a di-\nrectory structure is a daunting task.\nbooks\nalgorithms\ngraph algorithms\nmath\nscience\nfiction\nengineering\ncomputer science\n…\n…\n…\n…\n…\n…\n…\n…\nFigure 22.11\nA classiﬁcation DAG for a library information retrieval system.\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nVII. Other Topics\n22. Advanced Querying and \nInformation Retrieval\n849\n© The McGraw−Hill \nCompanies, 2001\n856\nChapter 22\nAdvanced Querying and Information Retrieval\n• The ﬁrst problem is determining what exactly the directory hierarchy should\nbe.\n• The second problem is, given a document, deciding which nodes of the direc-\ntory are categories relevant to the document.\nTo tackle the ﬁrst problem, portals such as Yahoo have teams of “internet librar-\nians” who come up with the classiﬁcation hierarchy and continually reﬁne it. The\nOpen Directory Project is a large collaborative effort, with different volunteers being\nresponsible for organizing different branches of the directory.\nThe second problem can also be tackled manually by librarians, or Web site main-\ntainers may be responsible for deciding where their sites should lie in the hierarchy.\nThere are also techniques for automatically deciding the location of documents based\non computing their similarity to documents that have already been classiﬁed.\n22.6\nSummary\n• Decision-support systems analyze online data collected by transaction-\nprocessing systems, to help people make business decisions. Since most or-\nganizations are extensively computerized today, a very large body of infor-\nmation is available for decision support. Decision-support systems come in\nvarious forms, including OLAP systems and data mining systems.\n• Online analytical processing (OLAP) tools help analysts view data summa-\nrized in different ways, so that they can gain insight into the functioning of an\norganization.\n\u0000 OLAP tools work on multidimensional data, characterized by dimension\nattributes and measure attributes.\n\u0000 The data cube consists of multidimensional data summarized in different\nways. Precomputing the data cube helps speed up queries on summaries\nof data.\n\u0000 Cross-tab displays permit users to view two dimensions of multidimen-\nsional data at a time, along with summaries of the data.\n\u0000 Drill down, rollup, slicing, and dicing are among the operations that users\nperform with OLAP tools.\n• The OLAP component of the SQL:1999 standard provides a variety of new func-\ntionality for data analysis, including new aggregate functions, cube and rollup\noperations, ranking functions, windowing functions, which support summa-\nrization on moving windows, and partitioning, with windowing and ranking\napplied inside each partition.\n• Data mining is the process of semiautomatically analyzing large databases\nto ﬁnd useful patterns. There are a number of applications of data mining,\nsuch as prediction of values based on past examples, ﬁnding of associations\nbetween purchases, and automatic clustering of people and movies.\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nVII. Other Topics\n22. Advanced Querying and \nInformation Retrieval\n850\n© The McGraw−Hill \nCompanies, 2001\n22.6\nSummary\n857\n• Classiﬁcation deals with predicting the class of test instances, by using at-\ntributes of the test instances, based on attributes of training instances, and the\nactual class of training instances. Classiﬁcation can be used, for instance, to\npredict credit-worthiness levels of new applicants, or to predict the perfor-\nmance of applicants to a university.\nThere are several types of classiﬁers, such as\n\u0000 Decision-tree classiﬁers. These perform classiﬁcation by constructing a\ntree based on training instances with leaves having class labels. The tree\nis traversed for each test instance to ﬁnd a leaf, and the class of the leaf is\nthe predicted class.\nSeveral techniques are available to construct decision trees, most of\nthem based on greedy heuristics.\n\u0000 Bayesian classiﬁers are simpler to construct than decision-tree classiﬁers,\nand work better in the case of missing/null attribute values.\n• Association rules identify items that co-occur frequently, for instance, items\nthat tend to be bought by the same customer. Correlations look for deviations\nfrom expected levels of association.\n• Other types of data mining include clustering, text mining, and data visual-\nization.\n• Data warehouses help gather and archive important operational data. Ware-\nhouses are used for decision support and analysis on historical data, for in-\nstance to predict trends. Data cleansing from input data sources is often a\nmajor task in data warehousing. Warehouse schemas tend to be multidimen-\nsional, involving one or a few very large fact tables and several much smaller\ndimension tables.\n• Information retrieval systems are used to store and query textual data such\nas documents. They use a simpler data model than do database systems, but\nprovide more powerful querying capabilities within the restricted model.\nQueries attempt to locate documents that are of interest by specifying, for\nexample, sets of keywords. The query that a user has in mind usually cannot\nbe stated precisely; hence, information-retrieval systems order answers on the\nbasis of potential relevance.\n• Relevance ranking makes use of several types of information, such as:\n\u0000 Term frequency: how important each term is to each document.\n\u0000 Inverse document frequency.\n\u0000 Site popularity. Page rank and hub/authority rank are two ways to assign\nimportance to sites on the basis of links to the site.\n• Similarity of documents is used to retrieve documents similar to an example\ndocument. Synonyms and homonyms complicate the task of information re-\ntrieval.\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nVII. Other Topics\n22. Advanced Querying and \nInformation Retrieval\n851\n© The McGraw−Hill \nCompanies, 2001\n858\nChapter 22\nAdvanced Querying and Information Retrieval\n• Precision and recall are two measures of the effectiveness of an information\nretrieval system.\n• Directory structures are used to classify documents with other similar docu-\nments.\nReview Terms\n• Decision-support systems\n• Statistical analysis\n• Multidimensional data\n\u0000 Measure attributes\n\u0000 Dimension attributes\n• Cross-tabulation\n• Data cube\n• Online analytical processing\n(OLAP)\n\u0000 Pivoting\n\u0000 Slicing and dicing\n\u0000 Rollup and drill down\n• Multidimensional OLAP (MOLAP)\n• Relational OLAP (ROLAP)\n• Hybrid OLAP (HOLAP)\n• Extended aggregation\n\u0000 Variance\n\u0000 Standard deviation\n\u0000 Correlation\n\u0000 Regression\n• Ranking functions\n\u0000 Rank\n\u0000 Dense rank\n\u0000 Partition by\n• Windowing\n• Data mining\n• Prediction\n• Associations\n• Classiﬁcation\n\u0000 Training data\n\u0000 Test data\n• Decision-tree classiﬁers\n\u0000 Partitioning attribute\n\u0000 Partitioning condition\n\u0000 Purity\n–– Gini measure\n–– Entropy measure\n\u0000 Information gain\n\u0000 Information content\n\u0000 Information gain ratio\n\u0000 Continuous-valued attribute\n\u0000 Categorical attribute\n\u0000 Binary split\n\u0000 Multiway split\n\u0000 Overﬁtting\n• Bayesian classiﬁers\n\u0000 Bayes theorem\n\u0000 Naive Bayesian classiﬁers\n• Regression\n\u0000 Linear regression\n\u0000 Curve ﬁtting\n• Association rules\n\u0000 Population\n\u0000 Support\n\u0000 Conﬁdence\n\u0000 Large itemsets\n• Other types of associations\n• Clustering\n\u0000 Hierarchical clustering\n\u0000 Agglomerative clustering\n\u0000 Divisive clustering\n• Text mining\n• Data visualization\n• Data warehousing\n\u0000 Gathering data\n\u0000 Source-driven architecture\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nVII. Other Topics\n22. Advanced Querying and \nInformation Retrieval\n852\n© The McGraw−Hill \nCompanies, 2001\nExercises\n859\n\u0000 Destination-driven architec-\nture\n\u0000 Data cleansing\n–– Merge–purge\n–– Householding\n• Warehouse schemas\n\u0000 Fact table\n\u0000 Dimension tables\n\u0000 Star schema\n• Information retrieval systems\n• Keyword search\n• Full text retrieval\n• Term\n• Relevance ranking\n\u0000 Term frequency\n\u0000 Inverse document frequency\n\u0000 Relevance\n\u0000 Proximity\n• Stop words\n• Relevance using hyperlinks\n\u0000 Site popularity\n\u0000 Page rank\n\u0000 Hub/authority ranking\n• Similarity-based retrieval\n• Synonyms\n• Homonyms\n• Inverted index\n• False drop\n• False negative\n• False positive\n• Precision\n• Recall\n• Web crawlers\n• Directories\n• Classiﬁcation hierarchy\nExercises\n22.1 For each of the SQL aggregate functions sum, count, min and max, show how\nto compute the aggregate value on a multiset S1 ∪S2, given the aggregate\nvalues on multisets S1 and S2.\nBased on the above, give expressions to compute aggregate values with\ngrouping on a subset S of the attributes of a relation r(A, B, C, D, E), given\naggregate values for grouping on attributes T ⊇S, for the following aggregate\nfunctions:\na. sum, count, min and max\nb. avg\nc. standard deviation\n22.2 Show how to express group by cube(a, b, c, d) using rollup; your answer should\nhave only one group by clause.\n22.3 Give an example of a pair of groupings that cannot be expressed by using a\nsingle group by clause with cube and rollup.\n22.4 Given a relation S(student, subject, marks), write a query to ﬁnd the top n\nstudents by total marks, by using ranking.\n22.5 Given relation r(a, b, d, d), Show how to use the extended SQL features to gen-\nerate a histogram of d versus a, dividing a into 20 equal-sized partitions (that\nis, where each partition contains 5 percent of the tuples in r, sorted by a).\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nVII. Other Topics\n22. Advanced Querying and \nInformation Retrieval\n853\n© The McGraw−Hill \nCompanies, 2001\n860\nChapter 22\nAdvanced Querying and Information Retrieval\n22.6 Write a query to ﬁnd cumulative balances, equivalent to that shown in Sec-\ntion 22.2.5, but without using the extended SQL windowing constructs.\n22.7 Consider the balance attribute of the account relation. Write an SQL query to\ncompute a histogram of balance values, dividing the range 0 to the maximum\naccount balance present, into three equal ranges.\n22.8 Consider the sales relation from Section 22.2. Write an SQL query to compute\nthe cube operation on the relation, giving the relation in Figure 22.2. Do not\nuse the with cube construct.\n22.9 Construct a decision tree classiﬁer with binary splits at each node, using tu-\nples in relation r(A, B, C) shown below as training data; attribute C denotes\nthe class. Show the ﬁnal tree, and with each node show the best split for each\nattribute along with its information gain value.\n(1, 2, a), (2, 1, a), (2, 5, b), (3, 3, b), (3, 6, b), (4, 5, b), (5, 5, c), (6, 3, b), (6, 7, c)\n22.10 Suppose there are two classiﬁcation rules, one that says that people with salaries\nbetween $10,000 and $20,000 have a credit rating of good, and another that says\nthat people with salaries between $20,000 and $30,000 have a credit rating of\ngood. Under what conditions can the rules be replaced, without any loss of in-\nformation, by a single rule that says people with salaries between $10,000 and\n$30,000 have a credit rating of good.\n22.11 Suppose half of all the transactions in a clothes shop purchase jeans, and one\nthird of all transactions in the shop purchase T-shirts. Suppose also that half\nof the transactions that purchase jeans also purchase T-shirts. Write down all\nthe (nontrivial) association rules you can deduce from the above information,\ngiving support and conﬁdence of each rule.\n22.12 Consider the problem of ﬁnding large itemsets.\na. Describe how to ﬁnd the support for a given collection of itemsets by using\na single scan of the data. Assume that the itemsets and associated informa-\ntion, such as counts, will ﬁt in memory.\nb. Suppose an itemset has support less than j. Show that no superset of this\nitemset can have support greater than or equal to j.\n22.13 Describe beneﬁts and drawbacks of a source-driven architecture for gathering\nof data at a data-warehouse, as compared to a destination-driven architecture.\n22.14 Consider the schema depicted in Figure 22.9. Give an SQL:1999 query to sum-\nmarize sales numbers and price by store and date, along with the hierarchies\non store and date.\n22.15 Compute the relevance (using appropriate deﬁnitions of term frequency and\ninverse document frequency) of each of the questions in this chapter to the\nquery “SQL relation.”\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nVII. Other Topics\n22. Advanced Querying and \nInformation Retrieval\n854\n© The McGraw−Hill \nCompanies, 2001\nBibliographical Notes\n861\n22.16 What is the difference between a false positive and a false drop? If it is essential\nthat no relevant information be missed by an information retrieval query, is it\nacceptable to have either false positives or false drops? Why?\n22.17 Suppose you want to ﬁnd documents that contain at least k of a given set of n\nkeywords. Suppose also you have a keyword index that gives you a (sorted) list\nof identiﬁers of documents that contain a speciﬁed keyword. Give an efﬁcient\nalgorithm to ﬁnd the desired set of documents.\nBibliographical Notes\nGray et al. [1995] and Gray et al. [1997] describe the data-cube operator. Efﬁcient algo-\nrithms for computing data cubes are described by Agarwal et al. [1996], Harinarayan\net al. [1996] and Ross and Srivastava [1997]. Descriptions of extended aggregation\nsupport in SQL:1999 can be found in the product manuals of database systems such\nas Oracle and IBM DB2. Deﬁnitions of statistical functions can be found in standard\nstatistics textbooks such as Bulmer [1979] and Ross [1999].\nWitten and Frank [1999] and Han and Kamber [2000] provide textbook coverage\nof data mining. Mitchell [1997] is a classic textbook on machine learning, and c",
    "precision": 0.024331870761866773,
    "recall": 0.9838709677419355,
    "iou": 0.024322169059011165,
    "f1": 0.04748929544569872,
    "gold_tokens_count": 62,
    "retrieved_tokens_count": 2507,
    "intersection_tokens": 61
  },
  {
    "config_name": "chars_weighted",
    "config": {
      "name": "chars_weighted",
      "index_prefix": "textbook_index",
      "chunking_strategy": "chars",
      "overlap": 0,
      "fusion": "weighted",
      "bm25_weight": 0.3,
      "tag_weight": 0.2,
      "top_k": 5,
      "embed_model": "sentence-transformers/all-MiniLM-L6-v2"
    },
    "question": "Define the concept of aggregation. Give two examples where this concept is useful.",
    "gold_text": "2.7.5 Aggregation One limitation of the E-R model is that it cannot express relationships among rela- tionships. To illustrate the need for such a construct, consider the ternary relationship works-on, which we saw earlier, between a employee, branch, and job (see Figure 2.13). Now, suppose we want to record managers for tasks performed by an employee at a branch; that is, we want to record managers for (employee, branch, job) combinations. Let us assume that there is an entity set manager. One alternative for representing this relationship is to create a quaternary relation- ship manages between employee, branch, job, and manager. (A quaternary relationship is required—a binary relationship between manager and employee would not permit us to represent which (branch, job) combinations of an employee are managed by which manager.) Using the basic E-R modeling constructs, we obtain the E-R diagram of Figure 2.18. (We have omitted the attributes of the entity sets, for simplicity.) It appears that the relationship sets works-on and manages can be combined into one single relationship set. Nevertheless, we should not combine them into a single relationship, since some employee, branch, job combinations many not have a manager. There is redundant information in the resultant figure, however, since every em- ployee, branch, job combination in manages is also in works-on. If the manager were a value rather than an manager entity, we could instead make manager a multivalued at- tribute of the relationship works-on. But doing so makes it more difficult (logically as well as in execution cost) to find, for example, employee-branch-job triples for which a manager is responsible. Since the manager is a manager entity, this alternative is ruled out in any case. The best way to model a situation such as the one just described is to use aggrega- tion. Aggregation is an abstraction through which relationships are treated as higher- level entities. Thus, for our example, we regard the relationship set works-on (relating the entity sets employee, branch, and job) as a higher-level entity set called works-on. Such an entity set is treated in the same manner as is any other entity set. We can then create a binary relationship manages between works-on and manager to represent who manages what tasks. Figure 2.19 shows a notation for aggregation commonly used to represent the above situation.",
    "retrieved_text": "loyees at each\nbranch of the bank separately, rather than the sum for the entire bank. To do so, we\nneed to partition the relation pt-works into groups based on the branch, and to apply\nthe aggregate function on each group.\nThe following expression using the aggregation operator G achieves the desired\nresult:\nbranch-nameGsum(salary)(pt-works)\nIn the expression, the attribute branch-name in the left-hand subscript of G indicates\nthat the input relation pt-works must be divided into groups based on the value of\nbranch-name. Figure 3.28 shows the resulting groups. The expression sum(salary) in\nthe right-hand subscript of G indicates that for each group of tuples (that is, each\nbranch), the aggregation function sum must be applied on the collection of values of\nthe salary attribute. The output relation consists of tuples with the branch name, and\nthe sum of the salaries for the branch, as shown in Figure 3.29.\nThe general form of the aggregation operation G is as follows:\nG1,G2,...,GnGF1(A1), F2(A2),..., Fm(Am)(E)\nwhere E is any relational-algebra expression; G1, G2, . . . , Gn constitute a list of at-\ntributes on which to group; each Fi is an aggregate function; and each Ai is an at-\nemployee-name\nbranch-name\nsalary\nRao\nAustin\n1500\nSato\nAustin\n1600\nJohnson\nDowntown\n1500\nLoreena\nDowntown\n1300\nPeterson\nDowntown\n2500\nAdams\nPerryridge\n1500\nBrown\nPerryridge\n1300\nGopal\nPerryridge\n5300\nFigure 3.28\nThe pt-works relation after grouping.\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nI. Data Models\n3. Relational Model\n115\n© The McGraw−Hill \nCompanies, 2001\n3.3\nExtended Relational-Algebra Operations\n107\nbranch-name\nsum of salary\nAustin\n3100\nDowntown\n5300\nPerryridge\n8100\nFigure 3.29\nResult of branch-nameGsum(salary)(pt-works).\ntribute name. The meaning of the operation is as follows. The tuples in the result of\nexpression E are partitioned into groups in such a way that\n1. All tuples in a group have the same values for G1, G2, . . . , Gn.\n2. Tuples in different groups have different values for G1, G2, . . . , Gn.\nThus, the groups can be identiﬁed by the values of attributes G1, G2, . . . , Gn. For each\ngroup (g1, g2, . . . , gn), the result has a tuple (g1, g2, . . . , gn, a1, a2, . . . , am) where, for\neach i, ai is the result of applying the aggregate function Fi on the multiset of values\nfor attribute Ai in the group.\nAs a special case of the aggregate operation, the list of attributes G1, G2, . . . , Gn can\nbe empty, in which case there is a single group containing all tuples in the relation.\nThis corresponds to aggregation without grouping.\nGoing back to our earlier example, if we want to ﬁnd the maximum salary for\npart-time employees at each branch, in addition to the sum of the salaries, we write\nthe expression\nbranch-nameGsum(salary),max(salary)(pt-works)\nAs in generalized projection, the result of an aggregation operation does not have a\nname. We can apply a rename operation to the result in order to give it a name. As\na notational convenience, attributes of an aggregation operation can be renamed as\nillustrated below:\nbranch-nameGsum(salary) as sum-salary,max(salary) as max-salary(pt-works)\nFigure 3.30 shows the result of the expression.\nbranch-name\nsum-salary\nmax-salary\nAustin\n3100\n1600\nDowntown\n5300\n2500\nPerryridge\n8100\n5300\nFigure 3.30\nResult of\nbranch-nameGsum(salary) as sum-salary,max(salary) as max-salary(pt-works).\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nI. Data Models\n3. Relational Model\n116\n© The McGraw−Hill \nCompanies, 2001\n108\nChapter 3\nRelational Model\nemployee-name\nstreet\ncity\nCoyote\nToon\nHollywood\nRabbit\nTunnel\nCarrotville\nSmith\nRevolver\nDeath Valley\nWilliams\nSeaview\nSeattle\nemployee-name\nbranch-name\nsalary\nCoyote\nMesa\n1500\nRabbit\nMesa\n1300\nGates\nRedmond\n5300\nWilliams\nRedmond\n1500\nFigure 3.31\nThe employee and ft-works relations.\n3.3.3\nOuter Join\nThe outer-join operation is an extension of the join operation to deal with missing\ninformation. Suppose that we have the relations with the following schemas, which\ncontain data on full-time employees:\nemployee (employee-name, street, city)\nft-works (employee-name, branch-name, salary)\nConsider the employee and ft-works relations in Figure 3.31. Suppose that we want\nto generate a single relation with all the information (street, city, branch name, and\nsalary) about full-time employees. A possible approach would be to use the natural-\njoin operation as follows:\nemployee\n\u0001 ft-works\nThe result of this expression appears in Figure 3.32. Notice that we have lost the street\nand city information about Smith, since the tuple describing Smith is absent from\nthe ft-works relation; similarly, we have lost the branch name and salary information\nabout Gates, since the tuple describing Gates is absent from the employee relation.\nWe can use the outer-join operation to avoid this loss of information. There are\nactually three forms of the operation: left outer join, denoted\n\u0001; right outer join, de-\nnoted\n\u0001 ; and full outer join, denoted\n\u0001 . All three forms of outer join compute the\njoin, and add extra tuples to the result of the join. The results of the expressions\nemployee-name\nstreet\ncity\nbranch-name\nsalary\nCoyote\nToon\nHollywood\nMesa\n1500\nRabbit\nTunnel\nCarrotville\nMesa\n1300\nWilliams\nSeaview\nSeattle\nRedmond\n1500\nFigure 3.32\nThe result of employee\n\u0001 ft-works.\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nI. Data Models\n3. Relational Model\n117\n© The McGraw−Hill \nCompanies, 2001\n3.3\nExtended Relational-Algebra Operations\n109\nemployee-name\nstreet\ncity\nbranch-name\nsalary\nCoyote\nToon\nHollywood\nMesa\n1500\nRabbit\nTunnel\nCarrotville\nMesa\n1300\nWilliams\nSeaview\nSeattle\nRedmond\n1500\nSmith\nRevolver\nDeath Valley\nnull\nnull\nFigure 3.33\nResult of employee\n\u0001 ft-works.\nemployee\n\u0001 ft-works,, employee\n\u0001 ft-works, and employee\n\u0001 ft-works appear in\nFigures 3.33, 3.34, and 3.35, respectively.\nThe left outer join (\n\u0001) takes all tuples in the left relation that did not match with\nany tuple in the right relation, pads the tuples with null values for all other attributes\nfrom the right relation, and adds them to the result of the natural join. In Figure 3.33,\ntuple (Smith, Revolver, Death Valley, null, null) is such a tuple. All information from\nthe left relation is present in the result of the left outer join.\nThe right outer join (\u0001 ) is symmetric with the left outer join: It pads tuples from\nthe right relation that did not match any from the left relation with nulls and adds\nthem to the result of the natural join. In Figure 3.34, tuple (Gates, null, null, Redmond,\n5300) is such a tuple. Thus, all information from the right relation is present in the\nresult of the right outer join.\nThe full outer join(\n\u0001 ) does both of those operations, padding tuples from the\nleft relation that did not match any from the right relation, as well as tuples from the\nright relation that did not match any from the left relation, and adding them to the\nresult of the join. Figure 3.35 shows the result of a full outer join.\nSince outer join operations may generate results containing null values, we need\nto specify how the different relational-algebra operations deal with null values. Sec-\ntion 3.3.4 deals with this issue.\nIt is interesting to note that the outer join operations can be expressed by the basic\nrelational-algebra operations. For instance, the left outer join operation, r\n\u0001 s, can\nbe written as\n(r\n\u0001 s) ∪(r −ΠR(r\n\u0001 s)) × {(null, . . . , null)}\nwhere the constant relation {(null, . . . , null)} is on the schema S −R.\nemployee-name\nstreet\ncity\nbranch-name\nsalary\nCoyote\nToon\nHollywood\nMesa\n1500\nRabbit\nTunnel\nCarrotville\nMesa\n1300\nWilliams\nSeaview\nSeattle\nRedmond\n1500\nGates\nnull\nnull\nRedmond\n5300\nFigure 3.34\nResult of employee\n\u0001 ft-works.\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nI. Data Models\n3. Relational Model\n118\n© The McGraw−Hill \nCompanies, 2001\n110\nChapter 3\nRelational Model\nemployee-name\nstreet\ncity\nbranch-name\nsalary\nCoyote\nToon\nHollywood\nMesa\n1500\nRabbit\nTunnel\nCarrotville\nMesa\n1300\nWilliams\nSeaview\nSeattle\nRedmond\n1500\nSmith\nRevolver\nDeath Valley\nnull\nnull\nGates\nnull\nnull\nRedmond\n5300\nFigure 3.35\nResult of employee\n\u0001 ft-works.\n3.3.4\nNull Values∗∗\nIn this section, we deﬁne how the various relational algebra operations deal with null\nvalues and complications that arise when a null value participates in an arithmetic\noperation or in a comparison. As we shall see, there is often more than one possible\nway of dealing with null values, and as a result our deﬁnitions can sometimes be\narbitrary. Operations and comparisons on null values should therefore be avoided,\nwhere possible.\nSince the special value null indicates “value unknown or nonexistent,” any arith-\nmetic operations (such as +, −, ∗, /) involving null values must return a null result.\nSimilarly, any comparisons (such as <, <=, >, >=, ̸=) involving a null value eval-\nuate to special value unknown; we cannot say for sure whether the result of the\ncomparison is true or false, so we say that the result is the new truth value unknown.\nComparisons involving nulls may occur inside Boolean expressions involving the\nand, or, and not operations. We must therefore deﬁne how the three Boolean opera-\ntions deal with the truth value unknown.\n• and: (true and unknown) = unknown; (false and unknown) = false; (unknown and\nunknown) = unknown.\n• or: (true or unknown) = true; (false or unknown) = unknown; (unknown or un-\nknown) = unknown.\n• not: (not unknown) = unknown.\nWe are now in a position to outline how the different relational operations deal\nwith null values. Our deﬁnitions follow those used in the SQL language.\n• select: The selection operation evaluates predicate P in σP (E) on each tuple t\nin E. If the predicate returns the value true, t is added to the result. Otherwise,\nif the predicate returns unknown or false, t is not added to the result.\n• join: Joins can be expressed as a cross product followed by a selection. Thus,\nthe deﬁnition of how selection handles nulls also deﬁnes how join operations\nhandle nulls.\nIn a natural join, say r\n\u0001 s, we can see from the above deﬁnition that if two\ntuples, tr ∈r and ts ∈s, both have a null value in a common attribute, then\nthe tuples do not match.\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nI. Data Models\n3. Relational Model\n119\n© The McGraw−Hill \nCompanies, 2001\n3.4\nModiﬁcation of the Database\n111\n• projection: The projection operation treats nulls just like any other value when\neliminating duplicates. Thus, if two tuples in the projection result are exactly\nthe same, and both have nulls in the same ﬁelds, they are treated as duplicates.\nThe decision is a little arbitrary since, without knowing the actual value,\nwe do not know if the two instances of null are duplicates or not.\n• union, intersection, difference: These operations treat nulls just as the projec-\ntion operation does; they treat tuples that have the same values on all ﬁelds as\nduplicates even if some of the ﬁelds have null values in both tuples.\nThe behavior is rather arbitrary, especially in the case of intersection and\ndifference, since we do not know if the actual values (if any) represented by\nthe nulls are the same.\n• generalized projection: We outlined how nulls are handled in expressions\nat the beginning of Section 3.3.4. Duplicate tuples containing null values are\nhandled as in the projection operation.\n• aggregate: When nulls occur in grouping attributes, the aggregate operation\ntreats them just as in projection: If two tuples are the same on all grouping\nattributes, the operation places them in the same group, even if some of their\nattribute values are null.\nWhen nulls occur in aggregated attributes, the operation deletes null values\nat the outset, before applying aggregation. If the resultant multiset is empty,\nthe aggregate result is null.\nNote that the treatment of nulls here is different from that in ordinary arith-\nmetic expressions; we could have deﬁned the result of an aggregate operation\nas null if even one of the aggregated values is null. However, this would mean\na single unknown value in a large group could make the aggregate result on\nthe group to be null, and we would lose a lot of useful information.\n• outer join: Outer join operations behave just like join operations, except on\ntuples that do not occur in the join result. Such tuples may be added to the\nresult (depending on whether the operation is\n\u0001,\n\u0001 , or\n\u0001 ), padded with\nnulls.\n3.4\nModiﬁcation of the Database\nWe have limited our attention until now to the extraction of information from the\ndatabase. In this section, we address how to add, remove, or change information in\nthe database.\nWe express database modiﬁcations by using the assignment operation. We make\nassignments to actual database relations by using the same notation as that described\nin Section 3.2.3 for assignment.\n3.4.1\nDeletion\nWe express a delete request in much the same way as a query. However, instead of\ndisplaying tuples to the user, we remove the selected tuples from the database. We\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nI. Data Models\n3. Relational Model\n120\n© The McGraw−Hill \nCompanies, 2001\n112\nChapter 3\nRelational Model\ncan delete only whole tuples; we cannot delete values on only particular attributes.\nIn relational algebra a deletion is expressed by\nr ←r −E\nwhere r is a relation and E is a relational-algebra query.\nHere are several examples of relational-algebra delete requests:\n• Delete all of Smith’s account records.\ndepositor ←depositor −σcustomer-name = “Smith” (depositor)\n• Delete all loans with amount in the range 0 to 50.\nloan ←loan −σamount≥0 and amount≤50 (loan)\n• Delete all accounts at branches located in Needham.\nr1 ←σbranch-city = “Needham” (account\n\u0001 branch)\nr2 ←Πbranch-name, account-number, balance (r1)\naccount ←account −r2\nNote that, in the ﬁnal example, we simpliﬁed our expression by using assign-\nment to temporary relations (r1 and r2).\n3.4.2\nInsertion\nTo insert data into a relation, we either specify a tuple to be inserted or write a query\nwhose result is a set of tuples to be inserted. Obviously, the attribute values for in-\nserted tuples must be members of the attribute’s domain. Similarly, tuples inserted\nmust be of the correct arity. The relational algebra expresses an insertion by\nr ←r ∪E\nwhere r is a relation and E is a relational-algebra expression. We express the insertion\nof a single tuple by letting E be a constant relation containing one tuple.\nSuppose that we wish to insert the fact that Smith has $1200 in account A-973 at\nthe Perryridge branch. We write\naccount ←account ∪{(A-973, “Perryridge”, 1200)}\ndepositor ←depositor ∪{(“Smith”, A-973)}\nMore generally, we might want to insert tuples on the basis of the result of a query.\nSuppose that we want to provide as a gift for all loan customers of the Perryridge\nbranch a new $200 savings account. Let the loan number serve as the account number\nfor this savings account. We write\nr1 ←(σbranch-name = “Perryridge” (borrower\n\u0001 loan))\nr2 ←Πloan-number, branch-name (r1)\naccount ←account ∪(r2 × {(200)})\ndepositor ←depositor ∪Πcustomer-name, loan-number (r1)\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nI. Data Models\n3. Relational Model\n121\n© The McGraw−Hill \nCompanies, 2001\n3.5\nViews\n113\nInstead of specifying a tuple as we did earlier, we specify a set of tuples that is in-\nserted into both the account and depositor relation. Each tuple in the account relation\nhas an account-number (which is the same as the loan number), a branch-name (Per-\nryridge), and the initial balance of the new account ($200). Each tuple in the depositor\nrelation has as customer-name the name of the loan customer who is being given the\nnew account and the same account number as the corresponding account tuple.\n3.4.3\nUpdating\nIn certain situations, we may wish to change a value in a tuple without changing all\nvalues in the tuple. We can use the generalized-projection operator to do this task:\nr ←ΠF1,F2,...,Fn(r)\nwhere each Fi is either the ith attribute of r, if the ith attribute is not updated, or, if\nthe attribute is to be updated, Fi is an expression, involving only constants and the\nattributes of r, that gives the new value for the attribute.\nIf we want to select some tuples from r and to update only them, we can use\nthe following expression; here, P denotes the selection condition that chooses which\ntuples to update:\nr ←ΠF1,F2,...,Fn(σP (r)) ∪(r −σP (r))\nTo illustrate the use of the update operation, suppose that interest payments are\nbeing made, and that all balances are to be increased by 5 percent. We write\naccount ←Πaccount-number, branch-name, balance ∗1.05 (account)\nNow suppose that accounts with balances over $10,000 receive 6 percent interest,\nwhereas all others receive 5 percent. We write\naccount ←ΠAN,BN, balance ∗1.06 (σbalance>10000 (account))\n∪ΠAN, BN balance ∗1.05 (σbalance≤10000 (account))\nwhere the abbreviations AN and BN stand for account-number and branch-name, re-\nspectively.\n3.5\nViews\nIn our examples up to this point, we have operated at the logical-model level. That\nis, we have assumed that the relations in thecollection we are given are the actual\nrelations stored in the database.\nIt is not desirable for all users to see the entire logical model. Security consider-\nations may require that certain data be hidden from users. Consider a person who\nneeds to know a customer’s loan number and branch name, but has no need to see\nthe loan amount. This person should see a relation described, in the relational alge-\nbra, by\nΠcustomer-name, loan-number, branch-name (borrower\n\u0001 loan)\nAside from security concerns, we may wish to create a personalized collection of\nrelations that is better matched to a certain user’s intuition than is the logical model.\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nI. Data Models\n3. Relational Model\n122\n© The McGraw−Hill \nCompanies, 2001\n114\nChapter 3\nRelational Model\nAn employee in the advertising department, for example, might like to see a relation\nconsisting of the customers who have either an account or a loan at the bank, and\nthe branches with which they do business. The relation that we would create for that\nemployee is\nΠbranch-name, customer-name (depositor\n\u0001 account)\n∪Πbranch-name, customer-name (borrower\n\u0001 loan)\nAny relation that is not part of the logical model, but is made visible to a user as a\nvirtual relation, is called a view. It is possible to support a large number of views on\ntop of any given set of actual relations.\n3.5.1\nView Deﬁnition\nWe deﬁne a view by using the create view statement. To deﬁne a view, we must give\nthe view a name, and must state the query that computes the view. The form of the\ncreate view statement is\ncreate view v as <query expression>\nwhere <query expression> is any legal relational-algebra query expression. The view\nname is represented by v.\nAs an example, consider the view consisting of branches and their customers. We\nwish this view to be called all-customer. We deﬁne this view as follows:\ncreate view all-customer as\nΠbranch-name, customer-name (depositor\n\u0001 account)\n∪Πbranch-name, customer-name (borrower\n\u0001 loan)\nOnce we have deﬁned a view, we can use the view name to refer to the virtual re-\nlation that the view generates. Using the view all-customer, we can ﬁnd all customers\nof the Perryridge branch by writing\nΠcustomer-name (σbranch-name = “Perryridge” (all-customer))\nRecall that we wrote the same query in Section 3.2.1 without using views.\nView names may appear in any place where a relation name may appear, so long\nas no update operations are executed on the views. We study the issue of update\noperations on views in Section 3.5.2.\nView deﬁnition differs from the relational-algebra assignment operation. Suppose\nthat we deﬁne relation r1 as follows:\nr1 ←Πbranch-name, customer-name (depositor\n\u0001 account)\n∪Πbranch-name, customer-name(borrower\n\u0001 loan\n\nration.\nA cluster is another form of organization for table data (see Section 11.7). The\nconcept, in this context, should not be confused with other meanings of the word\ncluster, such as those relating to hardware architecture. In a cluster, rows from dif-\nferent tables are stored together in the same block on the basis of some common\ncolumns. For example, a department table and an employee table could be clustered\nso that each row in the department table is stored together with all the employee\nrows for those employees who work in that department. The primary key/foreign\nkey values are used to determine the storage location. This organization gives per-\nformance beneﬁts when the two tables are joined, but without the space penalty of a\ndenormalized schema, since the values in the department table are not repeated for\neach employee. As a tradeoff, a query involving only the department table may have\nto involve a substantially larger number of blocks than if that table had been stored\non its own.\nThe cluster organization implies that a row belongs in a speciﬁc place; for example,\na new employee row must be inserted with the other rows for the same department.\nTherefore, an index on the clustering column is mandatory. An alternative organiza-\ntion is a hash cluster. Here, Oracle computes the location of a row by applying a hash\nfunction to the value for the cluster column. The hash function maps the row to a\nspeciﬁc block in the hash cluster. Since no index traversal is needed to access a row\naccording to its cluster column value, this organization can save signiﬁcant amounts\nof disk I/O. However, the number of hash buckets and other storage parameters must\nbe set carefully to avoid performance problems due to too many collisions or space\nwastage due to empty hash buckets.\nBoth the hash cluster and regular cluster organization can be applied to a single\ntable. Storing a table as a hash cluster with the primary key column as the cluster key\ncan allow an access based on a primary key value with a single disk I/O provided\nthat there is no overﬂow for that data block.\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nIII. Object−Based \nDatabases and XML\n8. Object−Oriented \nDatabases\n317\n© The McGraw−Hill \nCompanies, 2001\n928\nChapter 25\nOracle\n25.3.4\nIndex-Organized Tables\nIn an index organized table, records are stored in an Oracle B-tree index instead of in a\nheap. An index-organized table requires that a unique key be identiﬁed for use as the\nindex key. While an entry in a regular index contains the key value and row-id of the\nindexed row, an index-organized table replaces the row-id with the column values\nfor the remaining columns of the row. Compared to storing the data in a regular heap\ntable and creating an index on the key columns, index-organized table can improve\nboth performance and space utilization. Consider looking up all the column values\nof a row, given its primary key value. For a heap table, that would require an index\nprobe followed by a table access by row-id. For an index-organized table, only the\nindex probe is necessary.\nSecondary indices on nonkey columns of an index-organized table are different\nfrom indices on a regular heap table. In a heap table, each row has a ﬁxed row-id\nthat does not change. However, a B-tree is reorganized as it grows or shrinks when\nentries are inserted or deleted, and there is no guarantee that a row will stay in a\nﬁxed place inside an index-organized table. Hence, a secondary index on an index-\norganized table contains not normal row-ids, but logical row-ids instead. A logical\nrow-id consists of two parts: a physical row-id corresponding to where the row was\nwhen the index was created or last rebuilt and a value for the unique key. The phys-\nical row-id is referred to as a “guess” since it could be incorrect if the row has been\nmoved. If so, the other part of a logical row-id, the key value for the row, is used to\naccess the row; however, this access is slower than if the guess had been correct, since\nit involves a traversal of the B-tree for the index-organized table from the root all the\nway to the leaf nodes, potentially incurring several disk I/Os. However, if a table is\nhighly volatile and a large percentage of the guesses are likely to be wrong, it can be\nbetter to create the secondary index with only key values, since using an incorrect\nguess may result in a wasted disk I/O.\n25.3.5\nIndices\nOracle supports several different types of indices. The most commonly used type is a\nB-tree index, created on one or multiple columns. (Note: in the terminology of Oracle\n(as also in several other database systems) a B-tree index is what is referred to as a\nB+-tree index in Chapter 12.) Index entries have the following format: For an index\non columns col1, col2, and col3, each row in the table where at least one of the columns\nhas a nonnull value would result in the index entry\n< col1 >< col2 >< col3 >< row-id >\nwhere < coli > denotes the value for column i and < row-id > is the row-id for\nthe row. Oracle can optionally compress the preﬁx of the entry to save space. For\nexample, if there are many repeated combinations of < col1 >< col2 > values, the\nrepresentation of each distinct < col1 >< col2 > preﬁx can be shared between the\nentries that have that combination of values, rather than stored explicitly for each\nsuch entry. Preﬁx compression can lead to substantial space savings.\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nIII. Object−Based \nDatabases and XML\n8. Object−Oriented \nDatabases\n318\n© The McGraw−Hill \nCompanies, 2001\n25.3\nStorage and Indexing\n929\n25.3.6\nBitmap Indices\nBitmap indices (described in Section 12.9.4) use a bitmap representation for index\nentries, which can lead to substantial space saving (and therefore disk I/O savings),\nwhen the indexed column has a moderate number of distinct values. Bitmap indices\nin Oracle use the same kind of B-tree structure to store the entries as a regular in-\ndex. However, where a regular index on a column would have entries of the form\n< col1 >< row-id >, a bitmap index entry has the form\n< col1 >< startrow-id >< endrow-id >< compressedbitmap >\nThe bitmap conceptually represents the space of all possible rows in the table be-\ntween the start and end row-id. The number of such possible rows in a block depends\non how many rows can ﬁt into a block, which is a function of the number of columns\nin the table and their data types. Each bit in the bitmap represents one such possible\nrow in a block. If the column value of that row is that of the index entry, the bit is set\nto 1. If the row has some other value, or the row does not actually exist in the table,\nthe bit is set to 0. (It is possible that the row does not actually exist because a table\nblock may well have a smaller number of rows than the number that was calculated\nas the maximum possible.) If the difference is large, the result may be long strings\nof consecutive zeros in the bitmap, but the compression algorithm deals with such\nstrings of zeros, so the negative effect is limited.\nThe compression algorithm is a variation of a compression technique called Byte-\nAligned Bitmap Compression (BBC). Essentially, a section of the bitmap where the\ndistance between two consecutive ones is small enough is stored as verbatim bitmaps.\nIf the distance between two ones is sufﬁciently large—that is, there is a sufﬁcient\nnumber of adjacent zeros between them—a runlength of zeros, that is the number of\nzeros, is stored.\nBitmap indices allow multiple indices on the same table to be combined in the\nsame access path if there are multiple conditions on indexed columns in the where\nclause of a query. For example, for the condition\n(col1 = 1 or col1 = 2) and col2 > 5 and col3 <> 10\nOracle would be able to calculate which rows match the condition by performing\nBoolean operations on bitmaps from indices on the three columns. In this case, these\noperations would take place for each index:\n• For the index on col1, the bitmaps for key values 1 and 2 would be ored.\n• For the index on col2, all the bitmaps for key values > 5 would be merged in\nan operation that corresponds to a logical or.\n• For the index on col3, the bitmaps for key values 10 and null would be re-\ntrieved. Then, a Boolean and would be performed on the results from the ﬁrst\ntwo indices, followed by two Boolean minuses of the bitmaps for values 10\nand null for col3.\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nIII. Object−Based \nDatabases and XML\n8. Object−Oriented \nDatabases\n319\n© The McGraw−Hill \nCompanies, 2001\n930\nChapter 25\nOracle\nAll operations are performed directly on the compressed representation of the bit-\nmaps—no decompression is necessary—and the resulting (compressed) bitmap rep-\nresents those rows that match all the logical conditions.\nThe ability to use the Boolean operations to combine multiple indices is not lim-\nited to bitmap indices. Oracle can convert row-ids to the compressed bitmap repre-\nsentation, so it can use a regular B-tree index anywhere in a Boolean tree of bitmap\noperation simply by putting a row-id-to-bitmap operator on top of the index access\nin the execution plan.\nAs a rule of thumb, bitmap indices tend to be more space efﬁcient than regular\nB-tree indices if the number of distinct key values is less than half the number of\nrows in the table. For example, in a table with 1 million rows, an index on a column\nwith less than 500,000 distinct values would probably be smaller if it were created as\na bitmap index. For columns with a very small number of distinct values—for ex-\nample, columns referring to properties such as country, state, gender, marital status,\nand various status ﬂags—a bitmap index might require only a small fraction of the\nspace of a regular B-tree index. Any such space advantage can also give rise to corre-\nsponding performance advantages in the form of fewer disk I/Os when the index is\nscanned.\n25.3.7\nFunction-Based Indices\nIn addition to creating indices on one or multiple columns of a table, Oracle allows\nindices to be created on expressions that involve one or more columns, such as col1 +\ncol2 ∗5. For example, by creating an index on the expression upper(name), where upper\nis a function that returns the uppercase version of a string, and name is a column, it is\npossible to do case-insensitive searches on the name column. In order to ﬁnd all rows\nwith name “van Gogh” efﬁciently, the condition\nupper(name) = ’VAN GOGH’\nwould be used in the where clause of the query. Oracle then matches the condition\nwith the index deﬁnition and concludes that the index can be used to retrieve all the\nrows matching “van Gogh” regardless of how the name was capitalized when it was\nstored in the database. A function-based index can be created as either a bitmap or a\nB-tree index.\n25.3.8\nJoin Indices\nA join index is an index where the key columns are not in the table that is referenced\nby the row-ids in the index. Oracle supports bitmap join indices primarily for use\nwith star schemas (see Section 22.4.2). For example, if there is a column for product\nnames in a product dimension table, a bitmap join index on the fact table with this key\ncolumn could be used to retrieve the fact table rows that correspond to a product with\na speciﬁc name, although the name is not stored in the fact table. How the rows in\nthe fact and dimension tables correspond is based on a join condition that is speciﬁed\nwhen the index is created, and becomes part of the index metadata. When a query is\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nIII. Object−Based \nDatabases and XML\n8. Object−Oriented \nDatabases\n320\n© The McGraw−Hill \nCompanies, 2001\n25.3\nStorage and Indexing\n931\nprocessed, the optimizer will look for the same join condition in the where clause of\nthe query in order to determine if the join index is applicable.\nOracle allows bitmap join indices to have more than one key column and these\ncolumns can be in different tables. In all cases, the join conditions between the fact\ntable on which the index is built and the dimension tables must refer to unique keys\nin the dimension tables; that is, an indexed row in the fact table must correspond to\na unique row in each of the dimension tables.\nOracle can combine a bitmap join index on a fact table with other indices on the\nsame table—whether join indices or not—by using the operators for Boolean bitmap\noperations. For example, consider a schema with a fact table for sales, and dimension\ntables for customers, products, and time. Suppose a query requests information about\nsales to customers in a certain zip code who bought products in a certain product cat-\negory during a certain time period. If a multicolumn bitmap join index exists where\nthe key columns are the constrained dimension table columns (zip code, product cat-\negory and time), Oracle can use the join index to ﬁnd rows in the fact table that match\nthe constraining conditions. However, if individual, single-column indices exist for\nthe key columns (or a subset of them), Oracle can retrieve bitmaps for fact table rows\nthat match each individual condition, and use the Boolean and operation to generate\na fact table bitmap for those rows that satisfy all the conditions. If the query contains\nconditions on some columns of the fact table, indices on those columns could be in-\ncluded in the same access path, even if they were regular B-tree indices or domain\nindices (domain indices are described below in Section 25.3.9).\n25.3.9\nDomain Indices\nOracle allows tables to be indexed by index structures that are not native to Oracle.\nThis extensibility feature of the Oracle server allows software vendors to develop\nso-called cartridges with functionality for speciﬁc application domains, such as text,\nspatial data, and images, with indexing functionality beyond that provided by the\nstandard Oracle index types. In implementing the logic for creating, maintaining,\nand searching the index, the index designer must ensure that it adheres to a speciﬁc\nprotocol in its interaction with the Oracle server.\nA domain index must be registered in the data dictionary, together with the oper-\nators it supports. Oracle’s optimizer considers domain indices as one of the possible\naccess paths for a table. Oracle allows cost functions to be registered with the opera-\ntors so that the optimizer can compare the cost of using the domain index to those of\nother access paths.\nFor example, a domain index for advanced text searches may support an operator\ncontains. Once this operator has been registered, the domain index will be considered\nas an access path for a query like\nselect *\nfrom employees\nwhere contains(resume, ’LINUX’)\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nIII. Object−Based \nDatabases and XML\n8. Object−Oriented \nDatabases\n321\n© The McGraw−Hill \nCompanies, 2001\n932\nChapter 25\nOracle\nwhere resume is a text column in the employee table. The domain index can be stored\nin either an external data ﬁle or inside an Oracle index-organized table.\nA domain index can be combined with other (bitmap or B-tree) indices in the same\naccess path by converting between the row-id and bitmap representation and using\nBoolean bitmap operations.\n25.3.10\nPartitioning\nOracle supports various kinds of horizontal partitioning of tables and indices, and\nthis feature plays a major role in Oracle’s ability to support very large databases. The\nability to partition a table or index has advantages in many areas.\n• Backup and recovery are easier and faster, since they can be done on individ-\nual partitions rather than on the table as a whole.\n• Loading operations in a data warehousing environment are less intrusive:\ndata can be added to a partition, and then the partition added to a table, which\nis an instantaneous operation. Likewise, dropping a partition with obsolete\ndata from a table is very easy in a data warehouse that maintains a rolling\nwindow of historical data.\n• Query performance beneﬁts substantially, since the optimizer can recognize\nthat only a subset of the partitions of a table need to be accessed in order to\nresolve a query (partition pruning). Also, the optimizer can recognize that in\na join, it is not necessary to try to match all rows in one table with all rows in\nthe other, but that the joins need to be done only between matching pairs of\npartitions (partitionwise join).\nEach row in a partitioned table is associated with a speciﬁc partition. This associa-\ntion is based on the partitioning column or columns that are part of the deﬁnition of a\npartitioned table. There are several ways to map column values to partitions, giving\nrise to several types of partitioning, each with different characteristics: range, hash,\ncomposite, and list partitioning.\n25.3.10.1\nRange Partitioning\nIn range partitioning, the partitioning criteria are ranges of values. This type of par-\ntitioning is especially well suited to date columns, in which case all rows in the same\ndate range, say a day or a month, belong in the same partition. In a data warehouse\nwhere data are loaded from the transactional systems at regular intervals, range par-\ntitioning can be used to implement a rolling window of historical data efﬁciently.\nEach data load gets its own new partition, making the loading process faster and\nmore efﬁcient. The system actually loads the data into a separate table with the same\ncolumn deﬁnition as the partitioned table. It can then check the data for consistency,\ncleanse them, and index them. After that, the system can make the separate table a\nnew partition of the partitioned table, by a simple change to the metadata in the data\ndictionary—a nearly instantaneous operation.\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nIII. Object−Based \nDatabases and XML\n8. Object−Oriented \nDatabases\n322\n© The McGraw−Hill \nCompanies, 2001\n25.3\nStorage and Indexing\n933\nUp until the metadata change, the loading process does not affect the existing\ndata in the partitioned table in any way. There is no need to do any maintenance\nof existing indices as part of the loading. Old data can be removed from a table by\nsimply dropping its partition; this operation does not affect the other partitions.\nIn addition, queries in a data warehousing environment often contain conditions\nthat restrict them to a certain time period, such as a quarter or month. If date range\npartitioning is used, the query optimizer can restrict the data access to those parti-\ntions that are relevant to the query, and avoid a scan of the entire table.\n25.3.10.2\nHash Partitioning\nIn hash partitioning, a hash function maps rows to partitions according to the values\nin the partitioning columns. This type of partitioning is primarily useful when it is\nimportant to distribute the rows evenly among partitions or when partitionwise joins\nare important for query performance.\n25.3.10.3\nComposite Partitioning\nIn composite partitioning, the table is range partitioned, but each partition is subpar-\ntitioned by using hash partitioning. This type of partitioning combines the advan-\ntages of range partitioning and hash partitioning.\n25.3.10.4\nList Partitioning\nIn list partitioning, the values associated with a particular partition are stated in a\nlist. This type of partitioning is useful if the data in the partitioning column have a\nrelatively small set of discrete values. For instance, a table with a state column can be\nimplicitly partitioned by geographical region if each partition list has the states that\nbelong in the same region.\n25.3.11\nMaterialized Views\nThe materialized view feature (see Section 3.5.1) allows the result of an SQL query to\nbe stored in a table and used for later query processing. In addition, Oracle maintains\nthe materialized result, updating it when the tables that were referenced in the query\nare updated. Materialized views are used in data warehousing to speed up\n\n, 2001\n22.3\nData Mining\n841\n22.3.5\nClustering\nIntuitively, clustering refers to the problem of ﬁnding clusters of points in the given\ndata. The problem of clustering can be formalized from distance metrics in several\nways. One way is to phrase it as the problem of grouping points into k sets (for a\ngiven k) so that the average distance of points from the centroid of their assigned\ncluster is minimized.5 Another way is to group points so that the average distance\nbetween every pair of points in each cluster is minimized. There are other deﬁni-\ntions too; see the bibliographical notes for details. But the intuition behind all these\ndeﬁnitions is to group similar points together in a single set.\nAnother type of clustering appears in classiﬁcation systems in biology. (Such clas-\nsiﬁcation systems do not attempt to predict classes, rather they attempt to cluster re-\nlated items together.) For instance, leopards and humans are clustered under the class\nmammalia, while crocodiles and snakes are clustered under reptilia. Both mammalia\nand reptilia come under the common class chordata. The clustering of mammalia has\nfurther subclusters, such as carnivora and primates. We thus have hierarchical clus-\ntering. Given characteristics of different species, biologists have created a complex\nhierarchical clustering scheme grouping related species together at different levels of\nthe hierarchy.\nHierarchical clustering is also useful in other domains—for clustering documents,\nfor example. Internet directory systems (such as Yahoo’s) cluster related documents\nin a hierarchical fashion (see Section 22.5.5). Hierarchical clustering algorithms can\nbe classiﬁed as agglomerative clustering algorithms, which start by building small\nclusters and then creater higher levels, or divisive clustering algorithms, which ﬁrst\ncreate higher levels of the hierarchical clustering, then reﬁne each resulting cluster\ninto lower level clusters.\nThe statistics community has studied clustering extensively. Database research has\nprovided scalable clustering algorithms that can cluster very large data sets (that may\nnot ﬁt in memory). The Birch clustering algorithm is one such algorithm. Intuitively,\ndata points are inserted into a multidimensional tree structure (based on R-trees, de-\nscribed in Section 23.3.5.3), and guided to appropriate leaf nodes based on nearness\nto representative points in the internal nodes of the tree. Nearby points are thus clus-\ntered together in leaf nodes, and summarized if there are more points than ﬁt in\nmemory. Some postprocessing after insertion of all points gives the desired overall\nclustering. See the bibliographical notes for references to the Birch algorithm, and\nother techniques for clustering, including algorithms for hierarchical clustering.\nAn interesting application of clustering is to predict what new movies (or books,\nor music) a person is likely to be interested in, on the basis of:\n1. The person’s past preferences in movies\n2. Other people with similar past preferences\n3. The preferences of such people for new movies\n5.\nThe centroid of a set of points is deﬁned as a point whose coordinate on each dimension is the average\nof the coordinates of all the points of that set on that dimension. For example in two dimensions, the\ncentroid of a set of points { (x1, y1), (x2, y2), . . ., (xn, yn) } is given by (\nPn\ni=1 xi\nn\n,\nPn\ni=1 yi\nn\n)\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nVII. Other Topics\n22. Advanced Querying and \nInformation Retrieval\n835\n© The McGraw−Hill \nCompanies, 2001\n842\nChapter 22\nAdvanced Querying and Information Retrieval\nOne approach to this problem is as follows. To ﬁnd people with similar past prefer-\nences we create clusters of people based on their preferences for movies. The accuracy\nof clustering can be improved by previously clustering movies by their similarity, so\neven if people have not seen the same movies, if they have seen similar movies they\nwould be clustered together. We can repeat the clustering, alternately clustering peo-\nple, then movies, then people, and so on till we reache an equilibrium. Given a new\nuser, we ﬁnd a cluster of users most similar to that user, on the basis of the user’s\npreferences for movies already seen. We then predict movies in movie clusters that\nare popular with that user’s cluster as likely to be interesting to the new user. In fact,\nthis problem is an instance of collaborative ﬁltering, where users collaborate in the task\nof ﬁltering information to ﬁnd information of interest.\n22.3.6\nOther Types of Mining\nText mining applies data mining techniques to textual documents. For instance, there\nare tools that form clusters on pages that a user has visited; this helps users when\nthey browse the history of their browsing to ﬁnd pages they have visited earlier. The\ndistance between pages can be based, for instance, on common words in the pages\n(see Section 22.5.1.3). Another application is to classify pages into a Web directory\nautomatically, according to their similarity with other pages (see Section 22.5.5).\nData-visualization systems help users to examine large volumes of data, and to\ndetect patterns visually. Visual displays of data—such as maps, charts, and other\ngraphical representations—allow data to be presented compactly to users. A sin-\ngle graphical screen can encode as much information as a far larger number of text\nscreens. For example, if the user wants to ﬁnd out whether production problems at\nplants are correlated to the locations of the plants, the problem locations can be en-\ncoded in a special color—say, red—on a map. The user can then quickly discover\nlocations where problems are occurring. The user may then form hypotheses about\nwhy problems are occurring in those locations, and may verify the hypotheses quan-\ntitatively against the database.\nAs another example, information about values can be encoded as a color, and can\nbe displayed with as little as one pixel of screen area. To detect associations between\npairs of items, we can use a two-dimensional pixel matrix, with each row and each\ncolumn representing an item. The percentage of transactions that buy both items can\nbe encoded by the color intensity of the pixel. Items with high association will show\nup as bright pixels in the screen—easy to detect against the darker background.\nData visualization systems do not automatically detect patterns, but provide sys-\ntem support for users to detect patterns. Since humans are very good at detecting\nvisual patterns, data visualization is an important component of data mining.\n22.4\nData Warehousing\nLarge companies have presences in many places, each of which may generate a large\nvolume of data. For instance, large retail chains have hundreds or thousands of stores,\nwhereas insurance companies may have data from thousands of local branches. Fur-\nther, large organizations have a complex internal organization structure, and there-\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nVII. Other Topics\n22. Advanced Querying and \nInformation Retrieval\n836\n© The McGraw−Hill \nCompanies, 2001\n22.4\nData Warehousing\n843\ndata\nloaders\nDBMS\ndata warehouse\nquery and\nanalysis tools\ndata source n\ndata source 2\ndata source 1\n…\nFigure 22.8\nData-warehouse architecture.\nfore different data may be present in different locations, or on different operational\nsystems, or under different schemas. For instance, manufacturing-problem data and\ncustomer-complaint data may be stored on different database systems. Corporate de-\ncision makers require access to information from all such sources. Setting up queries\non individual sources is both cumbersome and inefﬁcient. Moreover, the sources of\ndata may store only current data, whereas decision makers may need access to past\ndata as well; for instance, information about how purchase patterns have changed in\nthe past year could be of great importance. Data warehouses provide a solution to\nthese problems.\nA data warehouse is a repository (or archive) of information gathered from mul-\ntiple sources, stored under a uniﬁed schema, at a single site. Once gathered, the data\nare stored for a long time, permitting access to historical data. Thus, data warehouses\nprovide the user a single consolidated interface to data, making decision-support\nqueries easier to write. Moreover, by accessing information for decision support from\na data warehouse, the decision maker ensures that online transaction-processing sys-\ntems are not affected by the decision-support workload.\n22.4.1\nComponents of a Data Warehouse\nFigure 22.8 shows the architecture of a typical data warehouse, and illustrates the\ngathering of data, the storage of data, and the querying and data-analysis support.\nAmong the issues to be addressed in building a warehouse are the following:\n• When and how to gather data. In a source-driven architecture for gather-\ning data, the data sources transmit new information, either continually (as\ntransaction processing takes place), or periodically (nightly, for example). In\na destination-driven architecture, the data warehouse periodically sends re-\nquests for new data to the sources.\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nVII. Other Topics\n22. Advanced Querying and \nInformation Retrieval\n837\n© The McGraw−Hill \nCompanies, 2001\n844\nChapter 22\nAdvanced Querying and Information Retrieval\nUnless updates at the sources are replicated at the warehouse via two-phase\ncommit, the warehouse will never be quite up to date with the sources. Two-\nphase commit is usually far too expensive to be an option, so data warehouses\ntypically have slightly out-of-date data. That, however, is usually not a prob-\nlem for decision-support systems.\n• What schema to use. Data sources that have been constructed independently\nare likely to have different schemas. In fact, they may even use different data\nmodels. Part of the task of a warehouse is to perform schema integration, and\nto convert data to the integrated schema before they are stored. As a result, the\ndata stored in the warehouse are not just a copy of the data at the sources. In-\nstead, they can be thought of as a materialized view of the data at the sources.\n• Data cleansing. The task of correcting and preprocessing data is called data\ncleansing. Data sources often deliver data with numerous minor inconsisten-\ncies, that can be corrected. For example, names are often misspelled, and ad-\ndresses may have street/area/city names misspelled, or zip codes entered in-\ncorrectly. These can be corrected to a reasonable extent by consulting a data-\nbase of street names and zip codes in each city. Address lists collected from\nmultiple sources may have duplicates that need to be eliminated in a merge–\npurge operation. Records for multiple individuals in a house may be grouped\ntogether so only one mailing is sent to each house; this operation is called\nhouseholding.\n• How to propagate updates. Updates on relations at the data sources must\nbe propagated to the data warehouse. If the relations at the data warehouse\nare exactly the same as those at the data source, the propagation is straight-\nforward. If they are not, the problem of propagating updates is basically the\nview-maintenance problem, which was discussed in Section 14.5.\n• What data to summarize. The raw data generated by a transaction-processing\nsystem may be too large to store online. However, we can answer many queries\nby maintaining just summary data obtained by aggregation on a relation,\nrather than maintaining the entire relation. For example, instead of storing\ndata about every sale of clothing, we can store total sales of clothing by item-\nname and category.\nSuppose that a relation r has been replaced by a summary relation s. Users\nmay still be permitted to pose queries as though the relation r were available\nonline. If the query requires only summary data, it may be possible to trans-\nform it into an equivalent one using s instead; see Section 14.5.\n22.4.2\nWarehouse Schemas\nData warehouses typically have schemas that are designed for data analysis, using\ntools such as OLAP tools. Thus, the data are usually multidimensional data, with di-\nmension attributes and measure attributes. Tables containing multidimensional data\nare called fact tables and are usually very large. A table recording sales information\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nVII. Other Topics\n22. Advanced Querying and \nInformation Retrieval\n838\n© The McGraw−Hill \nCompanies, 2001\n22.4\nData Warehousing\n845\nfor a retail store, with one tuple for each item that is sold, is a typical example of a fact\ntable. The dimensions of the sales table would include what the item is (usually an\nitem identiﬁer such as that used in bar codes), the date when the item is sold, which\nlocation (store) the item was sold from, which customer bought the item, and so on.\nThe measure attributes may include the number of items sold and the price of the\nitems.\nTo minimize storage requirements, dimension attributes are usually short identi-\nﬁers that are foreign keys into other other tables called dimension tables. For\ninstance, a fact table sales would have attributes item-id, store-id, customer-id, and date,\nand measure attributes number and price. The attribute store-id is a foreign key into\na dimension table store, which has other attributes such as store location (city, state,\ncountry). The item-id attribute of the sales table would be a foreign key into a di-\nmension table item-info, which would contain information such as the name of the\nitem, the category to which the item belongs, and other item details such as color and\nsize. The customer-id attribute would be a foreign key into a customer table containing\nattributes such as name and address of the customer. We can also view the date at-\ntribute as a foreign key into a date-info table giving the month, quarter, and year of\neach date.\nThe resultant schema appears in Figure 22.9. Such a schema, with a fact table,\nmultiple dimension tables, and foreign keys from the fact table to the dimension ta-\nbles, is called a star schema. More complex data warehouse designs may have multi-\nple levels of dimension tables; for instance, the item-info table may have an attribute\nmanufacturer-id that is a foreign key into another table giving details of the manufac-\nturer. Such schemas are called snowﬂake schemas. Complex data warehouse designs\nmay also have more than one fact table.\nitem-id\nstore-id\nstore-id\nitem-id\nitemname\ncolor\nsize\nitem-info\nsales\nstore\ncity\nstate\ncountry\ndate\nmonth\nquarter\nyear\ndate-info\nnumber\ndate\ncustomer-id\ncustomer\ncustomer-id\nname\nstreet\ncity\nstate\nzipcode\ncountry\ncategory\nprice\nFigure 22.9\nStar schema for a data warehouse.\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nVII. Other Topics\n22. Advanced Querying and \nInformation Retrieval\n839\n© The McGraw−Hill \nCompanies, 2001\n846\nChapter 22\nAdvanced Querying and Information Retrieval\n22.5\nInformation-Retrieval Systems\nThe ﬁeld of information retrieval has developed in parallel with the ﬁeld of databases.\nIn the traditional model used in the ﬁeld of information retrieval, information is orga-\nnized into documents, and it is assumed that there is a large number of documents.\nData contained in documents is unstructured, without any associated schema. The\nprocess of information retrieval consists of locating relevant documents, on the basis\nof user input, such as keywords or example documents.\nThe Web provides a convenient way to get to, and to interact with, information\nsources across the Internet. However, a persistent problem facing the Web is the ex-\nplosion of stored information, with little guidance to help the user to locate what\nis interesting. Information retrieval has played a critical role in making the Web a\nproductive and useful tool, especially for researchers.\nTraditional examples of information-retrieval systems are online library catalogs\nand online document-management systems such as those that store newspaper arti-\ncles. The data in such systems are organized as a collection of documents; a newspaper\narticle or a catalog entry (in a library catalog) are examples of documents. In the con-\ntext of the Web, usually each HTML page is considered to be a document.\nA user of such a system may want to retrieve a particular document or a particular\nclass of documents. The intended documents are typically described by a set of key-\nwords—for example, the keywords “database system” may be used to locate books\non database systems, and the keywords “stock” and “scandal” may be used to locate\narticles about stock-market scandals. Documents have associated with them a set of\nkeywords, and documents whose keywords contain those supplied by the user are\nretrieved.\nKeyword-based information retrieval can be used not only for retrieving textual\ndata, but also for retrieving other types of data, such as video or audio data, that\nhave descriptive keywords associated with them. For instance, a video movie may\nhave associated with it keywords such as its title, director, actors, type, and so on.\nThere are several differences between this model and the models used in tradi-\ntional database systems.\n• Database systems deal with several operations that are not addressed in infor-\nmation-retrieval systems. For instance, database systems deal with updates\nand with the associated transactional requirements of concurrency control\nand durability. These matters are viewed as less important in information sys-\ntems. Similarly, database systems deal with structured information organized\nwith relatively complex data models (such as the relational model or object-\noriented data models), whereas information-retrieval systems traditionally\nhave used a much simpler model, where the information in the database is\norganized simply as a collection of unstructured documents.\n• Information-retrieval systems deal with several issues that have not been ad-\ndressed adequately in database systems. For instance, the ﬁeld of information\nretrieval has dealt with the problems of managing unstructured documents,\nsuch as approximate searching by keywords, and of ranking of documents on\nestimated degree of relevance of the documents to the query.\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nVII. Other Topics\n22. Advanced Querying and \nInformation Retrieval\n840\n© The McGraw−Hill \nCompanies, 2001\n22.5\nInformation-Retrieval Systems\n847\n22.5.1\nKeyword Search\nInformation-retrieval systems typically allow query expressions formed using key-\nwords and the logical connectives and, or, and not. For example, a user could ask\nfor all documents that contain the keywords “motorcycle and maintenance,” or docu-\nments that contain the keywords “computer or microprocessor,” or even documents\nthat contain the keyword “computer but not database.” A query containing keywords\nwithout any of the above connectives is assumed to have ands implicitly connecting\nthe keywords.\nIn full text retrieval, all the words in each document are considered to be key-\nwords. For unstructured documents, full text retrieval is essential since there may be\nno information about what words in the document are keywords. We shall use the\nword term to refer to the words in a document, since all words are keywords.\nIn its simplest form an information retrieval system locates and returns all doc-\numents that contain all the keywords in the query, if the query has no connectives;\nconnectives are handled as you would expect. More sophisticated systems estimate\nrelevance of documents to a query so that the documents can be shown in order of\nestimated relevance. They use information about term occurrences, as well as hyper-\nlink information, to estimate relevance; Section 22.5.1.1 and 22.5.1.2 outline how to do\nso. Section 22.5.1.3 outlines how to deﬁne similarity of documents, and use similarity\nfor searching. Some systems a\n\n the hash join algorithm to compute outer joins is left for\nyou to do as an exercise (Exercise 13.11).\n13.6.5\nAggregation\nRecall the aggregation operator G, discussed in Section 3.3.2. For example, the oper-\nation\nbranch-nameGsum(balance)(account)\ngroups account tuples by branch, and computes the total balance of all the accounts\nat each branch.\nThe aggregation operation can be implemented in the same way as duplicate elim-\nination. We use either sorting or hashing, just as we did for duplicate elimination,\nbut based on the grouping attributes (branch-name in the preceding example). How-\never, instead of eliminating tuples with the same value for the grouping attribute, we\ngather them into groups, and apply the aggregation operations on each group to get\nthe result.\nThe cost estimate for implementing the aggregation operation is the same as the\ncost of duplicate elimination, for aggregate functions such as min, max, sum, count,\nand avg.\nInstead of gathering all the tuples in a group and then applying the aggregation\noperations, we can implement the aggregation operations sum, min, max, count, and\navg on the ﬂy as the groups are being constructed. For the case of sum, min, and\nmax, when two tuples in the same group are found, the system replaces them by\na single tuple containing the sum, min, or max, respectively, of the columns being\naggregated. For the count operation, it maintains a running count for each group for\nwhich a tuple has been found. Finally, we implement the avg operation by computing\nthe sum and the count values on the ﬂy, and ﬁnally dividing the sum by the count to\nget the average.\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nIV. Data Storage and \nQuerying\n13. Query Processing\n519\n© The McGraw−Hill \nCompanies, 2001\n518\nChapter 13\nQuery Processing\nIf all tuples of the result will ﬁt in memory, both the sort-based and the hash-based\nimplementations do not need to write any tuples to disk. As the tuples are read in,\nthey can be inserted in a sorted tree structure or in a hash index. When we use on the\nﬂy aggregation techniques, only one tuple needs to be stored for each of the groups.\nHence, the sorted tree structure or hash index will ﬁt in memory, and the aggregation\ncan be processed with just br block transfers, instead of with the 3br transfers that\nwould be required otherwise.\n13.7\nEvaluation of Expressions\nSo far, we have studied how individual relational operations are carried out. Now\nwe consider how to evaluate an expression containing multiple operations. The ob-\nvious way to evaluate an expression is simply to evaluate one operation at a time,\nin an appropriate order. The result of each evaluation is materialized in a temporary\nrelation for subsequent use. A disadvantage to this approach is the need to construct\nthe temporary relations, which (unless they are small) must be written to disk. An\nalternative approach is to evaluate several operations simultaneously in a pipeline,\nwith the results of one operation passed on to the next, without the need to store a\ntemporary relation.\nIn Sections 13.7.1 and 13.7.2, we consider both the materialization approach and\nthe pipelining approach. We shall see that the costs of these approaches can differ\nsubstantially, but also that there are cases where only the materialization approach is\nfeasible.\n13.7.1\nMaterialization\nIt is easiest to understand intuitively how to evaluate an expression by looking at a\npictorial representation of the expression in an operator tree. Consider the expression\nΠcustomer-name (σbalance<2500 (account)\n\u0001 customer)\nin Figure 13.10.\nΠ customer-name\nσ balance < 2500\naccount\ncustomer\nFigure 13.10\nPictorial representation of an expression.\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nIV. Data Storage and \nQuerying\n13. Query Processing\n520\n© The McGraw−Hill \nCompanies, 2001\n13.7\nEvaluation of Expressions\n519\nIf we apply the materialization approach, we start from the lowest-level operations\nin the expression (at the bottom of the tree). In our example, there is only one such op-\neration; the selection operation on account. The inputs to the lowest-level operations\nare relations in the database. We execute these operations by the algorithms that we\nstudied earlier, and we store the results in temporary relations. We can use these tem-\nporary relations to execute the operations at the next level up in the tree, where the\ninputs now are either temporary relations or relations stored in the database. In our\nexample, the inputs to the join are the customer relation and the temporary relation\ncreated by the selection on account. The join can now be evaluated, creating another\ntemporary relation.\nBy repeating the process, we will eventually evaluate the operation at the root of\nthe tree, giving the ﬁnal result of the expression. In our example, we get the ﬁnal\nresult by executing the projection operation at the root of the tree, using as input the\ntemporary relation created by the join.\nEvaluation as just described is called materialized evaluation, since the results of\neach intermediate operation are created (materialized) and then are used for evalua-\ntion of the next-level operations.\nThe cost of a materialized evaluation is not simply the sum of the costs of the oper-\nations involved. When we computed the cost estimates of algorithms, we ignored the\ncost of writing the result of the operation to disk. To compute the cost of evaluating\nan expression as done here, we have to add the costs of all the operations, as well\nas the cost of writing the intermediate results to disk. We assume that the records\nof the result accumulate in a buffer, and, when the buffer is full, they are written to\ndisk. The cost of writing out the result can be estimated as nr/fr, where nr is the\nestimated number of tuples in the result relation r, and fr is the blocking factor of the\nresult relation, that is, the number of records of r that will ﬁt in a block.\nDouble buffering (using two buffers, with one continuing execution of the al-\ngorithm while the other is being written out) allows the algorithm to execute more\nquickly by performing CPU activity in parallel with I/O activity.\n13.7.2\nPipelining\nWe can improve query-evaluation efﬁciency by reducing the number of temporary\nﬁles that are produced. We achieve this reduction by combining several relational op-\nerations into a pipeline of operations, in which the results of one operation are passed\nalong to the next operation in the pipeline. Evaluation as just described is called\npipelined evaluation. Combining operations into a pipeline eliminates the cost of\nreading and writing temporary relations.\nFor example, consider the expression (Πa1,a2(r\n\u0001 s)). If materialization were ap-\nplied, evaluation would involve creating a temporary relation to hold the result of the\njoin, and then reading back in the result to perform the projection. These operations\ncan be combined: When the join operation generates a tuple of its result, it passes that\ntuple immediately to the project operation for processing. By combining the join and\nthe projection, we avoid creating the intermediate result, and instead create the ﬁnal\nresult directly.\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nIV. Data Storage and \nQuerying\n13. Query Processing\n521\n© The McGraw−Hill \nCompanies, 2001\n520\nChapter 13\nQuery Processing\n13.7.2.1\nImplementation of Pipelining\nWe can implement a pipeline by constructing a single, complex operation that com-\nbines the operations that constitute the pipeline. Although this approach may be fea-\nsible for various frequently occurring situations, it is desirable in general to reuse the\ncode for individual operations in the construction of a pipeline. Therefore, each op-\neration in the pipeline is modeled as a separate process or thread within the system,\nwhich takes a stream of tuples from its pipelined inputs, and generates a stream of\ntuples for its output. For each pair of adjacent operations in the pipeline, the system\ncreates a buffer to hold tuples being passed from one operation to the next.\nIn the example of Figure 13.10, all three operations can be placed in a pipeline,\nwhich passes the results of the selection to the join as they are generated. In turn,\nit passes the results of the join to the projection as they are generated. The memory\nrequirements are low, since results of an operation are not stored for long. However,\nas a result of pipelining, the inputs to the operations are not available all at once for\nprocessing.\nPipelines can be executed in either of two ways:\n1. Demand driven\n2. Producer driven\nIn a demand-driven pipeline, the system makes repeated requests for tuples from\nthe operation at the top of the pipeline. Each time that an operation receives a request\nfor tuples, it computes the next tuple (or tuples) to be returned, and then returns\nthat tuple. If the inputs of the operation are not pipelined, the next tuple(s) to be\nreturned can be computed from the input relations, while the system keeps track of\nwhat has been returned so far. If it has some pipelined inputs, the operation also\nmakes requests for tuples from its pipelined inputs. Using the tuples received from\nits pipelined inputs, the operation computes tuples for its output, and passes them\nup to its parent.\nIn a producer-driven pipeline, operations do not wait for requests to produce\ntuples, but instead generate the tuples eagerly. Each operation at the bottom of a\npipeline continually generates output tuples, and puts them in its output buffer, until\nthe buffer is full. An operation at any other level of a pipeline generates output tuples\nwhen it gets input tuples from lower down in the pipeline, until its output buffer is\nfull. Once the operation uses a tuple from a pipelined input, it removes the tuple\nfrom its input buffer. In either case, once the output buffer is full, the operation waits\nuntil its parent operation removes tuples from the buffer, so that the buffer has space\nfor more tuples. At this point, the operation generates more tuples, until the buffer\nis full again. The operation repeats this process until all the output tuples have been\ngenerated.\nIt is necessary for the system to switch between operations only when an output\nbuffer is full, or an input buffer is empty and more input tuples are needed to gener-\nate any more output tuples. In a parallel-processing system, operations in a pipeline\nmay be run concurrently on distinct processors (see Chapter 20).\nUsing producer-driven pipelining can be thought of as pushing data up an oper-\nation tree from below, whereas using demand-driven pipelining can be thought of as\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nIV. Data Storage and \nQuerying\n13. Query Processing\n522\n© The McGraw−Hill \nCompanies, 2001\n13.7\nEvaluation of Expressions\n521\npulling data up an operation tree from the top. Whereas tuples are generated eagerly\nin producer-driven pipelining, they are generated lazily, on demand, in demand-\ndriven pipelining.\nEach operation in a demand-driven pipeline can be implemented as an iterator,\nwhich provides the following functions: open(), next(), and close(). After a call to\nopen(), each call to next() returns the next output tuple of the operation. The imple-\nmentation of the operation in turn calls open() and next() on its inputs, to get its input\ntuples when required. The function close() tells an iterator that no more tuples are\nrequired. The iterator maintains the state of its execution in between calls, so that\nsuccessive next() requests receive successive result tuples.\nFor example, for an iterator implementing the select operation using linear search,\nthe open() operation starts a ﬁle scan, and the iterator’s state records the point to\nwhich the ﬁle has been scanned. When the next() function is called, the ﬁle scan con-\ntinues from after the previous point; when the next tuple satisfying the selection is\nfound by scanning the ﬁle, the tuple is returned after storing the point where it was\nfound in the iterator state. A merge–join iterator’s open() operation would open its\ninputs, and if they are not already sorted, it would also sort the inputs. On calls to\nnext(), it would return the next pair of matching tuples. The state information would\nconsist of up to where each input had been scanned.\nDetails of the implementation of iterators are left for you to complete in Exer-\ncise 13.12. Demand-driven pipelining is used more commonly than producer-driven\npipelining, because it is easier to implement.\n13.7.2.2\nEvaluation Algorithms for Pipelining\nConsider a join operation whose left-hand–side input is pipelined. Since it is pipe-\nlined, the input is not available all at once for processing by the join operation. This\nunavailability limits the choice of join algorithm to be used. Merge join, for example,\ncannot be used if the inputs are not sorted, since it is not possible to sort a relation\nuntil all the tuples are available—thus, in effect, turning pipelining into materializa-\ntion. However, indexed nested-loop join can be used: As tuples are received for the\nleft-hand side of the join, they can be used to index the right-hand–side relation, and\nto generate tuples in the join result. This example illustrates that choices regarding\nthe algorithm used for an operation and choices regarding pipelining are not inde-\npendent.\nThe restrictions on the evaluation algorithms that are eligible for use are a limiting\nfactor for pipelining. As a result, despite the apparent advantages of pipelining, there\nare cases where materialization achieves lower overall cost. Suppose that the join of\nr and s is required, and input r is pipelined. If indexed nested-loop join is used to\nsupport pipelining, one access to disk may be needed for every tuple in the pipelined\ninput relation. The cost of this technique is nr ∗HTi, where HTi is the height of the\nindex on s. With materialization, the cost of writing out r would be br. With a join\ntechnique such as hash join, it may be possible to perform the join with a cost of\nabout 3(br + bs). If nr is substantially more than 4br + 3bs, materialization would be\ncheaper.\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nIV. Data Storage and \nQuerying\n13. Query Processing\n523\n© The McGraw−Hill \nCompanies, 2001\n522\nChapter 13\nQuery Processing\ndoner := false;\ndones := false;\nr := ∅;\ns := ∅;\nresult := ∅;\nwhile not doner or not dones do\nbegin\nif queue is empty, then wait until queue is not empty;\nt := top entry in queue;\nif t = Endr then doner := true\nelse if t = Ends then dones := true\nelse if t is from input r\nthen\nbegin\nr := r ∪{t};\nresult := result ∪({t}\n\u0001 s);\nend\nelse /* t is from input s */\nbegin\ns := s ∪{t};\nresult := result ∪(r\n\u0001 {t});\nend\nend\nFigure 13.11\nPipelined join algorithm.\nThe effective use of pipelining requires the use of evaluation algorithms that can\ngenerate output tuples even as tuples are received for the inputs to the operation. We\ncan distinguish between two cases:\n1. Only one of the inputs to a join is pipelined.\n2. Both inputs to the join are pipelined.\nIf only one of the inputs to a join is pipelined, indexed nested-loop join is a natural\nchoice. If the pipelined input tuples are sorted on the join attributes, and the join\ncondition is an equi-join, merge join can also be used. Hybrid hash–join can be used\ntoo, with the pipelined input as the probe relation. However, tuples that are not in the\nﬁrst partition will be output only after the entire pipelined input relation is received.\nHybrid hash–join is useful if the nonpipelined input ﬁts entirely in memory, or if at\nleast most of that input ﬁts in memory.\nIf both inputs are pipelined, the choice of join algorithms is more restricted. If both\ninputs are sorted on the join attribute, and the join condition is an equi-join, merge\njoin can be used. Another alternative is the pipelined join technique, shown in Figure\n13.11. The algorithm assumes that the input tuples for both input relations, r and s,\nare pipelined. Tuples made available for both relations are queued for processing in a\nsingle queue. Special queue entries, called Endr and Ends, which serve as end-of-ﬁle\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nIV. Data Storage and \nQuerying\n13. Query Processing\n524\n© The McGraw−Hill \nCompanies, 2001\n13.8\nSummary\n523\nmarkers, are inserted in the queue after all tuples from r and s (respectively) have\nbeen generated. For efﬁcient evaluation, appropriate indices should be built on the\nrelations r and s. As tuples are added to r and s, the indices must be kept up to date.\n13.8\nSummary\n• The ﬁrst action that the system must perform on a query is to translate the\nquery into its internal form, which (for relational database systems) is usually\nbased on the relational algebra. In the process of generating the internal form\nof the query, the parser checks the syntax of the user’s query, veriﬁes that the\nrelation names appearing in the query are names of relations in the database,\nand so on. If the query was expressed in terms of a view, the parser replaces all\nreferences to the view name with the relational-algebra expression to compute\nthe view.\n• Given a query, there are generally a variety of methods for computing the\nanswer. It is the responsibility of the query optimizer to transform the query\nas entered by the user into an equivalent query that can be computed more\nefﬁciently. Chapter 14 covers query optimization.\n• We can process simple selection operations by performing a linear scan, by\ndoing a binary search, or by making use of indices. We can handle complex\nselections by computing unions and intersections of the results of simple se-\nlections.\n• We can sort relations larger than memory by the external merge–sort algo-\nrithm.\n• Queries involving a natural join may be processed in several ways, depending\non the availability of indices and the form of physical storage for the relations.\n\u0000 If the join result is almost as large as the Cartesian product of the two\nrelations, a block nested-loop join strategy may be advantageous.\n\u0000 If indices are available, the indexed nested-loop join can be used.\n\u0000 If the relations are sorted, a merge join may be desirable. It may be advan-\ntageous to sort a relation prior to join computation (so as to allow use of\nthe merge join strategy).\n\u0000 The hash join algorithm partitions the relations into several pieces, such\nthat each piece of one of the relations ﬁts in memory. The partitioning is\ncarried out with a hash function on the join attributes, so that correspond-\ning pairs of partitions can be joined independently.\n• Duplicate elimination, projection, set operations (union, intersection and dif-\nference), and aggregation can be done by sorting or by hashing.\n• Outer join operations can be implemented by simple extensions of join algo-\nrithms.\n• Hashing and sorting are dual, in the sense that any operation such as du-\nplicate elimination, projection, aggregation, join, and outer join that can be\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nIV. Data Storage and \nQuerying\n13. Query Processing\n525\n© The McGraw−Hill \nCompanies, 2001\n524\nChapter 13\nQuery Processing\nimplemented by hashing can also be implemented by sorting, and vice versa;\nthat is, any operation that can be implemented by sorting can also be imple-\nmented by hashing.\n• An expression can be evaluated by means of materialization, where the sys-\ntem computes the result of each subexpression and stores it on disk, and then\nuses it to compute the result of the parent expression.\n• Pipelining helps to avoid writing the results of many subexpressions to disk,\nby using the results in the parent expression even as they are being generated.\nReview Terms\n• Query processing\n• Evaluation primitive\n• Query-execution plan\n• Query-evaluation plan\n• Query-execution engine\n• Measures of query cost\n• Seque\n\nllection of tables, each of which is assigned a\nunique name. Each table has a structure similar to that presented in Chapter 2, where\nwe represented E-R databases by tables. A row in a table represents a relationship\namong a set of values. Since a table is a collection of such relationships, there is a\nclose correspondence between the concept of table and the mathematical concept of\n79\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nI. Data Models\n3. Relational Model\n88\n© The McGraw−Hill \nCompanies, 2001\n80\nChapter 3\nRelational Model\nrelation, from which the relational data model takes its name. In what follows, we\nintroduce the concept of relation.\nIn this chapter, we shall be using a number of different relations to illustrate the\nvarious concepts underlying the relational data model. These relations represent part\nof a banking enterprise. They differ slightly from the tables that were used in Chap-\nter 2, so that we can simplify our presentation. We shall discuss criteria for the ap-\npropriateness of relational structures in great detail in Chapter 7.\n3.1.1\nBasic Structure\nConsider the account table of Figure 3.1. It has three column headers: account-number,\nbranch-name, and balance. Following the terminology of the relational model, we refer\nto these headers as attributes (as we did for the E-R model in Chapter 2). For each\nattribute, there is a set of permitted values, called the domain of that attribute. For\nthe attribute branch-name, for example, the domain is the set of all branch names. Let\nD1 denote the set of all account numbers, D2 the set of all branch names, and D3\nthe set of all balances. As we saw in Chapter 2, any row of account must consist of\na 3-tuple (v1, v2, v3), where v1 is an account number (that is, v1 is in domain D1),\nv2 is a branch name (that is, v2 is in domain D2), and v3 is a balance (that is, v3 is in\ndomain D3). In general, account will contain only a subset of the set of all possible\nrows. Therefore, account is a subset of\nD1 × D2 × D3\nIn general, a table of n attributes must be a subset of\nD1 × D2 × · · · × Dn−1 × Dn\nMathematicians deﬁne a relation to be a subset of a Cartesian product of a list of\ndomains. This deﬁnition corresponds almost exactly with our deﬁnition of table. The\nonly difference is that we have assigned names to attributes, whereas mathematicians\nrely on numeric “names,” using the integer 1 to denote the attribute whose domain\nappears ﬁrst in the list of domains, 2 for the attribute whose domain appears second,\nand so on. Because tables are essentially relations, we shall use the mathematical\naccount-number\nbranch-name\nbalance\nA-101\nDowntown\n500\nA-102\nPerryridge\n400\nA-201\nBrighton\n900\nA-215\nMianus\n700\nA-217\nBrighton\n750\nA-222\nRedwood\n700\nA-305\nRound Hill\n350\nFigure 3.1\nThe account relation.\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nI. Data Models\n3. Relational Model\n89\n© The McGraw−Hill \nCompanies, 2001\n3.1\nStructure of Relational Databases\n81\naccount-number\nbranch-name\nbalance\nA-101\nDowntown\n500\nA-215\nMianus\n700\nA-102\nPerryridge\n400\nA-305\nRound Hill\n350\nA-201\nBrighton\n900\nA-222\nRedwood\n700\nA-217\nBrighton\n750\nFigure 3.2\nThe account relation with unordered tuples.\nterms relation and tuple in place of the terms table and row. A tuple variable is a\nvariable that stands for a tuple; in other words, a tuple variable is a variable whose\ndomain is the set of all tuples.\nIn the account relation of Figure 3.1, there are seven tuples. Let the tuple variable t\nrefer to the ﬁrst tuple of the relation. We use the notation t[account-number] to denote\nthe value of t on the account-number attribute. Thus, t[account-number] = “A-101,” and\nt[branch-name] = “Downtown”. Alternatively, we may write t[1] to denote the value\nof tuple t on the ﬁrst attribute (account-number), t[2] to denote branch-name, and so on.\nSince a relation is a set of tuples, we use the mathematical notation of t ∈r to denote\nthat tuple t is in relation r.\nThe order in which tuples appear in a relation is irrelevant, since a relation is a\nset of tuples. Thus, whether the tuples of a relation are listed in sorted order, as in\nFigure 3.1, or are unsorted, as in Figure 3.2, does not matter; the relations in the two\nﬁgures above are the same, since both contain the same set of tuples.\nWe require that, for all relations r, the domains of all attributes of r be atomic. A\ndomain is atomic if elements of the domain are considered to be indivisible units.\nFor example, the set of integers is an atomic domain, but the set of all sets of integers\nis a nonatomic domain. The distinction is that we do not normally consider inte-\ngers to have subparts, but we consider sets of integers to have subparts—namely,\nthe integers composing the set. The important issue is not what the domain itself is,\nbut rather how we use domain elements in our database. The domain of all integers\nwould be nonatomic if we considered each integer to be an ordered list of digits. In\nall our examples, we shall assume atomic domains. In Chapter 9, we shall discuss\nextensions to the relational data model to permit nonatomic domains.\nIt is possible for several attributes to have the same domain. For example, sup-\npose that we have a relation customer that has the three attributes customer-name,\ncustomer-street, and customer-city, and a relation employee that includes the attribute\nemployee-name. It is possible that the attributes customer-name and employee-name will\nhave the same domain: the set of all person names, which at the physical level is\nthe set of all character strings. The domains of balance and branch-name, on the other\nhand, certainly ought to be distinct. It is perhaps less clear whether customer-name\nand branch-name should have the same domain. At the physical level, both customer\nnames and branch names are character strings. However, at the logical level, we may\nwant customer-name and branch-name to have distinct domains.\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nI. Data Models\n3. Relational Model\n90\n© The McGraw−Hill \nCompanies, 2001\n82\nChapter 3\nRelational Model\nOne domain value that is a member of any possible domain is the null value,\nwhich signiﬁes that the value is unknown or does not exist. For example, suppose\nthat we include the attribute telephone-number in the customer relation. It may be that\na customer does not have a telephone number, or that the telephone number is un-\nlisted. We would then have to resort to null values to signify that the value is un-\nknown or does not exist. We shall see later that null values cause a number of difﬁ-\nculties when we access or update the database, and thus should be eliminated if at\nall possible. We shall assume null values are absent initially, and in Section 3.3.4, we\ndescribe the effect of nulls on different operations.\n3.1.2\nDatabase Schema\nWhen we talk about a database, we must differentiate between the database schema,\nwhich is the logical design of the database, and a database instance, which is a snap-\nshot of the data in the database at a given instant in time.\nThe concept of a relation corresponds to the programming-language notion of a\nvariable. The concept of a relation schema corresponds to the programming-language\nnotion of type deﬁnition.\nIt is convenient to give a name to a relation schema, just as we give names to type\ndeﬁnitions in programming languages. We adopt the convention of using lower-\ncase names for relations, and names beginning with an uppercase letter for rela-\ntion schemas. Following this notation, we use Account-schema to denote the relation\nschema for relation account. Thus,\nAccount-schema = (account-number, branch-name, balance)\nWe denote the fact that account is a relation on Account-schema by\naccount(Account-schema)\nIn general, a relation schema consists of a list of attributes and their corresponding\ndomains. We shall not be concerned about the precise deﬁnition of the domain of\neach attribute until we discuss the SQL language in Chapter 4.\nThe concept of a relation instance corresponds to the programming language no-\ntion of a value of a variable. The value of a given variable may change with time;\nsimilarly the contents of a relation instance may change with time as the relation is\nupdated. However, we often simply say “relation” when we actually mean “relation\ninstance.”\nAs an example of a relation instance, consider the branch relation of Figure 3.3. The\nschema for that relation is\nBranch-schema = (branch-name, branch-city, assets)\nNote that the attribute branch-name appears in both Branch-schema and Account-\nschema. This duplication is not a coincidence. Rather, using common attributes in\nrelation schemas is one way of relating tuples of distinct relations. For example, sup-\npose we wish to ﬁnd the information about all of the accounts maintained in branches\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nI. Data Models\n3. Relational Model\n91\n© The McGraw−Hill \nCompanies, 2001\n3.1\nStructure of Relational Databases\n83\nbranch-name\nbranch-city\nassets\nBrighton\nBrooklyn\n7100000\nDowntown\nBrooklyn\n9000000\nMianus\nHorseneck\n400000\nNorth Town\nRye\n3700000\nPerryridge\nHorseneck\n1700000\nPownal\nBennington\n300000\nRedwood\nPalo Alto\n2100000\nRound Hill\nHorseneck\n8000000\nFigure 3.3\nThe branch relation.\nlocated in Brooklyn. We look ﬁrst at the branch relation to ﬁnd the names of all the\nbranches located in Brooklyn. Then, for each such branch, we would look in the ac-\ncount relation to ﬁnd the information about the accounts maintained at that branch.\nThis is not surprising—recall that the primary key attributes of a strong entity set\nappear in the table created to represent the entity set, as well as in the tables created\nto represent relationships that the entity set participates in.\nLet us continue our banking example. We need a relation to describe information\nabout customers. The relation schema is\nCustomer-schema = (customer-name, customer-street, customer-city)\nFigure 3.4 shows a sample relation customer (Customer-schema). Note that we have\nomitted the customer-id attribute, which we used Chapter 2, because now we want to\nhave smaller relation schemas in our running example of a bank database. We assume\nthat the customer name uniquely identiﬁes a customer—obviously this may not be\ntrue in the real world, but the assumption makes our examples much easier to read.\ncustomer-name\ncustomer-street\ncustomer-city\nAdams\nSpring\nPittsfield\nBrooks\nSenator\nBrooklyn\nCurry\nNorth\nRye\nGlenn\nSand Hill\nWoodside\nGreen\nWalnut\nStamford\nHayes\nMain\nHarrison\nJohnson\nAlma\nPalo Alto\nJones\nMain\nHarrison\nLindsay\nPark\nPittsfield\nSmith\nNorth\nRye\nTurner\nPutnam\nStamford\nWilliams\nNassau\nPrinceton\nFigure 3.4\nThe customer relation.\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nI. Data Models\n3. Relational Model\n92\n© The McGraw−Hill \nCompanies, 2001\n84\nChapter 3\nRelational Model\nIn a real-world database, the customer-id (which could be a social-security number, or\nan identiﬁer generated by the bank) would serve to uniquely identify customers.\nWe also need a relation to describe the association between customers and ac-\ncounts. The relation schema to describe this association is\nDepositor-schema = (customer-name, account-number)\nFigure 3.5 shows a sample relation depositor (Depositor-schema).\nIt would appear that, for our banking example, we could have just one relation\nschema, rather than several. That is, it may be easier for a user to think in terms of\none relation schema, rather than in terms of several. Suppose that we used only one\nrelation for our example, with schema\n(branch-name, branch-city, assets, customer-name, customer-street\ncustomer-city, account-number, balance)\nObserve that, if a customer has several accounts, we must list her address once for\neach account. That is, we must repeat certain information several times. This repeti-\ntion is wasteful and is avoided by the use of several relations, as in our example.\nIn addition, if a branch has no accounts (a newly created branch, say, that has no\ncustomers yet), we cannot construct a complete tuple on the preceding single rela-\ntion, because no data concerning customer and account are available yet. To represent\nincomplete tuples, we must use null values that signify that the value is unknown or\ndoes not exist. Thus, in our example, the values for customer-name, customer-street, and\nso on must be null. By using several relations, we can represent the branch informa-\ntion for a bank with no customers without using null values. We simply use a tuple\non Branch-schema to represent the information about the branch, and create tuples on\nthe other schemas only when the appropriate information becomes available.\nIn Chapter 7, we shall study criteria to help us decide when one set of relation\nschemas is more appropriate than another, in terms of information repetition and\nthe existence of null values. For now, we shall assume that the relation schemas are\ngiven.\nWe include two additional relations to describe data about loans maintained in the\nvarious branches in the bank:\ncustomer-name\naccount-number\nHayes\nA-102\nJohnson\nA-101\nJohnson\nA-201\nJones\nA-217\nLindsay\nA-222\nSmith\nA-215\nTurner\nA-305\nFigure 3.5\nThe depositor relation.\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nI. Data Models\n3. Relational Model\n93\n© The McGraw−Hill \nCompanies, 2001\n3.1\nStructure of Relational Databases\n85\nloan-number\nbranch-name\namount\nL-11\nRound Hill\n900\nL-14\nDowntown\n1500\nL-15\nPerryridge\n1500\nL-16\nPerryridge\n1300\nL-17\nDowntown\n1000\nL-23\nRedwood\n2000\nL-93\nMianus\n500\nFigure 3.6\nThe loan relation.\nLoan-schema = (loan-number, branch-name, amount)\nBorrower-schema = (customer-name, loan-number)\nFigures 3.6 and 3.7, respectively, show the sample relations loan (Loan-schema) and\nborrower (Borrower-schema).\nThe E-R diagram in Figure 3.8 depicts the banking enterprise that we have just\ndescribed. The relation schemas correspond to the set of tables that we might gener-\nate by the method outlined in Section 2.9. Note that the tables for account-branch and\nloan-branch have been combined into the tables for account and loan respectively. Such\ncombining is possible since the relationships are many to one from account and loan,\nrespectively, to branch, and, further, the participation of account and loan in the corre-\nsponding relationships is total, as the double lines in the ﬁgure indicate. Finally, we\nnote that the customer relation may contain information about customers who have\nneither an account nor a loan at the bank.\nThe banking enterprise described here will serve as our primary example in this\nchapter and in subsequent ones. On occasion, we shall need to introduce additional\nrelation schemas to illustrate particular points.\n3.1.3\nKeys\nThe notions of superkey, candidate key, and primary key, as discussed in Chapter 2,\nare also applicable to the relational model. For example, in Branch-schema, {branch-\ncustomer-name\nloan-number\nAdams\nL-16\nCurry\nL-93\nHayes\nL-15\nJackson\nL-14\nJones\nL-17\nSmith\nL-11\nSmith\nL-23\nWilliams\nL-17\nFigure 3.7\nThe borrower relation.\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nI. Data Models\n3. Relational Model\n94\n© The McGraw−Hill \nCompanies, 2001\n86\nChapter 3\nRelational Model\naccount-number\nbalance\naccount\nbranch-name\nassets\nbranch\naccount-branch\ncustomer-name\ncustomer-street\ncustomer-city\ncustomer\nloan-number\namount\ndepositor\nbranch-city\nloan-branch\nloan\nborrower\nFigure 3.8\nE-R diagram for the banking enterprise.\nname} and {branch-name, branch-city} are both superkeys. {branch-name, branch-city}\nis not a candidate key, because {branch-name} is a subset of {branch-name, branch-\ncity} and {branch-name} itself is a superkey. However, {branch-name} is a candidate\nkey, and for our purpose also will serve as a primary key. The attribute branch-city is\nnot a superkey, since two branches in the same city may have different names (and\ndifferent asset ﬁgures).\nLet R be a relation schema. If we say that a subset K of R is a superkey for R, we\nare restricting consideration to relations r(R) in which no two distinct tuples have\nthe same values on all attributes in K. That is, if t1 and t2 are in r and t1 ̸= t2, then\nt1[K] ̸= t2[K].\nIf a relational database schema is based on tables derived from an E-R schema, it\nis possible to determine the primary key for a relation schema from the primary keys\nof the entity or relationship sets from which the schema is derived:\n• Strong entity set. The primary key of the entity set becomes the primary key\nof the relation.\n• Weak entity set. The table, and thus the relation, corresponding to a weak\nentity set includes\n\u0000 The attributes of the weak entity set\n\u0000 The primary key of the strong entity set on which the weak entity set\ndepends\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nI. Data Models\n3. Relational Model\n95\n© The McGraw−Hill \nCompanies, 2001\n3.1\nStructure of Relational Databases\n87\nThe primary key of the relation consists of the union of the primary key of the\nstrong entity set and the discriminator of the weak entity set.\n• Relationship set. The union of the primary keys of the related entity sets be-\ncomes a superkey of the relation. If the relationship is many-to-many, this su-\nperkey is also the primary key. Section 2.4.2 describes how to determine the\nprimary keys in other cases. Recall from Section 2.9.3 that no table is gener-\nated for relationship sets linking a weak entity set to the corresponding strong\nentity set.\n• Combined tables. Recall from Section 2.9.3 that a binary many-to-one rela-\ntionship set from A to B can be represented by a table consisting of the at-\ntributes of A and attributes (if any exist) of the relationship set. The primary\nkey of the “many” entity set becomes the primary key of the relation (that is,\nif the relationship set is many to one from A to B, the primary key of A is\nthe primary key of the relation). For one-to-one relationship sets, the relation\nis constructed like that for a many-to-one relationship set. However, we can\nchoose either entity set’s primary key as the primary key of the relation, since\nboth are candidate keys.\n• Multivalued attributes. Recall from Section 2.9.5 that a multivalued attribute\nM is represented by a table consisting of the primary key of the entity set or\nrelationship set of which M is an attribute plus a column C holding an indi-\nvidual value of M. The primary key of the entity or relationship set, together\nwith the attribute C, becomes the primary key for the relation.\nFrom the preceding list, we see that a relation schema, say r1, derived from an E-R\nschema may include among its attributes the primary key of another relation schema,\nsay r2. This attribute is called a foreign key from r1, referencing r2. The relation r1\nis also called the referencing relation of the foreign key dependency, and r2 is called\nthe referenced relation of the foreign key. For example, the attribute branch-name in\nAccount-schema is a foreign key from Account-schema referencing Branch-schema, since\nbranch-name is the primary key of Branch-schema. In any database instance, given any\ntuple, say ta, from the account relation, there must be some tuple, say tb, in the branch\nrelation such that the value of the branch-name attribute of ta is the same as the value\nof the primary key, branch-name, of tb.\nIt is customary to list the primary key attributes of a relation schema before the\nother attributes; for example, the branch-name attribute of Branch-schema is listed ﬁrst,\nsince it is the primary key.\n3.1.4\nSchema Diagram\nA database schema, along with primary key and foreign key dependencies, can be\ndepicted pictorially by schema diagrams. Figure 3.9 shows the schema diagram for\nour banking enterprise. Each relation appears as a box, with the attributes listed in-\nside it and the relation name above it. If there are primary key attributes, a horizontal\nline crosses ",
    "precision": 0.06363214119832791,
    "recall": 0.791907514450867,
    "iou": 0.06258565555047967,
    "f1": 0.11779879621668099,
    "gold_tokens_count": 173,
    "retrieved_tokens_count": 2153,
    "intersection_tokens": 137
  },
  {
    "config_name": "chars_weighted",
    "config": {
      "name": "chars_weighted",
      "index_prefix": "textbook_index",
      "chunking_strategy": "chars",
      "overlap": 0,
      "fusion": "weighted",
      "bm25_weight": 0.3,
      "tag_weight": 0.2,
      "top_k": 5,
      "embed_model": "sentence-transformers/all-MiniLM-L6-v2"
    },
    "question": "A weak entity set can always be made into a strong entity set by adding to its attributes the primary key attributes of its identifying entity set. Outline what sort of redundancy will result if we do so.",
    "gold_text": "2.9.3.1 Redundancy of Tables A relationship set linking a weak entity set to the corresponding strong entity set is treated specially. As we noted in Section 2.6, these relationships are many-to-one and have no descriptive attributes. Furthermore, the primary key of a weak entity set in- cludes the primary key of the strong entity set. In the E-R diagram of Figure 2.16, the weak entity set payment is dependent on the strong entity set loan via the relation- ship set loan-payment. The primary key of payment is {loan-number, payment-number}, and the primary key of loan is {loan-number}. Since loan-payment has no descriptive attributes, the loan-payment table would have two columns, loan-number and payment- number. The table for the entity set payment has four columns, loan-number, payment- number, payment-date, and payment-amount. Every (loan-number, payment-number) com- bination in loan-payment would also be present in the payment table, and vice versa. Thus, the loan-payment table is redundant. In general, the table for the relationship set linking a weak entity set to its corresponding strong entity set is redundant and does not need to be present in a tabular representation of an E-R diagram.",
    "retrieved_text": "ity set in\nonly one ISA relationship; that is, entity sets in this diagram have only single inher-\nitance. If an entity set is a lower-level entity set in more than one ISA relationship,\nthen the entity set has multiple inheritance, and the resulting structure is said to be\na lattice.\n2.7.4\nConstraints on Generalizations\nTo model an enterprise more accurately, the database designer may choose to place\ncertain constraints on a particular generalization. One type of constraint involves\ndetermining which entities can be members of a given lower-level entity set. Such\nmembership may be one of the following:\n• Condition-deﬁned. In condition-deﬁned lower-level entity sets, membership\nis evaluated on the basis of whether or not an entity satisﬁes an explicit con-\ndition or predicate. For example, assume that the higher-level entity set ac-\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nI. Data Models\n2. Entity−Relationship \nModel\n62\n© The McGraw−Hill \nCompanies, 2001\n2.7\nExtended E-R Features\n53\ncount has the attribute account-type. All account entities are evaluated on the\ndeﬁning account-type attribute. Only those entities that satisfy the condition\naccount-type = “savings account” are allowed to belong to the lower-level en-\ntity set person. All entities that satisfy the condition account-type = “checking\naccount” are included in checking account. Since all the lower-level entities are\nevaluated on the basis of the same attribute (in this case, on account-type), this\ntype of generalization is said to be attribute-deﬁned.\n• User-deﬁned. User-deﬁned lower-level entity sets are not constrained by a\nmembership condition; rather, the database user assigns entities to a given en-\ntity set. For instance, let us assume that, after 3 months of employment, bank\nemployees are assigned to one of four work teams. We therefore represent the\nteams as four lower-level entity sets of the higher-level employee entity set. A\ngiven employee is not assigned to a speciﬁc team entity automatically on the\nbasis of an explicit deﬁning condition. Instead, the user in charge of this de-\ncision makes the team assignment on an individual basis. The assignment is\nimplemented by an operation that adds an entity to an entity set.\nA second type of constraint relates to whether or not entities may belong to more\nthan one lower-level entity set within a single generalization. The lower-level entity\nsets may be one of the following:\n• Disjoint. A disjointness constraint requires that an entity belong to no more\nthan one lower-level entity set. In our example, an account entity can satisfy\nonly one condition for the account-type attribute; an entity can be either a sav-\nings account or a checking account, but cannot be both.\n• Overlapping. In overlapping generalizations, the same entity may belong to\nmore than one lower-level entity set within a single generalization. For an\nillustration, consider the employee work team example, and assume that cer-\ntain managers participate in more than one work team. A given employee may\ntherefore appear in more than one of the team entity sets that are lower-level\nentity sets of employee. Thus, the generalization is overlapping.\nAs another example, suppose generalization applied to entity sets customer\nand employee leads to a higher-level entity set person. The generalization is\noverlapping if an employee can also be a customer.\nLower-level entity overlap is the default case; a disjointness constraint must be placed\nexplicitly on a generalization (or specialization). We can note a disjointedness con-\nstraint in an E-R diagram by adding the word disjoint next to the triangle symbol.\nA ﬁnal constraint, the completeness constraint on a generalization or specializa-\ntion, speciﬁes whether or not an entity in the higher-level entity set must belong to at\nleast one of the lower-level entity sets within the generalization/specialization. This\nconstraint may be one of the following:\n• Total generalization or specialization. Each higher-level entity must belong\nto a lower-level entity set.\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nI. Data Models\n2. Entity−Relationship \nModel\n63\n© The McGraw−Hill \nCompanies, 2001\n54\nChapter 2\nEntity-Relationship Model\n• Partial generalization or specialization. Some higher-level entities may not\nbelong to any lower-level entity set.\nPartial generalization is the default. We can specify total generalization in an E-R dia-\ngram by using a double line to connect the box representing the higher-level entity set\nto the triangle symbol. (This notation is similar to the notation for total participation\nin a relationship.)\nThe account generalization is total: All account entities must be either a savings\naccount or a checking account. Because the higher-level entity set arrived at through\ngeneralization is generally composed of only those entities in the lower-level entity\nsets, the completeness constraint for a generalized higher-level entity set is usually\ntotal. When the generalization is partial, a higher-level entity is not constrained to\nappear in a lower-level entity set. The work team entity sets illustrate a partial spe-\ncialization. Since employees are assigned to a team only after 3 months on the job,\nsome employee entities may not be members of any of the lower-level team entity sets.\nWe may characterize the team entity sets more fully as a partial, overlapping spe-\ncialization of employee. The generalization of checking-account and savings-account into\naccount is a total, disjoint generalization. The completeness and disjointness con-\nstraints, however, do not depend on each other. Constraint patterns may also be\npartial-disjoint and total-overlapping.\nWe can see that certain insertion and deletion requirements follow from the con-\nstraints that apply to a given generalization or specialization. For instance, when a\ntotal completeness constraint is in place, an entity inserted into a higher-level en-\ntity set must also be inserted into at least one of the lower-level entity sets. With a\ncondition-deﬁned constraint, all higher-level entities that satisfy the condition must\nbe inserted into that lower-level entity set. Finally, an entity that is deleted from a\nhigher-level entity set also is deleted from all the associated lower-level entity sets to\nwhich it belongs.\n2.7.5\nAggregation\nOne limitation of the E-R model is that it cannot express relationships among rela-\ntionships. To illustrate the need for such a construct, consider the ternary relationship\nworks-on, which we saw earlier, between a employee, branch, and job (see Figure 2.13).\nNow, suppose we want to record managers for tasks performed by an employee at a\nbranch; that is, we want to record managers for (employee, branch, job) combinations.\nLet us assume that there is an entity set manager.\nOne alternative for representing this relationship is to create a quaternary relation-\nship manages between employee, branch, job, and manager. (A quaternary relationship is\nrequired—a binary relationship between manager and employee would not permit us\nto represent which (branch, job) combinations of an employee are managed by which\nmanager.) Using the basic E-R modeling constructs, we obtain the E-R diagram of\nFigure 2.18. (We have omitted the attributes of the entity sets, for simplicity.)\nIt appears that the relationship sets works-on and manages can be combined into\none single relationship set. Nevertheless, we should not combine them into a single\nrelationship, since some employee, branch, job combinations many not have a manager.\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nI. Data Models\n2. Entity−Relationship \nModel\n64\n© The McGraw−Hill \nCompanies, 2001\n2.7\nExtended E-R Features\n55\nemployee\nbranch\nmanages\nmanager\njob\nworks-on\nFigure 2.18\nE-R diagram with redundant relationships.\nThere is redundant information in the resultant ﬁgure, however, since every em-\nployee, branch, job combination in manages is also in works-on. If the manager were a\nvalue rather than an manager entity, we could instead make manager a multivalued at-\ntribute of the relationship works-on. But doing so makes it more difﬁcult (logically as\nwell as in execution cost) to ﬁnd, for example, employee-branch-job triples for which\na manager is responsible. Since the manager is a manager entity, this alternative is\nruled out in any case.\nThe best way to model a situation such as the one just described is to use aggrega-\ntion. Aggregation is an abstraction through which relationships are treated as higher-\nlevel entities. Thus, for our example, we regard the relationship set works-on (relating\nthe entity sets employee, branch, and job) as a higher-level entity set called works-on.\nSuch an entity set is treated in the same manner as is any other entity set. We can\nthen create a binary relationship manages between works-on and manager to represent\nwho manages what tasks. Figure 2.19 shows a notation for aggregation commonly\nused to represent the above situation.\n2.7.6\nAlternative E-R Notations\nFigure 2.20 summarizes the set of symbols we have used in E-R diagrams. There is\nno universal standard for E-R diagram notation, and different books and E-R diagram\nsoftware use different notations; Figure 2.21 indicates some of the alternative nota-\ntions that are widely used. An entity set may be represented as a box with the name\noutside, and the attributes listed one below the other within the box. The primary\nkey attributes are indicated by listing them at the top, with a line separating them\nfrom the other attributes.\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nI. Data Models\n2. Entity−Relationship \nModel\n65\n© The McGraw−Hill \nCompanies, 2001\n56\nChapter 2\nEntity-Relationship Model\nbranch\nemployee\nmanages\nmanager\nworks-on\njob\nFigure 2.19\nE-R diagram with aggregation.\nCardinality constraints can be indicated in several different ways, as Figure 2.21\nshows. The labels ∗and 1 on the edges out of the relationship are sometimes used for\ndepicting many-to-many, one-to-one, and many-to-one relationships, as the ﬁgure\nshows. The case of one-to-many is symmetric to many-to-one, and is not shown. In\nanother alternative notation in the ﬁgure, relationship sets are represented by lines\nbetween entity sets, without diamonds; only binary relationships can be modeled\nthus. Cardinality constraints in such a notation are shown by “crow’s foot” notation,\nas in the ﬁgure.\n2.8\nDesign of an E-R Database Schema\nThe E-R data model gives us much ﬂexibility in designing a database schema to\nmodel a given enterprise. In this section, we consider how a database designer may\nselect from the wide range of alternatives. Among the designer’s decisions are:\n• Whether to use an attribute or an entity set to represent an object (discussed\nearlier in Section 2.2.1)\n• Whether a real-world concept is expressed more accurately by an entity set or\nby a relationship set (Section 2.2.2)\n• Whether to use a ternary relationship or a pair of binary relationships (Sec-\ntion 2.2.3)\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nI. Data Models\n2. Entity−Relationship \nModel\n66\n© The McGraw−Hill \nCompanies, 2001\n2.8\nDesign of an E-R Database Schema\n57\ntotal \nparticipation \nof entity set \nin relationship\nA\nR\nE\nmany-to-many\nrelationship\nR\nR\nE\nrole-\nname\nISA\n(specialization or\ngeneralization) \nmany-to-one\nrelationship\nR\nE\nR\nl..h\ncardinality \nlimits\ndiscriminating \nattribute of \nweak entity set\nISA\nISA\ntotal\ngeneralization\nISA\nattribute\nmultivalued\nattribute\nderived attribute\nweak entity set\nentity set\nE\nR\nA\nA\nR\nE\nA\nA\nprimary key\nrelationship set\nidentifying\nrelationship \nset for weak \nentity set\nrole indicator\none-to-one\nrelationship\nR\ndisjoint\ndisjoint\ngeneralization\nFigure 2.20\nSymbols used in the E-R notation.\n• Whether to use a strong or a weak entity set (Section 2.6); a strong entity set\nand its dependent weak entity sets may be regarded as a single “object” in the\ndatabase, since weak entities are existence dependent on a strong entity\n• Whether using generalization (Section 2.7.2) is appropriate; generalization, or\na hierarchy of ISA relationships, contributes to modularity by allowing com-\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nI. Data Models\n2. Entity−Relationship \nModel\n67\n© The McGraw−Hill \nCompanies, 2001\n58\nChapter 2\nEntity-Relationship Model\nR\nR\nmany-to-many\nrelationship\nentity set E with\nattributes A1, A2, A3\nand primary key A1\n*\n*\nR\n1\n1\nR\n*\n1\nR\nR\none-to-one\nrelationship\nmany-to-one\nrelationship\nE\nA1\nA2\nA3\nFigure 2.21\nAlternative E-R notations.\nmon attributes of similar entity sets to be represented in one place in an E-R\ndiagram\n• Whether using aggregation (Section 2.7.5) is appropriate; aggregation groups\na part of an E-R diagram into a single entity set, allowing us to treat the ag-\ngregate entity set as a single unit without concern for the details of its internal\nstructure.\nWe shall see that the database designer needs a good understanding of the enterprise\nbeing modeled to make these decisions.\n2.8.1\nDesign Phases\nA high-level data model serves the database designer by providing a conceptual\nframework in which to specify, in a systematic fashion, what the data requirements\nof the database users are, and how the database will be structured to fulﬁll these\nrequirements. The initial phase of database design, then, is to characterize fully the\ndata needs of the prospective database users. The database designer needs to interact\nextensively with domain experts and users to carry out this task. The outcome of this\nphase is a speciﬁcation of user requirements.\nNext, the designer chooses a data model, and by applying the concepts of the\nchosen data model, translates these requirements into a conceptual schema of the\ndatabase. The schema developed at this conceptual-design phase provides a detailed\noverview of the enterprise. Since we have studied only the E-R model so far, we shall\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nI. Data Models\n2. Entity−Relationship \nModel\n68\n© The McGraw−Hill \nCompanies, 2001\n2.8\nDesign of an E-R Database Schema\n59\nuse it to develop the conceptual schema. Stated in terms of the E-R model, the schema\nspeciﬁes all entity sets, relationship sets, attributes, and mapping constraints. The de-\nsigner reviews the schema to conﬁrm that all data requirements are indeed satisﬁed\nand are not in conﬂict with one another. She can also examine the design to remove\nany redundant features. Her focus at this point is describing the data and their rela-\ntionships, rather than on specifying physical storage details.\nA fully developed conceptual schema will also indicate the functional require-\nments of the enterprise. In a speciﬁcation of functional requirements, users describe\nthe kinds of operations (or transactions) that will be performed on the data. Example\noperations include modifying or updating data, searching for and retrieving speciﬁc\ndata, and deleting data. At this stage of conceptual design, the designer can review\nthe schema to ensure it meets functional requirements.\nThe process of moving from an abstract data model to the implementation of the\ndatabase proceeds in two ﬁnal design phases. In the logical-design phase, the de-\nsigner maps the high-level conceptual schema onto the implementation data model\nof the database system that will be used. The designer uses the resulting system-\nspeciﬁc database schema in the subsequent physical-design phase, in which the\nphysical features of the database are speciﬁed. These features include the form of ﬁle\norganization and the internal storage structures; they are discussed in Chapter 11.\nIn this chapter, we cover only the concepts of the E-R model as used in the concep-\ntual-schema-design phase. We have presented a brief overview of the database-design\nprocess to provide a context for the discussion of the E-R data model. Database design\nreceives a full treatment in Chapter 7.\nIn Section 2.8.2, we apply the two initial database-design phases to our banking-\nenterprise example. We employ the E-R data model to translate user requirements\ninto a conceptual design schema that is depicted as an E-R diagram.\n2.8.2\nDatabase Design for Banking Enterprise\nWe now look at the database-design requirements of a banking enterprise in more\ndetail, and develop a more realistic, but also more complicated, design than what\nwe have seen in our earlier examples. However, we do not attempt to model every\naspect of the database-design for a bank; we consider only a few aspects, in order to\nillustrate the process of database design.\n2.8.2.1\nData Requirements\nThe initial speciﬁcation of user requirements may be based on interviews with the\ndatabase users, and on the designer’s own analysis of the enterprise. The description\nthat arises from this design phase serves as the basis for specifying the conceptual\nstructure of the database. Here are the major characteristics of the banking enterprise.\n• The bank is organized into branches. Each branch is located in a particular\ncity and is identiﬁed by a unique name. The bank monitors the assets of each\nbranch.\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nI. Data Models\n2. Entity−Relationship \nModel\n69\n© The McGraw−Hill \nCompanies, 2001\n60\nChapter 2\nEntity-Relationship Model\n• Bank customers are identiﬁed by their customer-id values. The bank stores each\ncustomer’s name, and the street and city where the customer lives. Customers\nmay have accounts and can take out loans. A customer may be associated with\na particular banker, who may act as a loan ofﬁcer or personal banker for that\ncustomer.\n• Bank employees are identiﬁed by their employee-id values. The bank adminis-\ntration stores the name and telephone number of each employee, the names\nof the employee’s dependents, and the employee-id number of the employee’s\nmanager. The bank also keeps track of the employee’s start date and, thus,\nlength of employment.\n• The bank offers two types of accounts—savings and checking accounts. Ac-\ncounts can be held by more than one customer, and a customer can have more\nthan one account. Each account is assigned a unique account number. The\nbank maintains a record of each account’s balance, and the most recent date on\nwhich the account was accessed by each customer holding the account. In ad-\ndition, each savings account has an interest rate, and overdrafts are recorded\nfor each checking account.\n• A loan originates at a particular branch and can be held by one or more cus-\ntomers. A loan is identiﬁed by a unique loan number. For each loan, the bank\nkeeps track of the loan amount and the loan payments. Although a loan-\npayment number does not uniquely identify a particular payment among\nthose for all the bank’s loans, a payment number does identify a particular\npayment for a speciﬁc loan. The date and amount are recorded for each pay-\nment.\nIn a real banking enterprise, the bank would keep track of deposits and with-\ndrawals from savings and checking accounts, just as it keeps track of payments to\nloan accounts. Since the modeling requirements for that tracking are similar, and we\nwould like to keep our example application small, we do not keep track of such de-\nposits and withdrawals in our model.\n2.8.2.2\nEntity Sets Designation\nOur speciﬁcation of data requirements serves as the starting point for constructing a\nconceptual schema for the database. From the characteristics listed in Section 2.8.2.1,\nwe begin to identify entity sets and their attributes:\n• The branch entity set, with attributes branch-name, branch-city, and assets.\n• The customer entity set, with attributes customer-id, customer-name, customer-\nstreet; and customer-city. A possible additional attribute is banker-name.\n• The employee entity set, with attributes employee-id, employee-name, telephone-\nnumber, salary, and manager. Additional descriptive f\n\neatures are the multival-\nued attribute dependent-name, the base attribute start-date, and the derived at-\ntribute employment-length.\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nI. Data Models\n2. Entity−Relationship \nModel\n70\n© The McGraw−Hill \nCompanies, 2001\n2.8\nDesign of an E-R Database Schema\n61\n• Two account entity sets—savings-account and checking-account—with the com-\nmon attributes of account-number and balance; in addition, savings-account has\nthe attribute interest-rate and checking-account has the attribute overdraft-amount.\n• The loan entity set, with the attributes loan-number, amount, and originating-\nbranch.\n• The weak entity set loan-payment, with attributes payment-number, payment-\ndate, and payment-amount.\n2.8.2.3\nRelationship Sets Designation\nWe now return to the rudimentary design scheme of Section 2.8.2.2 and specify the\nfollowing relationship sets and mapping cardinalities. In the process, we also reﬁne\nsome of the decisions we made earlier regarding attributes of entity sets.\n• borrower, a many-to-many relationship set between customer and loan.\n• loan-branch, a many-to-one relationship set that indicates in which branch a\nloan originated. Note that this relationship set replaces the attribute originating-\nbranch of the entity set loan.\n• loan-payment, a one-to-many relationship from loan to payment, which docu-\nments that a payment is made on a loan.\n• depositor, with relationship attribute access-date, a many-to-many relationship\nset between customer and account, indicating that a customer owns an account.\n• cust-banker, with relationship attribute type, a many-to-one relationship set ex-\npressing that a customer can be advised by a bank employee, and that a bank\nemployee can advise one or more customers. Note that this relationship set\nhas replaced the attribute banker-name of the entity set customer.\n• works-for, a relationship set between employee entities with role indicators man-\nager and worker; the mapping cardinalities express that an employee works\nfor only one manager and that a manager supervises one or more employees.\nNote that this relationship set has replaced the manager attribute of employee.\n2.8.2.4\nE-R Diagram\nDrawing on the discussions in Section 2.8.2.3, we now present the completed E-R di-\nagram for our example banking enterprise. Figure 2.22 depicts the full representation\nof a conceptual model of a bank, expressed in terms of E-R concepts. The diagram in-\ncludes the entity sets, attributes, relationship sets, and mapping cardinalities arrived\nat through the design processes of Sections 2.8.2.1 and 2.8.2.2, and reﬁned in Section\n2.8.2.3.\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nI. Data Models\n2. Entity−Relationship \nModel\n71\n© The McGraw−Hill \nCompanies, 2001\n62\nChapter 2\nEntity-Relationship Model\ninterest-rate\noverdraft-amount\naccount-number\nbalance\nISA\ncustomer-name\ncustomer-street\ncustomer-id\ncustomer-city\ncustomer\nbranch-city\nbranch-name\nassets\nloan-number\namount\npayment-number\nloan-\npayment\npayment-date\ntype\ndependent-name\nemployee-id\nemployment-\nlength\n access-date\nborrower\nloan-branch\ncust-banker\ndepositor\nworks-for\nmanager\nworker\nemployee-name\ntelephone-number\nstart-date\nbranch\nloan\npayment\npayment-amount\naccount\nchecking-account\nsavings-account\nemployee\nFigure 2.22\nE-R diagram for a banking enterprise.\n2.9\nReduction of an E-R Schema to Tables\nWe can represent a database that conforms to an E-R database schema by a collection\nof tables. For each entity set and for each relationship set in the database, there is a\nunique table to which we assign the name of the corresponding entity set or relation-\nship set. Each table has multiple columns, each of which has a unique name.\nBoth the E-R model and the relational-database model are abstract, logical rep-\nresentations of real-world enterprises. Because the two models employ similar de-\nsign principles, we can convert an E-R design into a relational design. Converting a\ndatabase representation from an E-R diagram to a table format is the way we arrive\nat a relational-database design from an E-R diagram. Although important differences\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nI. Data Models\n2. Entity−Relationship \nModel\n72\n© The McGraw−Hill \nCompanies, 2001\n2.9\nReduction of an E-R Schema to Tables\n63\nexist between a relation and a table, informally, a relation can be considered to be a\ntable of values.\nIn this section, we describe how an E-R schema can be represented by tables; and\nin Chapter 3, we show how to generate a relational-database schema from an E-R\nschema.\nThe constraints speciﬁed in an E-R diagram, such as primary keys and cardinality\nconstraints, are mapped to constraints on the tables generated from the E-R diagram.\nWe provide more details about this mapping in Chapter 6 after describing how to\nspecify constraints on tables.\n2.9.1\nTabular Representation of Strong Entity Sets\nLet E be a strong entity set with descriptive attributes a1, a2, . . . , an. We represent\nthis entity by a table called E with n distinct columns, each of which corresponds to\none of the attributes of E. Each row in this table corresponds to one entity of the entity\nset E.\nAs an illustration, consider the entity set loan of the E-R diagram in Figure 2.8. This\nentity set has two attributes: loan-number and amount. We represent this entity set by\na table called loan, with two columns, as in Figure 2.23. The row\n(L-17, 1000)\nin the loan table means that loan number L-17 has a loan amount of $1000. We can\nadd a new entity to the database by inserting a row into a table. We can also delete or\nmodify rows.\nLet D1 denote the set of all loan numbers, and let D2 denote the set of all balances.\nAny row of the loan table must consist of a 2-tuple (v1, v2), where v1 is a loan (that\nis, v1 is in set D1) and v2 is an amount (that is, v2 is in set D2). In general, the loan\ntable will contain only a subset of the set of all possible rows. We refer to the set of all\npossible rows of loan as the Cartesian product of D1 and D2, denoted by\nD1 × D2\nIn general, if we have a table of n columns, we denote the Cartesian product of\nD1, D2, · · · , Dn by\nD1 × D2 × · · · × Dn−1 × Dn\nloan-number\namount\nL-11\n900\nL-14\n1500\nL-15\n1500\nL-16\n1300\nL-17\n1000\nL-23\n2000\nL-93\n500\nFigure 2.23\nThe loan table.\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nI. Data Models\n2. Entity−Relationship \nModel\n73\n© The McGraw−Hill \nCompanies, 2001\n64\nChapter 2\nEntity-Relationship Model\ncustomer-id\ncustomer-name\ncustomer-street\ncustomer-city\n019-28-3746\n182-73-6091\n192-83-7465\n244-66-8800\n321-12-3123\n335-57-7991\n336-66-9999\n677-89-9011\n963-96-3963\nSmith\nTurner\nJohnson\nCurry\nJones\nAdams\nLindsay\nHayes\nWilliams\nNorth\nPutnam\nAlma\nNorth\nMain\nSpring\nPark\nMain\nNassau\nRye\nStamford\nPalo Alto\nRye\nHarrison\nPittsfield\nPittsfield\nHarrison\nPrinceton\nFigure 2.24\nThe customer table.\nAs another example, consider the entity set customer of the E-R diagram in Fig-\nure 2.8. This entity set has the attributes customer-id, customer-name, customer-street,\nand customer-city. The table corresponding to customer has four columns, as in Fig-\nure 2.24.\n2.9.2\nTabular Representation of Weak Entity Sets\nLet A be a weak entity set with attributes a1, a2, . . . , am. Let B be the strong entity set\non which A depends. Let the primary key of B consist of attributes b1, b2, . . . , bn. We\nrepresent the entity set A by a table called A with one column for each attribute of\nthe set:\n{a1, a2, . . . , am} ∪{b1, b2, . . . , bn}\nAs an illustration, consider the entity set payment in the E-R diagram of Figure 2.16.\nThis entity set has three attributes: payment-number, payment-date, and payment-amount.\nThe primary key of the loan entity set, on which payment depends, is loan-number.\nThus, we represent payment by a table with four columns labeled loan-number, payment-\nnumber, payment-date, and payment-amount, as in Figure 2.25.\n2.9.3\nTabular Representation of Relationship Sets\nLet R be a relationship set, let a1, a2, . . . , am be the set of attributes formed by the\nunion of the primary keys of each of the entity sets participating in R, and let the\ndescriptive attributes (if any) of R be b1, b2, . . . , bn. We represent this relationship set\nby a table called R with one column for each attribute of the set:\n{a1, a2, . . . , am} ∪{b1, b2, . . . , bn}\nAs an illustration, consider the relationship set borrower in the E-R diagram of Fig-\nure 2.8. This relationship set involves the following two entity sets:\n• customer, with the primary key customer-id\n• loan, with the primary key loan-number\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nI. Data Models\n2. Entity−Relationship \nModel\n74\n© The McGraw−Hill \nCompanies, 2001\n2.9\nReduction of an E-R Schema to Tables\n65\nloan-number\npayment-number\npayment-date\npayment-amount\nL-11\n53\n7 June 2001\n125\nL-14\n69\n28 May 2001\n500\nL-15\n22\n23 May 2001\n300\nL-16\n58\n18 June 2001\n135\nL-17\n5\n10 May 2001\n50\nL-17\n6\n7 June 2001\n50\nL-17\n7\n17 June 2001\n100\nL-23\n11\n17 May 2001\n75\nL-93\n103\n3 June 2001\n900\nL-93\n104\n13 June 2001\n200\nFigure 2.25\nThe payment table.\nSince the relationship set has no attributes, the borrower table has two columns, la-\nbeled customer-id and loan-number, as shown in Figure 2.26.\n2.9.3.1\nRedundancy of Tables\nA relationship set linking a weak entity set to the corresponding strong entity set is\ntreated specially. As we noted in Section 2.6, these relationships are many-to-one and\nhave no descriptive attributes. Furthermore, the primary key of a weak entity set in-\ncludes the primary key of the strong entity set. In the E-R diagram of Figure 2.16, the\nweak entity set payment is dependent on the strong entity set loan via the relation-\nship set loan-payment. The primary key of payment is {loan-number, payment-number},\nand the primary key of loan is {loan-number}. Since loan-payment has no descriptive\nattributes, the loan-payment table would have two columns, loan-number and payment-\nnumber. The table for the entity set payment has four columns, loan-number, payment-\nnumber, payment-date, and payment-amount. Every (loan-number, payment-number) com-\nbination in loan-payment would also be present in the payment table, and vice versa.\nThus, the loan-payment table is redundant. In general, the table for the relationship set\ncustomer-id\nloan-number\n019-28-3746\nL-11\n019-28-3746\nL-23\n244-66-8800\nL-93\n321-12-3123\nL-17\n335-57-7991\nL-16\n555-55-5555\nL-14\n677-89-9011\nL-15\n963-96-3963\nL-17\nFigure 2.26\nThe borrower table.\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nI. Data Models\n2. Entity−Relationship \nModel\n75\n© The McGraw−Hill \nCompanies, 2001\n66\nChapter 2\nEntity-Relationship Model\nlinking a weak entity set to its corresponding strong entity set is redundant and does\nnot need to be present in a tabular representation of an E-R diagram.\n2.9.3.2\nCombination of Tables\nConsider a many-to-one relationship set AB from entity set A to entity set B. Using\nour table-construction scheme outlined previously, we get three tables: A, B, and AB.\nSuppose further that the participation of A in the relationship is total; that is, every\nentity a in the entity set A must participate in the relationship AB. Then we can\ncombine the tables A and AB to form a single table consisting of the union of columns\nof both tables.\nAs an illustration, consider the E-R diagram of Figure 2.27. The double line in the\nE-R diagram indicates that the participation of account in the account-branch is total.\nHence, an account cannot exist without being associated with a particular branch.\nFurther, the relationship set account-branch is many to one from account to branch.\nTherefore, we can combine the table for account-branch with the table for account and\nrequire only the following two tables:\n• account, with attributes account-number, balance, and branch-name\n• branch, with attributes branch-name, branch-city, and assets\n2.9.4\nComposite Attributes\nWe handle composite attributes by creating a separate attribute for each of the com-\nponent attributes; we do not create a separate column for the composite attribute\nitself. Suppose address is a composite attribute of entity set customer, and the com-\nponents of address are street and city. The table generated from customer would then\ncontain columns address-street and address-city; there is no separate column for address.\n2.9.5\nMultivalued Attributes\nWe have seen that attributes in an E-R diagram generally map directly into columns\nfor the appropriate tables. Multivalued attributes, however, are an exception; new\ntables are created for these attributes.\naccount-number\nbalance\naccount\nbranch-name\nbranch-city\nbranch\naccount-\nbranch\nassets\nFigure 2.27\nE-R diagram.\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nI. Data Models\n2. Entity−Relationship \nModel\n76\n© The McGraw−Hill \nCompanies, 2001\n2.9\nReduction of an E-R Schema to Tables\n67\nFor a multivalued attribute M, we create a table T with a column C that corre-\nsponds to M and columns corresponding to the primary key of the entity set or rela-\ntionship set of which M is an attribute. As an illustration, consider the E-R diagram\nin Figure 2.22. The diagram includes the multivalued attribute dependent-name. For\nthis multivalued attribute, we create a table dependent-name, with columns dname, re-\nferring to the dependent-name attribute of employee, and employee-id, representing the\nprimary key of the entity set employee. Each dependent of an employee is represented\nas a unique row in the table.\n2.9.6\nTabular Representation of Generalization\nThere are two different methods for transforming to a tabular form an E-R diagram\nthat includes generalization. Although we refer to the generalization in Figure 2.17\nin this discussion, we simplify it by including only the ﬁrst tier of lower-level entity\nsets—that is, savings-account and checking-account.\n1. Create a table for the higher-level entity set. For each lower-level entity set,\ncreate a table that includes a column for each of the attributes of that entity set\nplus a column for each attribute of the primary key of the higher-level entity\nset. Thus, for the E-R diagram of Figure 2.17, we have three tables:\n• account, with attributes account-number and balance\n• savings-account, with attributes account-number and interest-rate\n• checking-account, with attributes account-number and overdraft-amount\n2. An alternative representation is possible, if the generalization is disjoint and\ncomplete—that is, if no entity is a member of two lower-level entity sets di-\nrectly below a higher-level entity set, and if every entity in the higher level\nentity set is also a member of one of the lower-level entity sets. Here, do not\ncreate a table for the higher-level entity set. Instead, for each lower-level en-\ntity set, create a table that includes a column for each of the attributes of that\nentity set plus a column for each attribute of the higher-level entity set. Then,\nfor the E-R diagram of Figure 2.17, we have two tables.\n• savings-account, with attributes account-number, balance, and interest-rate\n• checking-account, with attributes account-number, balance, and overdraft-\namount\nThe savings-account and checking-account relations corresponding to these\ntables both have account-number as the primary key.\nIf the second method were used for an overlapping generalization, some values\nsuch as balance would be stored twice unnecessarily. Similarly, if the generalization\nwere not complete—that is, if some accounts were neither savings nor checking\naccounts—then such accounts could not be represented with the second method.\n2.9.7\nTabular Representation of Aggregation\nTransforming an E-R diagram containing aggregation to a tabular form is straight-\nforward. Consider the diagram of Figure 2.19. The table for the relationship set\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nI. Data Models\n2. Entity−Relationship \nModel\n77\n© The McGraw−Hill \nCompanies, 2001\n68\nChapter 2\nEntity-Relationship Model\nmanages between the aggregation of works-on and the entity set manager includes a\ncolumn for each attribute in the primary keys of the entity set manager and the rela-\ntionship set works-on. It would also include a column for any descriptive attributes,\nif they exist, of the relationship set manages. We then transform the relationship sets\nand entity sets within the aggregated entity.\n2.10\nThe Uniﬁed Modeling Language UML∗∗\nEntity-relationship diagrams help model the data representation component of a soft-\nware system. Data representation, however, forms only one part of an overall system\ndesign. Other components include models of user interactions with the system, spec-\niﬁcation of functional modules of the system and their interaction, etc. The Uniﬁed\nModeling Language (UML), is a proposed standard for creating speciﬁcations of var-\nious components of a software system. Some of the parts of UML are:\n• Class diagram. A class diagram is similar to an E-R diagram. Later in this\nsection we illustrate a few features of class diagrams and how they relate to\nE-R diagrams.\n• Use case diagram. Use case diagrams show the interaction between users and\nthe system, in particular the steps of tasks that users perform (such as with-\ndrawing money or registering for a course).\n• Activity diagram. Activity diagrams depict the ﬂow of tasks between various\ncomponents of a system.\n• Implementation diagram. Implementation diagrams show the system com-\nponents and their interconnections, both at the software component level and\nthe hardware component level.\nWe do not attempt to provide detailed coverage of the different parts of UML here.\nSee the bibliographic notes for references on UML. Instead we illustrate some features\nof UML through examples.\nFigure 2.28 shows several E-R diagram constructs and their equivalent UML class\ndiagram constructs. We describe these constructs below. UML shows entity sets as\nboxes and, unlike E-R, shows attributes within the box rather than as separate el-\nlipses. UML actually models objects, whereas E-R models entities. Objects are like\nentities, and have attributes, but additionally provide a set of functions (called meth-\nods) that can be invoked to compute values on the basis of attributes of the objects,\nor to update the object itself. Class diagrams can depict methods in addition to at-\ntributes. We cover objects in Chapter 8.\nWe represent binary relationship sets in UML by just drawing a line connecting\nthe entity sets. We write the relationship set name adjacent to the line. We may also\nspecify the role played by an entity set in a relationship set by writing the role name\non the line, adjacent to the entity set. Alternatively, we may write the relationship set\nname in a box, along with attributes of the relationship set, and connect the box by a\ndotted line to the line depicting the relationship set. This box can then be treated as\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nI. Data Models\n2. Entity−Relationship \nModel\n78\n© The McGraw−Hill \nCompanies, 2001\n2.10\nThe Uniﬁed Modeling Language UML∗∗\n69\ndisjoint\nE1\nE2\nR\nrole1\nrole2\nE2\nE2\nperson\ncustomer\nemployee\nperson\ncustomer\nemployee\nperson\nISA\ncustomer\nemployee\nperson\nISA\ncustomer\nemployee\n1. entity sets\n    and attributes\ncustomer-id\ncustomer-name\ncustomer-street\ncustomer-city\ncustomer\ncustomer-city\ncustomer-street\ncustomer\n2. relationships\na1\nrole1\nrole2\na1\na2\nR\nR\n3. cardinality\n    constraints \nR\n0..1\nR\nE1\nE2\nE1\nE1\nE2\nE2\nrole1\nrole2\nrole1\nrole2\n4. generalization and\n    specialization\nclass diagram in UML\nE-R diagram\nR\n0..*\n0..*\n0..1\ncustomer-name\ncustomer-id\n(overlapping \ngeneralization)\n(disjoint \ngeneralization)\na2\nE1\nE1\nFigure 2.28\nSymbols used in the UML class diagram notation.\nan entity set, in the same way as an aggregation in E-R diagrams and can participate\nin relationships with other entity sets.\nNonbinar\n\nn 2.1.2),\nthe role name is used instead of the name of the entity set, to form a unique attribute\nname.\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nI. Data Models\n2. Entity−Relationship \nModel\n46\n© The McGraw−Hill \nCompanies, 2001\n2.4\nDesign Issues\n37\nThe structure of the primary key for the relationship set depends on the map-\nping cardinality of the relationship set. As an illustration, consider the entity sets\ncustomer and account, and the relationship set depositor, with attribute access-date, in\nSection 2.1.2. Suppose that the relationship set is many to many. Then the primary\nkey of depositor consists of the union of the primary keys of customer and account.\nHowever, if a customer can have only one account—that is, if the depositor relation-\nship is many to one from customer to account—then the primary key of depositor is\nsimply the primary key of customer. Similarly, if the relationship is many to one from\naccount to customer—that is, each account is owned by at most one customer—then\nthe primary key of depositor is simply the primary key of account. For one-to-one re-\nlationships either primary key can be used.\nFor nonbinary relationships, if no cardinality constraints are present then the su-\nperkey formed as described earlier in this section is the only candidate key, and it\nis chosen as the primary key. The choice of the primary key is more complicated if\ncardinality constraints are present. Since we have not discussed how to specify cardi-\nnality constraints on nonbinary relations, we do not discuss this issue further in this\nchapter. We consider the issue in more detail in Section 7.3.\n2.4\nDesign Issues\nThe notions of an entity set and a relationship set are not precise, and it is possible\nto deﬁne a set of entities and the relationships among them in a number of differ-\nent ways. In this section, we examine basic issues in the design of an E-R database\nschema. Section 2.7.4 covers the design process in further detail.\n2.4.1\nUse of Entity Sets versus Attributes\nConsider the entity set employee with attributes employee-name and telephone-number.\nIt can easily be argued that a telephone is an entity in its own right with attributes\ntelephone-number and location (the ofﬁce where the telephone is located). If we take\nthis point of view, we must redeﬁne the employee entity set as:\n• The employee entity set with attribute employee-name\n• The telephone entity set with attributes telephone-number and location\n• The relationship set emp-telephone, which denotes the association between em-\nployees and the telephones that they have\nWhat, then, is the main difference between these two deﬁnitions of an employee?\nTreating a telephone as an attribute telephone-number implies that employees have\nprecisely one telephone number each. Treating a telephone as an entity telephone per-\nmits employees to have several telephone numbers (including zero) associated with\nthem. However, we could instead easily deﬁne telephone-number as a multivalued at-\ntribute to allow multiple telephones per employee.\nThe main difference then is that treating a telephone as an entity better models a\nsituation where one may want to keep extra information about a telephone, such as\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nI. Data Models\n2. Entity−Relationship \nModel\n47\n© The McGraw−Hill \nCompanies, 2001\n38\nChapter 2\nEntity-Relationship Model\nits location, or its type (mobile, video phone, or plain old telephone), or who all share\nthe telephone. Thus, treating telephone as an entity is more general than treating it\nas an attribute and is appropriate when the generality may be useful.\nIn contrast, it would not be appropriate to treat the attribute employee-name as an\nentity; it is difﬁcult to argue that employee-name is an entity in its own right (in contrast\nto the telephone). Thus, it is appropriate to have employee-name as an attribute of the\nemployee entity set.\nTwo natural questions thus arise: What constitutes an attribute, and what con-\nstitutes an entity set? Unfortunately, there are no simple answers. The distinctions\nmainly depend on the structure of the real-world enterprise being modeled, and on\nthe semantics associated with the attribute in question.\nA common mistake is to use the primary key of an entity set as an attribute of an-\nother entity set, instead of using a relationship. For example, it is incorrect to model\ncustomer-id as an attribute of loan even if each loan had only one customer. The re-\nlationship borrower is the correct way to represent the connection between loans and\ncustomers, since it makes their connection explicit, rather than implicit via an at-\ntribute.\nAnother related mistake that people sometimes make is to designate the primary\nkey attributes of the related entity sets as attributes of the relationship set. This should\nnot be done, since the primary key attributes are already implicit in the relationship.\n2.4.2\nUse of Entity Sets versus Relationship Sets\nIt is not always clear whether an object is best expressed by an entity set or a rela-\ntionship set. In Section 2.1.1, we assumed that a bank loan is modeled as an entity.\nAn alternative is to model a loan not as an entity, but rather as a relationship between\ncustomers and branches, with loan-number and amount as descriptive attributes. Each\nloan is represented by a relationship between a customer and a branch.\nIf every loan is held by exactly one customer and is associated with exactly one\nbranch, we may ﬁnd satisfactory the design where a loan is represented as a rela-\ntionship. However, with this design, we cannot represent conveniently a situation in\nwhich several customers hold a loan jointly. To handle such a situation, we must de-\nﬁne a separate relationship for each holder of the joint loan. Then, we must replicate\nthe values for the descriptive attributes loan-number and amount in each such relation-\nship. Each such relationship must, of course, have the same value for the descriptive\nattributes loan-number and amount.\nTwo problems arise as a result of the replication: (1) the data are stored multiple\ntimes, wasting storage space, and (2) updates potentially leave the data in an incon-\nsistent state, where the values differ in two relationships for attributes that are sup-\nposed to have the same value. The issue of how to avoid such replication is treated\nformally by normalization theory, discussed in Chapter 7.\nThe problem of replication of the attributes loan-number and amount is absent in\nthe original design of Section 2.1.1, because there loan is an entity set.\nOne possible guideline in determining whether to use an entity set or a relation-\nship set is to designate a relationship set to describe an action that occurs between\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nI. Data Models\n2. Entity−Relationship \nModel\n48\n© The McGraw−Hill \nCompanies, 2001\n2.4\nDesign Issues\n39\nentities. This approach can also be useful in deciding whether certain attributes may\nbe more appropriately expressed as relationships.\n2.4.3\nBinary versus n-ary Relationship Sets\nRelationships in databases are often binary. Some relationships that appear to be\nnonbinary could actually be better represented by several binary relationships. For\ninstance, one could create a ternary relationship parent, relating a child to his/her\nmother and father. However, such a relationship could also be represented by two\nbinary relationships, mother and father, relating a child to his/her mother and father\nseparately. Using the two relationships mother and father allows us record a child’s\nmother, even if we are not aware of the father’s identity; a null value would be\nrequired if the ternary relationship parent is used. Using binary relationship sets is\npreferable in this case.\nIn fact, it is always possible to replace a nonbinary (n-ary, for n > 2) relationship\nset by a number of distinct binary relationship sets. For simplicity, consider the ab-\nstract ternary (n = 3) relationship set R, relating entity sets A, B, and C. We replace\nthe relationship set R by an entity set E, and create three relationship sets:\n• RA, relating E and A\n• RB, relating E and B\n• RC, relating E and C\nIf the relationship set R had any attributes, these are assigned to entity set E; further,\na special identifying attribute is created for E (since it must be possible to distinguish\ndifferent entities in an entity set on the basis of their attribute values). For each rela-\ntionship (ai, bi, ci) in the relationship set R, we create a new entity ei in the entity set\nE. Then, in each of the three new relationship sets, we insert a relationship as follows:\n• (ei, ai) in RA\n• (ei, bi) in RB\n• (ei, ci) in RC\nWe can generalize this process in a straightforward manner to n-ary relationship\nsets. Thus, conceptually, we can restrict the E-R model to include only binary rela-\ntionship sets. However, this restriction is not always desirable.\n• An identifying attribute may have to be created for the entity set created to\nrepresent the relationship set. This attribute, along with the extra relationship\nsets required, increases the complexity of the design and (as we shall see in\nSection 2.9) overall storage requirements.\n• A n-ary relationship set shows more clearly that several entities participate in\na single relationship.\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nI. Data Models\n2. Entity−Relationship \nModel\n49\n© The McGraw−Hill \nCompanies, 2001\n40\nChapter 2\nEntity-Relationship Model\n• There may not be a way to translate constraints on the ternary relationship\ninto constraints on the binary relationships. For example, consider a constraint\nthat says that R is many-to-one from A, B to C; that is, each pair of entities\nfrom A and B is associated with at most one C entity. This constraint cannot\nbe expressed by using cardinality constraints on the relationship sets RA, RB,\nand RC.\nConsider the relationship set works-on in Section 2.1.2, relating employee, branch,\nand job. We cannot directly split works-on into binary relationships between employee\nand branch and between employee and job. If we did so, we would be able to record\nthat Jones is a manager and an auditor and that Jones works at Perryridge and Down-\ntown; however, we would not be able to record that Jones is a manager at Perryridge\nand an auditor at Downtown, but is not an auditor at Perryridge or a manager at\nDowntown.\nThe relationship set works-on can be split into binary relationships by creating a\nnew entity set as described above. However, doing so would not be very natural.\n2.4.4\nPlacement of Relationship Attributes\nThe cardinality ratio of a relationship can affect the placement of relationship at-\ntributes. Thus, attributes of one-to-one or one-to-many relationship sets can be as-\nsociated with one of the participating entity sets, rather than with the relationship\nset. For instance, let us specify that depositor is a one-to-many relationship set such\nthat one customer may have several accounts, but each account is held by only one\ncustomer. In this case, the attribute access-date, which speciﬁes when the customer last\naccessed that account, could be associated with the account entity set, as Figure 2.6 de-\npicts; to keep the ﬁgure simple, only some of the attributes of the two entity sets are\nshown. Since each account entity participates in a relationship with at most one in-\nstance of customer, making this attribute designation would have the same meaning\nA-101    24 May 1996\nA-215     3 June 1996\nA-102    10 June 1996\nA-305    28 May 1996\nA-201    17 June 1996\nA-222    24 June 1996\nA-217    23 May 1996\ncustomer (customer-name)\naccount (account-number, access-date)\ndepositor\nJohnson\nSmith\nHayes\nTurner\nJones\nLindsay\nFigure 2.6\nAccess-date as attribute of the account entity set.\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nI. Data Models\n2. Entity−Relationship \nModel\n50\n© The McGraw−Hill \nCompanies, 2001\n2.4\nDesign Issues\n41\nas would placing access-date with the depositor relationship set. Attributes of a one-to-\nmany relationship set can be repositioned to only the entity set on the “many” side of\nthe relationship. For one-to-one relationship sets, on the other hand, the relationship\nattribute can be associated with either one of the participating entities.\nThe design decision of where to place descriptive attributes in such cases—as a\nrelationship or entity attribute—should reﬂect the characteristics of the enterprise\nbeing modeled. The designer may choose to retain access-date as an attribute of depos-\nitor to express explicitly that an access occurs at the point of interaction between the\ncustomer and account entity sets.\nThe choice of attribute placement is more clear-cut for many-to-many relationship\nsets. Returning to our example, let us specify the perhaps more realistic case that\ndepositor is a many-to-many relationship set expressing that a customer may have\none or more accounts, and that an account can be held by one or more customers.\nIf we are to express the date on which a speciﬁc customer last accessed a speciﬁc\naccount, access-date must be an attribute of the depositor relationship set, rather than\neither one of the participating entities. If access-date were an attribute of account, for\ninstance, we could not determine which customer made the most recent access to a\njoint account. When an attribute is determined by the combination of participating\nentity sets, rather than by either entity separately, that attribute must be associated\nwith the many-to-many relationship set. Figure 2.7 depicts the placement of access-\ndate as a relationship attribute; again, to keep the ﬁgure simple, only some of the\nattributes of the two entity sets are shown.\nJohnson\nSmith\nHayes\nTurner\nJones\nLindsay\nA-101    \nA-215  \nA-102   \nA-305    \nA-201    \nA-222    \nA-217   \ncustomer(customer-name)\naccount(account-number)\ndepositor(access-date)\n24 May 1996\n  3 June 1996\n21 June 1996\n10 June 1996\n17 June 1996\n28 May 1996\n28 May 1996\n24 June 1996\n23 May 1996\nFigure 2.7\nAccess-date as attribute of the depositor relationship set.\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nI. Data Models\n2. Entity−Relationship \nModel\n51\n© The McGraw−Hill \nCompanies, 2001\n42\nChapter 2\nEntity-Relationship Model\n2.5\nEntity-Relationship Diagram\nAs we saw brieﬂy in Section 1.4, an E-R diagram can express the overall logical struc-\nture of a database graphically. E-R diagrams are simple and clear—qualities that may\nwell account in large part for the widespread use of the E-R model. Such a diagram\nconsists of the following major components:\n• Rectangles, which represent entity sets\n• Ellipses, which represent attributes\n• Diamonds, which represent relationship sets\n• Lines, which link attributes to entity sets and entity sets to relationship sets\n• Double ellipses, which represent multivalued attributes\n• Dashed ellipses, which denote derived attributes\n• Double lines, which indicate total participation of an entity in a relation-\nship set\n• Double rectangles, which represent weak entity sets (described later, in Sec-\ntion 2.6.)\nConsider the entity-relationship diagram in Figure 2.8, which consists of two en-\ntity sets, customer and loan, related through a binary relationship set borrower. The at-\ntributes associated with customer are customer-id, customer-name, customer-street, and\ncustomer-city. The attributes associated with loan are loan-number and amount. In Fig-\nure 2.8, attributes of an entity set that are members of the primary key are underlined.\nThe relationship set borrower may be many-to-many, one-to-many, many-to-one,\nor one-to-one. To distinguish among these types, we draw either a directed line (→)\nor an undirected line (—) between the relationship set and the entity set in question.\n• A directed line from the relationship set borrower to the entity set loan speci-\nﬁes that borrower is either a one-to-one or many-to-one relationship set, from\ncustomer to loan; borrower cannot be a many-to-many or a one-to-many rela-\ntionship set from customer to loan.\ncustomer-name\ncustomer-street\ncustomer-id\ncustomer-city\ncustomer\nloan-number\namount\nloan\nborrower\nFigure 2.8\nE-R diagram corresponding to customers and loans.\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nI. Data Models\n2. Entity−Relationship \nModel\n52\n© The McGraw−Hill \nCompanies, 2001\n2.5\nEntity-Relationship Diagram\n43\n• An undirected line from the relationship set borrower to the entity set loan spec-\niﬁes that borrower is either a many-to-many or one-to-many relationship set\nfrom customer to loan.\nReturning to the E-R diagram of Figure 2.8, we see that the relationship set borrower\nis many-to-many. If the relationship set borrower were one-to-many, from customer to\nloan, then the line from borrower to customer would be directed, with an arrow point-\ning to the customer entity set (Figure 2.9a). Similarly, if the relationship set borrower\nwere many-to-one from customer to loan, then the line from borrower to loan would\nhave an arrow pointing to the loan entity set (Figure 2.9b). Finally, if the relation-\nship set borrower were one-to-one, then both lines from borrower would have arrows:\ncustomer-name\ncustomer-street\ncustomer-id\ncustomer-city\ncustomer\nloan-number\namount\nloan\nborrower\n(a)\ncustomer-name\ncustomer-street\ncustomer-id\ncustomer-city\ncustomer\nloan-number\namount\nloan\nborrower\n(b)\ncustomer-name\ncustomer-street\ncustomer-id\ncustomer-city\ncustomer\nloan-number\namount\nloan\nborrower\n(c)\nFigure 2.9\nRelationships. (a) one to many. (b) many to one. (c) one-to-one.\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nI. Data Models\n2. Entity−Relationship \nModel\n53\n© The McGraw−Hill \nCompanies, 2001\n44\nChapter 2\nEntity-Relationship Model\ncustomer-name\ncustomer-street\ncustomer-id\ncustomer-city\ncustomer\nbalance\naccount\ndepositor\naccess-date\naccount-number\nFigure 2.10\nE-R diagram with an attribute attached to a relationship set.\none pointing to the loan entity set and one pointing to the customer entity set (Fig-\nure 2.9c).\nIf a relationship set has also some attributes associated with it, then we link these\nattributes to that relationship set. For example, in Figure 2.10, we have the access-\ndate descriptive attribute attached to the relationship set depositor to specify the most\nrecent date on which a customer accessed that account.\nFigure 2.11 shows how composite attributes can be represented in the E-R notation.\nHere, a composite attribute name, with component attributes ﬁrst-name, middle-initial,\nand last-name replaces the simple attribute customer-name of customer. Also, a compos-\nite attribute address, whose component attributes are street, city, state, and zip-code re-\nplaces the attributes customer-street and customer-city of customer. The attribute street is\nitself a composite attribute whose component attributes are street-number, street-name,\nand apartment number.\nFigure 2.11 also illustrates a multivalued attribute phone-number, depicted by a\ndouble ellipse, and a derived attribute age, depicted by a dashed ellipse.\ncity\nzip-code\nstreet\nstate\nname\ncustomer\ncustomer-id\nmiddle-initial\nlast-name\nfirst-name\nstreet-number\nstreet-name\napartment-number\naddress\nphone-number\ndate-of-birth\nage\nFigure 2.11\nE-R diagram with composite, multivalued, and derived attributes.\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nI. Data Models\n2. Entity−Relationship \nModel\n54\n© The McGraw−Hill \nCompanies, 2001\n2.5\nEntity-Relationship Diagram\n45\nemployee-id\nemployee-name\ntelephone-number\nemployee\nworks-for\nmanager\nworker\nFigure 2.12\nE-R diagram with role indicators.\nWe indicate roles in E-R diagrams by labeling the lines that connect diamonds\nto rectangles. Figure 2.12 shows the role indicators manager and worker between the\nemployee entity set and the works-for relationship set.\nNonbinary relationship sets\n\n can be speciﬁed easily in an E-R diagram. Figure 2.13\nconsists of the three entity sets employee, job, and branch, related through the relation-\nship set works-on.\nWe can specify some types of many-to-one relationships in the case of nonbinary\nrelationship sets. Suppose an employee can have at most one job in each branch (for\nexample, Jones cannot be a manager and an auditor at the same branch). This con-\nstraint can be speciﬁed by an arrow pointing to job on the edge from works-on.\nWe permit at most one arrow out of a relationship set, since an E-R diagram with\ntwo or more arrows out of a nonbinary relationship set can be interpreted in two\nways. Suppose there is a relationship set R between entity sets A1, A2, . . . , An, and the\nonly arrows are on the edges to entity sets Ai+1, Ai+2, . . . , An. Then, the two possible\ninterpretations are:\n1. A particular combination of entities from A1, A2, . . . , Ai can be associated with\nat most one combination of entities from Ai+1, Ai+2, . . . , An. Thus, the pri-\nmary key for the relationship R can be constructed by the union of the primary\nkeys of A1, A2, . . . , Ai.\nbranch\nbranch-city\nbranch-name\nassets\nemployee-id\ntitle\nlevel\nstreet\ncity\nemployee-name\nemployee\njob\nworks-on\nFigure 2.13\nE-R diagram with a ternary relationship.\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nI. Data Models\n2. Entity−Relationship \nModel\n55\n© The McGraw−Hill \nCompanies, 2001\n46\nChapter 2\nEntity-Relationship Model\nborrower\namount\nloan-number\nloan\ncustomer-city\ncustomer-id\ncustomer-name\ncustomer-street\ncustomer\nFigure 2.14\nTotal participation of an entity set in a relationship set.\n2. For each entity set Ak, i < k ≤n, each combination of the entities from the\nother entity sets can be associated with at most one entity from Ak. Each set\n{A1, A2, . . . , Ak−1, Ak+1, . . . , An}, for i < k ≤n, then forms a candidate key.\nEach of these interpretations has been used in different books and systems. To avoid\nconfusion, we permit only one arrow out of a relationship set, in which case the two\ninterpretations are equivalent. In Chapter 7 (Section 7.3) we study the notion of func-\ntional dependencies, which allow either of these interpretations to be speciﬁed in an\nunambiguous manner.\nDouble lines are used in an E-R diagram to indicate that the participation of an\nentity set in a relationship set is total; that is, each entity in the entity set occurs in at\nleast one relationship in that relationship set. For instance, consider the relationship\nborrower between customers and loans. A double line from loan to borrower, as in\nFigure 2.14, indicates that each loan must have at least one associated customer.\nE-R diagrams also provide a way to indicate more complex constraints on the num-\nber of times each entity participates in relationships in a relationship set. An edge\nbetween an entity set and a binary relationship set can have an associated minimum\nand maximum cardinality, shown in the form l..h, where l is the minimum and h\nthe maximum cardinality. A minimum value of 1 indicates total participation of the\nentity set in the relationship set. A maximum value of 1 indicates that the entity par-\nticipates in at most one relationship, while a maximum value ∗indicates no limit.\nNote that a label 1..∗on an edge is equivalent to a double line.\nFor example, consider Figure 2.15. The edge between loan and borrower has a car-\ndinality constraint of 1..1, meaning the minimum and the maximum cardinality are\nboth 1. That is, each loan must have exactly one associated customer. The limit 0..∗\non the edge from customer to borrower indicates that a customer can have zero or\nmore loans. Thus, the relationship borrower is one to many from customer to loan, and\nfurther the participation of loan in borrower is total.\nIt is easy to misinterpret the 0..∗on the edge between customer and borrower, and\nthink that the relationship borrower is many to one from customer to loan—this is\nexactly the reverse of the correct interpretation.\nIf both edges from a binary relationship have a maximum value of 1, the relation-\nship is one to one. If we had speciﬁed a cardinality limit of 1..∗on the edge between\ncustomer and borrower, we would be saying that each customer must have at least one\nloan.\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nI. Data Models\n2. Entity−Relationship \nModel\n56\n© The McGraw−Hill \nCompanies, 2001\n2.6\nWeak Entity Sets\n47\nborrower\namount\nloan-number\nloan\ncustomer-city\ncustomer-street\ncustomer\n0..*\n1..1\ncustomer-name\ncustomer-id\nFigure 2.15\nCardinality limits on relationship sets.\n2.6\nWeak Entity Sets\nAn entity set may not have sufﬁcient attributes to form a primary key. Such an entity\nset is termed a weak entity set. An entity set that has a primary key is termed a strong\nentity set.\nAs an illustration, consider the entity set payment, which has the three attributes:\npayment-number, payment-date, and payment-amount. Payment numbers are typically\nsequential numbers, starting from 1, generated separately for each loan. Thus, al-\nthough each payment entity is distinct, payments for different loans may share the\nsame payment number. Thus, this entity set does not have a primary key; it is a weak\nentity set.\nFor a weak entity set to be meaningful, it must be associated with another entity\nset, called the identifying or owner entity set. Every weak entity must be associated\nwith an identifying entity; that is, the weak entity set is said to be existence depen-\ndent on the identifying entity set. The identifying entity set is said to own the weak\nentity set that it identiﬁes. The relationship associating the weak entity set with the\nidentifying entity set is called the identifying relationship. The identifying relation-\nship is many to one from the weak entity set to the identifying entity set, and the\nparticipation of the weak entity set in the relationship is total.\nIn our example, the identifying entity set for payment is loan, and a relationship\nloan-payment that associates payment entities with their corresponding loan entities is\nthe identifying relationship.\nAlthough a weak entity set does not have a primary key, we nevertheless need a\nmeans of distinguishing among all those entities in the weak entity set that depend\non one particular strong entity. The discriminator of a weak entity set is a set of at-\ntributes that allows this distinction to be made. For example, the discriminator of the\nweak entity set payment is the attribute payment-number, since, for each loan, a pay-\nment number uniquely identiﬁes one single payment for that loan. The discriminator\nof a weak entity set is also called the partial key of the entity set.\nThe primary key of a weak entity set is formed by the primary key of the iden-\ntifying entity set, plus the weak entity set’s discriminator. In the case of the entity\nset payment, its primary key is {loan-number, payment-number}, where loan-number is\nthe primary key of the identifying entity set, namely loan, and payment-number dis-\ntinguishes payment entities within the same loan.\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nI. Data Models\n2. Entity−Relationship \nModel\n57\n© The McGraw−Hill \nCompanies, 2001\n48\nChapter 2\nEntity-Relationship Model\nThe identifying relationship set should have no descriptive attributes, since any\nrequired attributes can be associated with the weak entity set (see the discussion of\nmoving relationship-set attributes to participating entity sets in Section 2.2.1).\nA weak entity set can participate in relationships other than the identifying re-\nlationship. For instance, the payment entity could participate in a relationship with\nthe account entity set, identifying the account from which the payment was made. A\nweak entity set may participate as owner in an identifying relationship with another\nweak entity set. It is also possible to have a weak entity set with more than one iden-\ntifying entity set. A particular weak entity would then be identiﬁed by a combination\nof entities, one from each identifying entity set. The primary key of the weak entity\nset would consist of the union of the primary keys of the identifying entity sets, plus\nthe discriminator of the weak entity set.\nIn E-R diagrams, a doubly outlined box indicates a weak entity set, and a dou-\nbly outlined diamond indicates the corresponding identifying relationship. In Fig-\nure 2.16, the weak entity set payment depends on the strong entity set loan via the\nrelationship set loan-payment.\nThe ﬁgure also illustrates the use of double lines to indicate total participation—the\nparticipation of the (weak) entity set payment in the relationship loan-payment is total,\nmeaning that every payment must be related via loan-payment to some loan. Finally,\nthe arrow from loan-payment to loan indicates that each payment is for a single loan.\nThe discriminator of a weak entity set also is underlined, but with a dashed, rather\nthan a solid, line.\nIn some cases, the database designer may choose to express a weak entity set as\na multivalued composite attribute of the owner entity set. In our example, this alter-\nnative would require that the entity set loan have a multivalued, composite attribute\npayment, consisting of payment-number, payment-date, and payment-amount. A weak\nentity set may be more appropriately modeled as an attribute if it participates in only\nthe identifying relationship, and if it has few attributes. Conversely, a weak-entity-\nset representation will more aptly model a situation where the set participates in\nrelationships other than the identifying relationship, and where the weak entity set\nhas several attributes.\nloan-number\namount\nloan\npayment-number\npayment-amount\npayment\nloan-payment\npayment-date\nFigure 2.16\nE-R diagram with a weak entity set.\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nI. Data Models\n2. Entity−Relationship \nModel\n58\n© The McGraw−Hill \nCompanies, 2001\n2.7\nExtended E-R Features\n49\nAs another example of an entity set that can be modeled as a weak entity set,\nconsider offerings of a course at a university. The same course may be offered in\ndifferent semesters, and within a semester there may be several sections for the same\ncourse. Thus we can create a weak entity set course-offering, existence dependent on\ncourse; different offerings of the same course are identiﬁed by a semester and a section-\nnumber, which form a discriminator but not a primary key.\n2.7\nExtended E-R Features\nAlthough the basic E-R concepts can model most database features, some aspects of a\ndatabase may be more aptly expressed by certain extensions to the basic E-R model.\nIn this section, we discuss the extended E-R features of specialization, generalization,\nhigher- and lower-level entity sets, attribute inheritance, and aggregation.\n2.7.1\nSpecialization\nAn entity set may include subgroupings of entities that are distinct in some way\nfrom other entities in the set. For instance, a subset of entities within an entity set\nmay have attributes that are not shared by all the entities in the entity set. The E-R\nmodel provides a means for representing these distinctive entity groupings.\nConsider an entity set person, with attributes name, street, and city. A person may\nbe further classiﬁed as one of the following:\n• customer\n• employee\nEach of these person types is described by a set of attributes that includes all the at-\ntributes of entity set person plus possibly additional attributes. For example, customer\nentities may be described further by the attribute customer-id, whereas employee enti-\nties may be described further by the attributes employee-id and salary. The process of\ndesignating subgroupings within an entity set is called specialization. The special-\nization of person allows us to distinguish among persons according to whether they\nare employees or customers.\nAs another example, suppose the bank wishes to divide accounts into two cat-\negories, checking account and savings account. Savings accounts need a minimum\nbalance, but the bank may set interest rates differently for different customers, offer-\ning better rates to favored customers. Checking accounts have a ﬁxed interest rate,\nbut offer an overdraft facility; the overdraft amount on a checking account must be\nrecorded.\nThe bank could then create two specializations of account, namely savings-account\nand checking-account. As we saw earlier, account entities are described by the at-\ntributes account-number and balance. The entity set savings-account would have all the\nattributes of account and an additional attribute interest-rate. The entity set checking-\naccount would have all the attributes of account, and an additional attribute overdraft-\namount.\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nI. Data Models\n2. Entity−Relationship \nModel\n59\n© The McGraw−Hill \nCompanies, 2001\n50\nChapter 2\nEntity-Relationship Model\nWe can apply specialization repeatedly to reﬁne a design scheme. For instance,\nbank employees may be further classiﬁed as one of the following:\n• ofﬁcer\n• teller\n• secretary\nEach of these employee types is described by a set of attributes that includes all the\nattributes of entity set employee plus additional attributes. For example, ofﬁcer entities\nmay be described further by the attribute ofﬁce-number, teller entities by the attributes\nstation-number and hours-per-week, and secretary entities by the attribute hours-per-\nweek. Further, secretary entities may participate in a relationship secretary-for, which\nidentiﬁes which employees are assisted by a secretary.\nAn entity set may be specialized by more than one distinguishing feature. In our\nexample, the distinguishing feature among employee entities is the job the employee\nperforms. Another, coexistent, specialization could be based on whether the person\nis a temporary (limited-term) employee or a permanent employee, resulting in the\nentity sets temporary-employee and permanent-employee. When more than one special-\nization is formed on an entity set, a particular entity may belong to multiple spe-\ncializations. For instance, a given employee may be a temporary employee who is a\nsecretary.\nIn terms of an E-R diagram, specialization is depicted by a triangle component\nlabeled ISA, as Figure 2.17 shows. The label ISA stands for “is a” and represents, for\nexample, that a customer “is a” person. The ISA relationship may also be referred to as\na superclass-subclass relationship. Higher- and lower-level entity sets are depicted\nas regular entity sets—that is, as rectangles containing the name of the entity set.\n2.7.2\nGeneralization\nThe reﬁnement from an initial entity set into successive levels of entity subgroupings\nrepresents a top-down design process in which distinctions are made explicit. The\ndesign process may also proceed in a bottom-up manner, in which multiple entity\nsets are synthesized into a higher-level entity set on the basis of common features. The\ndatabase designer may have ﬁrst identiﬁed a customer entity set with the attributes\nname, street, city, and customer-id, and an employee entity set with the attributes name,\nstreet, city, employee-id, and salary.\nThere are similarities between the customer entity set and the employee entity set\nin the sense that they have several attributes in common. This commonality can be\nexpressed by generalization, which is a containment relationship that exists between\na higher-level entity set and one or more lower-level entity sets. In our example, person\nis the higher-level entity set and customer and employee are lower-level entity sets.\nHigher- and lower-level entity sets also may be designated by the terms superclass\nand subclass, respectively. The person entity set is the superclass of the customer and\nemployee subclasses.\nFor all practical purposes, generalization is a simple inversion of specialization.\nWe will apply both processes, in combination, in the course of designing the E-R\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nI. Data Models\n2. Entity−Relationship \nModel\n60\n© The McGraw−Hill \nCompanies, 2001\n2.7\nExtended E-R Features\n51\nstreet\nemployee\ncustomer\nofficer\nteller\nsecretary\nISA\nISA\nperson\nname\ncity\nhours-worked\noffice-number\nhours-worked\nstation-number\ncredit-rating\nsalary\nFigure 2.17\nSpecialization and generalization.\nschema for an enterprise. In terms of the E-R diagram itself, we do not distinguish be-\ntween specialization and generalization. New levels of entity representation will be\ndistinguished (specialization) or synthesized (generalization) as the design schema\ncomes to express fully the database application and the user requirements of the\ndatabase. Differences in the two approaches may be characterized by their starting\npoint and overall goal.\nSpecialization stems from a single entity set; it emphasizes differences among enti-\nties within the set by creating distinct lower-level entity sets. These lower-level entity\nsets may have attributes, or may participate in relationships, that do not apply to all\nthe entities in the higher-level entity set. Indeed, the reason a designer applies special-\nization is to represent such distinctive features. If customer and employee neither have\nattributes that person entities do not have nor participate in different relationships\nthan those in which person entities participate, there would be no need to specialize\nthe person entity set.\nGeneralization proceeds from the recognition that a number of entity sets share\nsome common features (namely, they are described by the same attributes and par-\nticipate in the same relationship sets). On the basis of their commonalities, generaliza-\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nI. Data Models\n2. Entity−Relationship \nModel\n61\n© The McGraw−Hill \nCompanies, 2001\n52\nChapter 2\nEntity-Relationship Model\ntion synthesizes these entity sets into a single, higher-level entity set. Generalization\nis used to emphasize the similarities among lower-level entity sets and to hide the\ndifferences; it also permits an economy of representation in that shared attributes are\nnot repeated.\n2.7.3\nAttribute Inheritance\nA crucial property of the higher- and lower-level entities created by specialization\nand generalization is attribute inheritance. The attributes of the higher-level entity\nsets are said to be inherited by the lower-level entity sets. For example, customer and\nemployee inherit the attributes of person. Thus, customer is described by its name, street,\nand city attributes, and additionally a customer-id attribute; employee is described by\nits name, street, and city attributes, and additionally employee-id and salary attributes.\nA lower-level entity set (or subclass) also inherits participation in the relationship\nsets in which its higher-level entity (or superclass) participates. The ofﬁcer, teller, and\nsecretary entity sets can participate in the works-for relationship set, since the super-\nclass employee participates in the works-for relationship. Attribute inheritance applies\nthrough all tiers of lower-level entity sets. The above entity sets can participate in any\nrelationships in which the person entity set participates.\nWhether a given portion of an E-R model was arrived at by specialization or gen-\neralization, the outcome is basically the same:\n• A higher-level entity set with attributes and relationships that apply to all of\nits lower-level entity sets\n• Lower-level entity sets with distinctive features that apply only within a par-\nticular lower-level entity set\nIn what follows, although we often refer to only generalization, the properties that\nwe discuss belong fully to both processes.\nFigure 2.17 depicts a hierarchy of entity sets. In the ﬁgure, employee is a lower-level\nentity set of person and a higher-level entity set of the ofﬁcer, teller, and secretary entity\nsets. In a hierarchy, a given entity set may be involved as a lower-level ent\n\noup\ntogether related attributes, making the modeling cleaner.\nNote also that a composite attribute may appear as a hierarchy. In the com-\nposite attribute address, its component attribute street can be further divided\ninto street-number, street-name, and apartment-number. Figure 2.2 depicts these\nexamples of composite attributes for the customer entity set.\n• Single-valued and multivalued attributes. The attributes in our examples all\nhave a single value for a particular entity. For instance, the loan-number at-\ntribute for a speciﬁc loan entity refers to only one loan number. Such attributes\nare said to be single valued. There may be instances where an attribute has\na set of values for a speciﬁc entity. Consider an employee entity set with the\nattribute phone-number. An employee may have zero, one, or several phone\nnumbers, and different employees may have different numbers of phones.\nThis type of attribute is said to be multivalued. As another example, an at-\n2.\nWe assume the address format used in the United States, which includes a numeric postal code called\na zip code.\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nI. Data Models\n2. Entity−Relationship \nModel\n39\n© The McGraw−Hill \nCompanies, 2001\n30\nChapter 2\nEntity-Relationship Model\nComposite\nAttributes\nComponent\nAttributes \nfirst-name  middle-initial  last-name\nstreet  city   state   postal-code\nstreet-number street-name  apartment-number\nname\naddress\nFigure 2.2\nComposite attributes customer-name and customer-address.\ntribute dependent-name of the employee entity set would be multivalued, since\nany particular employee may have zero, one, or more dependent(s).\nWhere appropriate, upper and lower bounds may be placed on the number\nof values in a multivalued attribute. For example, a bank may limit the num-\nber of phone numbers recorded for a single customer to two. Placing bounds\nin this case expresses that the phone-number attribute of the customer entity set\nmay have between zero and two values.\n• Derived attribute. The value for this type of attribute can be derived from\nthe values of other related attributes or entities. For instance, let us say that\nthe customer entity set has an attribute loans-held, which represents how many\nloans a customer has from the bank. We can derive the value for this attribute\nby counting the number of loan entities associated with that customer.\nAs another example, suppose that the customer entity set has an attribute\nage, which indicates the customer’s age. If the customer entity set also has an\nattribute date-of-birth, we can calculate age from date-of-birth and the current\ndate. Thus, age is a derived attribute. In this case, date-of-birth may be referred\nto as a base attribute, or a stored attribute. The value of a derived attribute is\nnot stored, but is computed when required.\nAn attribute takes a null value when an entity does not have a value for it. The\nnull value may indicate “not applicable”—that is, that the value does not exist for the\nentity. For example, one may have no middle name. Null can also designate that an\nattribute value is unknown. An unknown value may be either missing (the value does\nexist, but we do not have that information) or not known (we do not know whether or\nnot the value actually exists).\nFor instance, if the name value for a particular customer is null, we assume that\nthe value is missing, since every customer must have a name. A null value for the\napartment-number attribute could mean that the address does not include an apart-\nment number (not applicable), that an apartment number exists but we do not know\nwhat it is (missing), or that we do not know whether or not an apartment number is\npart of the customer’s address (unknown).\nA database for a banking enterprise may include a number of different entity sets.\nFor example, in addition to keeping track of customers and loans, the bank also\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nI. Data Models\n2. Entity−Relationship \nModel\n40\n© The McGraw−Hill \nCompanies, 2001\n2.1\nBasic Concepts\n31\nprovides accounts, which are represented by the entity set account with attributes\naccount-number and balance. Also, if the bank has a number of different branches, then\nwe may keep information about all the branches of the bank. Each branch entity set\nmay be described by the attributes branch-name, branch-city, and assets.\n2.1.2\nRelationship Sets\nA relationship is an association among several entities. For example, we can deﬁne\na relationship that associates customer Hayes with loan L-15. This relationship spec-\niﬁes that Hayes is a customer with loan number L-15.\nA relationship set is a set of relationships of the same type. Formally, it is a math-\nematical relation on n ≥2 (possibly nondistinct) entity sets. If E1, E2, . . . , En are\nentity sets, then a relationship set R is a subset of\n{(e1, e2, . . . , en) | e1 ∈E1, e2 ∈E2, . . . , en ∈En}\nwhere (e1, e2, . . . , en) is a relationship.\nConsider the two entity sets customer and loan in Figure 2.1. We deﬁne the rela-\ntionship set borrower to denote the association between customers and the bank loans\nthat the customers have. Figure 2.3 depicts this association.\nAs another example, consider the two entity sets loan and branch. We can deﬁne\nthe relationship set loan-branch to denote the association between a bank loan and the\nbranch in which that loan is maintained.\n555-55-5555 \nJackson      Dupont    Woodside\n321-12-3123 \nJones          Main        Harrison\n019-28-3746  \nSmith         North       Rye\n677-89-9011 \nHayes        Main        Harrison\n244-66-8800 \nCurry         North       Rye\n963-96-3963  \nWilliams    Nassau    Princeton\n335-57-7991 \nAdams       Spring     Pittsfield\nL-17   1000\nL-15   1500\nL-14   1500\nL-16   1300\nL-23   2000\nL-19     500\nL-11     900\nloan\ncustomer\nFigure 2.3\nRelationship set borrower.\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nI. Data Models\n2. Entity−Relationship \nModel\n41\n© The McGraw−Hill \nCompanies, 2001\n32\nChapter 2\nEntity-Relationship Model\nThe association between entity sets is referred to as participation; that is, the entity\nsets E1, E2, . . . , En participate in relationship set R. A relationship instance in an\nE-R schema represents an association between the named entities in the real-world\nenterprise that is being modeled. As an illustration, the individual customer entity\nHayes, who has customer identiﬁer 677-89-9011, and the loan entity L-15 participate\nin a relationship instance of borrower. This relationship instance represents that, in the\nreal-world enterprise, the person called Hayes who holds customer-id 677-89-9011 has\ntaken the loan that is numbered L-15.\nThe function that an entity plays in a relationship is called that entity’s role. Since\nentity sets participating in a relationship set are generally distinct, roles are implicit\nand are not usually speciﬁed. However, they are useful when the meaning of a re-\nlationship needs clariﬁcation. Such is the case when the entity sets of a relationship\nset are not distinct; that is, the same entity set participates in a relationship set more\nthan once, in different roles. In this type of relationship set, sometimes called a re-\ncursive relationship set, explicit role names are necessary to specify how an entity\nparticipates in a relationship instance. For example, consider an entity set employee\nthat records information about all the employees of the bank. We may have a rela-\ntionship set works-for that is modeled by ordered pairs of employee entities. The ﬁrst\nemployee of a pair takes the role of worker, whereas the second takes the role of man-\nager. In this way, all relationships of works-for are characterized by (worker, manager)\npairs; (manager, worker) pairs are excluded.\nA relationship may also have attributes called descriptive attributes. Consider a\nrelationship set depositor with entity sets customer and account. We could associate the\nattribute access-date to that relationship to specify the most recent date on which a\ncustomer accessed an account. The depositor relationship among the entities corre-\nsponding to customer Jones and account A-217 has the value “23 May 2001” for at-\ntribute access-date, which means that the most recent date that Jones accessed account\nA-217 was 23 May 2001.\nAs another example of descriptive attributes for relationships, suppose we have\nentity sets student and course which participate in a relationship set registered-for. We\nmay wish to store a descriptive attribute for-credit with the relationship, to record\nwhether a student has taken the course for credit, or is auditing (or sitting in on) the\ncourse.\nA relationship instance in a given relationship set must be uniquely identiﬁable\nfrom its participating entities, without using the descriptive attributes. To understand\nthis point, suppose we want to model all the dates when a customer accessed an\naccount. The single-valued attribute access-date can store a single access date only . We\ncannot represent multiple access dates by multiple relationship instances between the\nsame customer and account, since the relationship instances would not be uniquely\nidentiﬁable using only the participating entities. The right way to handle this case is\nto create a multivalued attribute access-dates, which can store all the access dates.\nHowever, there can be more than one relationship set involving the same entity\nsets. In our example the customer and loan entity sets participate in the relationship\nset borrower. Additionally, suppose each loan must have another customer who serves\nas a guarantor for the loan. Then the customer and loan entity sets may participate in\nanother relationship set, guarantor.\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nI. Data Models\n2. Entity−Relationship \nModel\n42\n© The McGraw−Hill \nCompanies, 2001\n2.2\nConstraints\n33\nThe relationship sets borrower and loan-branch provide an example of a binary rela-\ntionship set—that is, one that involves two entity sets. Most of the relationship sets in\na database system are binary. Occasionally, however, relationship sets involve more\nthan two entity sets.\nAs an example, consider the entity sets employee, branch, and job. Examples of job\nentities could include manager, teller, auditor, and so on. Job entities may have the at-\ntributes title and level. The relationship set works-on among employee, branch, and job is\nan example of a ternary relationship. A ternary relationship among Jones, Perryridge,\nand manager indicates that Jones acts as a manager at the Perryridge branch. Jones\ncould also act as auditor at the Downtown branch, which would be represented by\nanother relationship. Yet another relationship could be between Smith, Downtown,\nand teller, indicating Smith acts as a teller at the Downtown branch.\nThe number of entity sets that participate in a relationship set is also the degree of\nthe relationship set. A binary relationship set is of degree 2; a ternary relationship set\nis of degree 3.\n2.2\nConstraints\nAn E-R enterprise schema may deﬁne certain constraints to which the contents of a\ndatabase must conform. In this section, we examine mapping cardinalities and par-\nticipation constraints, which are two of the most important types of constraints.\n2.2.1\nMapping Cardinalities\nMapping cardinalities, or cardinality ratios, express the number of entities to which\nanother entity can be associated via a relationship set.\nMapping cardinalities are most useful in describing binary relationship sets, al-\nthough they can contribute to the description of relationship sets that involve more\nthan two entity sets. In this section, we shall concentrate on only binary relationship\nsets.\nFor a binary relationship set R between entity sets A and B, the mapping cardinal-\nity must be one of the following:\n• One to one. An entity in A is associated with at most one entity in B, and an\nentity in B is associated with at most one entity in A. (See Figure 2.4a.)\n• One to many. An entity in A is associated with any number (zero or more) of\nentities in B. An entity in B, however, can be associated with at most one entity\nin A. (See Figure 2.4b.)\n• Many to one. An entity in A is associated with at most one entity in B. An\nentity in B, however, can be associated with any number (zero or more) of\nentities in A. (See Figure 2.5a.)\n• Many to many. An entity in A is associated with any number (zero or more) of\nentities in B, and an entity in B is associated with any number (zero or more)\nof entities in A. (See Figure 2.5b.)\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nI. Data Models\n2. Entity−Relationship \nModel\n43\n© The McGraw−Hill \nCompanies, 2001\n34\nChapter 2\nEntity-Relationship Model\na1\na2\na3\na4\nb1\nb2\nb3\nb4\n(a)\n(b)\nA                      B\na1\na2\na3\nb1\nb2\nb3\nb4\nb5\nA                      B\nFigure 2.4\nMapping cardinalities. (a) One to one. (b) One to many.\nThe appropriate mapping cardinality for a particular relationship set obviously de-\npends on the real-world situation that the relationship set is modeling.\nAs an illustration, consider the borrower relationship set. If, in a particular bank, a\nloan can belong to only one customer, and a customer can have several loans, then\nthe relationship set from customer to loan is one to many. If a loan can belong to several\ncustomers (as can loans taken jointly by several business partners), the relationship\nset is many to many. Figure 2.3 depicts this type of relationship.\n2.2.2\nParticipation Constraints\nThe participation of an entity set E in a relationship set R is said to be total if every\nentity in E participates in at least one relationship in R. If only some entities in E\nparticipate in relationships in R, the participation of entity set E in relationship R is\nsaid to be partial. For example, we expect every loan entity to be related to at least\none customer through the borrower relationship. Therefore the participation of loan in\na1\na2\na3\na4\nb1\nb2\nb3\n(a)\n(b)\nA                      B\nA                       B\na5\na1\na2\na3\na4\nb1\nb2\nb3\nb4\nFigure 2.5\nMapping cardinalities. (a) Many to one. (b) Many to many.\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nI. Data Models\n2. Entity−Relationship \nModel\n44\n© The McGraw−Hill \nCompanies, 2001\n2.3\nKeys\n35\nthe relationship set borrower is total. In contrast, an individual can be a bank customer\nwhether or not she has a loan with the bank. Hence, it is possible that only some of\nthe customer entities are related to the loan entity set through the borrower relationship,\nand the participation of customer in the borrower relationship set is therefore partial.\n2.3\nKeys\nWe must have a way to specify how entities within a given entity set are distin-\nguished. Conceptually, individual entities are distinct; from a database perspective,\nhowever, the difference among them must be expressed in terms of their attributes.\nTherefore, the values of the attribute values of an entity must be such that they can\nuniquely identify the entity. In other words, no two entities in an entity set are allowed\nto have exactly the same value for all attributes.\nA key allows us to identify a set of attributes that sufﬁce to distinguish entities\nfrom each other. Keys also help uniquely identify relationships, and thus distinguish\nrelationships from each other.\n2.3.1\nEntity Sets\nA superkey is a set of one or more attributes that, taken collectively, allow us to iden-\ntify uniquely an entity in the entity set. For example, the customer-id attribute of the\nentity set customer is sufﬁcient to distinguish one customer entity from another. Thus,\ncustomer-id is a superkey. Similarly, the combination of customer-name and customer-id\nis a superkey for the entity set customer. The customer-name attribute of customer is not\na superkey, because several people might have the same name.\nThe concept of a superkey is not sufﬁcient for our purposes, since, as we saw, a\nsuperkey may contain extraneous attributes. If K is a superkey, then so is any superset\nof K. We are often interested in superkeys for which no proper subset is a superkey.\nSuch minimal superkeys are called candidate keys.\nIt is possible that several distinct sets of attributes could serve as a candidate key.\nSuppose that a combination of customer-name and customer-street is sufﬁcient to dis-\ntinguish among members of the customer entity set. Then, both {customer-id} and\n{customer-name, customer-street} are candidate keys. Although the attributes customer-\nid and customer-name together can distinguish customer entities, their combination\ndoes not form a candidate key, since the attribute customer-id alone is a candidate\nkey.\nWe shall use the term primary key to denote a candidate key that is chosen by\nthe database designer as the principal means of identifying entities within an entity\nset. A key (primary, candidate, and super) is a property of the entity set, rather than\nof the individual entities. Any two individual entities in the set are prohibited from\nhaving the same value on the key attributes at the same time. The designation of a\nkey represents a constraint in the real-world enterprise being modeled.\nCandidate keys must be chosen with care. As we noted, the name of a person is\nobviously not sufﬁcient, because there may be many people with the same name.\nIn the United States, the social-security number attribute of a person would be a\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nI. Data Models\n2. Entity−Relationship \nModel\n45\n© The McGraw−Hill \nCompanies, 2001\n36\nChapter 2\nEntity-Relationship Model\ncandidate key. Since non-U.S. residents usually do not have social-security numbers,\ninternational enterprises must generate their own unique identiﬁers. An alternative\nis to use some unique combination of other attributes as a key.\nThe primary key should be chosen such that its attributes are never, or very rarely,\nchanged. For instance, the address ﬁeld of a person should not be part of the primary\nkey, since it is likely to change. Social-security numbers, on the other hand, are guar-\nanteed to never change. Unique identiﬁers generated by enterprises generally do not\nchange, except if two enterprises merge; in such a case the same identiﬁer may have\nbeen issued by both enterprises, and a reallocation of identiﬁers may be required to\nmake sure they are unique.\n2.3.2\nRelationship Sets\nThe primary key of an entity set allows us to distinguish among the various entities of\nthe set. We need a similar mechanism to distinguish among the various relationships\nof a relationship set.\nLet R be a relationship set involving entity sets E1, E2, . . . , En. Let primary-key(Ei)\ndenote the set of attributes that forms the primary key for entity set Ei. Assume\nfor now that the attribute names of all primary keys are unique, and each entity set\nparticipates only once in the relationship. The composition of the primary key for\na relationship set depends on the set of attributes associated with the relationship\nset R.\nIf the relationship set R has no attributes associated with it, then the set of at-\ntributes\nprimary-key(E1) ∪primary-key(E2) ∪· · · ∪primary-key(En)\ndescribes an individual relationship in set R.\nIf the relationship set R has attributes a1, a2, · · · , am associated with it, then the set\nof attributes\nprimary-key(E1) ∪primary-key(E2) ∪· · · ∪primary-key(En) ∪{a1, a2, . . . , am}\ndescribes an individual relationship in set R.\nIn both of the above cases, the set of attributes\nprimary-key(E1) ∪primary-key(E2) ∪· · · ∪primary-key(En)\nforms a superkey for the relationship set.\nIn case the attribute names of primary keys are not unique across entity sets, the\nattributes are renamed to distinguish them; the name of the entity set combined with\nthe name of the attribute would form a unique name. In case an entity set participates\nmore than once in a relationship set (as in the works-for relationship in Sectio",
    "precision": 0.05084745762711865,
    "recall": 1.0,
    "iou": 0.05084745762711865,
    "f1": 0.0967741935483871,
    "gold_tokens_count": 81,
    "retrieved_tokens_count": 1593,
    "intersection_tokens": 81
  },
  {
    "config_name": "chars_weighted",
    "config": {
      "name": "chars_weighted",
      "index_prefix": "textbook_index",
      "chunking_strategy": "chars",
      "overlap": 0,
      "fusion": "weighted",
      "bm25_weight": 0.3,
      "tag_weight": 0.2,
      "top_k": 5,
      "embed_model": "sentence-transformers/all-MiniLM-L6-v2"
    },
    "question": "Describe the circumstances in which you would choose to use embedded SQL rather than SQL alone or only a general-purpose programming language.",
    "gold_text": "4.12 Embedded SQL SQL provides a powerful declarative query language. Writing queries in SQL is usu- ally much easier than coding the same queries in a general-purpose programming language. However, a programmer must have access to a database from a general- purpose programming language for at least two reasons: 1. Not all queries can be expressed in SQL, since SQL does not provide the full expressive power of a general-purpose language. That is, there exist queries that can be expressed in a language such as C, Java, or Cobol that cannot be expressed in SQL. To write such queries, we can embed SQL within a more powerful language. SQL is designed so that queries written in it can be optimized automatically and executed efficiently—and providing the full power of a programming language makes automatic optimization exceedingly difficult. 2. Nondeclarative actions—such as printing a report, interacting with a user, or sending the results of a query to a graphical user interface—cannot be done from within SQL. Applications usually have several components, and query- ing or updating data is only one component; other components are written in general-purpose programming languages. For an integrated application, the programs written in the programming language must be able to access the database. The SQL standard defines embeddings of SQL in a variety of programming lan- guages, such as C, Cobol, Pascal, Java, PL/I, and Fortran. A language in which SQL queries are embedded is referred to as a host language, and the SQL structures per- mitted in the host language constitute embedded SQL. Programs written in the host language can use the embedded SQL syntax to ac- cess and update data stored in a database. This embedded form of SQL extends the programmer’s ability to manipulate the database even further. In embedded SQL, all query processing is performed by the database system, which then makes the result of the query available to the program one tuple (record) at a time. An embedded SQL program must be processed by a special preprocessor prior to compilation. The preprocessor replaces embedded SQL requests with host-language declarations and procedure calls that allow run-time execution of the database ac- cesses. Then, the resulting program is compiled by the host-language compiler. To identify embedded SQL requests to the preprocessor, we use the EXEC SQL statement; it has the form EXEC SQL <embedded SQL statement > END-EXEC The exact syntax for embedded SQL requests depends on the language in which SQL is embedded. For instance, a semicolon is used instead of END-EXEC when SQL is embedded in C. The Java embedding of SQL (called SQLJ) uses the syntax # SQL { <embedded SQL statement > }; We place the statement SQL INCLUDE in the program to identify the place where the preprocessor should insert the special variables used for communication between the program and the database system. Variables of the host language can be used within embedded SQL statements, but they must be preceded by a colon (:) to distin- guish them from SQL variables. Embedded SQL statements are similar in form to the SQL statements that we de- scribed in this chapter. There are, however, several important differences, as we note here. To write a relational query, we use the declare cursor statement. The result of the query is not yet computed. Rather, the program must use the open and fetch com- mands (discussed later in this section) to obtain the result tuples. Consider the banking schema that we have used in this chapter. Assume that we have a host-language variable amount, and that we wish to find the names and cities of residence of customers who have more than amount dollars in any account. We can write this query as follows: EXEC SQL declare c cursor for select customer-name, customer-city from depositor, customer, account where depositor.customer-name= customer.customer-name and account.account-number= depositor.account-number and account.balance > :amount END-EXEC The variable c in the preceding expression is called a cursor for the query. We use this variable to identify the query in the open statement, which causes the query to be evaluated, and in the fetch statement, which causes the values of one tuple to be placed in host-language variables. The open statement for our sample query is as follows: EXEC SQL open c END-EXEC This statement causes the database system to execute the query and to save the results within a temporary relation. The query has a host-language variable (:amount); the query uses the value of the variable at the time the open statement was executed. If the SQL query results in an error, the database system stores an error diagnostic in the SQL communication-area (SQLCA) variables, whose declarations are inserted by the SQL INCLUDE statement. An embedded SQL program executes a series of fetch statements to retrieve tuples of the result. The fetch statement requires one host-language variable for each at- tribute of the result relation. For our example query, we need one variable to hold the customer-name value and another to hold the customer-city value. Suppose that those variables are cn and cc, respectively. Then the statement: EXEC SQL fetch c into :cn, :cc END-EXEC produces a tuple of the result relation. The program can then manipulate the vari- ables cn and cc by using the features of the host programming language. A single fetch request returns only one tuple. To obtain all tuples of the result, the program must contain a loop to iterate over all tuples. Embedded SQL assists the programmer in managing this iteration. Although a relation is conceptually a set, the tuples of the result of a query are in some fixed physical order. When the program executes an open statement on a cursor, the cursor is set to point to the first tuple of the result. Each time it executes a fetch statement, the cursor is updated to point to the next tuple of the result. When no further tuples remain to be processed, the variable SQLSTATE in the SQLCA is set to ’02000’ (meaning“no data”). Thus, we can use a while loop (or equivalent loop) to process each tuple of the result. We must use the close statement to tell the database system to delete the tempo- rary relation that held the result of the query. For our example, this statement takes the form EXEC SQL close c END-EXEC SQLJ, the Java embedding of SQL, provides a variation of the above scheme, where Java iterators are used in place of cursors. SQLJ associates the results of a query with an iterator, and the next() method of the Java iterator interface can be used to step through the result tuples, just as the preceding examples use fetch on the cursor. Embedded SQL expressions for database modification (update, insert, and delete) do not return a result. Thus, they are somewhat simpler to express. A database- modification request takes the form EXEC SQL < any valid update, insert, or delete> END-EXEC Host-language variables, preceded by a colon, may appear in the SQL database- modification expression. If an error condition arises in the execution of the statement, a diagnostic is set in the SQLCA. Database relations can also be updated through cursors. For example, if we want to add 100 to the balance attribute of every account where the branch name is“Per- ryridge”, we could declare a cursor as follows. declare c cursor for select * from account where branch-name = ‘Perryridge‘ for update We then iterate through the tuples by performing fetch operations on the cursor (as illustrated earlier), and after fetching each tuple we execute the following code update account set balance= balance + 100 where current of c Embedded SQL allows a host-language program to access the database, but it pro- vides no assistance in presenting results to the user or in generating reports. Most commercial database products include tools to assist application programmers in creating user interfaces and formatted reports. ",
    "retrieved_text": "3\nIf the above query were run on the tables in Figure 1.3, the system would ﬁnd that\nthe two accounts numbered A-101 and A-201 are owned by customer 192-83-7465\nand would print out the balances of the two accounts, namely 500 and 900.\nThere are a number of database query languages in use, either commercially or\nexperimentally. We study the most widely used query language, SQL, in Chapter 4.\nWe also study some other query languages in Chapter 5.\nThe levels of abstraction that we discussed in Section 1.3 apply not only to deﬁning\nor structuring data, but also to manipulating data. At the physical level, we must\ndeﬁne algorithms that allow efﬁcient access to data. At higher levels of abstraction,\nwe emphasize ease of use. The goal is to allow humans to interact efﬁciently with the\nsystem. The query processor component of the database system (which we study in\nChapters 13 and 14) translates DML queries into sequences of actions at the physical\nlevel of the database system.\n1.5.3\nDatabase Access from Application Programs\nApplication programs are programs that are used to interact with the database. Ap-\nplication programs are usually written in a host language, such as Cobol, C, C++, or\nJava. Examples in a banking system are programs that generate payroll checks, debit\naccounts, credit accounts, or transfer funds between accounts.\nTo access the database, DML statements need to be executed from the host lan-\nguage. There are two ways to do this:\n• By providing an application program interface (set of procedures) that can\nbe used to send DML and DDL statements to the database, and retrieve the\nresults.\nThe Open Database Connectivity (ODBC) standard deﬁned by Microsoft\nfor use with the C language is a commonly used application program inter-\nface standard. The Java Database Connectivity (JDBC) standard provides cor-\nresponding features to the Java language.\n• By extending the host language syntax to embed DML calls within the host\nlanguage program. Usually, a special character prefaces DML calls, and a pre-\nprocessor, called the DML precompiler, converts the DML statements to nor-\nmal procedure calls in the host language.\n1.6\nDatabase Users and Administrators\nA primary goal of a database system is to retrieve information from and store new\ninformation in the database. People who work with a database can be categorized as\ndatabase users or database administrators.\n1.6.1\nDatabase Users and User Interfaces\nThere are four different types of database-system users, differentiated by the way\nthey expect to interact with the system. Different types of user interfaces have been\ndesigned for the different types of users.\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\n1. Introduction\nText\n24\n© The McGraw−Hill \nCompanies, 2001\n14\nChapter 1\nIntroduction\n• Naive users are unsophisticated users who interact with the system by invok-\ning one of the application programs that have been written previously. For\nexample, a bank teller who needs to transfer $50 from account A to account B\ninvokes a program called transfer. This program asks the teller for the amount\nof money to be transferred, the account from which the money is to be trans-\nferred, and the account to which the money is to be transferred.\nAs another example, consider a user who wishes to ﬁnd her account bal-\nance over the World Wide Web. Such a user may access a form, where she\nenters her account number. An application program at the Web server then\nretrieves the account balance, using the given account number, and passes\nthis information back to the user.\nThe typical user interface for naive users is a forms interface, where the\nuser can ﬁll in appropriate ﬁelds of the form. Naive users may also simply\nread reports generated from the database.\n• Application programmers are computer professionals who write application\nprograms. Application programmers can choose from many tools to develop\nuser interfaces. Rapid application development (RAD) tools are tools that en-\nable an application programmer to construct forms and reports without writ-\ning a program. There are also special types of programming languages that\ncombine imperative control structures (for example, for loops, while loops\nand if-then-else statements) with statements of the data manipulation lan-\nguage. These languages, sometimes called fourth-generation languages, often\ninclude special features to facilitate the generation of forms and the display of\ndata on the screen. Most major commercial database systems include a fourth-\ngeneration language.\n• Sophisticated users interact with the system without writing programs. In-\nstead, they form their requests in a database query language. They submit\neach such query to a query processor, whose function is to break down DML\nstatements into instructions that the storage manager understands. Analysts\nwho submit queries to explore data in the database fall in this category.\nOnline analytical processing (OLAP) tools simplify analysts’ tasks by let-\nting them view summaries of data in different ways. For instance, an analyst\ncan see total sales by region (for example, North, South, East, and West), or by\nproduct, or by a combination of region and product (that is, total sales of each\nproduct in each region). The tools also permit the analyst to select speciﬁc re-\ngions, look at data in more detail (for example, sales by city within a region)\nor look at the data in less detail (for example, aggregate products together by\ncategory).\nAnother class of tools for analysts is data mining tools, which help them\nﬁnd certain kinds of patterns in data.\nWe study OLAP tools and data mining in Chapter 22.\n• Specialized users are sophisticated users who write specialized database\napplications that do not ﬁt into the traditional data-processing framework.\nAmong these applications are computer-aided design systems, knowledge-\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\n1. Introduction\nText\n25\n© The McGraw−Hill \nCompanies, 2001\n1.7\nTransaction Management\n15\nbase and expert systems, systems that store data with complex data types (for\nexample, graphics data and audio data), and environment-modeling systems.\nChapters 8 and 9 cover several of these applications.\n1.6.2\nDatabase Administrator\nOne of the main reasons for using DBMSs is to have central control of both the data\nand the programs that access those data. A person who has such central control over\nthe system is called a database administrator (DBA). The functions of a DBA include:\n• Schema deﬁnition. The DBA creates the original database schema by execut-\ning a set of data deﬁnition statements in the DDL.\n• Storage structure and access-method deﬁnition.\n• Schema and physical-organization modiﬁcation. The DBA carries out chang-\nes to the schema and physical organization to reﬂect the changing needs of the\norganization, or to alter the physical organization to improve performance.\n• Granting of authorization for data access. By granting different types of\nauthorization, the database administrator can regulate which parts of the data-\nbase various users can access. The authorization information is kept in a\nspecial system structure that the database system consults whenever some-\none attempts to access the data in the system.\n• Routine maintenance. Examples of the database administrator’s routine\nmaintenance activities are:\n\u0000 Periodically backing up the database, either onto tapes or onto remote\nservers, to prevent loss of data in case of disasters such as ﬂooding.\n\u0000 Ensuring that enough free disk space is available for normal operations,\nand upgrading disk space as required.\n\u0000 Monitoring jobs running on the database and ensuring that performance\nis not degraded by very expensive tasks submitted by some users.\n1.7\nTransaction Management\nOften, several operations on the database form a single logical unit of work. An ex-\nample is a funds transfer, as in Section 1.2, in which one account (say A) is debited and\nanother account (say B) is credited. Clearly, it is essential that either both the credit\nand debit occur, or that neither occur. That is, the funds transfer must happen in its\nentirety or not at all. This all-or-none requirement is called atomicity. In addition, it\nis essential that the execution of the funds transfer preserve the consistency of the\ndatabase. That is, the value of the sum A + B must be preserved. This correctness\nrequirement is called consistency. Finally, after the successful execution of a funds\ntransfer, the new values of accounts A and B must persist, despite the possibility of\nsystem failure. This persistence requirement is called durability.\nA transaction is a collection of operations that performs a single logical function\nin a database application. Each transaction is a unit of both atomicity and consis-\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\n1. Introduction\nText\n26\n© The McGraw−Hill \nCompanies, 2001\n16\nChapter 1\nIntroduction\ntency. Thus, we require that transactions do not violate any database-consistency\nconstraints. That is, if the database was consistent when a transaction started, the\ndatabase must be consistent when the transaction successfully terminates. However,\nduring the execution of a transaction, it may be necessary temporarily to allow incon-\nsistency, since either the debit of A or the credit of B must be done before the other.\nThis temporary inconsistency, although necessary, may lead to difﬁculty if a failure\noccurs.\nIt is the programmer’s responsibility to deﬁne properly the various transactions,\nso that each preserves the consistency of the database. For example, the transaction to\ntransfer funds from account A to account B could be deﬁned to be composed of two\nseparate programs: one that debits account A, and another that credits account B. The\nexecution of these two programs one after the other will indeed preserve consistency.\nHowever, each program by itself does not transform the database from a consistent\nstate to a new consistent state. Thus, those programs are not transactions.\nEnsuring the atomicity and durability properties is the responsibility of the data-\nbase system itself—speciﬁcally, of the transaction-management component. In the\nabsence of failures, all transactions complete successfully, and atomicity is achieved\neasily. However, because of various types of failure, a transaction may not always\ncomplete its execution successfully. If we are to ensure the atomicity property, a failed\ntransaction must have no effect on the state of the database. Thus, the database must\nbe restored to the state in which it was before the transaction in question started exe-\ncuting. The database system must therefore perform failure recovery, that is, detect\nsystem failures and restore the database to the state that existed prior to the occur-\nrence of the failure.\nFinally, when several transactions update the database concurrently, the consis-\ntency of data may no longer be preserved, even though each individual transac-\ntion is correct. It is the responsibility of the concurrency-control manager to control\nthe interaction among the concurrent transactions, to ensure the consistency of the\ndatabase.\nDatabase systems designed for use on small personal computers may not have\nall these features. For example, many small systems allow only one user to access\nthe database at a time. Others do not offer backup and recovery, leaving that to the\nuser. These restrictions allow for a smaller data manager, with fewer requirements for\nphysical resources—especially main memory. Although such a low-cost, low-feature\napproach is adequate for small personal databases, it is inadequate for a medium- to\nlarge-scale enterprise.\n1.8\nDatabase System Structure\nA database system is partitioned into modules that deal with each of the responsi-\nbilites of the overall system. The functional components of a database system can be\nbroadly divided into the storage manager and the query processor components.\nThe storage manager is important because databases typically require a large\namount of storage space. Corporate databases range in size from hundreds of gi-\ngabytes to, for the largest databases, terabytes of data. A gigabyte is 1000 megabytes\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\n1. Introduction\nText\n27\n© The McGraw−Hill \nCompanies, 2001\n1.8\nDatabase System Structure\n17\n(1 billion bytes), and a terabyte is 1 million megabytes (1 trillion bytes). Since the\nmain memory of computers cannot store this much information, the information is\nstored on disks. Data are moved between disk storage and main memory as needed.\nSince the movement of data to and from disk is slow relative to the speed of the cen-\ntral processing unit, it is imperative that the database system structure the data so as\nto minimize the need to move data between disk and main memory.\nThe query processor is important because it helps the database system simplify\nand facilitate access to data. High-level views help to achieve this goal; with them,\nusers of the system are not be burdened unnecessarily with the physical details of the\nimplementation of the system. However, quick processing of updates and queries\nis important. It is the job of the database system to translate updates and queries\nwritten in a nonprocedural language, at the logical level, into an efﬁcient sequence of\noperations at the physical level.\n1.8.1\nStorage Manager\nA storage manager is a program module that provides the interface between the low-\nlevel data stored in the database and the application programs and queries submit-\nted to the system. The storage manager is responsible for the interaction with the ﬁle\nmanager. The raw data are stored on the disk using the ﬁle system, which is usu-\nally provided by a conventional operating system. The storage manager translates\nthe various DML statements into low-level ﬁle-system commands. Thus, the storage\nmanager is responsible for storing, retrieving, and updating data in the database.\nThe storage manager components include:\n• Authorization and integrity manager, which tests for the satisfaction of in-\ntegrity constraints and checks the authority of users to access data.\n• Transaction manager, which ensures that the database remains in a consistent\n(correct) state despite system failures, and that concurrent transaction execu-\ntions proceed without conﬂicting.\n• File manager, which manages the allocation of space on disk storage and the\ndata structures used to represent information stored on disk.\n• Buffer manager, which is responsible for fetching data from disk storage into\nmain memory, and deciding what data to cache in main memory. The buffer\nmanager is a critical part of the database system, since it enables the database\nto handle data sizes that are much larger than the size of main memory.\nThe storage manager implements several data structures as part of the physical\nsystem implementation:\n• Data ﬁles, which store the database itself.\n• Data dictionary, which stores metadata about the structure of the database, in\nparticular the schema of the database.\n• Indices, which provide fast access to data items that hold particular values.\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\n1. Introduction\nText\n28\n© The McGraw−Hill \nCompanies, 2001\n18\nChapter 1\nIntroduction\n1.8.2\nThe Query Processor\nThe query processor components include\n• DDL interpreter, which interprets DDL statements and records the deﬁnitions\nin the data dictionary.\n• DML compiler, which translates DML statements in a query language into an\nevaluation plan consisting of low-level instructions that the query evaluation\nengine understands.\nA query can usually be translated into any of a number of alternative eval-\nuation plans that all give the same result. The DML compiler also performs\nquery optimization, that is, it picks the lowest cost evaluation plan from amo-\nng the alternatives.\n• Query evaluation engine, which executes low-level instructions generated by\nthe DML compiler.\nFigure 1.4 shows these components and the connections among them.\n1.9\nApplication Architectures\nMost users of a database system today are not present at the site of the database\nsystem, but connect to it through a network. We can therefore differentiate between\nclient machines, on which remote database users work, and server machines, on\nwhich the database system runs.\nDatabase applications are usually partitioned into two or three parts, as in Fig-\nure 1.5. In a two-tier architecture, the application is partitioned into a component\nthat resides at the client machine, which invokes database system functionality at the\nserver machine through query language statements. Application program interface\nstandards like ODBC and JDBC are used for interaction between the client and the\nserver.\nIn contrast, in a three-tier architecture, the client machine acts as merely a front\nend and does not contain any direct database calls. Instead, the client end communi-\ncates with an application server, usually through a forms interface. The application\nserver in turn communicates with a database system to access data. The business\nlogic of the application, which says what actions to carry out under what conditions,\nis embedded in the application server, instead of being distributed across multiple\nclients. Three-tier applications are more appropriate for large applications, and for\napplications that run on the World Wide Web.\n1.10\nHistory of Database Systems\nData processing drives the growth of computers, as it has from the earliest days of\ncommercial computers. In fact, automation of data processing tasks predates com-\nputers. Punched cards, invented by Hollerith, were used at the very beginning of the\ntwentieth century to record U.S. census data, and mechanical systems were used to\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\n1. Introduction\nText\n29\n© The McGraw−Hill \nCompanies, 2001\n1.10\nHistory of Database Systems\n19\nnaive users\n(tellers, agents, \nweb-users) \nquery processor\nstorage manager\ndisk storage\nindices\nstatistical data\ndata\ndata dictionary\napplication\nprogrammers\napplication\ninterfaces\napplication\nprogram\nobject code\ncompiler and\nlinker\nbuffer manager\nfile manager\nauthorization\nand integrity\n manager\ntransaction\nmanager\nDML compiler \nand organizer\nquery evaluation\nengine\nDML queries\nDDL interpreter\napplication\nprograms\nquery\ntools\nadministration\ntools\nsophisticated\nusers\n(analysts)\ndatabase\nadministrator\nuse\nwrite\nuse\nuse\nFigure 1.4\nSystem structure.\nprocess the cards and tabulate results. Punched cards were later widely used as a\nmeans of entering data into computers.\nTechniques for data storage and processing have evolved over the years:\n• 1950s and early 1960s: Magnetic tapes were developed for data storage. Data\nprocessing tasks such as payroll were automated, with data stored on tapes.\nProcessing of data consisted of reading data from one or more tapes and\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\n1. Introduction\nText\n30\n© The McGraw−Hill \nCompanies, 2001\n20\nChapter 1\nIntroduction\nuser\napplication server\ndatabase system\nuser\ndatabase system\nb.  three-tier architecture\na.  two-tier architecture\nnetwork\nserver\nclient\napplication\nnetwork\napplication client\nFigure 1.5\nTwo-tier and three-tier architectures.\nwriting data to a new tape. Data could also be input from punched card decks,\nand output to printers. For example, salary raises were processed by entering\nthe raises on punched cards and reading the punched card deck in synchro-\nnization with a tape containing the master salary details. The records had to\nbe in the same sorted order. The salary raises would be added to the salary\nread from the master tape, and written to a new tape; the new tape would\nbecome the new master tape.\nTapes (and card decks) could be read only sequentially, and data sizes were\nmuch larger than main memory; thus, data process\n\nfound at the end of that chapter.\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nII. Relational Databases\nIntroduction\n140\n© The McGraw−Hill \nCompanies, 2001\nP\nA\nR\nT\n2\nRelational Databases\nA relational database is a shared repository of data. To make data from a relational\ndatabase available to users, we have to address several issues. One is how users spec-\nify requests for data: Which of the various query languages do they use? Chapter 4\ncovers the SQL language, which is the most widely used query language today. Chap-\nter 5 covers two other query languages, QBE and Datalog, which offer alternative\napproaches to querying relational data.\nAnother issue is data integrity and security; databases need to protect data from\ndamage by user actions, whether unintentional or intentional. The integrity main-\ntenance component of a database ensures that updates do not violate integrity con-\nstraints that have been speciﬁed on the data. The security component of a database\nincludes authentication of users, and access control, to restrict the permissible actions\nfor each user. Chapter 6 covers integrity and security issues. Security and integrity\nissues are present regardless of the data model, but for concreteness we study them\nin the context of the relational model. Integrity constraints form the basis of relational\ndatabase design, which we study in Chapter 7.\nRelational database design—the design of the relational schema—is the ﬁrst step\nin building a database application. Schema design was covered informally in ear-\nlier chapters. There are, however, principles that can be used to distinguish good\ndatabase designs from bad ones. These are formalized by means of several “normal\nforms,” which offer different tradeoffs between the possibility of inconsistencies and\nthe efﬁciency of certain queries. Chapter 7 describes the formal design of relational\nschemas.\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nII. Relational Databases\n4. SQL\n141\n© The McGraw−Hill \nCompanies, 2001\nC\nH\nA\nP\nT\nE\nR\n4\nSQL\nThe formal languages described in Chapter 3 provide a concise notation for repre-\nsenting queries. However, commercial database systems require a query language\nthat is more user friendly. In this chapter, we study SQL, the most inﬂuential commer-\ncially marketed query language, SQL. SQL uses a combination of relational-algebra\nand relational-calculus constructs.\nAlthough we refer to the SQL language as a “query language,” it can do much\nmore than just query a database. It can deﬁne the structure of the data, modify data\nin the database, and specify security constraints.\nIt is not our intention to provide a complete users’ guide for SQL. Rather, we\npresent SQL’s fundamental constructs and concepts. Individual implementations of\nSQL may differ in details, or may support only a subset of the full language.\n4.1\nBackground\nIBM developed the original version of SQL at its San Jose Research Laboratory (now\nthe Almaden Research Center). IBM implemented the language, originally called Se-\nquel, as part of the System R project in the early 1970s. The Sequel language has\nevolved since then, and its name has changed to SQL (Structured Query Language).\nMany products now support the SQL language. SQL has clearly established itself as\nthe standard relational-database language.\nIn 1986, the American National Standards Institute (ANSI) and the International\nOrganization for Standardization (ISO) published an SQL standard, called SQL-86.\nIBM published its own corporate SQL standard, the Systems Application Architec-\nture Database Interface (SAA-SQL) in 1987. ANSI published an extended standard for\nSQL, SQL-89, in 1989. The next version of the standard was SQL-92 standard, and the\nmost recent version is SQL:1999. The bibliographic notes provide references to these\nstandards.\n135\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nII. Relational Databases\n4. SQL\n142\n© The McGraw−Hill \nCompanies, 2001\n136\nChapter 4\nSQL\nIn this chapter, we present a survey of SQL, based mainly on the widely imple-\nmented SQL-92 standard. The SQL:1999 standard is a superset of the SQL-92 standard;\nwe cover some features of SQL:1999 in this chapter, and provide more detailed cov-\nerage in Chapter 9. Many database systems support some of the new constructs in\nSQL:1999, although currently no database system supports all the new constructs. You\nshould also be aware that some database systems do not even support all the fea-\ntures of SQL-92, and that many databases provide nonstandard features that we do\nnot cover here.\nThe SQL language has several parts:\n• Data-deﬁnition language (DDL). The SQL DDL provides commands for deﬁn-\ning relation schemas, deleting relations, and modifying relation schemas.\n• Interactive data-manipulation language (DML). The SQL DML includes a\nquery language based on both the relational algebra and the tuple relational\ncalculus. It includes also commands to insert tuples into, delete tuples from,\nand modify tuples in the database.\n• View deﬁnition. The SQL DDL includes commands for deﬁning views.\n• Transaction control. SQL includes commands for specifying the beginning\nand ending of transactions.\n• Embedded SQL and dynamic SQL. Embedded and dynamic SQL deﬁne how\nSQL statements can be embedded within general-purpose programming lan-\nguages, such as C, C++, Java, PL/I, Cobol, Pascal, and Fortran.\n• Integrity. The SQL DDL includes commands for specifying integrity constraints\nthat the data stored in the database must satisfy. Updates that violate integrity\nconstraints are disallowed.\n• Authorization. The SQL DDL includes commands for specifying access rights\nto relations and views.\nIn this chapter, we cover the DML and the basic DDL features of SQL. We also\nbrieﬂy outline embedded and dynamic SQL, including the ODBC and JDBC standards\nfor interacting with a database from programs written in the C and Java languages.\nSQL features supporting integrity and authorization are described in Chapter 6, while\nChapter 9 outlines object-oriented extensions to SQL.\nThe enterprise that we use in the examples in this chapter, and later chapters, is a\nbanking enterprise with the following relation schemas:\nBranch-schema = (branch-name, branch-city, assets)\nCustomer-schema = (customer-name, customer-street, customer-city)\nLoan-schema = (loan-number, branch-name, amount)\nBorrower-schema = (customer-name, loan-number)\nAccount-schema = (account-number, branch-name, balance)\nDepositor-schema = (customer-name, account-number)\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nII. Relational Databases\n4. SQL\n143\n© The McGraw−Hill \nCompanies, 2001\n4.2\nBasic Structure\n137\nNote that in this chapter, as elsewhere in the text, we use hyphenated names for\nschema, relations, and attributes for ease of reading. In actual SQL systems, however,\nhyphens are not valid parts of a name (they are treated as the minus operator). A\nsimple way of translating the names we use to valid SQL names is to replace all hy-\nphens by the underscore symbol (“ ”). For example, we use branch name in place of\nbranch-name.\n4.2\nBasic Structure\nA relational database consists of a collection of relations, each of which is assigned\na unique name. Each relation has a structure similar to that presented in Chapter 3.\nSQL allows the use of null values to indicate that the value either is unknown or does\nnot exist. It allows a user to specify which attributes cannot be assigned null values,\nas we shall discuss in Section 4.11.\nThe basic structure of an SQL expression consists of three clauses: select, from, and\nwhere.\n• The select clause corresponds to the projection operation of the relational al-\ngebra. It is used to list the attributes desired in the result of a query.\n• The from clause corresponds to the Cartesian-product operation of the rela-\ntional algebra. It lists the relations to be scanned in the evaluation of the ex-\npression.\n• The where clause corresponds to the selection predicate of the relational alge-\nbra. It consists of a predicate involving attributes of the relations that appear\nin the from clause.\nThat the term select has different meaning in SQL than in the relational algebra is an\nunfortunate historical fact. We emphasize the different interpretations here to mini-\nmize potential confusion.\nA typical SQL query has the form\nselect A1, A2, . . . , An\nfrom r1, r2, . . . , rm\nwhere P\nEach Ai represents an attribute, and each ri a relation. P is a predicate. The query is\nequivalent to the relational-algebra expression\nΠA1, A2,...,An(σP (r1 × r2 × · · · × rm))\nIf the where clause is omitted, the predicate P is true. However, unlike the result of a\nrelational-algebra expression, the result of the SQL query may contain multiple copies\nof some tuples; we shall return to this issue in Section 4.2.8.\nSQL forms the Cartesian product of the relations named in the from clause,\nperforms a relational-algebra selection using the where clause predicate, and then\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nII. Relational Databases\n4. SQL\n144\n© The McGraw−Hill \nCompanies, 2001\n138\nChapter 4\nSQL\nprojects the result onto the attributes of the select clause. In practice, SQL may con-\nvert the expression into an equivalent form that can be processed more efﬁciently.\nHowever, we shall defer concerns about efﬁciency to Chapters 13 and 14.\n4.2.1\nThe select Clause\nThe result of an SQL query is, of course, a relation. Let us consider a simple query\nusing our banking example, “Find the names of all branches in the loan relation”:\nselect branch-name\nfrom loan\nThe result is a relation consisting of a single attribute with the heading branch-name.\nFormal query languages are based on the mathematical notion of a relation being\na set. Thus, duplicate tuples never appear in relations. In practice, duplicate elimina-\ntion is time-consuming. Therefore, SQL (like most other commercial query languages)\nallows duplicates in relations as well as in the results of SQL expressions. Thus, the\npreceding query will list each branch-name once for every tuple in which it appears in\nthe loan relation.\nIn those cases where we want to force the elimination of duplicates, we insert the\nkeyword distinct after select. We can rewrite the preceding query as\nselect distinct branch-name\nfrom loan\nif we want duplicates removed.\nSQL allows us to use the keyword all to specify explicitly that duplicates are not\nremoved:\nselect all branch-name\nfrom loan\nSince duplicate retention is the default, we will not use all in our examples. To ensure\nthe elimination of duplicates in the results of our example queries, we will use dis-\ntinct whenever it is necessary. In most queries where distinct is not used, the exact\nnumber of duplicate copies of each tuple present in the query result is not important.\nHowever, the number is important in certain applications; we return to this issue in\nSection 4.2.8.\nThe asterisk symbol “ * ” can be used to denote “all attributes.” Thus, the use of\nloan.* in the preceding select clause would indicate that all attributes of loan are to be\nselected. A select clause of the form select * indicates that all attributes of all relations\nappearing in the from clause are selected.\nThe select clause may also contain arithmetic expressions involving the operators\n+, −, ∗, and / operating on constants or attributes of tuples. For example, the query\nselect loan-number, branch-name, amount * 100\nfrom loan\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nII. Relational Databases\n4. SQL\n145\n© The McGraw−Hill \nCompanies, 2001\n4.2\nBasic Structure\n139\nwill return a relation that is the same as the loan relation, except that the attribute\namount is multiplied by 100.\nSQL also provides special data types, such as various forms of the date type, and\nallows several arithmetic functions to operate on these types.\n4.2.2\nThe where Clause\nLet us illustrate the use of the where clause in SQL. Consider the query “Find all loan\nnumbers for loans made at the Perryridge branch with loan amounts greater that\n$1200.” This query can be written in SQL as:\nselect loan-number\nfrom loan\nwhere branch-name = ’Perryridge’ and amount > 1200\nSQL uses the logical connectives and, or, and not—rather than the mathematical\nsymbols ∧, ∨, and ¬ —in the where clause. The operands of the logical connectives\ncan be expressions involving the comparison operators <, <=, >, >=, =, and <>.\nSQL allows us to use the comparison operators to compare strings and arithmetic\nexpressions, as well as special types, such as date types.\nSQL includes a between comparison operator to simplify where clauses that spec-\nify that a value be less than or equal to some value and greater than or equal to some\nother value. If we wish to ﬁnd the loan number of those loans with loan amounts\nbetween $90,000 and $100,000, we can use the between comparison to write\nselect loan-number\nfrom loan\nwhere amount between 90000 and 100000\ninstead of\nselect loan-number\nfrom loan\nwhere amount <= 100000 and amount >= 90000\nSimilarly, we can use the not between comparison operator.\n4.2.3\nThe from Clause\nFinally, let us discuss the use of the from clause. The from clause by itself deﬁnes a\nCartesian product of the relations in the clause. Since the natural join is deﬁned in\nterms of a Cartesian product, a selection, and a projection, it is a relatively simple\nmatter to write an SQL expression for the natural join.\nWe write the relational-algebra expression\nΠcustomer-name, loan-number, amount (borrower\n\u0001 loan)\nfor the query “For all customers who have a loan from the bank, ﬁnd their names,\nloan numbers and loan amount.” In SQL, this query can be written as\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nII. Relational Databases\n4. SQL\n146\n© The McGraw−Hill \nCompanies, 2001\n140\nChapter 4\nSQL\nselect customer-name, borrower.loan-number, amount\nfrom borrower, loan\nwhere borrower.loan-number = loan.loan-number\nNotice that SQL uses the notation relation-name.attribute-name, as does the relational\nalgebra, to avoid ambiguity in cases where an attribute appears in the schema of more\nthan one relation. We could have written borrower.customer-name instead of customer-\nname in the select clause. However, since the attribute customer-name appears in only\none of the relations named in the from clause, there is no ambiguity when we write\ncustomer-name.\nWe can extend the preceding query and consider a more complicated case in which\nwe require also that the loan be from the Perryridge branch: “Find the customer\nnames, loan numbers, and loan amounts for all loans at the Perryridge branch.” To\nwrite this query, we need to state two constraints in the where clause, connected by\nthe logical connective and:\nselect customer-name, borrower.loan-number, amount\nfrom borrower, loan\nwhere borrower.loan-number = loan.loan-number and\nbranch-name = ’Perryridge’\nSQL includes extensions to perform natural joins and outer joins in the from clause.\nWe discuss these extensions in Section 4.10.\n4.2.4\nThe Rename Operation\nSQL provides a mechanism for renaming both relations and attributes. It uses the as\nclause, taking the form:\nold-name as new-name\nThe as clause can appear in both the select and from clauses.\nConsider again the query that we used earlier:\nselect customer-name, borrower.loan-number, amount\nfrom borrower, loan\nwhere borrower.loan-number = loan.loan-number\nThe result of this query is a relation with the following attributes:\ncustomer-name, loan-number, amount.\nThe names of the attributes in the result are derived from the names of the attributes\nin the relations in the from clause.\nWe cannot, however, always derive names in this way, for several reasons: First,\ntwo relations in the from clause may have attributes with the same name, in which\ncase an attribute name is duplicated in the result. Second, if we used an arithmetic\nexpression in the select clause, the resultant attribute does not have a name. Third,\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nII. Relational Databases\n4. SQL\n147\n© The McGraw−Hill \nCompanies, 2001\n4.2\nBasic Structure\n141\neven if an attribute name can be derived from the base relations as in the preced-\ning example, we may want to change the attribute name in the result. Hence, SQL\nprovides a way of renaming the attributes of a result relation.\nFor example, if we want the attribute name loan-number to be replaced with the\nname loan-id, we can rewrite the preceding query as\nselect customer-name, borrower.loan-number as loan-id, amount\nfrom borrower, loan\nwhere borrower.loan-number = loan.loan-number\n4.2.5\nTuple Variables\nThe as clause is particularly useful in deﬁning the notion of tuple variables, as is\ndone in the tuple relational calculus. A tuple variable in SQL must be associated with\na particular relation. Tuple variables are deﬁned in the from clause by way of the as\nclause. To illustrate, we rewrite the query “For all customers who have a loan from\nthe bank, ﬁnd their names, loan numbers, and loan amount” as\nselect customer-name, T.loan-number, S.amount\nfrom borrower as T, loan as S\nwhere T.loan-number = S.loan-number\nNote that we deﬁne a tuple variable in the from clause by placing it after the name of\nthe relation with which it is associated, with the keyword as in between (the keyword\nas is optional). When we write expressions of the form relation-name.attribute-name,\nthe relation name is, in effect, an implicitly deﬁned tuple variable.\nTuple variables are most useful for comparing two tuples in the same relation.\nRecall that, in such cases, we could use the rename operation in the relational algebra.\nSuppose that we want the query “Find the names of all branches that have assets\ngreater than at least one branch located in Brooklyn.” We can write the SQL expression\nselect distinct T.branch-name\nfrom branch as T, branch as S\nwhere T.assets > S.assets and S.branch-city = ’Brooklyn’\nObserve that we could not use the notation branch.asset, since it would not be clear\nwhich reference to branch is intended.\nSQL permits us to use the notation (v1, v2, . . . , vn) to denote a tuple of arity n con-\ntaining values v1, v2, . . . , vn. The comparison operators can be used on tuples, and\nthe ordering is deﬁned lexicographically. For example, (a1, a2) <= (b1, b2) is true if\na1 < b1, or (a1 = b1) ∧(a2 <= b2); similarly, the two tuples are equal if all their\nattributes are equal.\n4.2.6\nString Operations\nSQL speciﬁes strings by enclosing them in single quotes, for example, ’Perryridge’,\nas we saw earlier. A single quote character that is part of a string can be speciﬁed by\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nII. Relational Databases\n4. SQL\n148\n© The McGraw−Hill \nCompanies, 2001\n142\nChapter 4\nSQL\nusing two single quote characters; for example the string “It’s right” can be speciﬁed\nby ’It”s right’.\nThe most commonly used operation on strings is pattern matching using the op-\nerator like. We describe patterns by using two special characters:\n• Percent (%): The % character matches any substring.\n• Underscore ( ): The\ncharacter matches any character.\nPatterns are case sensitive; that is, uppercase characters do not match lowercase char-\nacters, or vice versa. To illustrate pattern matching, we consider the following exam-\nples:\n• ’Perry%’ matches any string beginning with “Perry”.\n• ’%idge%’ matches any string containing “idge” as a substring, for example,\n’Perryridge’, ’Rock Ridge’, ’Mianus Bridge’, and ’Ridgeway’.\n• ’\n’ matches any string of exactly three characters.\n• ’\n%’ matches any string of at least three characters.\nSQL expresses patterns by using the like comparison operator. Consider the query\n“Find the names of all customers whose street address includes the substring ‘Main’.”\nThis query can be written as\nselect customer-name\nfrom customer\nwhere customer-street like ’%Main%’\nFor pattern\n\ne value d, we can use extract (ﬁeld from\nd), where ﬁeld can be one of year, month, day, hour, minute, or second.\nSQL allows comparison operations on all the domains listed here, and it allows\nboth arithmetic and comparison operations on the various numeric domains. SQL\nalso provides a data type called interval, and it allows computations based on dates\nand times and on intervals. For example, if x and y are of type date, then x −y is an\ninterval whose value is the number of days from date x to date y. Similarly, adding\nor subtracting an interval to a date or time gives back a date or time, respectively.\nIt is often useful to compare values from compatible domains. For example, since\nevery small integer is an integer, a comparison x < y, where x is a small integer and\ny is an integer (or vice versa), makes sense. We make such a comparison by casting\nsmall integer x as an integer. A transformation of this sort is called a type coercion.\nType coercion is used routinely in common programming languages, as well as in\ndatabase systems.\nAs an illustration, suppose that the domain of customer-name is a character string\nof length 20, and the domain of branch-name is a character string of length 15. Al-\nthough the string lengths might differ, standard SQL will consider the two domains\ncompatible.\nAs we discussed in Chapter 3, the null value is a member of all domains. For cer-\ntain attributes, however, null values may be inappropriate. Consider a tuple in the\ncustomer relation where customer-name is null. Such a tuple gives a street and city for\nan anonymous customer; thus, it does not contain useful information. In cases such\nas this, we wish to forbid null values, and we do so by restricting the domain of\ncustomer-name to exclude null values.\nSQL allows the domain declaration of an attribute to include the speciﬁcation not\nnull and thus prohibits the insertion of a null value for this attribute. Any database\nmodiﬁcation that would cause a null to be inserted in a not null domain generates\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nII. Relational Databases\n4. SQL\n176\n© The McGraw−Hill \nCompanies, 2001\n170\nChapter 4\nSQL\nan error diagnostic. There are many situations where we want to avoid null values.\nIn particular, it is essential to prohibit null values in the primary key of a relation\nschema. Thus, in our bank example, in the customer relation, we must prohibit a null\nvalue for the attribute customer-name, which is the primary key for customer.\n4.11.2\nSchema Deﬁnition in SQL\nWe deﬁne an SQL relation by using the create table command:\ncreate table r(A1D1, A2D2, . . . , AnDn,\n⟨integrity-constraint1⟩,\n. . . ,\n⟨integrity-constraintk⟩)\nwhere r is the name of the relation, each Ai is the name of an attribute in the schema\nof relation r, and Di is the domain type of values in the domain of attribute Ai. The\nallowed integrity constraints include\n• primary key (Aj1, Aj2, . . . , Ajm): The primary key speciﬁcation says that at-\ntributes Aj1, Aj2, . . . , Ajm form the primary key for the relation. The primary\nkey attributes are required to be non-null and unique; that is, no tuple can have\na null value for a primary key attribute, and no two tuples in the relation can\nbe equal on all the primary-key attributes.1 Although the primary key speciﬁ-\ncation is optional, it is generally a good idea to specify a primary key for each\nrelation.\n• check(P): The check clause speciﬁes a predicate P that must be satisﬁed by\nevery tuple in the relation.\nThe create table command also includes other integrity constraints, which we shall\ndiscuss in Chapter 6.\nFigure 4.8 presents a partial SQL DDL deﬁnition of our bank database. Note that,\nas in earlier chapters, we do not attempt to model precisely the real world in the\nbank-database example. In the real world, multiple people may have the same name,\nso customer-name would not be a primary key customer; a customer-id would more\nlikely be used as a primary key. We use customer-name as a primary key to keep our\ndatabase schema simple and short.\nIf a newly inserted or modiﬁed tuple in a relation has null values for any primary-\nkey attribute, or if the tuple has the same value on the primary-key attributes as does\nanother tuple in the relation, SQL ﬂags an error and prevents the update. Similarly, it\nﬂags an error and prevents the update if the check condition on the tuple fails.\nBy default null is a legal value for every attribute in SQL, unless the attribute is\nspeciﬁcally stated to be not null. An attribute can be declared to be not null in the\nfollowing way:\naccount-number char(10) not null\n1.\nIn SQL-89, primary-key attributes were not implicitly declared to be not null; an explicit not null\ndeclaration was required.\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nII. Relational Databases\n4. SQL\n177\n© The McGraw−Hill \nCompanies, 2001\n4.11\nData-Deﬁnition Language\n171\ncreate table customer\n(customer-name\nchar(20),\ncustomer-street\nchar(30),\ncustomer-city\nchar(30),\nprimary key (customer-name))\ncreate table branch\n(branch-name\nchar(15),\nbranch-city\nchar(30),\nassets\ninteger,\nprimary key (branch-name),\ncheck (assets >= 0))\ncreate table account\n(account-number char(10),\nbranch-name\nchar(15),\nbalance\ninteger,\nprimary key (account-number),\ncheck (balance >= 0))\ncreate table depositor\n(customer-name\nchar(20),\naccount-number\nchar(10),\nprimary key (customer-name, account-number))\nFigure 4.8\nSQL data deﬁnition for part of the bank database.\nSQL also supports an integrity constraint\nunique (Aj1, Aj2, . . . , Ajm)\nThe unique speciﬁcation says that attributes Aj1, Aj2, . . . , Ajm form a candidate key;\nthat is, no two tuples in the relation can be equal on all the primary-key attributes.\nHowever, candidate key attributes are permitted to be null unless they have explicitly\nbeen declared to be not null. Recall that a null value does not equal any other value.\nThe treatment of nulls here is the same as that of the unique construct deﬁned in\nSection 4.6.4.\nA common use of the check clause is to ensure that attribute values satisfy spec-\niﬁed conditions, in effect creating a powerful type system. For instance, the check\nclause in the create table command for relation branch checks that the value of assets\nis nonnegative. As another example, consider the following:\ncreate table student\n(name\nchar(15) not null,\nstudent-id\nchar(10),\ndegree-level\nchar(15),\nprimary key (student-id),\ncheck (degree-level in (’Bachelors’, ’Masters’, ’Doctorate’)))\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nII. Relational Databases\n4. SQL\n178\n© The McGraw−Hill \nCompanies, 2001\n172\nChapter 4\nSQL\nHere, we use the check clause to simulate an enumerated type, by specifying that\ndegree-level must be one of ’Bachelors’, ’Masters’, or ’Doctorate’. We consider more\ngeneral forms of check conditions, as well as a class of constraints called referential\nintegrity constraints, in Chapter 6.\nA newly created relation is empty initially. We can use the insert command to load\ndata into the relation. Many relational-database products have special bulk loader\nutilities to load an initial set of tuples into a relation.\nTo remove a relation from an SQL database, we use the drop table command. The\ndrop table command deletes all information about the dropped relation from the\ndatabase. The command\ndrop table r\nis a more drastic action than\ndelete from r\nThe latter retains relation r, but deletes all tuples in r. The former deletes not only all\ntuples of r, but also the schema for r. After r is dropped, no tuples can be inserted\ninto r unless it is re-created with the create table command.\nWe use the alter table command to add attributes to an existing relation. All tuples\nin the relation are assigned null as the value for the new attribute. The form of the\nalter table command is\nalter table r add A D\nwhere r is the name of an existing relation, A is the name of the attribute to be added,\nand D is the domain of the added attribute. We can drop attributes from a relation by\nthe command\nalter table r drop A\nwhere r is the name of an existing relation, and A is the name of an attribute of the\nrelation. Many database systems do not support dropping of attributes, although\nthey will allow an entire table to be dropped.\n4.12\nEmbedded SQL\nSQL provides a powerful declarative query language. Writing queries in SQL is usu-\nally much easier than coding the same queries in a general-purpose programming\nlanguage. However, a programmer must have access to a database from a general-\npurpose programming language for at least two reasons:\n1. Not all queries can be expressed in SQL, since SQL does not provide the full\nexpressive power of a general-purpose language. That is, there exist queries\nthat can be expressed in a language such as C, Java, or Cobol that cannot be\nexpressed in SQL. To write such queries, we can embed SQL within a more\npowerful language.\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nII. Relational Databases\n4. SQL\n179\n© The McGraw−Hill \nCompanies, 2001\n4.12\nEmbedded SQL\n173\nSQL is designed so that queries written in it can be optimized automatically\nand executed efﬁciently—and providing the full power of a programming\nlanguage makes automatic optimization exceedingly difﬁcult.\n2. Nondeclarative actions—such as printing a report, interacting with a user, or\nsending the results of a query to a graphical user interface—cannot be done\nfrom within SQL. Applications usually have several components, and query-\ning or updating data is only one component; other components are written in\ngeneral-purpose programming languages. For an integrated application, the\nprograms written in the programming language must be able to access the\ndatabase.\nThe SQL standard deﬁnes embeddings of SQL in a variety of programming lan-\nguages, such as C, Cobol, Pascal, Java, PL/I, and Fortran. A language in which SQL\nqueries are embedded is referred to as a host language, and the SQL structures per-\nmitted in the host language constitute embedded SQL.\nPrograms written in the host language can use the embedded SQL syntax to ac-\ncess and update data stored in a database. This embedded form of SQL extends the\nprogrammer’s ability to manipulate the database even further. In embedded SQL, all\nquery processing is performed by the database system, which then makes the result\nof the query available to the program one tuple (record) at a time.\nAn embedded SQL program must be processed by a special preprocessor prior to\ncompilation. The preprocessor replaces embedded SQL requests with host-language\ndeclarations and procedure calls that allow run-time execution of the database ac-\ncesses. Then, the resulting program is compiled by the host-language compiler. To\nidentify embedded SQL requests to the preprocessor, we use the EXEC SQL statement;\nit has the form\nEXEC SQL <embedded SQL statement > END-EXEC\nThe exact syntax for embedded SQL requests depends on the language in which\nSQL is embedded. For instance, a semicolon is used instead of END-EXEC when SQL\nis embedded in C. The Java embedding of SQL (called SQLJ) uses the syntax\n# SQL { <embedded SQL statement > };\nWe place the statement SQL INCLUDE in the program to identify the place where\nthe preprocessor should insert the special variables used for communication between\nthe program and the database system. Variables of the host language can be used\nwithin embedded SQL statements, but they must be preceded by a colon (:) to distin-\nguish them from SQL variables.\nEmbedded SQL statements are similar in form to the SQL statements that we de-\nscribed in this chapter. There are, however, several important differences, as we note\nhere.\nTo write a relational query, we use the declare cursor statement. The result of the\nquery is not yet computed. Rather, the program must use the open and fetch com-\nmands (discussed later in this section) to obtain the result tuples.\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nII. Relational Databases\n4. SQL\n180\n© The McGraw−Hill \nCompanies, 2001\n174\nChapter 4\nSQL\nConsider the banking schema that we have used in this chapter. Assume that we\nhave a host-language variable amount, and that we wish to ﬁnd the names and cities\nof residence of customers who have more than amount dollars in any account. We can\nwrite this query as follows:\nEXEC SQL\ndeclare c cursor for\nselect customer-name, customer-city\nfrom depositor, customer, account\nwhere depositor.customer-name = customer.customer-name and\naccount.account-number = depositor.account-number and\naccount.balance > :amount\nEND-EXEC\nThe variable c in the preceding expression is called a cursor for the query. We use\nthis variable to identify the query in the open statement, which causes the query to\nbe evaluated, and in the fetch statement, which causes the values of one tuple to be\nplaced in host-language variables.\nThe open statement for our sample query is as follows:\nEXEC SQL open c END-EXEC\nThis statement causes the database system to execute the query and to save the results\nwithin a temporary relation. The query has a host-language variable (:amount); the\nquery uses the value of the variable at the time the open statement was executed.\nIf the SQL query results in an error, the database system stores an error diagnostic\nin the SQL communication-area (SQLCA) variables, whose declarations are inserted\nby the SQL INCLUDE statement.\nAn embedded SQL program executes a series of fetch statements to retrieve tuples\nof the result. The fetch statement requires one host-language variable for each at-\ntribute of the result relation. For our example query, we need one variable to hold the\ncustomer-name value and another to hold the customer-city value. Suppose that those\nvariables are cn and cc, respectively. Then the statement:\nEXEC SQL fetch c into :cn, :cc END-EXEC\nproduces a tuple of the result relation. The program can then manipulate the vari-\nables cn and cc by using the features of the host programming language.\nA single fetch request returns only one tuple. To obtain all tuples of the result,\nthe program must contain a loop to iterate over all tuples. Embedded SQL assists the\nprogrammer in managing this iteration. Although a relation is conceptually a set, the\ntuples of the result of a query are in some ﬁxed physical order. When the program\nexecutes an open statement on a cursor, the cursor is set to point to the ﬁrst tuple\nof the result. Each time it executes a fetch statement, the cursor is updated to point\nto the next tuple of the result. When no further tuples remain to be processed, the\nvariable SQLSTATE in the SQLCA is set to ’02000’ (meaning “no data”). Thus, we can\nuse a while loop (or equivalent loop) to process each tuple of the result.\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nII. Relational Databases\n4. SQL\n181\n© The McGraw−Hill \nCompanies, 2001\n4.13\nDynamic SQL\n175\nWe must use the close statement to tell the database system to delete the tempo-\nrary relation that held the result of the query. For our example, this statement takes\nthe form\nEXEC SQL close c END-EXEC\nSQLJ, the Java embedding of SQL, provides a variation of the above scheme, where\nJava iterators are used in place of cursors. SQLJ associates the results of a query with\nan iterator, and the next() method of the Java iterator interface can be used to step\nthrough the result tuples, just as the preceding examples use fetch on the cursor.\nEmbedded SQL expressions for database modiﬁcation (update, insert, and delete)\ndo not return a result. Thus, they are somewhat simpler to express. A database-\nmodiﬁcation request takes the form\nEXEC SQL < any valid update, insert, or delete> END-EXEC\nHost-language variables, preceded by a colon, may appear in the SQL database-\nmodiﬁcation expression. If an error condition arises in the execution of the statement,\na diagnostic is set in the SQLCA.\nDatabase relations can also be updated through cursors. For example, if we want\nto add 100 to the balance attribute of every account where the branch name is “Per-\nryridge”, we could declare a cursor as follows.\ndeclare c cursor for\nselect *\nfrom account\nwhere branch-name = ‘Perryridge‘\nfor update\nWe then iterate through the tuples by performing fetch operations on the cursor (as\nillustrated earlier), and after fetching each tuple we execute the following code\nupdate account\nset balance = balance + 100\nwhere current of c\nEmbedded SQL allows a host-language program to access the database, but it pro-\nvides no assistance in presenting results to the user or in generating reports. Most\ncommercial database products include tools to assist application programmers in\ncreating user interfaces and formatted reports. We discuss such tools in Chapter 5\n(Section 5.3).\n4.13\nDynamic SQL\nThe dynamic SQL component of SQL allows programs to construct and submit SQL\nqueries at run time. In contrast, embedded SQL statements must be completely present\nat compile time; they are compiled by the embedded SQL preprocessor. Using dy-\nnamic SQL, programs can create SQL queries as strings at run time (perhaps based on\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nII. Relational Databases\n4. SQL\n182\n© The McGraw−Hill \nCompanies, 2001\n176\nChapter 4\nSQL\ninput from the user) and can either have them executed immediately or have them\nprepared for subsequent use. Preparing a dynamic SQL statement compiles it, and\nsubsequent uses of the prepared statement use the compiled version.\nSQL deﬁnes standards for embedding dynamic SQL calls in a host language, such\nas C, as in the following example.\nchar * sqlprog = ”update account set balance = balance ∗1.05\nwhere account-number = ?”\nEXEC SQL prepare dynprog from :sqlprog;\nchar account[10] = ”A-101”;\nEXEC SQL execute dynprog using :account;\nThe dynamic SQL program contains a ?, which is a place holder for a value that is\nprovided when the SQL program is executed.\nHowever, the syntax above requires extensions to the language or a preprocessor\nfor the extended language. An alternative that is very widely used is to use an appli-\ncation program interface to send SQL queries or updates to a database system, and\nnot make any changes in the programming language itself.\nIn the rest of this section, we look at two standards for connecting to an SQL\ndatabase and performing queries and updates. One, ODBC, is an application pro-\ngram interface for the C language, while the other, JDBC, is an application program\ninterface for the Java language.\nTo understand these standards, we need to understand the concept of SQL ses-\nsions. The user or application connects to an SQL server, establishing a session; exe-\ncutes a series of statements; and ﬁnally disconnects the session. Thus, all activities of\nthe user or application are in the context of an SQL session. In addition to the normal\nSQL commands, a session can also contain commands to commit the work carried out\nin the session, or to rollback the work carried out in the session.\n4.13.1\nODBC∗∗\nThe Open DataBase Connectivity (ODBC) standard deﬁnes a way for an application\nprogram to communicate with a database server. ODBC deﬁnes an application pro-\ngram interface (API) that applications can use to open a connection with a database,\nsend queries and updates, and get back results. Applications such as graphical user\ninterfaces, statistics packages, and spreadsheets can make use of the same ODBC API\nto connect to any database server that supports ODBC.\nEach database system supporting ODBC provides a library that must be linked\nwith the client program. When the client program makes an ODBC API call, the code\nin the library communicates with the server to carry out the requested action, and\nfetch results.\nFigure 4.9 shows an example of C code using the ODBC API. The ﬁrst step in using\nODBC to communicate with a server is to set up a connection with the server. To do\nso, the program ﬁrst allocates an SQ\n\ntype system of\nobject-oriented databases, combined with relations as the basis for storage of data.\nIt applies inheritance to relations, not just to types. The object-relational data model\nprovides a smooth migration path from relational databases, which is attractive to\nrelational database vendors. As a result, the SQL:1999 standard includes a number\nof object-oriented features in its type system, while continuing to use the relational\nmodel as the underlying model.\nThe XML language was initially designed as a way of adding markup informa-\ntion to text documents, but has become important because of its applications in data\nexchange. XML provides a way to represent data that have nested structure, and fur-\nthermore allows a great deal of ﬂexibility in structuring of data, which is important\nfor certain kinds of nontraditional data. Chapter 10 describes the XML language, and\nthen presents different ways of expressing queries on data represented in XML, and\ntransforming XML data from one form to another.\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nIII. Object−Based \nDatabases and XML\n8. Object−Oriented \nDatabases\n308\n© The McGraw−Hill \nCompanies, 2001\nP\nA\nR\nT\n8\nCase Studies\nThis part describes how different database systems integrate the various concepts\ndescribed earlier in the book. Speciﬁcally, three widely used database systems—IBM\nDB2, Oracle, and Microsoft SQL Server—are covered in Chapters 25, 26, and 27. These\nthree represent three of the most widely used database systems.\nEach of these chapters highlights unique features of each database system: tools,\nSQL variations and extensions, and system architecture, including storage organiza-\ntion, query processing, concurrency control and recovery, and replication.\nThe chapters cover only key aspects of the database products they describe, and\ntherefore should not be regarded as a comprehensive coverage of the product. Fur-\nthermore, since products are enhanced regularly, details of the product may change.\nWhen using a particular product version, be sure to consult the user manuals for\nspeciﬁc details.\nKeep in mind that the chapters in this part use industrial rather than academic\nterminology. For instance, they use table instead of relation, row instead of tuple,\nand column instead of attribute.\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nIII. Object−Based \nDatabases and XML\n8. Object−Oriented \nDatabases\n309\n© The McGraw−Hill \nCompanies, 2001\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nIII. Object−Based \nDatabases and XML\n8. Object−Oriented \nDatabases\n310\n© The McGraw−Hill \nCompanies, 2001\nC\nH\nA\nP\nT\nE\nR\n2\n5\nOracle\nHakan Jakobsson\nOracle Corporation\nWhen Oracle was founded in 1977 as Software Development Laboratories by Larry\nEllison, Bob Miner, and Ed Oates, there were no commercial relational database prod-\nucts. The company, which was later renamed Oracle, set out to build a relational\ndatabase management system as a commercial product, and was the ﬁrst to reach the\nmarket. Since then, Oracle has held a leading position in the relational database mar-\nket, but over the years its product and service offerings have grown beyond the rela-\ntional database server. In addition to tools directly related to database development\nand management, Oracle sells business intelligence tools, including a multidimen-\nsional database management system (Oracle Express), query and analysis tools, data-\nmining products, and an application server with close integration to the database\nserver.\nIn addition to database-related servers and tools, the company also offers appli-\ncation software for enterprise resource planning and customer-relationship manage-\nment, including areas such as ﬁnancials, human resources, manufacturing, market-\ning, sales, and supply chain management. Oracle’s Business OnLine unit offers ser-\nvices in these areas as an application service provider.\nThis chapter surveys a subset of the features, options, and functionality of Oracle\nproducts. New versions of the products are being developed continually, so all prod-\nuct descriptions are subject to change. The feature set described here is based on the\nﬁrst release of Oracle9i.\n25.1\nDatabase Design and Querying Tools\nOracle provides a variety of tools for database design, querying, report generation\nand data analysis, including OLAP.\n921\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nIII. Object−Based \nDatabases and XML\n8. Object−Oriented \nDatabases\n311\n© The McGraw−Hill \nCompanies, 2001\n922\nChapter 25\nOracle\n25.1.1\nDatabase Design Tools\nMost of Oracle’s design tools are included in the Oracle Internet Development Suite.\nThis is a suite of tools for various aspects of application development, including tools\nfor forms development, data modeling, reporting, and querying. The suite supports\nthe UML standard (see Section 2.10) for development modeling. It provides class\nmodeling to generate code for the business components for Java framework as well\nas activity modeling for general-purpose control ﬂow modeling. The suite also sup-\nports XML for data exchange with other UML tools.\nThe major database design tool in the suite is Oracle Designer, which translates\nbusiness logic and data ﬂows into a schema deﬁnitions and procedural scripts for\napplication logic. It supports such modeling techniques as E-R diagrams, information\nengineering, and object analysis and design. Oracle Designer stores the design in\nOracle Repository, which serves as a single point of metadata for the application.\nThe metadata can then be used to generate forms and reports. Oracle Repository\nprovides conﬁguration management for database objects, forms applications, Java\nclasses, XML ﬁles, and other types of ﬁles.\nThe suite also contains application development tools for generating forms, re-\nports, and tools for various aspects of Java and XML-based development. The busi-\nness intelligence component provides JavaBeans for analytic functionality such as\ndata visualization, querying, and analytic calculations.\nOracle also has an application development tool for data warehousing, Oracle\nWarehouse Builder. Warehouse Builder is a tool for design and deployment of all as-\npects of a data warehouse, including schema design, data mapping and transforma-\ntions, data load processing, and metadata management. Oracle Warehouse Builder\nsupports both 3NF and star schemas and can also import designs from Oracle De-\nsigner.\n25.1.2\nQuerying Tools\nOracle provides tools for ad-hoc querying, report generation and data analysis, in-\ncluding OLAP.\nOracle Discoverer is a Web-based, ad hoc query, reporting, analysis and Web pub-\nlishing tool for end users and data analysts. It allows users to drill up and down on\nresult sets, pivot data, and store calculations as reports that can be published in a\nvariety of formats such as spreadsheets or HTML. Discoverer has wizards to help end\nusers visualize data as graphs. Oracle9i has supports a rich set of analytical func-\ntions, such as ranking and moving aggregation in SQL. Discoverer’s ad hoc query\ninterface can generate SQL that takes advantage of this functionality and can pro-\nvide end users with rich analytical functionality. Since the processing takes place in\nthe relational database management system, Discoverer does not require a complex\nclient-side calculation engine and there is a version of Discoverer that is browser\nbased.\nOracle Express Server is a multidimensional database server. It supports a wide\nvariety of analytical queries as well as forecasting, modeling, and scenario manage-\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nIII. Object−Based \nDatabases and XML\n8. Object−Oriented \nDatabases\n312\n© The McGraw−Hill \nCompanies, 2001\n25.2\nSQL Variations and Extensions\n923\nment. It can use the relational database management system as a back end for storage\nor use its own multidimensional storage of the data.\nWith the introduction of OLAP services in Oracle9i, Oracle is moving away from\nsupporting a separate storage engine and moving most of the calculations into SQL.\nThe result is a model where all the data reside in the relational database management\nsystem and where any remaining calculations that cannot be performed in SQL are\ndone in a calculation engine running on the database server. The model also provides\na Java OLAP application programmer interface.\nThere are many reasons for moving away from a separate multidimensional stor-\nage engine:\n• A relational engine can scale to much larger data sets.\n• A common security model can be used for the analytical applications and the\ndata warehouse.\n• Multidimensional modeling can be integrated with data warehouse modeling.\n• The relational database management system has a larger set of features and\nfunctionality in many areas such as high availability, backup and recovery,\nand third-party tool support.\n• There is no need to train database administrators for two database engines.\nThe main challenge with moving away from a separate multidimensional database\nengine is to provide the same performance. A multidimensional database manage-\nment system that materializes all or large parts of a data cube can offer very fast\nresponse times for many calculations. Oracle has approached this problem in two\nways.\n• Oracle has added SQL support for a wide range of analytical functions, in-\ncluding cube, rollup, grouping sets, ranks, moving aggregation, lead and lag\nfunctions, histogram buckets, linear regression, and standard deviation, along\nwith the ability to optimize the execution of such functions in the database en-\ngine.\n• Oracle has extended materialized views to permit analytical functions, in par-\nticular grouping sets. The ability to materialize parts or all of the cube is key\nto the performance of a multidimensional database management system and\nmaterialized views give a relational database management system the ability\nto do the same thing.\n25.2\nSQL Variations and Extensions\nOracle9i supports all core SQL:1999 features fully or partially, with some minor ex-\nceptions such as distinct data types. In addition, Oracle supports a large number of\nother language constructs, some of which conform with SQL:1999, while others are\nOracle-speciﬁc in syntax or functionality. For example, Oracle supports the OLAP\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nIII. Object−Based \nDatabases and XML\n8. Object−Oriented \nDatabases\n313\n© The McGraw−Hill \nCompanies, 2001\n924\nChapter 25\nOracle\noperations described in Section 22.2, including ranking, moving aggregation, cube,\nand rollup.\nA few examples of Oracle SQL extensions are:\n• connect by, which is a form of tree traversal that allows transitive closure-\nstyle calculations in a single SQL statement. It is an Oracle-speciﬁc syntax for\na feature that Oracle has had since the 1980s.\n• Upsert and multitable inserts. The upsert operation combines update and in-\nsert, and is useful for merging new data with old data in data warehousing\napplications. If a new row has the same key value as an old row, the old row is\nupdated (for example by adding the measure values from the new row), oth-\nerwise the new row is inserted into the table. Multitable inserts allow multiple\ntables to be updated based on a single scan of new data.\n• with clause, which is described in Section 4.8.2.\n25.2.1\nObject-Relational Features\nOracle has extensive support for object-relational constructs, including:\n• Object types. A single-inheritance model is supported for type hierarchies.\n• Collection types. Oracle supports varrays which are variable length arrays,\nand nested tables.\n• Object tables. These are used to store objects while providing a relational\nview of the attributes of the objects.\n• Table functions. These are functions that produce sets of rows as output, and\ncan be used in the from clause of a query. Table functions in Oracle can be\nnested. If a table function is used to express some form of data transformation,\nnesting multiple functions allows multiple transformations to be expressed in\na single statement.\n• Object views. These provide a virtual object table view of data stored in a\nregular relational table. They allow data to be accessed or viewed in an object-\noriented style even if the data are really stored in a traditional relational for-\nmat.\n• Methods. These can be written in PL/SQL, Java, or C.\n• User-deﬁned aggregate functions. These can be used in SQL statements in the\nsame way as built-in functions such as sum and count.\n• XML data types. These can be used to store and index XML documents.\nOracle has two main procedural languages, PL/SQL and Java. PL/SQL was Oracle’s\noriginal language for stored procedures and it has syntax similar to that used in the\nAda language. Java is supported through a Java virtual machine inside the database\nengine. Oracle provides a package to encapsulate related procedures, functions, and\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nIII. Object−Based \nDatabases and XML\n8. Object−Oriented \nDatabases\n314\n© The McGraw−Hill \nCompanies, 2001\n25.3\nStorage and Indexing\n925\nvariables into single units. Oracle supports SQLJ (SQL embedded in Java) and JDBC,\nand provides a tool to generate Java class deﬁnitions corresponding to user-deﬁned\ndatabase types.\n25.2.2\nTriggers\nOracle provides several types of triggers and several options for when and how they\nare invoked. (See Section 6.4 for an introduction to triggers in SQL.) Triggers can be\nwritten in PL/SQL or Java or as C callouts.\nFor triggers that execute on DML statements such as insert, update, and delete,\nOracle supports row triggers and statement triggers. Row triggers execute once for\nevery row that is affected (updated or deleted, for example) by the DML operation.\nA statement trigger is executed just once per statement. In each case, the trigger can\nbe deﬁned as either a before or after trigger, depending on whether it is to be invoked\nbefore or after the DML operation is carried out.\nOracle allows the creation of instead of triggers for views that cannot be subject\nto DML operations. Depending on the view deﬁnition, it may not be possible for Or-\nacle to translate a DML statement on a view to modiﬁcations of the underlying base\ntables unambiguously. Hence, DML operations on views are subject to numerous re-\nstrictions. A user can create an instead of trigger on a view to specify manually what\noperations on the base tables are to occur in response to the DML operation on the\nview. Oracle executes the trigger instead of the DML operation and therefore pro-\nvides a mechanism to circumvent the restrictions on DML operations against views.\nOracle also has triggers that execute on a variety of other events, like database\nstartup or shutdown, server error messages, user logon or logoff, and DDL statements\nsuch as create, alter and drop statements.\n25.3\nStorage and Indexing\nIn Oracle parlance, a database consists of information stored in ﬁles and is accessed\nthrough an instance, which is a shared memory area and a set of processes that inter-\nact with the data in the ﬁles.\n25.3.1\nTable Spaces\nA database consists of one or more logical storage units called table spaces. Each\ntable space, in turn, consists of one or more physical structures called data ﬁles. These\nmay be either ﬁles managed by the operating system or raw devices.\nUsually, an Oracle database will have the following table spaces:\n• The system table space, which is always created. It contains the data dictio-\nnary tables and storage for triggers and stored procedures.\n• Table spaces created to store user data. While user data can be stored in the\nsystem table space, it is often desirable to separate the user data from the sys-\ntem data. Usually, the decision about what other table spaces should be cre-\nated is based on performance, availability, maintainability, and ease of admin-\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nIII. Object−Based \nDatabases and XML\n8. Object−Oriented \nDatabases\n315\n© The McGraw−Hill \nCompanies, 2001\n926\nChapter 25\nOracle\nistration. For example, having multiple table spaces can be useful for partial\nbackup and recovery operations.\n• Temporary table spaces. Many database operations require sorting the data,\nand the sort routine may have to store data temporarily on disk if the sort\ncannot be done in memory. Temporary table spaces are allocated for sorting,\nto make the space management operations involved in spilling to disk more\nefﬁcient.\nTable spaces can also be used as a means of moving data between databases. For\nexample, it is common to move data from a transactional system to a data warehouse\nat regular intervals. Oracle allows moving all the data in a table space from one sys-\ntem to the other by simply copying the ﬁles and exporting and importing a small\namount of data dictionary metadata. These operations can be much faster than un-\nloading the data from one database and then using a loader to insert it into the other.\nA requirement for this feature is that both systems use the same operating system.\n25.3.2\nSegments\nThe space in a table space is divided into units, called segments, that each contain\ndata for a speciﬁc data structure. There are four types of segments.\n• Data segments. Each table in a table space has its own data segment where\nthe table data are stored unless the table is partitioned; if so, there is one data\nsegment per partition. (Partitioning in Oracle is described in Section 25.3.10.)\n• Index segments. Each index in a table space has its own index segment, except\nfor partitioned indices, which have one index segment per partition.\n• Temporary segments. These are segments used when a sort operation needs\nto write data to disk or when data are inserted into a temporary table.\n• Rollback segments. These segments contain undo information so that an un-\ncommitted transaction can be rolled back. They also play an important roll in\nOracle’s concurrency control model and for database recovery, described in\nSections 25.5.1 and 25.5.2.\nBelow the level of segment, space is allocated at a level of granularity called extent.\nEach extent consists of a set of contiguous database blocks. A database block is the\nlowest level of granularity at which Oracle performs disk I/O. A database block does\nnot have to be the same as an operating system block in size, but should be a multiple\nthereof.\nOracle provides storage parameters that allow for detailed control of how space is\nallocated and managed, parameters such as:\n• The size of a new extent that is to be allocated to provide room for rows that\nare inserted into a table.\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nIII. Object−Based \nDatabases and XML\n8. Object−Oriented \nDatabases\n316\n© The McGraw−Hill \nCompanies, 2001\n25.3\nStorage and Indexing\n927\n• The percentage of space utilization at which a database block is considered full\nand at which no more rows will be inserted into that block. (Leaving some free\nspace in a block can allow the existing rows to grow in size through updates,\nwithout running out of space in the block.)\n25.3.3\nTables\nA standard table in Oracle is heap organized; that is, the storage location of a row in\na table is not based on the values contained in the row, and is ﬁxed when the row\nis inserted. However, if the table is partitioned, the content of the row affects the\npartition in which it is stored. There are several features and variations.\nOracle supports nested tables; that is, a table can have a column whose data type\nis another table. The nested table is not stored in line in the parent table, but is stored\nin a separate table.\nOracle supports temporary tables where the duration of the data is either the trans-\naction in which the data are inserted, or the user session. The data are private to the\nsession and are automatically removed at the end of its du\n\nies on local schemas at each of the sites where the query has to be exe-\ncuted. The query results have to be translated back into the global schema.\nThe task is simpliﬁed by writing wrappers for each data source, which pro-\nvide a view of the local data in the global schema. Wrappers also translate\nqueries on the global schema into queries on the local schema, and translate\nresults back into the global schema. Wrappers may be provided by individual\nsites, or may be written separately as part of the multidatabase system.\nWrappers can even be used to provide a relational view of nonrelational\ndata sources, such as Web pages (possibly with forms interfaces), ﬂat ﬁles,\nhierarchical and network databases, and directory systems.\n• Some data sources may provide only limited query capabilities; for instance,\nthey may support selections, but not joins. They may even restrict the form\nof selections, allowing selections only on certain ﬁelds; Web data sources with\nform interfaces are an example of such data sources. Queries may therefore\nhave to be broken up, to be partly performed at the data source and partly at\nthe site issuing the query.\n• In general, more than one site may need to be accessed to answer a given\nquery. Answers retrieved from the sites may have to be processed to remove\nduplicates. Suppose one site contains account tuples satisfying the selection\nbalance < 100, while another contains account tuples satisfying balance > 50.\nA query on the entire account relation would require access to both sites and\nremoval of duplicate answers resulting from tuples with balance between 50\nand 100, which are replicated at both sites.\n• Global query optimization in a heterogeneous database is difﬁcult, since the\nquery execution system may not know what the costs are of alternative query\nplans at different sites. The usual solution is to rely on only local-level opti-\nmization, and just use heuristics at the global level.\nMediator systems are systems that integrate multiple heterogeneous data sources,\nproviding an integrated global view of the data and providing query facilities on\nthe global view. Unlike full-ﬂedged multidatabase systems, mediator systems do not\nbother about transaction processing. (The terms mediator and multidatabase are of-\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nVI. Database System \nArchitecture\n19. Distributed Databases\n737\n© The McGraw−Hill \nCompanies, 2001\n19.9\nDirectory Systems\n741\nten used in an interchangeable fashion, and systems that are called mediators may\nsupport limited forms of transactions.) The term virtual database is used to refer\nto multidatabase/mediator systems, since they provide the appearance of a single\ndatabase with a global schema, although data exist on multiple sites in local schemas.\n19.9\nDirectory Systems\nConsider an organization that wishes to make data about its employees available to\na variety of people in the organization; example of the kinds of data would include\nname, designation, employee-id, address, email address, phone number, fax num-\nber, and so on. In the precomputerization days, organizations would create physical\ndirectories of employees and distribute them across the organization. Even today,\ntelephone companies create physical directories of customers.\nIn general, a directory is a listing of information about some class of objects such as\npersons. Directories can be used to ﬁnd information about a speciﬁc object, or in the\nreverse direction to ﬁnd objects that meet a certain requirement. In the world of phys-\nical telephone directories, directories that satisfy lookups in the forward direction are\ncalled white pages, while directories that satisfy lookups in the reverse direction are\ncalled yellow pages.\nIn today’s networked world, the need for directories is still present and, if any-\nthing, even more important. However, directories today need to be available over a\ncomputer network, rather than in a physical (paper) form.\n19.9.1\nDirectory Access Protocols\nDirectory information can be made available through Web interfaces, as many orga-\nnizations, and phone companies in particular do. Such interfaces are good for hu-\nmans. However, programs too, need to access directory information. Directories can\nbe used for storing other types of information, much like ﬁle system directories. For\ninstance, Web browsers can store personal bookmarks and other browser settings in\na directory system. A user can thus access the same settings from multiple locations,\nsuch as at home and at work, without having to share a ﬁle system.\nSeveral directory access protocols have been developed to provide a standardized\nway of accessing data in a directory. The most widely used among them today is the\nLightweight Directory Access Protocol (LDAP).\nObviously all the types of data in our examples can be stored without much trou-\nble in a database system, and accessed through protocols such as JDBC or ODBC. The\nquestion then is, why come up with a specialized protocol for accessing directory\ninformation? There are at least two answers to the question.\n• First, directory access protocols are simpliﬁed protocols that cater to a lim-\nited type of access to data. They evolved in parallel with the database access\nprotocols.\n• Second, and more important, directory systems provide a simple mechanism\nto name objects in a hierarchical fashion, similar to ﬁle system directory names,\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nVI. Database System \nArchitecture\n19. Distributed Databases\n738\n© The McGraw−Hill \nCompanies, 2001\n742\nChapter 19\nDistributed Databases\nwhich can be used in a distributed directory system to specify what informa-\ntion is stored in each of the directory servers. For example, a particular direc-\ntory server may store information for Bell Laboratories employees in Murray\nHill, while another may store information for Bell Laboratories employees in\nBangalore, giving both sites autonomy in controlling their local data. The di-\nrectory access protocol can be used to obtain data from both directories, across\na network. More importantly, the directory system can be set up to automati-\ncally forward queries made at one site to the other site, without user interven-\ntion.\nFor these reasons, several organizations have directory systems to make organiza-\ntional information available online. As may be expected, several directory implemen-\ntations ﬁnd it beneﬁcial to use relational databases to store data, instead of creating\nspecial-purpose storage systems.\n19.9.2\nLDAP: Lightweight Directory Access Protocol\nIn general a directory system is implemented as one or more servers, which service\nmultiple clients. Clients use the application programmer interface deﬁned by direc-\ntory system to communicate with the directory servers. Directory access protocols\nalso deﬁne a data model and access control.\nThe X.500 directory access protocol, deﬁned by the International Organization for\nStandardization (ISO), is a standard for accessing directory information. However,\nthe protocol is rather complex, and is not widely used. The Lightweight Directory\nAccess Protocol (LDAP) provides many of the X.500 features, but with less complex-\nity, and is widely used. In the rest of this section, we shall outline the data model and\naccess protocol details of LDAP.\n19.9.2.1\nLDAP Data Model\nIn LDAP directories store entries, which are similar to objects. Each entry must have a\ndistinguished name (DN), which uniquely identiﬁes the entry. A DN is in turn made\nup of a sequence of relative distinguished names (RDNs). For example, an entry may\nhave the following distinguished name.\ncn=Silberschatz, ou=Bell Labs, o=Lucent, c=USA\nAs you can see, the distinguished name in this example is a combination of a name\nand (organizational) address, starting with a person’s name, then giving the orga-\nnizational unit (ou), the organization (o), and country (c). The order of the compo-\nnents of a distinguished name reﬂects the normal postal address order, rather than\nthe reverse order used in specifying path names for ﬁles. The set of RDNs for a DN is\ndeﬁned by the schema of the directory system.\nEntries can also have attributes. LDAP provides binary, string, and time types, and\nadditionally the types tel for telephone numbers, and PostalAddress for addresses\n(lines separated by a “$” character). Unlike those in the relational model, attributes\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nVI. Database System \nArchitecture\n19. Distributed Databases\n739\n© The McGraw−Hill \nCompanies, 2001\n19.9\nDirectory Systems\n743\nare multivalued by default, so it is possible to store multiple telephone numbers or\naddresses for an entry.\nLDAP allows the deﬁnition of object classes with attribute names and types. In-\nheritance can be used in deﬁning object classes. Moreover, entries can be speciﬁed to\nbe of one or more object classes. It is not necessary that there be a single most-speciﬁc\nobject class to which an entry belongs.\nEntries are organized into a directory information tree (DIT), according to their\ndistinguished names. Entries at the leaf level of the tree usually represent speciﬁc\nobjects. Entries that are internal nodes represent objects such as organizational units,\norganizations, or countries. The children of a node have a DN containing all the RDNs\nof the parent, and one or more additional RDNs. For instance, an internal node may\nhave a DN c=USA, and all entries below it have the value USA for the RDN c.\nThe entire distinguished name need not be stored in an entry; The system can\ngenerate the distinguished name of an entry by traversing up the DIT from the entry,\ncollecting the RDN=value components to create the full distinguished name.\nEntries may have more than one distinguished name—for example, an entry for a\nperson in more than one organization. To deal with such cases, the leaf level of a DIT\ncan be an alias, which points to an entry in another branch of the tree.\n19.9.2.2\nData Manipulation\nUnlike SQL, LDAP does not deﬁne either a data-deﬁnition language or a data manip-\nulation language. However, LDAP deﬁnes a network protocol for carrying out data\ndeﬁnition and manipulation. Users of LDAP can either use an application program-\nming interface, or use tools provided by various vendors to perform data deﬁnition\nand manipulation. LDAP also deﬁnes a ﬁle format called LDAP Data Interchange\nFormat (LDIF) that can be used for storing and exchanging information.\nThe querying mechanism in LDAP is very simple, consisting of just selections and\nprojections, without any join. A query must specify the following:\n• A base—that is, a node within a DIT—by giving its distinguished name (the\npath from the root to the node).\n• A search condition, which can be a Boolean combination of conditions on in-\ndividual attributes. Equality, matching by wild-card characters, and approxi-\nmate equality (the exact deﬁnition of approximate equality is system depen-\ndent) are supported.\n• A scope, which can be just the base, the base and its children, or the entire\nsubtree beneath the base.\n• Attributes to return.\n• Limits on number of results and resource consumption.\nThe query can also specify whether to automatically dereference aliases; if alias deref-\nerences are turned off, alias entries can be returned as answers.\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nVI. Database System \nArchitecture\n19. Distributed Databases\n740\n© The McGraw−Hill \nCompanies, 2001\n744\nChapter 19\nDistributed Databases\nOne way of querying an LDAP data source is by using LDAP URLs. Examples of\nLDAP URLs are:\nldap:://aura.research.bell-labs.com/o=Lucent,c=USA\nldap:://aura.research.bell-labs.com/o=Lucent,c=USA??sub?cn=Korth\nThe ﬁrst URL returns all attributes of all entries at the server with organization being\nLucent, and country being USA. The second URL executes a search query (selection)\ncn=Korth on the subtree of the node with distinguished name o=Lucent, c=USA. The\nquestion marks in the URL separate different ﬁelds. The ﬁrst ﬁeld is the distinguished\nname, here o=Lucent,c=USA. The second ﬁeld, the list of attributes to return, is left\nempty, meaning return all attributes. The third attribute, sub, indicates that the entire\nsubtree is to be searched. The last parameter is the search condition.\nA second way of querying an LDAP directory is by using an application program-\nming interface. Figure 19.6 shows a piece of C code used to connect to an LDAP server\nand run a query against the server. The code ﬁrst opens a connection to an LDAP\nserver by ldap open and ldap bind. It then executes a query by ldap search s. The\narguments to ldap search s are the LDAP connection handle, the DN of the base from\nwhich the search should be done, the scope of the search, the search condition, the\nlist of attributes to be returned, and an attribute called attrsonly, which, if set to 1,\nwould result in only the schema of the result being returned, without any actual tu-\nples. The last argument is an output argument that returns the result of the search as\nan LDAPMessage structure.\nThe ﬁrst for loop iterates over and prints each entry in the result. Note that an\nentry may have multiple attributes, and the second for loop prints each attribute.\nSince attributes in LDAP may be multivalued, the third for loop prints each value of\nan attribute. The calls ldap msgfree and ldap value free free memory that is allocated\nby the LDAP libraries. Figure 19.6 does not show code for handling error conditions.\nThe LDAP API also contains functions to create, update, and delete entries, as well\nas other operations on the DIT. Each function call behaves like a separate transaction;\nLDAP does not support atomicity of multiple updates.\n19.9.2.3\nDistributed Directory Trees\nInformation about an organization may be split into multiple DITs, each of which\nstores information about some entries. The sufﬁx of a DIT is a sequence of RDN=value\npairs that identify what information the DIT stores; the pairs are concatenated to the\nrest of the distinguished name generated by traversing from the entry to the root.\nFor instance, the sufﬁx of a DIT may be o=Lucent, c=USA, while another may have\nthe sufﬁx o=Lucent, c=India. The DITs may be organizationally and geographically\nseparated.\nA node in a DIT may contain a referral to another node in another DIT; for in-\nstance, the organizational unit Bell Labs under o=Lucent, c=USA may have its own\nDIT, in which case the DIT for o=Lucent, c=USA would have a node ou=Bell Labs\nrepresenting a referral to the DIT for Bell Labs.\nReferrals are the key component that help organize a distributed collection of di-\nrectories into an integrated system. When a server gets a query on a DIT, it may\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nVI. Database System \nArchitecture\n19. Distributed Databases\n741\n© The McGraw−Hill \nCompanies, 2001\n19.9\nDirectory Systems\n745\n#include <stdio.h>\n#include <ldap.h>\nmain() {\nLDAP *ld;\nLDAPMessage *res, *entry;\nchar *dn, *attr, *attrList[] = {“telephoneNumber”, NULL};\nBerElement *ptr;\nint vals, i;\nld = ldap open(“aura.research.bell-labs.com”, LDAP PORT);\nldap simple bind(ld, “avi”, “avi-passwd”) ;\nldap search s(ld, “o=Lucent, c=USA”, LDAP SCOPE SUBTREE, “cn=Korth”,\nattrList, /*attrsonly*/ 0, &res);\nprintf(“found %d entries”, ldap count entries(ld, res));\nfor (entry=ldap ﬁrst entry(ld, res); entry != NULL;\nentry = ldap next entry(ld, entry)\n{\ndn = ldap get dn(ld, entry);\nprintf(“dn: %s”, dn);\nldap memfree(dn);\nfor (attr = ldap ﬁrst attribute(ld, entry, &ptr);\nattr ! NULL;\nattr = ldap next attribute(ld, entry, ptr))\n{\nprintf(“%s: ”, attr);\nvals = ldap get values(ld, entry, attr);\nfor (i=0; vals[i] != NULL; i++)\nprintf(“%s, ”, vals[i]);\nldap value free(vals);\n}\n}\nldap msgfree(res);\nldap unbind(ld);\n}\nFigure 19.6\nExample of LDAP code in C.\nreturn a referral to the client, which then issues a query on the referenced DIT. Ac-\ncess to the referenced DIT is transparent, proceeding without the user’s knowledge.\nAlternatively, the server itself may issue the query to the referred DIT and return the\nresults along with locally computed results.\nThe hierarchical naming mechanism used by LDAP helps break up control of in-\nformation across parts of an organization. The referral facility then helps integrate all\nthe directories in an organization into a single virtual directory.\nAlthough it is not an LDAP requirement, organizations often choose to break up\ninformation either by geography (for instance, an organization may maintain a direc-\ntory for each site where the organization has a large presence) or by organizational\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nVI. Database System \nArchitecture\n19. Distributed Databases\n742\n© The McGraw−Hill \nCompanies, 2001\n746\nChapter 19\nDistributed Databases\nstructure (for instance, each organizational unit, such as department, maintains its\nown directory).\nMany LDAP implementations support master–slave and multimaster replication\nof DITs, although replication is not part of the current LDAP version 3 standard. Work\non standardizing replication in LDAP is in progress.\n19.10\nSummary\n• A distributed database system consists of a collection of sites, each of which\nmaintains a local database system. Each site is able to process local transac-\ntions: those transactions that access data in only that single site. In addition, a\nsite may participate in the execution of global transactions; those transactions\nthat access data in several sites. The execution of global transactions requires\ncommunication among the sites.\n• Distributed databases may be homogeneous, where all sites have a common\nschema and database system code, or heterogeneous, where the schemas and\nsystem codes may differ.\n• There are several issues involved in storing a relation in the distributed data-\nbase, including replication and fragmentation. It is essential that the system\nminimize the degree to which a user needs to be aware of how a relation is\nstored.\n• A distributed system may suffer from the same types of failure that can afﬂict\na centralized system. There are, however, additional failures with which we\nneed to deal in a distributed environment, including the failure of a site, the\nfailure of a link, loss of a message, and network partition. Each of these prob-\nlems needs to be considered in the design of a distributed recovery scheme.\n• To ensure atomicity, all the sites in which a transaction T executed must agree\non the ﬁnal outcome of the execution. T either commits at all sites or aborts at\nall sites. To ensure this property, the transaction coordinator of T must execute\na commit protocol. The most widely used commit protocol is the two-phase\ncommit protocol.\n• The two-phase commit protocol may lead to blocking, the situation in which\nthe fate of a transaction cannot be determined until a failed site (the coordi-\nnator) recovers. We can use the three-phase commit protocol to reduce the\nprobability of blocking.\n• Persistent messaging provides an alternative model for handling distributed\ntransactions. The model breaks a single transaction into parts that are exe-\ncuted at different databases. Persistent messages (which are guaranteed to be\ndelivered exactly once, regardless of failures), are sent to remote sites to re-\nquest actions to be taken there. While persistent messaging avoids the block-\ning problem, application developers have to write code to handle various\ntypes of failures.\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nVI. Database System \nArchitecture\n19. Distributed Databases\n743\n© The McGraw−Hill \nCompanies, 2001\n19.10\nSummary\n747\n• The various concurrency-control schemes used in a centralized system can be\nmodiﬁed for use in a distributed environment.\n\u0000 In the case of locking protocols, the only change that needs to be incor-\nporated is in the way that the lo",
    "precision": 0.16666666666666666,
    "recall": 0.9872122762148338,
    "iou": 0.16630762602326582,
    "f1": 0.2851865533801256,
    "gold_tokens_count": 391,
    "retrieved_tokens_count": 2316,
    "intersection_tokens": 386
  },
  {
    "config_name": "chars_rrf",
    "config": {
      "name": "chars_rrf",
      "index_prefix": "textbook_index",
      "chunking_strategy": "chars",
      "overlap": 0,
      "fusion": "rrf",
      "bm25_weight": 0.3,
      "tag_weight": 0.2,
      "top_k": 5,
      "embed_model": "sentence-transformers/all-MiniLM-L6-v2"
    },
    "question": "List four significant differences between a file-processing system and a DBMS.",
    "gold_text": "1.2 Database Systems versus File Systems Consider part of a savings-bank enterprise that keeps information about all cus- tomers and savings accounts. One way to keep the information on a computer is to store it in operating system files. To allow users to manipulate the information, the system has a number of application programs that manipulate the files, including • A program to debit or credit an account • A program to add a new account • A program to find the balance of an account • A program to generate monthly statements System programmers wrote these application programs to meet the needs of the bank. New application programs are added to the system as the need arises. For exam- ple, suppose that the savings bank decides to offer checking accounts. As a result, the bank creates new permanent files that contain information about all the checking accounts maintained in the bank, and it may have to write new application programs to deal with situations that do not arise in savings accounts, such as overdrafts. Thus, as time goes by, the system acquires more files and more application programs. This typical file-processing system is supported by a conventional operating sys- tem. The system stores permanent records in various files, and it needs different application programs to extract records from, and add records to, the appropriate files. Before database management systems (DBMSs) came along, organizations usu- ally stored information in such systems. Keeping organizational information in a file-processing system has a number of major disadvantages: • Data redundancy and inconsistency. Since different programmers create the files and application programs over a long period, the various files are likely to have different formats and the programs may be written in several pro- gramming languages. Moreover, the same information may be duplicated in several places (files). For example, the address and telephone number of a par- ticular customer may appear in a file that consists of savings-account records and in a file that consists of checking-account records. This redundancy leads to higher storage and access cost. In addition, it may lead to data inconsis- tency; that is, the various copies of the same data may no longer agree. For example, a changed customer address may be reflected in savings-account records but not elsewhere in the system. • Difficulty in accessing data. Suppose that one of the bank officers needs to find out the names of all customers who live within a particular postal-code area. The officer asks the data-processing department to generate such a list. Because the designers of the original system did not anticipate this request, there is no application program on hand to meet it. There is, however, an ap- plication program to generate the list of all customers. The bank officer hasnow two choices: either obtain the list of all customers and extract the needed information manually or ask a system programmer to write the necessary application program. Both alternatives are obviously unsatisfactory. Suppose that such a program is written, and that, several days later, the same officer needs to trim that list to include only those customers who have an account balance of $10,000 or more. As expected, a program to generate such a list does not exist. Again, the officer has the preceding two options, neither of which is satisfactory. The point here is that conventional file-processing environments do not al- low needed data to be retrieved in a convenient and efficient manner. More responsive data-retrieval systems are required for general use. • Data isolation. Because data are scattered in various files, and files may be in different formats, writing new application programs to retrieve the appropri- ate data is difficult. • Integrity problems. The data values stored in the database must satisfy cer- tain types of consistency constraints. For example, the balance of a bank ac- count may never fall below a prescribed amount (say, $25). Developers enforce these constraints in the system by adding appropriate code in the various ap- plication programs. However, when new constraints are added, it is difficult to change the programs to enforce them. The problem is compounded when constraints involve several data items from different files. • Atomicity problems. A computer system, like any other mechanical or elec- trical device, is subject to failure. In many applications, it is crucial that, if a failure occurs, the data be restored to the consistent state that existed prior to the failure. Consider a program to transfer $50 from account A to account B. If a system failure occurs during the execution of the program, it is possible that the $50 was removed from account A but was not credited to account B, resulting in an inconsistent database state. Clearly, it is essential to database consistency that either both the credit and debit occur, or that neither occur. That is, the funds transfer must be atomic—it must happen in its entirety or not at all. It is difficult to ensure atomicity in a conventional file-processing system. • Concurrent-access anomalies. For the sake of overall performance of the sys- tem and faster response, many systems allow multiple users to update the data simultaneously. In such an environment, interaction of concurrent up- dates may result in inconsistent data. Consider bank account A, containing $500. If two customers withdraw funds (say $50 and $100 respectively) from account A at about the same time, the result of the concurrent executions may leave the account in an incorrect (or inconsistent) state. Suppose that the pro- grams executing on behalf of each withdrawal read the old balance, reduce that value by the amount being withdrawn, and write the result back. If the two programs run concurrently, they may both read the value $500, and write back $450 and $400, respectively. Depending on which one writes the valuelast, the account may contain either $450 or $400, rather than the correct value of $350. To guard against this possibility, the system must maintain some form of supervision. But supervision is difficult to provide because data may be accessed by many different application programs that have not been coordi- nated previously. • Security problems. Not every user of the database system should be able to access all the data. For example, in a banking system, payroll personnel need to see only that part of the database that has information about the various bank employees. They do not need access to information about customer ac- counts. But, since application programs are added to the system in an ad hoc manner, enforcing such security constraints is difficult. These difficulties, among others, prompted the development of database systems. In what follows, we shall see the concepts and algorithms that enable database sys- tems to solve the problems with file-processing systems. In most of this book, we use a bank enterprise as a running example of a typical data-processing application found in a corporation.",
    "retrieved_text": "For Evaluation Only.\ndddddd\nFor Evaluation Only.\nCopyright (c) by Foxit Software Company, 2004\nEdited by Foxit PDF Editor\nFor Evaluation Only.\nCopyright (c) by Foxit Software Company, 2004\nEdited by Foxit PDF Editor\nComputer \nScience\nVolume 1\nSilberschatz−Korth−Sudarshan  •  Database System Concepts, Fourth Edition  \nFront Matter \n1\nPreface \n1\n1. Introduction \n11\nText \n11\nI. Data Models \n35\nIntroduction \n35\n2. Entity−Relationship Model \n36\n3. Relational Model \n87\nII. Relational Databases \n140\nIntroduction \n140\n4. SQL \n141\n5. Other Relational Languages \n194\n6. Integrity and Security \n229\n7. Relational−Database Design \n260\nIII. Object−Based Databases and XML \n307\nIntroduction \n307\n8. Object−Oriented Databases \n308\n9. Object−Relational Databases \n337\n10. XML \n363\nIV. Data Storage and Querying \n393\nIntroduction \n393\n11. Storage and File Structure \n394\n12. Indexing and Hashing \n446\n13. Query Processing \n494\n14. Query Optimization \n529\nV. Transaction Management \n563\nIntroduction \n563\n15. Transactions \n564\n16. Concurrency Control \n590\n17. Recovery System \n637\niii\nVI. Database System Architecture \n679\nIntroduction \n679\n18. Database System Architecture \n680\n19. Distributed Databases \n705\n20. Parallel Databases \n750\nVII. Other Topics \n773\nIntroduction \n773\n21. Application Development and Administration \n774\n22. Advanced Querying and Information Retrieval \n810\n23. Advanced Data Types and New Applications \n856\n24. Advanced Transaction Processing \n884\niv\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nFront Matter\nPreface\n1\n© The McGraw−Hill \nCompanies, 2001\nPreface\nDatabase management has evolved from a specialized computer application to a\ncentral component of a modern computing environment, and, as a result, knowl-\nedge about database systems has become an essential part of an education in com-\nputer science. In this text, we present the fundamental concepts of database manage-\nment. These concepts include aspects of database design, database languages, and\ndatabase-system implementation.\nThis text is intended for a ﬁrst course in databases at the junior or senior under-\ngraduate, or ﬁrst-year graduate, level. In addition to basic material for a ﬁrst course,\nthe text contains advanced material that can be used for course supplements, or as\nintroductory material for an advanced course.\nWe assume only a familiarity with basic data structures, computer organization,\nand a high-level programming language such as Java, C, or Pascal. We present con-\ncepts as intuitive descriptions, many of which are based on our running example of\na bank enterprise. Important theoretical results are covered, but formal proofs are\nomitted. The bibliographical notes contain pointers to research papers in which re-\nsults were ﬁrst presented and proved, as well as references to material for further\nreading. In place of proofs, ﬁgures and examples are used to suggest why a result is\ntrue.\nThe fundamental concepts and algorithms covered in the book are often based\non those used in existing commercial or experimental database systems. Our aim is\nto present these concepts and algorithms in a general setting that is not tied to one\nparticular database system. Details of particular commercial database systems are\ndiscussed in Part 8, “Case Studies.”\nIn this fourth edition of Database System Concepts, we have retained the overall style\nof the ﬁrst three editions, while addressing the evolution of database management.\nSeveral new chapters have been added to cover new technologies. Every chapter has\nbeen edited, and most have been modiﬁed extensively. We shall describe the changes\nin detail shortly.\nxv\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nFront Matter\nPreface\n2\n© The McGraw−Hill \nCompanies, 2001\nxvi\nPreface\nOrganization\nThe text is organized in eight major parts, plus three appendices:\n• Overview (Chapter 1). Chapter 1 provides a general overview of the nature\nand purpose of database systems. We explain how the concept of a database\nsystem has developed, what the common features of database systems are,\nwhat a database system does for the user, and how a database system inter-\nfaces with operating systems. We also introduce an example database applica-\ntion: a banking enterprise consisting of multiple bank branches. This example\nis used as a running example throughout the book. This chapter is motiva-\ntional, historical, and explanatory in nature.\n• Data models (Chapters 2 and 3). Chapter 2 presents the entity-relationship\nmodel. This model provides a high-level view of the issues in database design,\nand of the problems that we encounter in capturing the semantics of realistic\napplications within the constraints of a data model. Chapter 3 focuses on the\nrelational data model, covering the relevant relational algebra and relational\ncalculus.\n• Relational databases (Chapters 4 through 7). Chapter 4 focuses on the most\ninﬂuential of the user-oriented relational languages: SQL. Chapter 5 covers\ntwo other relational languages, QBE and Datalog. These two chapters describe\ndata manipulation: queries, updates, insertions, and deletions. Algorithms\nand design issues are deferred to later chapters. Thus, these chapters are suit-\nable for introductory courses or those individuals who want to learn the basics\nof database systems, without getting into the details of the internal algorithms\nand structure.\nChapter 6 presents constraints from the standpoint of database integrity\nand security; Chapter 7 shows how constraints can be used in the design of\na relational database. Referential integrity; mechanisms for integrity mainte-\nnance, such as triggers and assertions; and authorization mechanisms are pre-\nsented in Chapter 6. The theme of this chapter is the protection of the database\nfrom accidental and intentional damage.\nChapter 7 introduces the theory of relational database design. The theory\nof functional dependencies and normalization is covered, with emphasis on\nthe motivation and intuitive understanding of each normal form. The overall\nprocess of database design is also described in detail.\n• Object-based databases and XML (Chapters 8 through 10). Chapter 8 covers\nobject-oriented databases. It introduces the concepts of object-oriented pro-\ngramming, and shows how these concepts form the basis for a data model.\nNo prior knowledge of object-oriented languages is assumed. Chapter 9 cov-\ners object-relational databases, and shows how the SQL:1999 standard extends\nthe relational data model to include object-oriented features, such as inheri-\ntance, complex types, and object identity.\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nFront Matter\nPreface\n3\n© The McGraw−Hill \nCompanies, 2001\nPreface\nxvii\nChapter 10 covers the XML standard for data representation, which is see-\ning increasing use in data communication and in the storage of complex data\ntypes. The chapter also describes query languages for XML.\n• Data storage and querying (Chapters 11 through 14). Chapter 11 deals with\ndisk, ﬁle, and ﬁle-system structure, and with the mapping of relational and\nobject data to a ﬁle system. A variety of data-access techniques are presented\nin Chapter 12, including hashing, B+-tree indices, and grid ﬁle indices. Chap-\nters 13 and 14 address query-evaluation algorithms, and query optimization\nbased on equivalence-preserving query transformations.\nThese chapters provide an understanding of the internals of the storage and\nretrieval components of a database.\n• Transaction management (Chapters 15 through 17). Chapter 15 focuses on\nthe fundamentals of a transaction-processing system, including transaction\natomicity, consistency, isolation, and durability, as well as the notion of serial-\nizability.\nChapter 16 focuses on concurrency control and presents several techniques\nfor ensuring serializability, including locking, timestamping, and optimistic\n(validation) techniques. The chapter also covers deadlock issues. Chapter 17\ncovers the primary techniques for ensuring correct transaction execution de-\nspite system crashes and disk failures. These techniques include logs, shadow\npages, checkpoints, and database dumps.\n• Database system architecture (Chapters 18 through 20). Chapter 18 covers\ncomputer-system architecture, and describes the inﬂuence of the underlying\ncomputer system on the database system. We discuss centralized systems,\nclient–server systems, parallel and distributed architectures, and network\ntypes in this chapter. Chapter 19 covers distributed database systems, revis-\niting the issues of database design, transaction management, and query eval-\nuation and optimization, in the context of distributed databases. The chap-\nter also covers issues of system availability during failures and describes the\nLDAP directory system.\nChapter 20, on parallel databases explores a variety of parallelization tech-\nniques, including I/O parallelism, interquery and intraquery parallelism, and\ninteroperation and intraoperation parallelism. The chapter also describes\nparallel-system design.\n• Other topics (Chapters 21 through 24). Chapter 21 covers database appli-\ncation development and administration. Topics include database interfaces,\nparticularly Web interfaces, performance tuning, performance benchmarks,\nstandardization, and database issues in e-commerce. Chapter 22 covers query-\ning techniques, including decision support systems, and information retrieval.\nTopics covered in the area of decision support include online analytical pro-\ncessing (OLAP) techniques, SQL:1999 support for OLAP, data mining, and data\nwarehousing. The chapter also describes information retrieval techniques for\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nFront Matter\nPreface\n4\n© The McGraw−Hill \nCompanies, 2001\nxviii\nPreface\nquerying textual data, including hyperlink-based techniques used in Web\nsearch engines.\nChapter 23 covers advanced data types and new applications, including\ntemporal data, spatial and geographic data, multimedia data, and issues in the\nmanagement of mobile and personal databases. Finally, Chapter 24 deals with\nadvanced transaction processing. We discuss transaction-processing monitors,\nhigh-performance transaction systems, real-time transaction systems, and\ntransactional workﬂows.\n• Case studies (Chapters 25 through 27). In this part we present case studies of\nthree leading commercial database systems, including Oracle, IBM DB2, and\nMicrosoft SQL Server. These chapters outline unique features of each of these\nproducts, and describe their internal structure. They provide a wealth of in-\nteresting information about the respective products, and help you see how the\nvarious implementation techniques described in earlier parts are used in real\nsystems. They also cover several interesting practical aspects in the design of\nreal systems.\n• Online appendices. Although most new database applications use either the\nrelational model or the object-oriented model, the network and hierarchical\ndata models are still in use. For the beneﬁt of readers who wish to learn about\nthese data models, we provide appendices describing the network and hier-\narchical data models, in Appendices A and B respectively; the appendices are\navailable only online (http://www.bell-labs.com/topic/books/db-book).\nAppendix C describes advanced relational database design, including the\ntheory of multivalued dependencies, join dependencies, and the project-join\nand domain-key normal forms. This appendix is for the beneﬁt of individuals\nwho wish to cover the theory of relational database design in more detail, and\ninstructors who wish to do so in their courses. This appendix, too, is available\nonly online, on the Web page of the book.\nThe Fourth Edition\nThe production of this fourth edition has been guided by the many comments and\nsuggestions we received concerning the earlier editions, by our own observations\nwhile teaching at IIT Bombay, and by our analysis of the directions in which database\ntechnology is evolving.\nOur basic procedure was to rewrite the material in each chapter, bringing the older\nmaterial up to date, adding discussions on recent developments in database technol-\nogy, and improving descriptions of topics that students found difﬁcult to understand.\nEach chapter now has a list of review terms, which can help you review key topics\ncovered in the chapter. We have also added a tools section at the end of most chap-\nters, which provide information on software tools related to the topic of the chapter.\nWe have also added new exercises, and updated references.\nWe have added a new chapter covering XML, and three case study chapters cov-\nering the leading commercial database systems, including Oracle, IBM DB2, and Mi-\ncrosoft SQL Server.\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nFront Matter\nPreface\n5\n© The McGraw−Hill \nCompanies, 2001\nPreface\nxix\nWe have organized the chapters into several parts, and reorganized the contents\nof several chapters. For the beneﬁt of those readers familiar with the third edition,\nwe explain the main changes here:\n• Entity-relationship model. We have improved our coverage of the entity-\nrelationship (E-R) model. More examples have been added, and some changed,\nto give better intuition to the reader. A summary of alternative E-R notations\nhas been added, along with a new section on UML.\n• Relational databases. Our coverage of SQL in Chapter 4 now references the\nSQL:1999 standard, which was approved after publication of the third edition.\nSQL coverage has been signiﬁcantly expanded to include the with clause, ex-\npanded coverage of embedded SQL, and coverage of ODBC and JDBC whose\nusage has increased greatly in the past few years. Coverage of Quel has been\ndropped from Chapter 5, since it is no longer in wide use. Coverage of QBE\nhas been revised to remove some ambiguities and to add coverage of the QBE\nversion used in the Microsoft Access database.\nChapter 6 now covers integrity constraints and security. Coverage of se-\ncurity has been moved to Chapter 6 from its third-edition position of Chap-\nter 19. Chapter 6 also covers triggers. Chapter 7 covers relational-database\ndesign and normal forms. Discussion of functional dependencies has been\nmoved into Chapter 7 from its third-edition position of Chapter 6. Chapter\n7 has been signiﬁcantly rewritten, providing several short-cut algorithms for\ndealing with functional dependencies and extended coverage of the overall\ndatabase design process. Axioms for multivalued dependency inference, PJNF\nand DKNF, have been moved into an appendix.\n• Object-based databases. Coverage of object orientation in Chapter 8 has been\nimproved, and the discussion of ODMG updated. Object-relational coverage in\nChapter 9 has been updated, and in particular the SQL:1999 standard replaces\nthe extended SQL used in the third edition.\n• XML. Chapter 10, covering XML, is a new chapter in the fourth edition.\n• Storage, indexing, and query processing. Coverage of storage and ﬁle struc-\ntures, in Chapter 11, has been updated; this chapter was Chapter 10 in the\nthird edition. Many characteristics of disk drives and other storage mecha-\nnisms have changed greatly in the past few years, and our coverage has been\ncorrespondingly updated. Coverage of RAID has been updated to reﬂect tech-\nnology trends. Coverage of data dictionaries (catalogs) has been extended.\nChapter 12, on indexing, now includes coverage of bitmap indices; this\nchapter was Chapter 11 in the third edition. The B+-tree insertion algorithm\nhas been simpliﬁed, and pseudocode has been provided for search. Parti-\ntioned hashing has been dropped, since it is not in signiﬁcant use.\nOur treatment of query processing has been reorganized, with the earlier\nchapter (Chapter 12 in the third edition) split into two chapters, one on query\nprocessing (Chapter 13) and another on query optimization (Chapter 14). All\ndetails regarding cost estimation and query optimization have been moved\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nFront Matter\nPreface\n6\n© The McGraw−Hill \nCompanies, 2001\nxx\nPreface\nto Chapter 14, allowing Chapter 13 to concentrate on query processing algo-\nrithms. We have dropped several detailed (and tedious) formulae for calcu-\nlating the exact number of I/O operations for different operations. Chapter 14\nnow has pseudocode for optimization algorithms, and new sections on opti-\nmization of nested subqueries and on materialized views.\n• Transaction processing. Chapter 15, which provides an introduction to trans-\nactions, has been updated; this chapter was numbered Chapter 13 in the third\nedition. Tests for view serializability have been dropped.\nChapter 16, on concurrency control, includes a new section on implemen-\ntation of lock managers, and a section on weak levels of consistency, which\nwas in Chapter 20 of the third edition. Concurrency control of index structures\nhas been expanded, providing details of the crabbing protocol, which is a sim-\npler alternative to the B-link protocol, and next-key locking to avoid the phan-\ntom problem. Chapter 17, on recovery, now includes coverage of the ARIES\nrecovery algorithm. This chapter also covers remote backup systems for pro-\nviding high availability despite failures, an increasingly important feature in\n“24 × 7” applications.\nAs in the third edition, instructors can choose between just introducing\ntransaction-processing concepts (by covering only Chapter 15), or offering de-\ntailed coverage (based on Chapters 15 through 17).\n• Database system architectures. Chapter 18, which provides an overview of\ndatabase system architectures, has been updated to cover current technology;\nthis was Chapter 16 in the third edition. The order of the parallel database\nchapter and the distributed database chapters has been ﬂipped. While the cov-\nerage of parallel database query processing techniques in Chapter 20\n(which was Chapter 16 in the third edition) is mainly of interest to those who\nwish to learn about database internals, distributed databases, now covered in\nChapter 19, is a topic that is more fundamental; it is one that anyone dealing\nwith databases should be familiar with.\nChapter 19 on distributed databases has been signiﬁcantly rewritten, to re-\nduce the emphasis on naming and transparency and to increase coverage of\noperation during failures, including concurrency control techniques to pro-\nvide high availability. Coverage of three-phase commit protocol has been ab-\nbreviated, as has distributed detection of global deadlocks, since neither is\nused much in practice. Coverage of query processing issues in heterogeneous\ndatabases has been moved up from Chapter 20 of the third edition. There is\na new section on directory systems, in particular LDAP, since these are quite\nwidely used as a mechanism for making information available in a distributed\nsetting.\n• Other topics. Although we have modiﬁed and updated the entire text, we\nconcentrated our presentation of material pertaining to ongoing database re-\nsearch and new database applications in four new chapters, from Chapter 21\nto Chapter 24.\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nFront Matter\nPreface\n7\n© The McGraw−Hill \nCompanies, 2001\nPreface\nxxi\nChapter 21 is new in the fourth edition and covers application develop-\nment and administration. The description of how to build Web interfaces to\ndatabases, including servlets and other mechanisms for server-side scripting,\nis new. The section on performance tuning, which was earlier in Chapter 19,\nhas new material on the famous 5-minute rule and the 1-minute rule, as well\nas some new examples. Coverage of materialized view selection is also new.\nCoverage of benchmarks and standards has been updated. There is a new sec-\ntion on e-commerce, focusing on database issues in e-commerce, and a new\nsection on dealing with legacy systems.\nChapter 22, which covers advanced querying and inform\n\nation retrieval,\nincludes new material on OLAP, particulary on SQL:1999 extensions for data\nanalysis. Coverage of data warehousing and data mining has also been ex-\ntended greatly. Coverage of information retrieval has been signiﬁcantly ex-\ntended, particulary in the area of Web searching. Earlier versions of this ma-\nterial were in Chapter 21 of the third edition.\nChapter 23, which covers advanced data types and new applications, has\nmaterial on temporal data, spatial data, multimedia data, and mobile data-\nbases. This material is an updated version of material that was in Chapter 21\nof the third edition. Chapter 24, which covers advanced transaction process-\ning, contains updated versions of sections on TP monitors, workﬂow systems,\nmain-memory and real-time databases, long-duration transactions, and trans-\naction management in multidatabases, which appeared in Chapter 20 of the\nthird edition.\n• Case studies. The case studies covering Oracle, IBM DB2 and Microsoft SQL\nServer are new to the fourth edition. These chapters outline unique features\nof each of these products, and describe their internal structure.\nInstructor’s Note\nThe book contains both basic and advanced material, which might not be covered in\na single semester. We have marked several sections as advanced, using the symbol\n“∗∗”. These sections may be omitted if so desired, without a loss of continuity.\nIt is possible to design courses by using various subsets of the chapters. We outline\nsome of the possibilities here:\n• Chapter 5 can be omitted if students will not be using QBE or Datalog as part\nof the course.\n• If object orientation is to be covered in a separate advanced course, Chapters\n8 and 9, and Section 11.9, can be omitted. Alternatively, they could constitute\nthe foundation of an advanced course in object databases.\n• Chapter 10 (XML) and Chapter 14 (query optimization) can be omitted from\nan introductory course.\n• Both our coverage of transaction processing (Chapters 15 through 17) and our\ncoverage of database-system architecture (Chapters 18 through 20) consist of\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nFront Matter\nPreface\n8\n© The McGraw−Hill \nCompanies, 2001\nxxii\nPreface\nan overview chapter (Chapters 15 and 18, respectively), followed by chap-\nters with details. You might choose to use Chapters 15 and 18, while omitting\nChapters 16, 17, 19, and 20, if you defer these latter chapters to an advanced\ncourse.\n• Chapters 21 through 24 are suitable for an advanced course or for self-study\nby students, although Section 21.1 may be covered in a ﬁrst database course.\nModel course syllabi, based on the text, can be found on the Web home page of the\nbook (see the following section).\nWeb Page and Teaching Supplements\nA Web home page for the book is available at the URL:\nhttp://www.bell-labs.com/topic/books/db-book\nThe Web page contains:\n• Slides covering all the chapters of the book\n• Answers to selected exercises\n• The three appendices\n• An up-to-date errata list\n• Supplementary material contributed by users of the book\nA complete solution manual will be made available only to faculty. For more infor-\nmation about how to get a copy of the solution manual, please send electronic mail to\ncustomer.service@mcgraw-hill.com. In the United States, you may call 800-338-3987.\nThe McGraw-Hill Web page for this book is\nhttp://www.mhhe.com/silberschatz\nContacting Us and Other Users\nWe provide a mailing list through which users of our book can communicate among\nthemselves and with us. If you wish to be on the list, please send a message to\ndb-book@research.bell-labs.com, include your name, afﬁliation, title, and electronic\nmail address.\nWe have endeavored to eliminate typos, bugs, and the like from the text. But, as in\nnew releases of software, bugs probably remain; an up-to-date errata list is accessible\nfrom the book’s home page. We would appreciate it if you would notify us of any\nerrors or omissions in the book that are not on the current list of errata.\nWe would be glad to receive suggestions on improvements to the books. We also\nwelcome any contributions to the book Web page that could be of use to other read-\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nFront Matter\nPreface\n9\n© The McGraw−Hill \nCompanies, 2001\nPreface\nxxiii\ners, such as programming exercises, project suggestions, online labs and tutorials,\nand teaching tips.\nE-mail should be addressed to db-book@research.bell-labs.com. Any other cor-\nrespondence should be sent to Avi Silberschatz, Bell Laboratories, Room 2T-310, 600\nMountain Avenue, Murray Hill, NJ 07974, USA.\nAcknowledgments\nThis edition has beneﬁted from the many useful comments provided to us by the\nnumerous students who have used the third edition. In addition, many people have\nwritten or spoken to us about the book, and have offered suggestions and comments.\nAlthough we cannot mention all these people here, we especially thank the following:\n• Phil Bernhard, Florida Institute of Technology; Eitan M. Gurari, The Ohio State\nUniversity; Irwin Levinstein, Old Dominion University; Ling Liu, Georgia In-\nstitute of Technology; Ami Motro, George Mason University; Bhagirath Nara-\nhari, Meral Ozsoyoglu, Case Western Reserve University; and Odinaldo Ro-\ndriguez, King’s College London; who served as reviewers of the book and\nwhose comments helped us greatly in formulating this fourth edition.\n• Soumen Chakrabarti, Sharad Mehrotra, Krithi Ramamritham, Mike Reiter,\nSunita Sarawagi, N. L. Sarda, and Dilys Thomas, for extensive and invaluable\nfeedback on several chapters of the book.\n• Phil Bohannon, for writing the ﬁrst draft of Chapter 10 describing XML.\n• Hakan Jakobsson (Oracle), Sriram Padmanabhan (IBM), and C´esar Galindo-\nLegaria, Goetz Graefe, Jos´e A. Blakeley, Kalen Delaney, Michael Rys, Michael\nZwilling, Sameet Agarwal, Thomas Casey (all of Microsoft) for writing the\nappendices describing the Oracle, IBM DB2, and Microsoft SQL Server database\nsystems.\n• Yuri Breitbart, for help with the distributed database chapter; Mike Reiter, for\nhelp with the security sections; and Jim Melton, for clariﬁcations on SQL:1999.\n• Marilyn Turnamian and Nandprasad Joshi, whose excellent secretarial assis-\ntance was essential for timely completion of this fourth edition.\nThe publisher was Betsy Jones. The senior developmental editor was Kelley\nButcher. The project manager was Jill Peter. The executive marketing manager was\nJohn Wannemacher. The cover illustrator was Paul Tumbaugh while the cover de-\nsigner was JoAnne Schopler. The freelance copyeditor was George Watson. The free-\nlance proofreader was Marie Zartman. The supplement producer was Jodi Banowetz.\nThe designer was Rick Noel. The freelance indexer was Tobiah Waldron.\nThis edition is based on the three previous editions, so we thank once again the\nmany people who helped us with the ﬁrst three editions, including R. B. Abhyankar,\nDon Batory, Haran Boral, Paul Bourgeois, Robert Brazile, Michael Carey, J. Edwards,\nChristos Faloutsos, Homma Farian, Alan Fekete, Shashi Gadia, Jim Gray, Le Gruen-\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nFront Matter\nPreface\n10\n© The McGraw−Hill \nCompanies, 2001\nxxiv\nPreface\nwald, Ron Hitchens, Yannis Ioannidis, Hyoung-Joo Kim, Won Kim, Henry Korth (fa-\nther of Henry F.), Carol Kroll, Gary Lindstrom, Dave Maier, Keith Marzullo, Fletcher\nMattox, Alberto Mendelzon, Hector Garcia-Molina, Ami Motro, Anil Nigam, Cyril\nOrji, Bruce Porter, Jim Peterson, K. V. Raghavan, Mark Roth, Marek Rusinkiewicz,\nS. Seshadri, Shashi Shekhar, Amit Sheth, Nandit Soparkar, Greg Speegle, and Mari-\nanne Winslett. Lyn Dupr´e copyedited the third edition and Sara Strandtman edited\nthe text of the third edition. Greg Speegle, Dawn Bezviner, and K. V. Raghavan helped\nus to prepare the instructor’s manual for earlier editions. The new cover is an evo-\nlution of the covers of the ﬁrst three editions; Marilyn Turnamian created an early\ndraft of the cover design for this edition. The idea of using ships as part of the cover\nconcept was originally suggested to us by Bruce Stephan.\nFinally, Sudarshan would like to acknowledge his wife, Sita, for her love and sup-\nport, two-year old son Madhur for his love, and mother, Indira, for her support. Hank\nwould like to acknowledge his wife, Joan, and his children, Abby and Joe, for their\nlove and understanding. Avi would like to acknowledge his wife Haya, and his son,\nAaron, for their patience and support during the revision of this book.\nA. S.\nH. F. K.\nS. S.\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\n1. Introduction\nText\n11\n© The McGraw−Hill \nCompanies, 2001\nC\nH\nA\nP\nT\nE\nR\n1\nIntroduction\nA database-management system (DBMS) is a collection of interrelated data and a\nset of programs to access those data. The collection of data, usually referred to as the\ndatabase, contains information relevant to an enterprise. The primary goal of a DBMS\nis to provide a way to store and retrieve database information that is both convenient\nand efﬁcient.\nDatabase systems are designed to manage large bodies of information. Manage-\nment of data involves both deﬁning structures for storage of information and pro-\nviding mechanisms for the manipulation of information. In addition, the database\nsystem must ensure the safety of the information stored, despite system crashes or\nattempts at unauthorized access. If data are to be shared among several users, the\nsystem must avoid possible anomalous results.\nBecause information is so important in most organizations, computer scientists\nhave developed a large body of concepts and techniques for managing data. These\nconcepts and technique form the focus of this book. This chapter brieﬂy introduces\nthe principles of database systems.\n1.1\nDatabase System Applications\nDatabases are widely used. Here are some representative applications:\n• Banking: For customer information, accounts, and loans, and banking transac-\ntions.\n• Airlines: For reservations and schedule information. Airlines were among the\nﬁrst to use databases in a geographically distributed manner—terminals sit-\nuated around the world accessed the central database system through phone\nlines and other data networks.\n• Universities: For student information, course registrations, and grades.\n1\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\n1. Introduction\nText\n12\n© The McGraw−Hill \nCompanies, 2001\n2\nChapter 1\nIntroduction\n• Credit card transactions: For purchases on credit cards and generation of month-\nly statements.\n• Telecommunication: For keeping records of calls made, generating monthly bills,\nmaintaining balances on prepaid calling cards, and storing information about\nthe communication networks.\n• Finance: For storing information about holdings, sales, and purchases of ﬁnan-\ncial instruments such as stocks and bonds.\n• Sales: For customer, product, and purchase information.\n• Manufacturing: For management of supply chain and for tracking production\nof items in factories, inventories of items in warehouses/stores, and orders for\nitems.\n• Human resources: For information about employees, salaries, payroll taxes and\nbeneﬁts, and for generation of paychecks.\nAs the list illustrates, databases form an essential part of almost all enterprises today.\nOver the course of the last four decades of the twentieth century, use of databases\ngrew in all enterprises. In the early days, very few people interacted directly with\ndatabase systems, although without realizing it they interacted with databases in-\ndirectly—through printed reports such as credit card statements, or through agents\nsuch as bank tellers and airline reservation agents. Then automated teller machines\ncame along and let users interact directly with databases. Phone interfaces to com-\nputers (interactive voice response systems) also allowed users to deal directly with\ndatabases—a caller could dial a number, and press phone keys to enter information\nor to select alternative options, to ﬁnd ﬂight arrival/departure times, for example, or\nto register for courses in a university.\nThe internet revolution of the late 1990s sharply increased direct user access to\ndatabases. Organizations converted many of their phone interfaces to databases into\nWeb interfaces, and made a variety of services and information available online. For\ninstance, when you access an online bookstore and browse a book or music collec-\ntion, you are accessing data stored in a database. When you enter an order online,\nyour order is stored in a database. When you access a bank Web site and retrieve\nyour bank balance and transaction information, the information is retrieved from the\nbank’s database system. When you access a Web site, information about you may be\nretrieved from a database, to select which advertisements should be shown to you.\nFurthermore, data about your Web accesses may be stored in a database.\nThus, although user interfaces hide details of access to a database, and most people\nare not even aware they are dealing with a database, accessing databases forms an\nessential part of almost everyone’s life today.\nThe importance of database systems can be judged in another way—today, data-\nbase system vendors like Oracle are among the largest software companies in the\nworld, and database systems form an important part of the product line of more\ndiversiﬁed companies like Microsoft and IBM.\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\n1. Introduction\nText\n13\n© The McGraw−Hill \nCompanies, 2001\n1.2\nDatabase Systems versus File Systems\n3\n1.2\nDatabase Systems versus File Systems\nConsider part of a savings-bank enterprise that keeps information about all cus-\ntomers and savings accounts. One way to keep the information on a computer is\nto store it in operating system ﬁles. To allow users to manipulate the information, the\nsystem has a number of application programs that manipulate the ﬁles, including\n• A program to debit or credit an account\n• A program to add a new account\n• A program to ﬁnd the balance of an account\n• A program to generate monthly statements\nSystem programmers wrote these application programs to meet the needs of the\nbank.\nNew application programs are added to the system as the need arises. For exam-\nple, suppose that the savings bank decides to offer checking accounts. As a result,\nthe bank creates new permanent ﬁles that contain information about all the checking\naccounts maintained in the bank, and it may have to write new application programs\nto deal with situations that do not arise in savings accounts, such as overdrafts. Thus,\nas time goes by, the system acquires more ﬁles and more application programs.\nThis typical ﬁle-processing system is supported by a conventional operating sys-\ntem. The system stores permanent records in various ﬁles, and it needs different\napplication programs to extract records from, and add records to, the appropriate\nﬁles. Before database management systems (DBMSs) came along, organizations usu-\nally stored information in such systems.\nKeeping organizational information in a ﬁle-processing system has a number of\nmajor disadvantages:\n• Data redundancy and inconsistency. Since different programmers create the\nﬁles and application programs over a long period, the various ﬁles are likely\nto have different formats and the programs may be written in several pro-\ngramming languages. Moreover, the same information may be duplicated in\nseveral places (ﬁles). For example, the address and telephone number of a par-\nticular customer may appear in a ﬁle that consists of savings-account records\nand in a ﬁle that consists of checking-account records. This redundancy leads\nto higher storage and access cost. In addition, it may lead to data inconsis-\ntency; that is, the various copies of the same data may no longer agree. For\nexample, a changed customer address may be reﬂected in savings-account\nrecords but not elsewhere in the system.\n• Difﬁculty in accessing data. Suppose that one of the bank ofﬁcers needs to\nﬁnd out the names of all customers who live within a particular postal-code\narea. The ofﬁcer asks the data-processing department to generate such a list.\nBecause the designers of the original system did not anticipate this request,\nthere is no application program on hand to meet it. There is, however, an ap-\nplication program to generate the list of all customers. The bank ofﬁcer has\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\n1. Introduction\nText\n14\n© The McGraw−Hill \nCompanies, 2001\n4\nChapter 1\nIntroduction\nnow two choices: either obtain the list of all customers and extract the needed\ninformation manually or ask a system programmer to write the necessary\napplication program. Both alternatives are obviously unsatisfactory. Suppose\nthat such a program is written, and that, several days later, the same ofﬁcer\nneeds to trim that list to include only those customers who have an account\nbalance of $10,000 or more. As expected, a program to generate such a list does\nnot exist. Again, the ofﬁcer has the preceding two options, neither of which is\nsatisfactory.\nThe point here is that conventional ﬁle-processing environments do not al-\nlow needed data to be retrieved in a convenient and efﬁcient manner. More\nresponsive data-retrieval systems are required for general use.\n• Data isolation. Because data are scattered in various ﬁles, and ﬁles may be in\ndifferent formats, writing new application programs to retrieve the appropri-\nate data is difﬁcult.\n• Integrity problems. The data values stored in the database must satisfy cer-\ntain types of consistency constraints. For example, the balance of a bank ac-\ncount may never fall below a prescribed amount (say, $25). Developers enforce\nthese constraints in the system by adding appropriate code in the various ap-\nplication programs. However, when new constraints are added, it is difﬁcult\nto change the programs to enforce them. The problem is compounded when\nconstraints involve several data items from different ﬁles.\n• Atomicity problems. A computer system, like any other mechanical or elec-\ntrical device, is subject to failure. In many applications, it is crucial that, if a\nfailure occurs, the data be restored to the consistent state that existed prior to\nthe failure. Consider a program to transfer $50 from account A to account B.\nIf a system failure occurs during the execution of the program, it is possible\nthat the $50 was removed from account A but was not credited to account B,\nresulting in an inconsistent database state. Clearly, it is essential to database\nconsistency that either both the credit and debit occur, or that neither occur.\nThat is, the funds transfer must be atomic—it must happen in its entirety or\nnot at all. It is difﬁcult to ensure atomicity in a conventional ﬁle-processing\nsystem.\n• Concurrent-access anomalies. For the sake of overall performance of the sys-\ntem and faster response, many systems allow multiple users to update the\ndata simultaneously. In such an environment, interaction of concurrent up-\ndates may result in inconsistent data. Consider bank account A, containing\n$500. If two customers withdraw funds (say $50 and $100 respectively) from\naccount A at about the same time, the result of the concurrent executions may\nleave the account in an incorrect (or inconsistent) state. Suppose that the pro-\ngrams executing on behalf of each withdrawal read the old balance, reduce\nthat value by the amount being withdrawn, and write the result back. If the\ntwo programs run concurrently, they may both read the value $500, and write\nback $450 and $400, respectively. Depending on which one writes the value\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\n1. Introduction\nText\n15\n© The McGraw−Hill \nCompanies, 2001\n1.3\nView of Data\n5\nlast, the account may contain either $450 or $400, rather than the correct value\nof $350. To guard against this possibili\n\n starting from the student appli-\ncation procedure.\nb. Indicate acceptable termination states, and which steps involve human in-\ntervention.\nc. Indicate possible errors (including deadline expiry) and how they are dealt\nwith.\nd. Study how much of the workﬂow has been automated at your university.\n24.4 Like database systems, workﬂow systems also require concurrency and recov-\nery management. List three reasons why we cannot simply apply a relational\ndatabase system using 2PL, physical undo logging, and 2PC.\n24.5 If the entire database ﬁts in main memory, do we still need a database system\nto manage the data? Explain your answer.\n24.6 Consider a main-memory database system recovering from a system crash.\nExplain the relative merits of\n• Loading the entire database back into main memory before resuming trans-\naction processing\n• Loading data as it is requested by transactions\n24.7 In the group-commit technique, how many transactions should be part of a\ngroup? Explain your answer.\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nVII. Other Topics\n24. Advanced Transaction \nProcessing\n910\n© The McGraw−Hill \nCompanies, 2001\nBibliographical Notes\n917\n24.8 Is a high-performance transaction system necessarily a real-time system? Why\nor why not?\n24.9 In a database system using write-ahead logging, what is the worst-case num-\nber of disk accesses required to read a data item? Explain why this presents a\nproblem to designers of real-time database systems.\n24.10 Explain why it may be impractical to require serializability for long-duration\ntransactions.\n24.11 Consider a multithreaded process that delivers messages from a durable queue\nof persistent messages. Different threads may run concurrently, attempting to\ndeliver different messages. In case of a delivery failure, the message must be\nrestored in the queue. Model the actions that each thread carries out as a mul-\ntilevel transaction, so that locks on the queue need not be held till a message is\ndelivered.\n24.12 Discuss the modiﬁcations that need to be made in each of the recovery schemes\ncovered in Chapter 17 if we allow nested transactions. Also, explain any differ-\nences that result if we allow multilevel transactions.\n24.13 What is the purpose of compensating transactions? Present two examples of\ntheir use.\n24.14 Consider a multidatabase system in which it is guaranteed that at most one\nglobal transaction is active at any time, and every local site ensures local seri-\nalizability.\na. Suggest ways in which the multidatabase system can ensure that there is\nat most one active global transaction at any time.\nb. Show by example that it is possible for a nonserializable global schedule\nto result despite the assumptions.\n24.15 Consider a multidatabase system in which every local site ensures local serial-\nizability, and all global transactions are read only.\na. Show by example that nonserializable executions may result in such a sys-\ntem.\nb. Show how you could use a ticket scheme to ensure global serializability.\nBibliographical Notes\nGray and Edwards [1995] provides an overview of TP monitor architectures; Gray\nand Reuter [1993] provides a detailed (and excellent) textbook description of tran-\nsaction-processing systems, including chapters on TP monitors. Our description of TP\nmonitors is modeled on these two sources. X/Open [1991] deﬁnes the X/Open XA\ninterface. Transaction processing in Tuxedo is described in Huffman [1993]. Wipﬂer\n[1987] is one of several texts on application development using CICS.\nFischer [2001] is a handbook on workﬂow systems. A reference model for work-\nﬂows, proposed by the Workﬂow Management Coalition, is presented in Hollinsworth\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nVII. Other Topics\n24. Advanced Transaction \nProcessing\n911\n© The McGraw−Hill \nCompanies, 2001\n918\nChapter 24\nAdvanced Transaction Processing\n[1994]. The Web site of the coalition is www.wfmc.org. Our description of workﬂows\nfollows the model of Rusinkiewicz and Sheth [1995].\nReuter [1989] presents ConTracts, a method for grouping transactions into multi-\ntransaction activities. Some issues related to workﬂows were addressed in the work\non long-running activities described by Dayal et al. [1990] and Dayal et al. [1991]. The\nauthors propose event–condition–action rules as a technique for specifying work-\nﬂows. Jin et al. [1993] describes workﬂow issues in telecommunication applications.\nGarcia-Molina and Salem [1992] provides an overview of main-memory databases.\nJagadish et al. [1993] describes a recovery algorithm designed for main-memory data-\nbases. A storage manager for main-memory databases is described in Jagadish et al.\n[1994].\nTransaction processing in real-time databases is discussed by Abbott and Garcia-\nMolina [1999] and Dayal et al. [1990]. Barclay et al. [1982] describes a real-time data-\nbase system used in a telecommunications switching system. Complexity and\ncorrectness issues in real-time databases are addressed by Korth et al. [1990b] and\nSoparkar et al. [1995]. Concurrency control and scheduling in real-time databases are\ndiscussed by Haritsa et al. [1990], Hong et al. [1993], and Pang et al. [1995]. Ozsoyoglu\nand Snodgrass [1995] is a survey of research in real-time and temporal databases.\nNested and multilevel transactions are presented by Lynch [1983], Moss [1982],\nMoss [1985], Lynch and Merritt [1986], Fekete et al. [1990b], Fekete et al. [1990a], Ko-\nrth and Speegle [1994], and Pu et al. [1988]. Theoretical aspects of multilevel transac-\ntions are presented in Lynch et al. [1988] and Weihl and Liskov [1990].\nSeveral extended-transaction models have been deﬁned including Sagas (Garcia-\nMolina and Salem [1987]), ACTA (Chrysanthis and Ramamritham [1994]), the Con-\nTract model (Wachter and Reuter [1992]), ARIES (Mohan et al. [1992] and Rothermel\nand Mohan [1989]), and the NT/PV model (Korth and Speegle [1994]).\nSplitting transactions to achieve higher performance is addressed in Shasha et al.\n[1995]. A model for concurrency in nested transactions systems is presented in Beeri\net al. [1989]. Relaxation of serializability is discussed in Garcia-Molina [1983] and\nSha et al. [1988]. Recovery in nested transaction systems is discussed by Moss [1987],\nHaerder and Rothermel [1987], Rothermel and Mohan [1989]. Multilevel transaction\nmanagement is discussed in Weikum [1991].\nGray [1981], Skarra and Zdonik [1989], Korth and Speegle [1988], and Korth and\nSpeegle [1990] discuss long-duration transactions. Transaction processing for\nlong-duration transactions is considered by Weikum and Schek [1984], Haerder and\nRothermel [1987], Weikum et al. [1990], and Korth et al. [1990a]. Salem et al. [1994]\npresents an extension of 2PL for long-duration transactions by allowing the early\nrelease of locks under certain circumstances. Transaction processing in design and\nsoftware-engineering applications is discussed in Korth et al. [1988], Kaiser [1990],\nand Weikum [1991].\nTransaction processing in multidatabase systems is discussed in Breitbart et al.\n[1990], Breitbart et al. [1991], Breitbart et al. [1992], Soparkar et al. [1991], Mehrotra\net al. [1992b] and Mehrotra et al. [1992a]. The ticket scheme is presented in Geor-\ngakopoulos et al. [1994]. 2LSR is introduced in Mehrotra et al. [1991]. An earlier ap-\nproach, called quasi-serializability, is presented in Du and Elmagarmid [1989].\n\n\nrs at the base, and a shared-\nnothing architecture at the top, with possibly a shared-disk architecture in the mid-\ndle. Figure 18.8d illustrates a hierarchical architecture with shared-memory nodes\nconnected together in a shared-nothing architecture. Commercial parallel database\nsystems today run on several of these architectures.\nAttempts to reduce the complexity of programming such systems have yielded\ndistributed virtual-memory architectures, where logically there is a single shared\nmemory, but physically there are multiple disjoint memory systems; the virtual-\nmemory-mapping hardware, coupled with system software, allows each processor\nto view the disjoint memories as a single virtual memory. Since access speeds differ,\ndepending on whether the page is available locally or not, such an architecture is also\nreferred to as a nonuniform memory architecture (NUMA).\n18.4\nDistributed Systems\nIn a distributed database system, the database is stored on several computers. The\ncomputers in a distributed system communicate with one another through various\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nVI. Database System \nArchitecture\n18. Database System \nArchitecture\n695\n© The McGraw−Hill \nCompanies, 2001\n698\nChapter 18\nDatabase System Architectures\ncommunication media, such as high-speed networks or telephone lines. They do not\nshare main memory or disks. The computers in a distributed system may vary in size\nand function, ranging from workstations up to mainframe systems.\nThe computers in a distributed system are referred to by a number of different\nnames, such as sites or nodes, depending on the context in which they are mentioned.\nWe mainly use the term site, to emphasize the physical distribution of these systems.\nThe general structure of a distributed system appears in Figure 18.9.\nThe main differences between shared-nothing parallel databases and distributed\ndatabases are that distributed databases are typically geographically separated, are\nseparately administered, and have a slower interconnection. Another major differ-\nence is that, in a distributed database system, we differentiate between local and\nglobal transactions. A local transaction is one that accesses data only from sites\nwhere the transaction was initiated. A global transaction, on the other hand, is one\nthat either accesses data in a site different from the one at which the transaction was\ninitiated, or accesses data in several different sites.\nThere are several reasons for building distributed database systems, including\nsharing of data, autonomy, and availability.\n• Sharing data. The major advantage in building a distributed database system\nis the provision of an environment where users at one site may be able to\naccess the data residing at other sites. For instance, in a distributed banking\nsystem, where each branch stores data related to that branch, it is possible for\na user in one branch to access data in another branch. Without this capability,\na user wishing to transfer funds from one branch to another would have to\nresort to some external mechanism that would couple existing systems.\n• Autonomy. The primary advantage of sharing data by means of data distri-\nbution is that each site is able to retain a degree of control over data that\nsite A\nsite C\nsite B\ncommunication\nvia network\nnetwork\nFigure 18.9\nA distributed system.\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nVI. Database System \nArchitecture\n18. Database System \nArchitecture\n696\n© The McGraw−Hill \nCompanies, 2001\n18.4\nDistributed Systems\n699\nare stored locally. In a centralized system, the database administrator of the\ncentral site controls the database. In a distributed system, there is a global\ndatabase administrator responsible for the entire system. A part of these re-\nsponsibilities is delegated to the local database administrator for each site.\nDepending on the design of the distributed database system, each adminis-\ntrator may have a different degree of local autonomy. The possibility of local\nautonomy is often a major advantage of distributed databases.\n• Availability. If one site fails in a distributed system, the remaining sites may\nbe able to continue operating. In particular, if data items are replicated in sev-\neral sites, a transaction needing a particular data item may ﬁnd that item in\nany of several sites. Thus, the failure of a site does not necessarily imply the\nshutdown of the system.\nThe failure of one site must be detected by the system, and appropriate\naction may be needed to recover from the failure. The system must no longer\nuse the services of the failed site. Finally, when the failed site recovers or is\nrepaired, mechanisms must be available to integrate it smoothly back into the\nsystem.\nAlthough recovery from failure is more complex in distributed systems\nthan in centralized systems, the ability of most of the system to continue to\noperate despite the failure of one site results in increased availability. Avail-\nability is crucial for database systems used for real-time applications. Loss of\naccess to data by, for example, an airline may result in the loss of potential\nticket buyers to competitors.\n18.4.1\nAn Example of a Distributed Database\nConsider a banking system consisting of four branches in four different cities. Each\nbranch has its own computer, with a database of all the accounts maintained at that\nbranch. Each such installation is thus a site. There also exists one single site that\nmaintains information about all the branches of the bank. Each branch maintains\n(among others) a relation account(Account-schema), where\nAccount-schema = (account-number, branch-name, balance)\nThe site containing information about all the branches of the bank maintains the re-\nlation branch(Branch-schema), where\nBranch-schema = (branch-name, branch-city, assets)\nThere are other relations maintained at the various sites; we ignore them for the pur-\npose of our example.\nTo illustrate the difference between the two types of transactions—local and\nglobal—at the sites, consider a transaction to add $50 to account number A-177\nlocated at the Valleyview branch. If the transaction was initiated at the Valleyview\nbranch, then it is considered local; otherwise, it is considered global. A transaction\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nVI. Database System \nArchitecture\n18. Database System \nArchitecture\n697\n© The McGraw−Hill \nCompanies, 2001\n700\nChapter 18\nDatabase System Architectures\nto transfer $50 from account A-177 to account A-305, which is located at the Hillside\nbranch, is a global transaction, since accounts in two different sites are accessed as a\nresult of its execution.\nIn an ideal distributed database system, the sites would share a common global\nschema (although some relations may be stored only at some sites), all sites would\nrun the same distributed database-management software, and the sites would be\naware of each other’s existence. If a distributed database is built from scratch, it\nwould indeed be possible to achieve the above goals. However, in reality a dis-\ntributed database has to be constructed by linking together multiple already-existing\ndatabase systems, each with its own schema and possibly running different database-\nmanagement software. Such systems are sometimes called multidatabase systems\nor heterogeneous distributed database systems. We discuss these systems in Sec-\ntion 19.8, where we show how to achieve a degree of global control despite the het-\nerogeneity of the component systems.\n18.4.2\nImplementation Issues\nAtomicity of transactions is an important issue in building a distributed database sys-\ntem. If a transaction runs across two sites, unless the system designers are careful, it\nmay commit at one site and abort at another, leading to an inconsistent state. Trans-\naction commit protocols ensure such a situation cannot arise. The two-phase commit\nprotocol (2PC) is the most widely used of these protocols.\nThe basic idea behind 2PC is for each site to execute the transaction till just before\ncommit, and then leave the commit decision to a single coordinator site; the trans-\naction is said to be in the ready state at a site at this point. The coordinator decides\nto commit the transaction only if the transaction reaches the ready state at every site\nwhere it executed; otherwise (for example, if the transaction aborts at any site), the\ncoordinator decides to abort the transaction. Every site where the transaction exe-\ncuted must follow the decision of the coordinator. If a site fails when a transaction is\nin ready state, when the site recovers from failure it should be in a position to either\ncommit or abort the transaction, depending on the decision of the coordinator. The\n2PC protocol is described in detail in Section 19.4.1.\nConcurrency control is another issue in a distributed database. Since a transac-\ntion may access data items at several sites, transaction managers at several sites may\nneed to coordinate to implement concurrency control. If locking is used (as is almost\nalways the case in practice), locking can be performed locally at the sites containing\naccessed data items, but there is also a possibility of deadlock involving transactions\noriginating at multiple sites. Therefore deadlock detection needs to be carried out\nacross multiple sites. Failures are more common in distributed systems since not only\nmay computers fail, but communication links may also fail. Replication of data items,\nwhich is the key to the continued functioning of distributed databases when failures\noccur, further complicates concurrency control. Section 19.5 provides detailed cover-\nage of concurrency control in distributed databases.\nThe standard transaction models, based on multiple actions carried out by a single\nprogram unit, are often inappropriate for carrying out tasks that cross the boundaries\nof databases that cannot or will not cooperate to implement protocols such as 2PC.\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nVI. Database System \nArchitecture\n18. Database System \nArchitecture\n698\n© The McGraw−Hill \nCompanies, 2001\n18.5\nNetwork Types\n701\nAlternative approaches, based on persistent messaging for communication, are gener-\nally used for such tasks.\nWhen the tasks to be carried out are complex, involving multiple databases and/or\nmultiple interactions with humans, coordination of the tasks and ensuring transac-\ntion properties for the tasks become more complicated. Workﬂow management systems\nare systems designed to help with carrying out such tasks. Section 19.4.3 describes\npersistent messaging, while Section 24.2 describes workﬂow management systems.\nIn case an organization has to choose between a distributed architecture and a\ncentralized architecture for implementing an application, the system architect must\nbalance the advantages against the disadvantages of distribution of data. We have al-\nready seen the advantages of using distributed databases. The primary disadvantage\nof distributed database systems is the added complexity required to ensure proper\ncoordination among the sites. This increased complexity takes various forms:\n• Software-development cost. It is more difﬁcult to implement a distributed\ndatabase system; thus, it is more costly.\n• Greater potential for bugs. Since the sites that constitute the distributed sys-\ntem operate in parallel, it is harder to ensure the correctness of algorithms,\nespecially operation during failures of part of the system, and recovery from\nfailures. The potential exists for extremely subtle bugs.\n• Increased processing overhead. The exchange of messages and the additional\ncomputation required to achieve intersite coordination are a form of overhead\nthat does not arise in centralized systems.\nThere are several approaches to distributed database design, ranging from fully\ndistributed designs to ones that include a large degree of centralization. We study\nthem in Chapter 19.\n18.5\nNetwork Types\nDistributed databases and client–server systems are built around communication\nnetworks. There are basically two types of networks: local-area networks and wide-\narea networks. The main difference between the two is the way in which they are\ndistributed geographically. In local-area networks, processors are distributed over\nsmall geographical areas, such as a single building or a number of adjacent build-\nings. In wide-area networks, on the other hand, a number of autonomous processors\nare distributed over a large geographical area (such as the United States or the en-\ntire world). These differences imply major variations in the speed and reliability of\nthe communication network, and are reﬂected in the distributed operating-system\ndesign.\n18.5.1\nLocal-Area Networks\nLocal-area networks (LANs) (Figure 18.10) emerged in the early 1970s as a way\nfor computers to communicate and to share data with one another. People recog-\nnized that, for many enterprises, numerous small computers, each with its own self-\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nVI. Database System \nArchitecture\n18. Database System \nArchitecture\n699\n© The McGraw−Hill \nCompanies, 2001\n702\nChapter 18\nDatabase System Architectures\nworkstation\nprinter\nCPU server\nprocessors\nComputer-System Structures\nPC\nworkstation\nfile server\nprocessors\ngateway\nFigure 18.10\nLocal-area network.\ncontained applications, are more economical than a single large system. Because each\nsmall computer is likely to need access to a full complement of peripheral devices\n(such as disks and printers), and because some form of data sharing is likely to oc-\ncur in a single enterprise, it was a natural step to connect these small systems into a\nnetwork.\nLANs are generally used in an ofﬁce environment. All the sites in such systems\nare close to one another, so the communication links tend to have a higher speed and\nlower error rate than do their counterparts in wide-area networks. The most common\nlinks in a local-area network are twisted pair, coaxial cable, ﬁber optics, and, increas-\ningly, wireless connections. Communication speeds range from a few megabits per\nsecond (for wireless local-area networks), to 1 gigabit per second for Gigabit Ether-\nnet. Standard Ethernet runs at 10 megabits per second, while Fast Ethernet run at 100\nmegabits per second.\nA storage-area network (SAN) is a special type of high-speed local-area network\ndesigned to connect large banks of storage devices (disks) to computers that use the\ndata. Thus storage-area networks help build large-scale shared-disk systems. The moti-\nvation for using storage-area networks to connect multiple computers to large banks\nof storage devices is essentially the same as that for shared-disk databases, namely\n• Scalability by adding more computers\n• High availability, since data is still accessible even if a computer fails\nRAID organizations are used in the storage devices to ensure high availability of the\ndata, permitting processing to continue even if individual disks fail. Storage area\nnetworks are usually built with redundancy, such as multiple paths between nodes,\nso if a component such as a link or a connection to the network fails, the network\ncontinues to function.\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nVI. Database System \nArchitecture\n18. Database System \nArchitecture\n700\n© The McGraw−Hill \nCompanies, 2001\n18.6\nSummary\n703\n18.5.2\nWide-Area Networks\nWide-area networks (WANs) emerged in the late 1960s, mainly as an academic re-\nsearch project to provide efﬁcient communication among sites, allowing hardware\nand software to be shared conveniently and economically by a wide community of\nusers. Systems that allowed remote terminals to be connected to a central computer\nvia telephone lines were developed in the early 1960s, but they were not true WANs.\nThe ﬁrst WAN to be designed and developed was the Arpanet. Work on the Arpanet\nbegan in 1968. The Arpanet has grown from a four-site experimental network to a\nworldwide network of networks, the Internet, comprising hundreds of millions of\ncomputer systems. Typical links on the Internet are ﬁber-optic lines and, sometimes,\nsatellite channels. Data rates for wide-area links typically range from a few megabits\nper second to hundreds of gigabits per second. The last link, to end user sites, is of-\nten based on digital subscriber loop (DSL) technology supporting a few megabits per\nsecond), or cable modem (supporting 10 megabits per second), or dial-up modem\nconnections over phone lines (supporting up to 56 kilobits per second).\nWANs can be classiﬁed into two types:\n• In discontinuous connection WANs, such as those based on wireless connec-\ntions, hosts are connected to the network only part of the time.\n• In continuous connection WANs, such as the wired Internet, hosts are con-\nnected to the network at all times.\nNetworks that are not continuously connected typically do not allow transactions\nacross sites, but may keep local copies of remote data, and refresh the copies peri-\nodically (every night, for instance). For applications where consistency is not critical,\nsuch as sharing of documents, groupware systems such as Lotus Notes allow up-\ndates of remote data to be made locally, and the updates are then propagated back\nto the remote site periodically. There is a potential for conﬂicting updates at differ-\nent sites, conﬂicts that have to be detected and resolved. A mechanism for detecting\nconﬂicting updates is described later, in Section 23.5.4; the resolution mechanism for\nconﬂicting updates is, however, application dependent.\n18.6\nSummary\n• Centralized database systems run entirely on a single computer. With the\ngrowth of personal computers and local-area networking, the database front-\nend functionality has moved increasingly to clients, with server systems pro-\nviding the back-end functionality. Client–server interface protocols have\nhelped the growth of client–server database systems.\n• Servers can be either transaction servers or data servers, although the use\nof transaction servers greatly exceeds the use of data servers for providing\ndatabase services.\n\u0000 Transaction servers have multiple processes, possibly running on multiple\nprocessors. So that these processes have access to common data, such as\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nVI. Database System \nArchitecture\n18. Database System \nArchitecture\n701\n© The McGraw−Hill \nCompanies, 2001\n704\nChapter 18\nDatabase System Architectures\nthe database buffer, systems store such data in shared memory. In addition\nto processes that handle queries, there are system processes that carry out\ntasks such as lock and log management and checkpointing.\n\u0000 Data server systems supply raw data to clients. Such systems strive to\nminimize communication between clients and servers by caching data\nand locks at the clients. Parallel database systems use similar optimiza-\ntions.\n• Parallel database systems consist of multiple processors and multiple disks\nconnected by a fast interconnection network. Speedup measures how much\nwe can increase processing speed by increasing parallelism, for a single trans-\naction. Scaleup measures how well we can handle an increased number of\ntransactions by increasing parallelism. Interference, skew, and start–up costs\nact as barriers to getting ideal speedup and scaleup.\n• Parallel database architectures include the shared-memory, shared-disk,\nshared-nothing, and hierarchical architectures. These architectures have dif-\nferent tradeoffs of scalability versus communication speed.\n• A distributed database is a collection of partially independent databases that\n(ideally) share a common schema, and coordinate processing of transactions\nthat access nonlocal data. The processors communicate with one another thro-\nugh a communication network that handles routing and connection strategies.\n• Principally, there ar\n\ning programs were forced\nto process data in a particular order, by reading and merging data from tapes\nand card decks.\n• Late 1960s and 1970s: Widespread use of hard disks in the late 1960s changed\nthe scenario for data processing greatly, since hard disks allowed direct access\nto data. The position of data on disk was immaterial, since any location on disk\ncould be accessed in just tens of milliseconds. Data were thus freed from the\ntyranny of sequentiality. With disks, network and hierarchical databases could\nbe created that allowed data structures such as lists and trees to be stored on\ndisk. Programmers could construct and manipulate these data structures.\nA landmark paper by Codd [1970] deﬁned the relational model, and non-\nprocedural ways of querying data in the relational model, and relational\ndatabases were born. The simplicity of the relational model and the possibil-\nity of hiding implementation details completely from the programmer were\nenticing indeed. Codd later won the prestigious Association of Computing\nMachinery Turing Award for his work.\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\n1. Introduction\nText\n31\n© The McGraw−Hill \nCompanies, 2001\n1.11\nSummary\n21\n• 1980s: Although academically interesting, the relational model was not used\nin practice initially, because of its perceived performance disadvantages; re-\nlational databases could not match the performance of existing network and\nhierarchical databases. That changed with System R, a groundbreaking project\nat IBM Research that developed techniques for the construction of an efﬁcient\nrelational database system. Excellent overviews of System R are provided by\nAstrahan et al. [1976] and Chamberlin et al. [1981]. The fully functional Sys-\ntem R prototype led to IBM’s ﬁrst relational database product, SQL/DS. Initial\ncommercial relational database systems, such as IBM DB2, Oracle, Ingres, and\nDEC Rdb, played a major role in advancing techniques for efﬁcient process-\ning of declarative queries. By the early 1980s, relational databases had become\ncompetitive with network and hierarchical database systems even in the area\nof performance. Relational databases were so easy to use that they eventually\nreplaced network/hierarchical databases; programmers using such databases\nwere forced to deal with many low-level implementation details, and had to\ncode their queries in a procedural fashion. Most importantly, they had to keep\nefﬁciency in mind when designing their programs, which involved a lot of\neffort. In contrast, in a relational database, almost all these low-level tasks\nare carried out automatically by the database, leaving the programmer free to\nwork at a logical level. Since attaining dominance in the 1980s, the relational\nmodel has reigned supreme among data models.\nThe 1980s also saw much research on parallel and distributed databases, as\nwell as initial work on object-oriented databases.\n• Early 1990s: The SQL language was designed primarily for decision support\napplications, which are query intensive, yet the mainstay of databases in the\n1980s was transaction processing applications, which are update intensive.\nDecision support and querying re-emerged as a major application area for\ndatabases. Tools for analyzing large amounts of data saw large growths in\nusage.\nMany database vendors introduced parallel database products in this pe-\nriod. Database vendors also began to add object-relational support to their\ndatabases.\n• Late 1990s: The major event was the explosive growth of the World Wide Web.\nDatabases were deployed much more extensively than ever before. Database\nsystems now had to support very high transaction processing rates, as well as\nvery high reliability and 24×7 availability (availability 24 hours a day, 7 days a\nweek, meaning no downtime for scheduled maintenance activities). Database\nsystems also had to support Web interfaces to data.\n1.11\nSummary\n• A database-management system (DBMS) consists of a collection of interre-\nlated data and a collection of programs to access that data. The data describe\none particular enterprise.\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\n1. Introduction\nText\n32\n© The McGraw−Hill \nCompanies, 2001\n22\nChapter 1\nIntroduction\n• The primary goal of a DBMS is to provide an environment that is both conve-\nnient and efﬁcient for people to use in retrieving and storing information.\n• Database systems are ubiquitous today, and most people interact, either di-\nrectly or indirectly, with databases many times every day.\n• Database systems are designed to store large bodies of information. The man-\nagement of data involves both the deﬁnition of structures for the storage of\ninformation and the provision of mechanisms for the manipulation of infor-\nmation. In addition, the database system must provide for the safety of the\ninformation stored, in the face of system crashes or attempts at unauthorized\naccess. If data are to be shared among several users, the system must avoid\npossible anomalous results.\n• A major purpose of a database system is to provide users with an abstract\nview of the data. That is, the system hides certain details of how the data are\nstored and maintained.\n• Underlying the structure of a database is the data model: a collection of con-\nceptual tools for describing data, data relationships, data semantics, and data\nconstraints. The entity-relationship (E-R) data model is a widely used data\nmodel, and it provides a convenient graphical representation to view data, re-\nlationships and constraints. The relational data model is widely used to store\ndata in databases. Other data models are the object-oriented model, the object-\nrelational model, and semistructured data models.\n• The overall design of the database is called the database schema. A database\nschema is speciﬁed by a set of deﬁnitions that are expressed using a data-\ndeﬁnition language (DDL).\n• A data-manipulation language (DML) is a language that enables users to ac-\ncess or manipulate data. Nonprocedural DMLs, which require a user to specify\nonly what data are needed, without specifying exactly how to get those data,\nare widely used today.\n• Database users can be categorized into several classes, and each class of users\nusually uses a different type of interface to the database.\n• A database system has several subsystems.\n\u0000 The transaction manager subsystem is responsible for ensuring that the\ndatabase remains in a consistent (correct) state despite system failures.\nThe transaction manager also ensures that concurrent transaction execu-\ntions proceed without conﬂicting.\n\u0000 The query processor subsystem compiles and executes DDL and DML\nstatements.\n\u0000 The storage manager subsystem provides the interface between the low-\nlevel data stored in the database and the application programs and queries\nsubmitted to the system.\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\n1. Introduction\nText\n33\n© The McGraw−Hill \nCompanies, 2001\nExercises\n23\n• Database applications are typically broken up into a front-end part that runs at\nclient machines and a part that runs at the back end. In two-tier architectures,\nthe front-end directly communicates with a database running at the back end.\nIn three-tier architectures, the back end part is itself broken up into an appli-\ncation server and a database server.\nReview Terms\n• Database\nmanagement\nsystem\n(DBMS)\n• Database systems applications\n• File systems\n• Data inconsistency\n• Consistency constraints\n• Data views\n• Data abstraction\n• Database instance\n• Schema\n\u0000 Database schema\n\u0000 Physical schema\n\u0000 Logical schema\n• Physical data independence\n• Data models\n\u0000 Entity-relationship model\n\u0000 Relational data model\n\u0000 Object-oriented data model\n\u0000 Object-relational data model\n• Database languages\n\u0000 Data deﬁnition language\n\u0000 Data manipulation language\n\u0000 Query language\n• Data dictionary\n• Metadata\n• Application program\n• Database administrator (DBA)\n• Transactions\n• Concurrency\n• Client and server machines\nExercises\n1.1 List four signiﬁcant differences between a ﬁle-processing system and a DBMS.\n1.2 This chapter has described several major advantages of a database system. What\nare two disadvantages?\n1.3 Explain the difference between physical and logical data independence.\n1.4 List ﬁve responsibilities of a database management system. For each responsi-\nbility, explain the problems that would arise if the responsibility were not dis-\ncharged.\n1.5 What are ﬁve main functions of a database administrator?\n1.6 List seven programming languages that are procedural and two that are non-\nprocedural. Which group is easier to learn and use? Explain your answer.\n1.7 List six major steps that you would take in setting up a database for a particular\nenterprise.\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\n1. Introduction\nText\n34\n© The McGraw−Hill \nCompanies, 2001\n24\nChapter 1\nIntroduction\n1.8 Consider a two-dimensional integer array of size n × m that is to be used in\nyour favorite programming language. Using the array as an example, illustrate\nthe difference (a) between the three levels of data abstraction, and (b) between\na schema and instances.\nBibliographical Notes\nWe list below general purpose books, research paper collections, and Web sites on\ndatabases. Subsequent chapters provide references to material on each topic outlined\nin this chapter.\nTextbooks covering database systems include Abiteboul et al. [1995], Date [1995],\nElmasri and Navathe [2000], O’Neil and O’Neil [2000], Ramakrishnan and Gehrke\n[2000], and Ullman [1988]. Textbook coverage of transaction processing is provided\nby Bernstein and Newcomer [1997] and Gray and Reuter [1993].\nSeveral books contain collections of research papers on database management.\nAmong these are Bancilhon and Buneman [1990], Date [1986], Date [1990], Kim [1995],\nZaniolo et al. [1997], and Stonebraker and Hellerstein [1998].\nA review of accomplishments in database management and an assessment of fu-\nture research challenges appears in Silberschatz et al. [1990], Silberschatz et al. [1996]\nand Bernstein et al. [1998]. The home page of the ACM Special Interest Group on\nManagement of Data (see www.acm.org/sigmod) provides a wealth of information\nabout database research. Database vendor Web sites (see the tools section below)\nprovide details about their respective products.\nCodd [1970] is the landmark paper that introduced the relational model. Discus-\nsions concerning the evolution of DBMSs and the development of database technol-\nogy are offered by Fry and Sibley [1976] and Sibley [1976].\nTools\nThere are a large number of commercial database systems in use today. The ma-\njor ones include: IBM DB2 (www.ibm.com/software/data), Oracle (www.oracle.com),\nMicrosoft SQL Server (www.microsoft.com/sql), Informix (www.informix.com), and\nSybase (www.sybase.com). Some of these systems are available free for personal or\nnoncommercial use, or for development, but are not free for actual deployment.\nThere are also a number of free/public domain database systems; widely used\nones include MySQL (www.mysql.com) and PostgresSQL (www.postgressql.org).\nA more complete list of links to vendor Web sites and other information is avail-\nable from the home page of this book, at www.research.bell-labs.com/topic/books/db-\nbook.\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nI. Data Models\nIntroduction\n35\n© The McGraw−Hill \nCompanies, 2001\nP\nA\nR\nT\n1\nData Models\nA data model is a collection of conceptual tools for describing data, data relation-\nships, data semantics, and consistency constraints. In this part, we study two data\nmodels—the entity–relationship model and the relational model.\nThe entity–relationship (E-R) model is a high-level data model. It is based on a\nperception of a real world that consists of a collection of basic objects, called entities,\nand of relationships among these objects.\nThe relational model is a lower-level model. It uses a collection of tables to repre-\nsent both data and the relationships among those data. Its conceptual simplicity has\nled to its widespread adoption; today a vast majority of database products are based\non the relational model. Designers often formulate database schema design by ﬁrst\nmodeling data at a high level, using the E-R model, and then translating it into the\nthe relational model.\nWe shall study other data models later in the book. The object-oriented data model,\nfor example, extends the representation of entities by adding notions of encapsula-\ntion, methods (functions), and object identity. The object-relational data model com-\nbines features of the object-oriented data model and the relational data model. Chap-\nters 8 and 9, respectively, cover these two data models.\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nI. Data Models\n2. Entity−Relationship \nModel\n36\n© The McGraw−Hill \nCompanies, 2001\nC\nH\nA\nP\nT\nE\nR\n2\nEntity-Relationship Model\nThe entity-relationship (E-R) data model perceives the real world as consisting of\nbasic objects, called entities, and relationships among these objects. It was developed\nto facilitate database design by allowing speciﬁcation of an enterprise schema, which\nrepresents the overall logical structure of a database. The E-R data model is one of sev-\neral semantic data models; the semantic aspect of the model lies in its representation\nof the meaning of the data. The E-R model is very useful in mapping the meanings\nand interactions of real-world enterprises onto a conceptual schema. Because of this\nusefulness, many database-design tools draw on concepts from the E-R model.\n2.1\nBasic Concepts\nThe E-R data model employs three basic notions: entity sets, relationship sets, and\nattributes.\n2.1.1\nEntity Sets\nAn entity is a “thing” or “object” in the real world that is distinguishable from all\nother objects. For example, each person in an enterprise is an entity. An entity has a\nset of properties, and the values for some set of properties may uniquely identify an\nentity. For instance, a person may have a person-id property whose value uniquely\nidentiﬁes that person. Thus, the value 677-89-9011 for person-id would uniquely iden-\ntify one particular person in the enterprise. Similarly, loans can be thought of as enti-\nties, and loan number L-15 at the Perryridge branch uniquely identiﬁes a loan entity.\nAn entity may be concrete, such as a person or a book, or it may be abstract, such as\na loan, or a holiday, or a concept.\nAn entity set is a set of entities of the same type that share the same properties, or\nattributes. The set of all persons who are customers at a given bank, for example, can\nbe deﬁned as the entity set customer. Similarly, the entity set loan might represent the\n27\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nI. Data Models\n2. Entity−Relationship \nModel\n37\n© The McGraw−Hill \nCompanies, 2001\n28\nChapter 2\nEntity-Relationship Model\nset of all loans awarded by a particular bank. The individual entities that constitute a\nset are said to be the extension of the entity set. Thus, all the individual bank customers\nare the extension of the entity set customer.\nEntity sets do not need to be disjoint. For example, it is possible to deﬁne the entity\nset of all employees of a bank (employee) and the entity set of all customers of the bank\n(customer). A person entity may be an employee entity, a customer entity, both, or neither.\nAn entity is represented by a set of attributes. Attributes are descriptive proper-\nties possessed by each member of an entity set. The designation of an attribute for an\nentity set expresses that the database stores similar information concerning each en-\ntity in the entity set; however, each entity may have its own value for each attribute.\nPossible attributes of the customer entity set are customer-id, customer-name, customer-\nstreet, and customer-city. In real life, there would be further attributes, such as street\nnumber, apartment number, state, postal code, and country, but we omit them to\nkeep our examples simple. Possible attributes of the loan entity set are loan-number\nand amount.\nEach entity has a value for each of its attributes. For instance, a particular customer\nentity may have the value 321-12-3123 for customer-id, the value Jones for customer-\nname, the value Main for customer-street, and the value Harrison for customer-city.\nThe customer-id attribute is used to uniquely identify customers, since there may\nbe more than one customer with the same name, street, and city. In the United States,\nmany enterprises ﬁnd it convenient to use the social-security number of a person1\nas an attribute whose value uniquely identiﬁes the person. In general the enterprise\nwould have to create and assign a unique identiﬁer for each customer.\nFor each attribute, there is a set of permitted values, called the domain, or value\nset, of that attribute. The domain of attribute customer-name might be the set of all\ntext strings of a certain length. Similarly, the domain of attribute loan-number might\nbe the set of all strings of the form “L-n” where n is a positive integer.\nA database thus includes a collection of entity sets, each of which contains any\nnumber of entities of the same type. Figure 2.1 shows part of a bank database that\nconsists of two entity sets: customer and loan.\nFormally, an attribute of an entity set is a function that maps from the entity set into\na domain. Since an entity set may have several attributes, each entity can be described\nby a set of (attribute, data value) pairs, one pair for each attribute of the entity set. For\nexample, a particular customer entity may be described by the set {(customer-id, 677-\n89-9011), (customer-name, Hayes), (customer-street, Main), (customer-city, Harrison)},\nmeaning that the entity describes a person named Hayes whose customer identiﬁer\nis 677-89-9011 and who resides at Main Street in Harrison. We can see, at this point,\nan integration of the abstract schema with the actual enterprise being modeled. The\nattribute values describing an entity will constitute a signiﬁcant portion of the data\nstored in the database.\nAn attribute, as used in the E-R model, can be characterized by the following at-\ntribute types.\n1.\nIn the United States, the government assigns to each person in the country a unique number, called a\nsocial-security number, to identify that person uniquely. Each person is supposed to have only one social-\nsecurity number, and no two people are supposed to have the same social-security number.\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nI. Data Models\n2. Entity−Relationship \nModel\n38\n© The McGraw−Hill \nCompanies, 2001\n2.1\nBasic Concepts\n29\n555-55-5555 Jackson    Dupont  Woodside\n321-12-3123  Jones         Main       Harrison\n019-28-3746 Smith        North     Rye\n677-89-9011 Hayes       Main      Harrison\n244-66-8800 Curry        North     Rye\n 963-96-3963 Williams  Nassau   Princeton\n335-57-7991 Adams      Spring    Pittsfield\nL-17   1000\nL-15   1500\nL-14   1500\nL-16   1300\nL-23   2000\nL-19     500\nL-11     900\nloan\ncustomer\nFigure 2.1\nEntity sets customer and loan.\n• Simple and composite attributes. In our examples thus far, the attributes have\nbeen simple; that is, they are not divided into subparts. Composite attributes,\non the other hand, can be divided into subparts (that is, other attributes). For\nexample, an attribute name could be structured as a composite attribute con-\nsisting of ﬁrst-name, middle-initial, and last-name. Using composite attributes in\na design schema is a good choice if a user will wish to refer to an entire at-\ntribute on some occasions, and to only a component of the attribute on other\noccasions. Suppose we were to substitute for the customer entity-set attributes\ncustomer-street and customer-city the composite attribute address with the at-\ntributes street, city, state, and zip-code.2 Composite attributes help us to gr",
    "precision": 0.14935822637106183,
    "recall": 0.9365853658536586,
    "iou": 0.14786291875240662,
    "f1": 0.25763166722576314,
    "gold_tokens_count": 410,
    "retrieved_tokens_count": 2571,
    "intersection_tokens": 384
  },
  {
    "config_name": "chars_rrf",
    "config": {
      "name": "chars_rrf",
      "index_prefix": "textbook_index",
      "chunking_strategy": "chars",
      "overlap": 0,
      "fusion": "rrf",
      "bm25_weight": 0.3,
      "tag_weight": 0.2,
      "top_k": 5,
      "embed_model": "sentence-transformers/all-MiniLM-L6-v2"
    },
    "question": "What are the components of a query processor?",
    "gold_text": "The query processor components include • DDL interpreter, which interprets DDL statements and records the definitions in the data dictionary. • DML compiler, which translates DML statements in a query language into an evaluation plan consisting of low-level instructions that the query evaluation engine understands. A query can usually be translated into any of a number of alternative eval- uation plans that all give the same result. The DML compiler also performs query optimization, that is, it picks the lowest cost evaluation plan from amo- ng the alternatives. • Query evaluation engine, which executes low-level instructions generated by the DML compiler.",
    "retrieved_text": "3\nIf the above query were run on the tables in Figure 1.3, the system would ﬁnd that\nthe two accounts numbered A-101 and A-201 are owned by customer 192-83-7465\nand would print out the balances of the two accounts, namely 500 and 900.\nThere are a number of database query languages in use, either commercially or\nexperimentally. We study the most widely used query language, SQL, in Chapter 4.\nWe also study some other query languages in Chapter 5.\nThe levels of abstraction that we discussed in Section 1.3 apply not only to deﬁning\nor structuring data, but also to manipulating data. At the physical level, we must\ndeﬁne algorithms that allow efﬁcient access to data. At higher levels of abstraction,\nwe emphasize ease of use. The goal is to allow humans to interact efﬁciently with the\nsystem. The query processor component of the database system (which we study in\nChapters 13 and 14) translates DML queries into sequences of actions at the physical\nlevel of the database system.\n1.5.3\nDatabase Access from Application Programs\nApplication programs are programs that are used to interact with the database. Ap-\nplication programs are usually written in a host language, such as Cobol, C, C++, or\nJava. Examples in a banking system are programs that generate payroll checks, debit\naccounts, credit accounts, or transfer funds between accounts.\nTo access the database, DML statements need to be executed from the host lan-\nguage. There are two ways to do this:\n• By providing an application program interface (set of procedures) that can\nbe used to send DML and DDL statements to the database, and retrieve the\nresults.\nThe Open Database Connectivity (ODBC) standard deﬁned by Microsoft\nfor use with the C language is a commonly used application program inter-\nface standard. The Java Database Connectivity (JDBC) standard provides cor-\nresponding features to the Java language.\n• By extending the host language syntax to embed DML calls within the host\nlanguage program. Usually, a special character prefaces DML calls, and a pre-\nprocessor, called the DML precompiler, converts the DML statements to nor-\nmal procedure calls in the host language.\n1.6\nDatabase Users and Administrators\nA primary goal of a database system is to retrieve information from and store new\ninformation in the database. People who work with a database can be categorized as\ndatabase users or database administrators.\n1.6.1\nDatabase Users and User Interfaces\nThere are four different types of database-system users, differentiated by the way\nthey expect to interact with the system. Different types of user interfaces have been\ndesigned for the different types of users.\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\n1. Introduction\nText\n24\n© The McGraw−Hill \nCompanies, 2001\n14\nChapter 1\nIntroduction\n• Naive users are unsophisticated users who interact with the system by invok-\ning one of the application programs that have been written previously. For\nexample, a bank teller who needs to transfer $50 from account A to account B\ninvokes a program called transfer. This program asks the teller for the amount\nof money to be transferred, the account from which the money is to be trans-\nferred, and the account to which the money is to be transferred.\nAs another example, consider a user who wishes to ﬁnd her account bal-\nance over the World Wide Web. Such a user may access a form, where she\nenters her account number. An application program at the Web server then\nretrieves the account balance, using the given account number, and passes\nthis information back to the user.\nThe typical user interface for naive users is a forms interface, where the\nuser can ﬁll in appropriate ﬁelds of the form. Naive users may also simply\nread reports generated from the database.\n• Application programmers are computer professionals who write application\nprograms. Application programmers can choose from many tools to develop\nuser interfaces. Rapid application development (RAD) tools are tools that en-\nable an application programmer to construct forms and reports without writ-\ning a program. There are also special types of programming languages that\ncombine imperative control structures (for example, for loops, while loops\nand if-then-else statements) with statements of the data manipulation lan-\nguage. These languages, sometimes called fourth-generation languages, often\ninclude special features to facilitate the generation of forms and the display of\ndata on the screen. Most major commercial database systems include a fourth-\ngeneration language.\n• Sophisticated users interact with the system without writing programs. In-\nstead, they form their requests in a database query language. They submit\neach such query to a query processor, whose function is to break down DML\nstatements into instructions that the storage manager understands. Analysts\nwho submit queries to explore data in the database fall in this category.\nOnline analytical processing (OLAP) tools simplify analysts’ tasks by let-\nting them view summaries of data in different ways. For instance, an analyst\ncan see total sales by region (for example, North, South, East, and West), or by\nproduct, or by a combination of region and product (that is, total sales of each\nproduct in each region). The tools also permit the analyst to select speciﬁc re-\ngions, look at data in more detail (for example, sales by city within a region)\nor look at the data in less detail (for example, aggregate products together by\ncategory).\nAnother class of tools for analysts is data mining tools, which help them\nﬁnd certain kinds of patterns in data.\nWe study OLAP tools and data mining in Chapter 22.\n• Specialized users are sophisticated users who write specialized database\napplications that do not ﬁt into the traditional data-processing framework.\nAmong these applications are computer-aided design systems, knowledge-\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\n1. Introduction\nText\n25\n© The McGraw−Hill \nCompanies, 2001\n1.7\nTransaction Management\n15\nbase and expert systems, systems that store data with complex data types (for\nexample, graphics data and audio data), and environment-modeling systems.\nChapters 8 and 9 cover several of these applications.\n1.6.2\nDatabase Administrator\nOne of the main reasons for using DBMSs is to have central control of both the data\nand the programs that access those data. A person who has such central control over\nthe system is called a database administrator (DBA). The functions of a DBA include:\n• Schema deﬁnition. The DBA creates the original database schema by execut-\ning a set of data deﬁnition statements in the DDL.\n• Storage structure and access-method deﬁnition.\n• Schema and physical-organization modiﬁcation. The DBA carries out chang-\nes to the schema and physical organization to reﬂect the changing needs of the\norganization, or to alter the physical organization to improve performance.\n• Granting of authorization for data access. By granting different types of\nauthorization, the database administrator can regulate which parts of the data-\nbase various users can access. The authorization information is kept in a\nspecial system structure that the database system consults whenever some-\none attempts to access the data in the system.\n• Routine maintenance. Examples of the database administrator’s routine\nmaintenance activities are:\n\u0000 Periodically backing up the database, either onto tapes or onto remote\nservers, to prevent loss of data in case of disasters such as ﬂooding.\n\u0000 Ensuring that enough free disk space is available for normal operations,\nand upgrading disk space as required.\n\u0000 Monitoring jobs running on the database and ensuring that performance\nis not degraded by very expensive tasks submitted by some users.\n1.7\nTransaction Management\nOften, several operations on the database form a single logical unit of work. An ex-\nample is a funds transfer, as in Section 1.2, in which one account (say A) is debited and\nanother account (say B) is credited. Clearly, it is essential that either both the credit\nand debit occur, or that neither occur. That is, the funds transfer must happen in its\nentirety or not at all. This all-or-none requirement is called atomicity. In addition, it\nis essential that the execution of the funds transfer preserve the consistency of the\ndatabase. That is, the value of the sum A + B must be preserved. This correctness\nrequirement is called consistency. Finally, after the successful execution of a funds\ntransfer, the new values of accounts A and B must persist, despite the possibility of\nsystem failure. This persistence requirement is called durability.\nA transaction is a collection of operations that performs a single logical function\nin a database application. Each transaction is a unit of both atomicity and consis-\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\n1. Introduction\nText\n26\n© The McGraw−Hill \nCompanies, 2001\n16\nChapter 1\nIntroduction\ntency. Thus, we require that transactions do not violate any database-consistency\nconstraints. That is, if the database was consistent when a transaction started, the\ndatabase must be consistent when the transaction successfully terminates. However,\nduring the execution of a transaction, it may be necessary temporarily to allow incon-\nsistency, since either the debit of A or the credit of B must be done before the other.\nThis temporary inconsistency, although necessary, may lead to difﬁculty if a failure\noccurs.\nIt is the programmer’s responsibility to deﬁne properly the various transactions,\nso that each preserves the consistency of the database. For example, the transaction to\ntransfer funds from account A to account B could be deﬁned to be composed of two\nseparate programs: one that debits account A, and another that credits account B. The\nexecution of these two programs one after the other will indeed preserve consistency.\nHowever, each program by itself does not transform the database from a consistent\nstate to a new consistent state. Thus, those programs are not transactions.\nEnsuring the atomicity and durability properties is the responsibility of the data-\nbase system itself—speciﬁcally, of the transaction-management component. In the\nabsence of failures, all transactions complete successfully, and atomicity is achieved\neasily. However, because of various types of failure, a transaction may not always\ncomplete its execution successfully. If we are to ensure the atomicity property, a failed\ntransaction must have no effect on the state of the database. Thus, the database must\nbe restored to the state in which it was before the transaction in question started exe-\ncuting. The database system must therefore perform failure recovery, that is, detect\nsystem failures and restore the database to the state that existed prior to the occur-\nrence of the failure.\nFinally, when several transactions update the database concurrently, the consis-\ntency of data may no longer be preserved, even though each individual transac-\ntion is correct. It is the responsibility of the concurrency-control manager to control\nthe interaction among the concurrent transactions, to ensure the consistency of the\ndatabase.\nDatabase systems designed for use on small personal computers may not have\nall these features. For example, many small systems allow only one user to access\nthe database at a time. Others do not offer backup and recovery, leaving that to the\nuser. These restrictions allow for a smaller data manager, with fewer requirements for\nphysical resources—especially main memory. Although such a low-cost, low-feature\napproach is adequate for small personal databases, it is inadequate for a medium- to\nlarge-scale enterprise.\n1.8\nDatabase System Structure\nA database system is partitioned into modules that deal with each of the responsi-\nbilites of the overall system. The functional components of a database system can be\nbroadly divided into the storage manager and the query processor components.\nThe storage manager is important because databases typically require a large\namount of storage space. Corporate databases range in size from hundreds of gi-\ngabytes to, for the largest databases, terabytes of data. A gigabyte is 1000 megabytes\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\n1. Introduction\nText\n27\n© The McGraw−Hill \nCompanies, 2001\n1.8\nDatabase System Structure\n17\n(1 billion bytes), and a terabyte is 1 million megabytes (1 trillion bytes). Since the\nmain memory of computers cannot store this much information, the information is\nstored on disks. Data are moved between disk storage and main memory as needed.\nSince the movement of data to and from disk is slow relative to the speed of the cen-\ntral processing unit, it is imperative that the database system structure the data so as\nto minimize the need to move data between disk and main memory.\nThe query processor is important because it helps the database system simplify\nand facilitate access to data. High-level views help to achieve this goal; with them,\nusers of the system are not be burdened unnecessarily with the physical details of the\nimplementation of the system. However, quick processing of updates and queries\nis important. It is the job of the database system to translate updates and queries\nwritten in a nonprocedural language, at the logical level, into an efﬁcient sequence of\noperations at the physical level.\n1.8.1\nStorage Manager\nA storage manager is a program module that provides the interface between the low-\nlevel data stored in the database and the application programs and queries submit-\nted to the system. The storage manager is responsible for the interaction with the ﬁle\nmanager. The raw data are stored on the disk using the ﬁle system, which is usu-\nally provided by a conventional operating system. The storage manager translates\nthe various DML statements into low-level ﬁle-system commands. Thus, the storage\nmanager is responsible for storing, retrieving, and updating data in the database.\nThe storage manager components include:\n• Authorization and integrity manager, which tests for the satisfaction of in-\ntegrity constraints and checks the authority of users to access data.\n• Transaction manager, which ensures that the database remains in a consistent\n(correct) state despite system failures, and that concurrent transaction execu-\ntions proceed without conﬂicting.\n• File manager, which manages the allocation of space on disk storage and the\ndata structures used to represent information stored on disk.\n• Buffer manager, which is responsible for fetching data from disk storage into\nmain memory, and deciding what data to cache in main memory. The buffer\nmanager is a critical part of the database system, since it enables the database\nto handle data sizes that are much larger than the size of main memory.\nThe storage manager implements several data structures as part of the physical\nsystem implementation:\n• Data ﬁles, which store the database itself.\n• Data dictionary, which stores metadata about the structure of the database, in\nparticular the schema of the database.\n• Indices, which provide fast access to data items that hold particular values.\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\n1. Introduction\nText\n28\n© The McGraw−Hill \nCompanies, 2001\n18\nChapter 1\nIntroduction\n1.8.2\nThe Query Processor\nThe query processor components include\n• DDL interpreter, which interprets DDL statements and records the deﬁnitions\nin the data dictionary.\n• DML compiler, which translates DML statements in a query language into an\nevaluation plan consisting of low-level instructions that the query evaluation\nengine understands.\nA query can usually be translated into any of a number of alternative eval-\nuation plans that all give the same result. The DML compiler also performs\nquery optimization, that is, it picks the lowest cost evaluation plan from amo-\nng the alternatives.\n• Query evaluation engine, which executes low-level instructions generated by\nthe DML compiler.\nFigure 1.4 shows these components and the connections among them.\n1.9\nApplication Architectures\nMost users of a database system today are not present at the site of the database\nsystem, but connect to it through a network. We can therefore differentiate between\nclient machines, on which remote database users work, and server machines, on\nwhich the database system runs.\nDatabase applications are usually partitioned into two or three parts, as in Fig-\nure 1.5. In a two-tier architecture, the application is partitioned into a component\nthat resides at the client machine, which invokes database system functionality at the\nserver machine through query language statements. Application program interface\nstandards like ODBC and JDBC are used for interaction between the client and the\nserver.\nIn contrast, in a three-tier architecture, the client machine acts as merely a front\nend and does not contain any direct database calls. Instead, the client end communi-\ncates with an application server, usually through a forms interface. The application\nserver in turn communicates with a database system to access data. The business\nlogic of the application, which says what actions to carry out under what conditions,\nis embedded in the application server, instead of being distributed across multiple\nclients. Three-tier applications are more appropriate for large applications, and for\napplications that run on the World Wide Web.\n1.10\nHistory of Database Systems\nData processing drives the growth of computers, as it has from the earliest days of\ncommercial computers. In fact, automation of data processing tasks predates com-\nputers. Punched cards, invented by Hollerith, were used at the very beginning of the\ntwentieth century to record U.S. census data, and mechanical systems were used to\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\n1. Introduction\nText\n29\n© The McGraw−Hill \nCompanies, 2001\n1.10\nHistory of Database Systems\n19\nnaive users\n(tellers, agents, \nweb-users) \nquery processor\nstorage manager\ndisk storage\nindices\nstatistical data\ndata\ndata dictionary\napplication\nprogrammers\napplication\ninterfaces\napplication\nprogram\nobject code\ncompiler and\nlinker\nbuffer manager\nfile manager\nauthorization\nand integrity\n manager\ntransaction\nmanager\nDML compiler \nand organizer\nquery evaluation\nengine\nDML queries\nDDL interpreter\napplication\nprograms\nquery\ntools\nadministration\ntools\nsophisticated\nusers\n(analysts)\ndatabase\nadministrator\nuse\nwrite\nuse\nuse\nFigure 1.4\nSystem structure.\nprocess the cards and tabulate results. Punched cards were later widely used as a\nmeans of entering data into computers.\nTechniques for data storage and processing have evolved over the years:\n• 1950s and early 1960s: Magnetic tapes were developed for data storage. Data\nprocessing tasks such as payroll were automated, with data stored on tapes.\nProcessing of data consisted of reading data from one or more tapes and\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\n1. Introduction\nText\n30\n© The McGraw−Hill \nCompanies, 2001\n20\nChapter 1\nIntroduction\nuser\napplication server\ndatabase system\nuser\ndatabase system\nb.  three-tier architecture\na.  two-tier architecture\nnetwork\nserver\nclient\napplication\nnetwork\napplication client\nFigure 1.5\nTwo-tier and three-tier architectures.\nwriting data to a new tape. Data could also be input from punched card decks,\nand output to printers. For example, salary raises were processed by entering\nthe raises on punched cards and reading the punched card deck in synchro-\nnization with a tape containing the master salary details. The records had to\nbe in the same sorted order. The salary raises would be added to the salary\nread from the master tape, and written to a new tape; the new tape would\nbecome the new master tape.\nTapes (and card decks) could be read only sequentially, and data sizes were\nmuch larger than main memory; thus, data process\n\nies on local schemas at each of the sites where the query has to be exe-\ncuted. The query results have to be translated back into the global schema.\nThe task is simpliﬁed by writing wrappers for each data source, which pro-\nvide a view of the local data in the global schema. Wrappers also translate\nqueries on the global schema into queries on the local schema, and translate\nresults back into the global schema. Wrappers may be provided by individual\nsites, or may be written separately as part of the multidatabase system.\nWrappers can even be used to provide a relational view of nonrelational\ndata sources, such as Web pages (possibly with forms interfaces), ﬂat ﬁles,\nhierarchical and network databases, and directory systems.\n• Some data sources may provide only limited query capabilities; for instance,\nthey may support selections, but not joins. They may even restrict the form\nof selections, allowing selections only on certain ﬁelds; Web data sources with\nform interfaces are an example of such data sources. Queries may therefore\nhave to be broken up, to be partly performed at the data source and partly at\nthe site issuing the query.\n• In general, more than one site may need to be accessed to answer a given\nquery. Answers retrieved from the sites may have to be processed to remove\nduplicates. Suppose one site contains account tuples satisfying the selection\nbalance < 100, while another contains account tuples satisfying balance > 50.\nA query on the entire account relation would require access to both sites and\nremoval of duplicate answers resulting from tuples with balance between 50\nand 100, which are replicated at both sites.\n• Global query optimization in a heterogeneous database is difﬁcult, since the\nquery execution system may not know what the costs are of alternative query\nplans at different sites. The usual solution is to rely on only local-level opti-\nmization, and just use heuristics at the global level.\nMediator systems are systems that integrate multiple heterogeneous data sources,\nproviding an integrated global view of the data and providing query facilities on\nthe global view. Unlike full-ﬂedged multidatabase systems, mediator systems do not\nbother about transaction processing. (The terms mediator and multidatabase are of-\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nVI. Database System \nArchitecture\n19. Distributed Databases\n737\n© The McGraw−Hill \nCompanies, 2001\n19.9\nDirectory Systems\n741\nten used in an interchangeable fashion, and systems that are called mediators may\nsupport limited forms of transactions.) The term virtual database is used to refer\nto multidatabase/mediator systems, since they provide the appearance of a single\ndatabase with a global schema, although data exist on multiple sites in local schemas.\n19.9\nDirectory Systems\nConsider an organization that wishes to make data about its employees available to\na variety of people in the organization; example of the kinds of data would include\nname, designation, employee-id, address, email address, phone number, fax num-\nber, and so on. In the precomputerization days, organizations would create physical\ndirectories of employees and distribute them across the organization. Even today,\ntelephone companies create physical directories of customers.\nIn general, a directory is a listing of information about some class of objects such as\npersons. Directories can be used to ﬁnd information about a speciﬁc object, or in the\nreverse direction to ﬁnd objects that meet a certain requirement. In the world of phys-\nical telephone directories, directories that satisfy lookups in the forward direction are\ncalled white pages, while directories that satisfy lookups in the reverse direction are\ncalled yellow pages.\nIn today’s networked world, the need for directories is still present and, if any-\nthing, even more important. However, directories today need to be available over a\ncomputer network, rather than in a physical (paper) form.\n19.9.1\nDirectory Access Protocols\nDirectory information can be made available through Web interfaces, as many orga-\nnizations, and phone companies in particular do. Such interfaces are good for hu-\nmans. However, programs too, need to access directory information. Directories can\nbe used for storing other types of information, much like ﬁle system directories. For\ninstance, Web browsers can store personal bookmarks and other browser settings in\na directory system. A user can thus access the same settings from multiple locations,\nsuch as at home and at work, without having to share a ﬁle system.\nSeveral directory access protocols have been developed to provide a standardized\nway of accessing data in a directory. The most widely used among them today is the\nLightweight Directory Access Protocol (LDAP).\nObviously all the types of data in our examples can be stored without much trou-\nble in a database system, and accessed through protocols such as JDBC or ODBC. The\nquestion then is, why come up with a specialized protocol for accessing directory\ninformation? There are at least two answers to the question.\n• First, directory access protocols are simpliﬁed protocols that cater to a lim-\nited type of access to data. They evolved in parallel with the database access\nprotocols.\n• Second, and more important, directory systems provide a simple mechanism\nto name objects in a hierarchical fashion, similar to ﬁle system directory names,\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nVI. Database System \nArchitecture\n19. Distributed Databases\n738\n© The McGraw−Hill \nCompanies, 2001\n742\nChapter 19\nDistributed Databases\nwhich can be used in a distributed directory system to specify what informa-\ntion is stored in each of the directory servers. For example, a particular direc-\ntory server may store information for Bell Laboratories employees in Murray\nHill, while another may store information for Bell Laboratories employees in\nBangalore, giving both sites autonomy in controlling their local data. The di-\nrectory access protocol can be used to obtain data from both directories, across\na network. More importantly, the directory system can be set up to automati-\ncally forward queries made at one site to the other site, without user interven-\ntion.\nFor these reasons, several organizations have directory systems to make organiza-\ntional information available online. As may be expected, several directory implemen-\ntations ﬁnd it beneﬁcial to use relational databases to store data, instead of creating\nspecial-purpose storage systems.\n19.9.2\nLDAP: Lightweight Directory Access Protocol\nIn general a directory system is implemented as one or more servers, which service\nmultiple clients. Clients use the application programmer interface deﬁned by direc-\ntory system to communicate with the directory servers. Directory access protocols\nalso deﬁne a data model and access control.\nThe X.500 directory access protocol, deﬁned by the International Organization for\nStandardization (ISO), is a standard for accessing directory information. However,\nthe protocol is rather complex, and is not widely used. The Lightweight Directory\nAccess Protocol (LDAP) provides many of the X.500 features, but with less complex-\nity, and is widely used. In the rest of this section, we shall outline the data model and\naccess protocol details of LDAP.\n19.9.2.1\nLDAP Data Model\nIn LDAP directories store entries, which are similar to objects. Each entry must have a\ndistinguished name (DN), which uniquely identiﬁes the entry. A DN is in turn made\nup of a sequence of relative distinguished names (RDNs). For example, an entry may\nhave the following distinguished name.\ncn=Silberschatz, ou=Bell Labs, o=Lucent, c=USA\nAs you can see, the distinguished name in this example is a combination of a name\nand (organizational) address, starting with a person’s name, then giving the orga-\nnizational unit (ou), the organization (o), and country (c). The order of the compo-\nnents of a distinguished name reﬂects the normal postal address order, rather than\nthe reverse order used in specifying path names for ﬁles. The set of RDNs for a DN is\ndeﬁned by the schema of the directory system.\nEntries can also have attributes. LDAP provides binary, string, and time types, and\nadditionally the types tel for telephone numbers, and PostalAddress for addresses\n(lines separated by a “$” character). Unlike those in the relational model, attributes\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nVI. Database System \nArchitecture\n19. Distributed Databases\n739\n© The McGraw−Hill \nCompanies, 2001\n19.9\nDirectory Systems\n743\nare multivalued by default, so it is possible to store multiple telephone numbers or\naddresses for an entry.\nLDAP allows the deﬁnition of object classes with attribute names and types. In-\nheritance can be used in deﬁning object classes. Moreover, entries can be speciﬁed to\nbe of one or more object classes. It is not necessary that there be a single most-speciﬁc\nobject class to which an entry belongs.\nEntries are organized into a directory information tree (DIT), according to their\ndistinguished names. Entries at the leaf level of the tree usually represent speciﬁc\nobjects. Entries that are internal nodes represent objects such as organizational units,\norganizations, or countries. The children of a node have a DN containing all the RDNs\nof the parent, and one or more additional RDNs. For instance, an internal node may\nhave a DN c=USA, and all entries below it have the value USA for the RDN c.\nThe entire distinguished name need not be stored in an entry; The system can\ngenerate the distinguished name of an entry by traversing up the DIT from the entry,\ncollecting the RDN=value components to create the full distinguished name.\nEntries may have more than one distinguished name—for example, an entry for a\nperson in more than one organization. To deal with such cases, the leaf level of a DIT\ncan be an alias, which points to an entry in another branch of the tree.\n19.9.2.2\nData Manipulation\nUnlike SQL, LDAP does not deﬁne either a data-deﬁnition language or a data manip-\nulation language. However, LDAP deﬁnes a network protocol for carrying out data\ndeﬁnition and manipulation. Users of LDAP can either use an application program-\nming interface, or use tools provided by various vendors to perform data deﬁnition\nand manipulation. LDAP also deﬁnes a ﬁle format called LDAP Data Interchange\nFormat (LDIF) that can be used for storing and exchanging information.\nThe querying mechanism in LDAP is very simple, consisting of just selections and\nprojections, without any join. A query must specify the following:\n• A base—that is, a node within a DIT—by giving its distinguished name (the\npath from the root to the node).\n• A search condition, which can be a Boolean combination of conditions on in-\ndividual attributes. Equality, matching by wild-card characters, and approxi-\nmate equality (the exact deﬁnition of approximate equality is system depen-\ndent) are supported.\n• A scope, which can be just the base, the base and its children, or the entire\nsubtree beneath the base.\n• Attributes to return.\n• Limits on number of results and resource consumption.\nThe query can also specify whether to automatically dereference aliases; if alias deref-\nerences are turned off, alias entries can be returned as answers.\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nVI. Database System \nArchitecture\n19. Distributed Databases\n740\n© The McGraw−Hill \nCompanies, 2001\n744\nChapter 19\nDistributed Databases\nOne way of querying an LDAP data source is by using LDAP URLs. Examples of\nLDAP URLs are:\nldap:://aura.research.bell-labs.com/o=Lucent,c=USA\nldap:://aura.research.bell-labs.com/o=Lucent,c=USA??sub?cn=Korth\nThe ﬁrst URL returns all attributes of all entries at the server with organization being\nLucent, and country being USA. The second URL executes a search query (selection)\ncn=Korth on the subtree of the node with distinguished name o=Lucent, c=USA. The\nquestion marks in the URL separate different ﬁelds. The ﬁrst ﬁeld is the distinguished\nname, here o=Lucent,c=USA. The second ﬁeld, the list of attributes to return, is left\nempty, meaning return all attributes. The third attribute, sub, indicates that the entire\nsubtree is to be searched. The last parameter is the search condition.\nA second way of querying an LDAP directory is by using an application program-\nming interface. Figure 19.6 shows a piece of C code used to connect to an LDAP server\nand run a query against the server. The code ﬁrst opens a connection to an LDAP\nserver by ldap open and ldap bind. It then executes a query by ldap search s. The\narguments to ldap search s are the LDAP connection handle, the DN of the base from\nwhich the search should be done, the scope of the search, the search condition, the\nlist of attributes to be returned, and an attribute called attrsonly, which, if set to 1,\nwould result in only the schema of the result being returned, without any actual tu-\nples. The last argument is an output argument that returns the result of the search as\nan LDAPMessage structure.\nThe ﬁrst for loop iterates over and prints each entry in the result. Note that an\nentry may have multiple attributes, and the second for loop prints each attribute.\nSince attributes in LDAP may be multivalued, the third for loop prints each value of\nan attribute. The calls ldap msgfree and ldap value free free memory that is allocated\nby the LDAP libraries. Figure 19.6 does not show code for handling error conditions.\nThe LDAP API also contains functions to create, update, and delete entries, as well\nas other operations on the DIT. Each function call behaves like a separate transaction;\nLDAP does not support atomicity of multiple updates.\n19.9.2.3\nDistributed Directory Trees\nInformation about an organization may be split into multiple DITs, each of which\nstores information about some entries. The sufﬁx of a DIT is a sequence of RDN=value\npairs that identify what information the DIT stores; the pairs are concatenated to the\nrest of the distinguished name generated by traversing from the entry to the root.\nFor instance, the sufﬁx of a DIT may be o=Lucent, c=USA, while another may have\nthe sufﬁx o=Lucent, c=India. The DITs may be organizationally and geographically\nseparated.\nA node in a DIT may contain a referral to another node in another DIT; for in-\nstance, the organizational unit Bell Labs under o=Lucent, c=USA may have its own\nDIT, in which case the DIT for o=Lucent, c=USA would have a node ou=Bell Labs\nrepresenting a referral to the DIT for Bell Labs.\nReferrals are the key component that help organize a distributed collection of di-\nrectories into an integrated system. When a server gets a query on a DIT, it may\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nVI. Database System \nArchitecture\n19. Distributed Databases\n741\n© The McGraw−Hill \nCompanies, 2001\n19.9\nDirectory Systems\n745\n#include <stdio.h>\n#include <ldap.h>\nmain() {\nLDAP *ld;\nLDAPMessage *res, *entry;\nchar *dn, *attr, *attrList[] = {“telephoneNumber”, NULL};\nBerElement *ptr;\nint vals, i;\nld = ldap open(“aura.research.bell-labs.com”, LDAP PORT);\nldap simple bind(ld, “avi”, “avi-passwd”) ;\nldap search s(ld, “o=Lucent, c=USA”, LDAP SCOPE SUBTREE, “cn=Korth”,\nattrList, /*attrsonly*/ 0, &res);\nprintf(“found %d entries”, ldap count entries(ld, res));\nfor (entry=ldap ﬁrst entry(ld, res); entry != NULL;\nentry = ldap next entry(ld, entry)\n{\ndn = ldap get dn(ld, entry);\nprintf(“dn: %s”, dn);\nldap memfree(dn);\nfor (attr = ldap ﬁrst attribute(ld, entry, &ptr);\nattr ! NULL;\nattr = ldap next attribute(ld, entry, ptr))\n{\nprintf(“%s: ”, attr);\nvals = ldap get values(ld, entry, attr);\nfor (i=0; vals[i] != NULL; i++)\nprintf(“%s, ”, vals[i]);\nldap value free(vals);\n}\n}\nldap msgfree(res);\nldap unbind(ld);\n}\nFigure 19.6\nExample of LDAP code in C.\nreturn a referral to the client, which then issues a query on the referenced DIT. Ac-\ncess to the referenced DIT is transparent, proceeding without the user’s knowledge.\nAlternatively, the server itself may issue the query to the referred DIT and return the\nresults along with locally computed results.\nThe hierarchical naming mechanism used by LDAP helps break up control of in-\nformation across parts of an organization. The referral facility then helps integrate all\nthe directories in an organization into a single virtual directory.\nAlthough it is not an LDAP requirement, organizations often choose to break up\ninformation either by geography (for instance, an organization may maintain a direc-\ntory for each site where the organization has a large presence) or by organizational\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nVI. Database System \nArchitecture\n19. Distributed Databases\n742\n© The McGraw−Hill \nCompanies, 2001\n746\nChapter 19\nDistributed Databases\nstructure (for instance, each organizational unit, such as department, maintains its\nown directory).\nMany LDAP implementations support master–slave and multimaster replication\nof DITs, although replication is not part of the current LDAP version 3 standard. Work\non standardizing replication in LDAP is in progress.\n19.10\nSummary\n• A distributed database system consists of a collection of sites, each of which\nmaintains a local database system. Each site is able to process local transac-\ntions: those transactions that access data in only that single site. In addition, a\nsite may participate in the execution of global transactions; those transactions\nthat access data in several sites. The execution of global transactions requires\ncommunication among the sites.\n• Distributed databases may be homogeneous, where all sites have a common\nschema and database system code, or heterogeneous, where the schemas and\nsystem codes may differ.\n• There are several issues involved in storing a relation in the distributed data-\nbase, including replication and fragmentation. It is essential that the system\nminimize the degree to which a user needs to be aware of how a relation is\nstored.\n• A distributed system may suffer from the same types of failure that can afﬂict\na centralized system. There are, however, additional failures with which we\nneed to deal in a distributed environment, including the failure of a site, the\nfailure of a link, loss of a message, and network partition. Each of these prob-\nlems needs to be considered in the design of a distributed recovery scheme.\n• To ensure atomicity, all the sites in which a transaction T executed must agree\non the ﬁnal outcome of the execution. T either commits at all sites or aborts at\nall sites. To ensure this property, the transaction coordinator of T must execute\na commit protocol. The most widely used commit protocol is the two-phase\ncommit protocol.\n• The two-phase commit protocol may lead to blocking, the situation in which\nthe fate of a transaction cannot be determined until a failed site (the coordi-\nnator) recovers. We can use the three-phase commit protocol to reduce the\nprobability of blocking.\n• Persistent messaging provides an alternative model for handling distributed\ntransactions. The model breaks a single transaction into parts that are exe-\ncuted at different databases. Persistent messages (which are guaranteed to be\ndelivered exactly once, regardless of failures), are sent to remote sites to re-\nquest actions to be taken there. While persistent messaging avoids the block-\ning problem, application developers have to write code to handle various\ntypes of failures.\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nVI. Database System \nArchitecture\n19. Distributed Databases\n743\n© The McGraw−Hill \nCompanies, 2001\n19.10\nSummary\n747\n• The various concurrency-control schemes used in a centralized system can be\nmodiﬁed for use in a distributed environment.\n\u0000 In the case of locking protocols, the only change that needs to be incor-\nporated is in the way that the lo\n\nery frequently.\n12.10\nSummary\n• Many queries reference only a small proportion of the records in a ﬁle. To\nreduce the overhead in searching for these records, we can construct indices\nfor the ﬁles that store the database.\n• Index-sequential ﬁles are one of the oldest index schemes used in database\nsystems. To permit fast retrieval of records in search-key order, records are\nstored sequentially, and out-of-order records are chained together. To allow\nfast random access, we use an index structure.\n• There are two types of indices that we can use: dense indices and sparse\nindices. Dense indices contain entries for every search-key value, whereas\nsparse indices contain entries only for some search-key values.\n• If the sort order of a search key matches the sort order of a relation, an index\non the search key is called a primary index. The other indices are called sec-\nondary indices. Secondary indices improve the performance of queries that use\nsearch keys other than the primary one. However, they impose an overhead\non modiﬁcation of the database.\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nIV. Data Storage and \nQuerying\n12. Indexing and Hashing\n489\n© The McGraw−Hill \nCompanies, 2001\n488\nChapter 12\nIndexing and Hashing\n• The primary disadvantage of the index-sequential ﬁle organization is that per-\nformance degrades as the ﬁle grows. To overcome this deﬁciency, we can use\na B+-tree index.\n• A B+-tree index takes the form of a balanced tree, in which every path from the\nroot of the tree to a leaf of the tree is of the same length. The height of a B+-\ntree is proportional to the logarithm to the base N of the number of records\nin the relation, where each nonleaf node stores N pointers; the value of N is\noften around 50 or 100. B+-trees are much shorter than other balanced binary-\ntree structures such as AVL trees, and therefore require fewer disk accesses to\nlocate records.\n• Lookup on B+-trees is straightforward and efﬁcient. Insertion and deletion,\nhowever, are somewhat more complicated, but still efﬁcient. The number of\noperations required for lookup, insertion, and deletion on B+-trees is propor-\ntional to the logarithm to the base N of the number of records in the relation,\nwhere each nonleaf node stores N pointers.\n• We can use B+-trees for indexing a ﬁle containing records, as well as to orga-\nnize records into a ﬁle.\n• B-tree indices are similar to B+-tree indices. The primary advantage of a B-tree\nis that the B-tree eliminates the redundant storage of search-key values. The\nmajor disadvantages are overall complexity and reduced fanout for a given\nnode size. System designers almost universally prefer B+-tree indices over B-\ntree indices in practice.\n• Sequential ﬁle organizations require an index structure to locate data. File or-\nganizations based on hashing, by contrast, allow us to ﬁnd the address of a\ndata item directly by computing a function on the search-key value of the de-\nsired record. Since we do not know at design time precisely which search-key\nvalues will be stored in the ﬁle, a good hash function to choose is one that as-\nsigns search-key values to buckets such that the distribution is both uniform\nand random.\n• Static hashing uses hash functions in which the set of bucket addresses is ﬁxed.\nSuch hash functions cannot easily accommodate databases that grow signiﬁ-\ncantly larger over time. There are several dynamic hashing techniques that allow\nthe hash function to be modiﬁed. One example is extendable hashing, which\ncopes with changes in database size by splitting and coalescing buckets as the\ndatabase grows and shrinks.\n• We can also use hashing to create secondary indices; such indices are called\nhash indices. For notational convenience, we assume hash ﬁle organizations\nhave an implicit hash index on the search key used for hashing.\n• Ordered indices such as B+-trees and hash indices can be used for selections\nbased on equality conditions involving single attributes. When multiple\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nIV. Data Storage and \nQuerying\n12. Indexing and Hashing\n490\n© The McGraw−Hill \nCompanies, 2001\nExercises\n489\nattributes are involved in a selection condition, we can intersect record iden-\ntiﬁers retrieved from multiple indices.\n• Grid ﬁles provide a general means of indexing on multiple attributes.\n• Bitmap indices provide a very compact representation for indexing attributes\nwith very few distinct values. Intersection operations are extremely fast on\nbitmaps, making them ideal for supporting queries on multiple attributes.\nReview Terms\n• Access types\n• Access time\n• Insertion time\n• Deletion time\n• Space overhead\n• Ordered index\n• Primary index\n• Clustering index\n• Secondary index\n• Nonclustering index\n• Index-sequential ﬁle\n• Index record/entry\n• Dense index\n• Sparse index\n• Multilevel index\n• Sequential scan\n• B+-Tree index\n• Balanced tree\n• B+-Tree ﬁle organization\n• B-Tree index\n• Static hashing\n• Hash ﬁle organization\n• Hash index\n• Bucket\n• Hash function\n• Bucket overﬂow\n• Skew\n• Closed hashing\n• Dynamic hashing\n• Extendable hashing\n• Multiple-key access\n• Indices on multiple keys\n• Grid ﬁles\n• Bitmap index\n• Bitmap operations\n\u0000 Intersection\n\u0000 Union\n\u0000 Complement\n\u0000 Existence bitmap\nExercises\n12.1 When is it preferable to use a dense index rather than a sparse index? Explain\nyour answer.\n12.2 Since indices speed query processing, why might they not be kept on several\nsearch keys? List as many reasons as possible.\n12.3 What is the difference between a primary index and a secondary index?\n12.4 Is it possible in general to have two primary indices on the same relation for\ndifferent search keys? Explain your answer.\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nIV. Data Storage and \nQuerying\n12. Indexing and Hashing\n491\n© The McGraw−Hill \nCompanies, 2001\n490\nChapter 12\nIndexing and Hashing\n12.5 Construct a B+-tree for the following set of key values:\n(2, 3, 5, 7, 11, 17, 19, 23, 29, 31)\nAssume that the tree is initially empty and values are added in ascending or-\nder. Construct B+-trees for the cases where the number of pointers that will ﬁt\nin one node is as follows:\na. Four\nb. Six\nc. Eight\n12.6 For each B+-tree of Exercise 12.5, show the steps involved in the following\nqueries:\na. Find records with a search-key value of 11.\nb. Find records with a search-key value between 7 and 17, inclusive.\n12.7 For each B+-tree of Exercise 12.5, show the form of the tree after each of the\nfollowing series of operations:\na. Insert 9.\nb. Insert 10.\nc. Insert 8.\nd. Delete 23.\ne. Delete 19.\n12.8 Consider the modiﬁed redistribution scheme for B+-trees described in page\n463. What is the expected height of the tree as a function of n?\n12.9 Repeat Exercise 12.5 for a B-tree.\n12.10 Explain the distinction between closed and open hashing. Discuss the relative\nmerits of each technique in database applications.\n12.11 What are the causes of bucket overﬂow in a hash ﬁle organization? What can\nbe done to reduce the occurrence of bucket overﬂows?\n12.12 Suppose that we are using extendable hashing on a ﬁle that contains records\nwith the following search-key values:\n2, 3, 5, 7, 11, 17, 19, 23, 29, 31\nShow the extendable hash structure for this ﬁle if the hash function is h(x) = x\nmod 8 and buckets can hold three records.\n12.13 Show how the extendable hash structure of Exercise 12.12 changes as the result\nof each of the following steps:\na. Delete 11.\nb. Delete 31.\nc. Insert 1.\nd. Insert 15.\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nIV. Data Storage and \nQuerying\n12. Indexing and Hashing\n492\n© The McGraw−Hill \nCompanies, 2001\nBibliographical Notes\n491\n12.14 Give pseudocode for deletion of entries from an extendable hash structure,\nincluding details of when and how to coalesce buckets. Do not bother about\nreducing the size of the bucket address table.\n12.15 Suggest an efﬁcient way to test if the bucket address table in extendable hash-\ning can be reduced in size, by storing an extra count with the bucket address\ntable. Give details of how the count should be maintained when buckets are\nsplit, coalesced or deleted.\n(Note: Reducing the size of the bucket address table is an expensive oper-\nation, and subsequent inserts may cause the table to grow again. Therefore, it\nis best not to reduce the size as soon as it is possible to do so, but instead do\nit only if the number of index entries becomes small compared to the bucket\naddress table size.)\n12.16 Why is a hash structure not the best choice for a search key on which range\nqueries are likely?\n12.17 Consider a grid ﬁle in which we wish to avoid overﬂow buckets for perfor-\nmance reasons. In cases where an overﬂow bucket would be needed, we in-\nstead reorganize the grid ﬁle. Present an algorithm for such a reorganization.\n12.18 Consider the account relation shown in Figure 12.25.\na. Construct a bitmap index on the attributes branch-name and balance, divid-\ning balance values into 4 ranges: below 250, 250 to below 500, 500 to below\n750, and 750 and above.\nb. Consider a query that requests all accounts in Downtown with a balance of\n500 or more. Outline the steps in answering the query, and show the ﬁnal\nand intermediate bitmaps constructed to answer the query.\n12.19 Show how to compute existence bitmaps from other bitmaps. Make sure that\nyour technique works even in the presence of null values, by using a bitmap\nfor the value null.\n12.20 How does data encryption affect index schemes? In particular, how might it\naffect schemes that attempt to store data in sorted order?\nBibliographical Notes\nDiscussions of the basic data structures in indexing and hashing can be found in\nCormen et al. [1990]. B-tree indices were ﬁrst introduced in Bayer [1972] and Bayer\nand McCreight [1972]. B+-trees are discussed in Comer [1979], Bayer and Unterauer\n[1977] and Knuth [1973]. The bibliographic notes in Chapter 16 provides references to\nresearch on allowing concurrent accesses and updates on B+-trees. Gray and Reuter\n[1993] provide a good description of issues in the implementation of B+-trees.\nSeveral alternative tree and treelike search structures have been proposed. Tries\nare trees whose structure is based on the “digits” of keys (for example, a dictionary\nthumb index, which has one entry for each letter). Such trees may not be balanced\nin the sense that B+-trees are. Tries are discussed by Ramesh et al. [1989], Orenstein\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nIV. Data Storage and \nQuerying\n12. Indexing and Hashing\n493\n© The McGraw−Hill \nCompanies, 2001\n492\nChapter 12\nIndexing and Hashing\n[1982], Litwin [1981] and Fredkin [1960]. Related work includes the digital B-trees of\nLomet [1981].\nKnuth [1973] analyzes a large number of different hashing techniques. Several dy-\nnamic hashing schemes exist. Extendable hashing was introduced by Fagin et al.\n[1979]. Linear hashing was introduced by Litwin [1978] and Litwin [1980]; Larson\n[1982] presents a performance analysis of linear hashing. Ellis [1987] examined con-\ncurrency with linear hashing. Larson [1988] presents a variant of linear hashing. An-\nother scheme, called dynamic hashing, was proposed by Larson [1978]. An alterna-\ntive given by Ramakrishna and Larson [1989] allows retrieval in a single disk access\nat the price of a high overhead for a small fraction of database modiﬁcations. Par-\ntitioned hashing is an extension of hashing to multiple attributes, and is covered in\nRivest [1976], Burkhard [1976] and Burkhard [1979].\nThe grid ﬁle structure appears in Nievergelt et al. [1984] and Hinrichs [1985].\nBitmap indices, and variants called bit-sliced indices and projection indices are de-\nscribed in O’Neil and Quass [1997]. They were ﬁrst introduced in the IBM Model\n204 ﬁle manager on the AS 400 platform. They provide very large speedups on cer-\ntain types of queries, and are today implemented on most database systems. Recent\nresearch on bitmap indices includes Wu and Buchmann [1998], Chan and Ioannidis\n[1998], Chan and Ioannidis [1999], and Johnson [1999a].\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nIV. Data Storage and \nQuerying\n13. Query Processing\n494\n© The McGraw−Hill \nCompanies, 2001\nC\nH\nA\nP\nT\nE\nR\n1\n3\nQuery Processing\nQuery processing refers to the range of activities involved in extracting data from\na database. The activities include translation of queries in high-level database lan-\nguages into expressions that can be used at the physical level of the ﬁle system, a\nvariety of query-optimizing transformations, and actual evaluation of queries.\n13.1\nOverview\nThe steps involved in processing a query appear in Figure 13.1. The basic steps are\n1. Parsing and translation\n2. Optimization\n3. Evaluation\nBefore query processing can begin, the system must translate the query into a us-\nable form. A language such as SQL is suitable for human use, but is ill-suited to be\nthe system’s internal representation of a query. A more useful internal representation\nis one based on the extended relational algebra.\nThus, the ﬁrst action the system must take in query processing is to translate a\ngiven query into its internal form. This translation process is similar to the work\nperformed by the parser of a compiler. In generating the internal form of the query,\nthe parser checks the syntax of the user’s query, veriﬁes that the relation names ap-\npearing in the query are names of the relations in the database, and so on. The sys-\ntem constructs a parse-tree representation of the query, which it then translates into\na relational-algebra expression. If the query was expressed in terms of a view, the\ntranslation phase also replaces all uses of the view by the relational-algebra expres-\n493\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nIV. Data Storage and \nQuerying\n13. Query Processing\n495\n© The McGraw−Hill \nCompanies, 2001\n494\nChapter 13\nQuery Processing\nquery\noutput\nquery\nparser and\ntranslator\nevaluation engine\nrelational algebra\nexpression\nexecution plan\noptimizer\ndata\nstatistics\nabout data\nFigure 13.1\nSteps in query processing.\nsion that deﬁnes the view.1 Most compiler texts cover parsing (see the bibliographical\nnotes).\nGiven a query, there are generally a variety of methods for computing the answer.\nFor example, we have seen that, in SQL, a query could be expressed in several differ-\nent ways. Each SQL query can itself be translated into a relational-algebra expression\nin one of several ways. Furthermore, the relational-algebra representation of a query\nspeciﬁes only partially how to evaluate a query; there are usually several ways to\nevaluate relational-algebra expressions. As an illustration, consider the query\nselect balance\nfrom account\nwhere balance < 2500\nThis query can be translated into either of the following relational-algebra expres-\nsions:\n• σbalance<2500 (Πbalance (account))\n• Πbalance (σbalance<2500 (account))\nFurther, we can execute each relational-algebra operation by one of several dif-\nferent algorithms. For example, to implement the preceding selection, we can search\nevery tuple in account to ﬁnd tuples with balance less than 2500. If a B+-tree index is\navailable on the attribute balance, we can use the index instead to locate the tuples.\nTo specify fully how to evaluate a query, we need not only to provide the relational-\nalgebra expression, but also to annotate it with instructions specifying how to eval-\n1.\nFor materialized views, the expression deﬁning the view has already been evaluated and stored. There-\nfore, the stored relation can be used, instead of uses of the view being replaced by the expression deﬁning\nthe view. Recursive views are handled differently, via a ﬁxed-point procedure, as discussed in Section 5.2.6.\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nIV. Data Storage and \nQuerying\n13. Query Processing\n496\n© The McGraw−Hill \nCompanies, 2001\n13.2\nMeasures of Query Cost\n495\nΠ balance\nσ balance < 2500; use index 1\naccount\nFigure 13.2\nA query-evaluation plan.\nuate each operation. Annotations may state the algorithm to be used for a speciﬁc\noperation, or the particular index or indices to use. A relational-algebra operation\nannotated with instructions on how to evaluate it is called an evaluation primitive.\nA sequence of primitive operations that can be used to evaluate a query is a query-\nexecution plan or query-evaluation plan. Figure 13.2 illustrates an evaluation plan\nfor our example query, in which a particular index (denoted in the ﬁgure as “in-\ndex 1”) is speciﬁed for the selection operation. The query-execution engine takes a\nquery-evaluation plan, executes that plan, and returns the answers to the query.\nThe different evaluation plans for a given query can have different costs. We do not\nexpect users to write their queries in a way that suggests the most efﬁcient evaluation\nplan. Rather, it is the responsibility of the system to construct a query-evaluation plan\nthat minimizes the cost of query evaluation. Chapter 14 describes query optimization\nin detail.\nOnce the query plan is chosen, the query is evaluated with that plan, and the result\nof the query is output.\nThe sequence of steps already described for processing a query is representa-\ntive; not all databases exactly follow those steps. For instance, instead of using the\nrelational-algebra representation, several databases use an annotated parse-tree rep-\nresentation based on the structure of the given SQL query. However, the concepts that\nwe describe here form the basis of query processing in databases.\nIn order to optimize a query, a query optimizer must know the cost of each oper-\nation. Although the exact cost is hard to compute, since it depends on many param-\neters such as actual memory available to the operation, it is possible to get a rough\nestimate of execution cost for each operation.\nSection 13.2 outlines how we measure the cost of a query. Sections 13.3 through\n13.6 cover the evaluation of individual relational-algebra operations. Several opera-\ntions may be grouped together into a pipeline, in which each of the operations starts\nworking on its input tuples even as they are being generated by another operation.\nIn Section 13.7, we examine how to coordinate the execution of multiple operations\nin a query evaluation plan, in particular, how to use pipelined operations to avoid\nwriting intermediate results to disk.\n13.2\nMeasures of Query Cost\nThe cost of query evaluation can be measured in terms of a number of different re-\nsources, including disk accesses, CPU time to execute a query, and, in a distributed\nor parallel database system, the cost of communication (which we discuss later, in\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nIV. Data Storage and \nQuerying\n13. Query Processing\n497\n© The McGraw−Hill \nCompanies, 2001\n496\nChapter 13\nQuery Processing\nChapters 19 and 20). The response time for a query-evaluation plan (that is, the clock\ntime required to execute the plan), assuming no other activity is going on on the com-\nputer, would account for all these costs, and could be used as a good measure of the\ncost of the plan.\nIn large database systems, however, disk accesses (which we measure as the num-\nber of transfers of blocks from disk) are usually the most important cost, since disk ac-\ncesses are slow compared to in-memory operations. Moreover, CPU speeds have been\nimproving much faster than have disk speeds. Thus, it is likely that the time spent in\ndisk activity will continue to dominate the total time to execute a query. Finally, esti-\nmating the CPU time is relatively hard, compared to estimating the disk-access cost.\nTherefore, most people consider the disk-access cost a reasonable measure of the cost\nof a query-evaluation plan.\nWe use the number of block transfers from disk as a measure of the actual cost. To\nsimplify our computation of disk-access cost, we assume that all t\n\nfound at the end of that chapter.\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nII. Relational Databases\nIntroduction\n140\n© The McGraw−Hill \nCompanies, 2001\nP\nA\nR\nT\n2\nRelational Databases\nA relational database is a shared repository of data. To make data from a relational\ndatabase available to users, we have to address several issues. One is how users spec-\nify requests for data: Which of the various query languages do they use? Chapter 4\ncovers the SQL language, which is the most widely used query language today. Chap-\nter 5 covers two other query languages, QBE and Datalog, which offer alternative\napproaches to querying relational data.\nAnother issue is data integrity and security; databases need to protect data from\ndamage by user actions, whether unintentional or intentional. The integrity main-\ntenance component of a database ensures that updates do not violate integrity con-\nstraints that have been speciﬁed on the data. The security component of a database\nincludes authentication of users, and access control, to restrict the permissible actions\nfor each user. Chapter 6 covers integrity and security issues. Security and integrity\nissues are present regardless of the data model, but for concreteness we study them\nin the context of the relational model. Integrity constraints form the basis of relational\ndatabase design, which we study in Chapter 7.\nRelational database design—the design of the relational schema—is the ﬁrst step\nin building a database application. Schema design was covered informally in ear-\nlier chapters. There are, however, principles that can be used to distinguish good\ndatabase designs from bad ones. These are formalized by means of several “normal\nforms,” which offer different tradeoffs between the possibility of inconsistencies and\nthe efﬁciency of certain queries. Chapter 7 describes the formal design of relational\nschemas.\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nII. Relational Databases\n4. SQL\n141\n© The McGraw−Hill \nCompanies, 2001\nC\nH\nA\nP\nT\nE\nR\n4\nSQL\nThe formal languages described in Chapter 3 provide a concise notation for repre-\nsenting queries. However, commercial database systems require a query language\nthat is more user friendly. In this chapter, we study SQL, the most inﬂuential commer-\ncially marketed query language, SQL. SQL uses a combination of relational-algebra\nand relational-calculus constructs.\nAlthough we refer to the SQL language as a “query language,” it can do much\nmore than just query a database. It can deﬁne the structure of the data, modify data\nin the database, and specify security constraints.\nIt is not our intention to provide a complete users’ guide for SQL. Rather, we\npresent SQL’s fundamental constructs and concepts. Individual implementations of\nSQL may differ in details, or may support only a subset of the full language.\n4.1\nBackground\nIBM developed the original version of SQL at its San Jose Research Laboratory (now\nthe Almaden Research Center). IBM implemented the language, originally called Se-\nquel, as part of the System R project in the early 1970s. The Sequel language has\nevolved since then, and its name has changed to SQL (Structured Query Language).\nMany products now support the SQL language. SQL has clearly established itself as\nthe standard relational-database language.\nIn 1986, the American National Standards Institute (ANSI) and the International\nOrganization for Standardization (ISO) published an SQL standard, called SQL-86.\nIBM published its own corporate SQL standard, the Systems Application Architec-\nture Database Interface (SAA-SQL) in 1987. ANSI published an extended standard for\nSQL, SQL-89, in 1989. The next version of the standard was SQL-92 standard, and the\nmost recent version is SQL:1999. The bibliographic notes provide references to these\nstandards.\n135\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nII. Relational Databases\n4. SQL\n142\n© The McGraw−Hill \nCompanies, 2001\n136\nChapter 4\nSQL\nIn this chapter, we present a survey of SQL, based mainly on the widely imple-\nmented SQL-92 standard. The SQL:1999 standard is a superset of the SQL-92 standard;\nwe cover some features of SQL:1999 in this chapter, and provide more detailed cov-\nerage in Chapter 9. Many database systems support some of the new constructs in\nSQL:1999, although currently no database system supports all the new constructs. You\nshould also be aware that some database systems do not even support all the fea-\ntures of SQL-92, and that many databases provide nonstandard features that we do\nnot cover here.\nThe SQL language has several parts:\n• Data-deﬁnition language (DDL). The SQL DDL provides commands for deﬁn-\ning relation schemas, deleting relations, and modifying relation schemas.\n• Interactive data-manipulation language (DML). The SQL DML includes a\nquery language based on both the relational algebra and the tuple relational\ncalculus. It includes also commands to insert tuples into, delete tuples from,\nand modify tuples in the database.\n• View deﬁnition. The SQL DDL includes commands for deﬁning views.\n• Transaction control. SQL includes commands for specifying the beginning\nand ending of transactions.\n• Embedded SQL and dynamic SQL. Embedded and dynamic SQL deﬁne how\nSQL statements can be embedded within general-purpose programming lan-\nguages, such as C, C++, Java, PL/I, Cobol, Pascal, and Fortran.\n• Integrity. The SQL DDL includes commands for specifying integrity constraints\nthat the data stored in the database must satisfy. Updates that violate integrity\nconstraints are disallowed.\n• Authorization. The SQL DDL includes commands for specifying access rights\nto relations and views.\nIn this chapter, we cover the DML and the basic DDL features of SQL. We also\nbrieﬂy outline embedded and dynamic SQL, including the ODBC and JDBC standards\nfor interacting with a database from programs written in the C and Java languages.\nSQL features supporting integrity and authorization are described in Chapter 6, while\nChapter 9 outlines object-oriented extensions to SQL.\nThe enterprise that we use in the examples in this chapter, and later chapters, is a\nbanking enterprise with the following relation schemas:\nBranch-schema = (branch-name, branch-city, assets)\nCustomer-schema = (customer-name, customer-street, customer-city)\nLoan-schema = (loan-number, branch-name, amount)\nBorrower-schema = (customer-name, loan-number)\nAccount-schema = (account-number, branch-name, balance)\nDepositor-schema = (customer-name, account-number)\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nII. Relational Databases\n4. SQL\n143\n© The McGraw−Hill \nCompanies, 2001\n4.2\nBasic Structure\n137\nNote that in this chapter, as elsewhere in the text, we use hyphenated names for\nschema, relations, and attributes for ease of reading. In actual SQL systems, however,\nhyphens are not valid parts of a name (they are treated as the minus operator). A\nsimple way of translating the names we use to valid SQL names is to replace all hy-\nphens by the underscore symbol (“ ”). For example, we use branch name in place of\nbranch-name.\n4.2\nBasic Structure\nA relational database consists of a collection of relations, each of which is assigned\na unique name. Each relation has a structure similar to that presented in Chapter 3.\nSQL allows the use of null values to indicate that the value either is unknown or does\nnot exist. It allows a user to specify which attributes cannot be assigned null values,\nas we shall discuss in Section 4.11.\nThe basic structure of an SQL expression consists of three clauses: select, from, and\nwhere.\n• The select clause corresponds to the projection operation of the relational al-\ngebra. It is used to list the attributes desired in the result of a query.\n• The from clause corresponds to the Cartesian-product operation of the rela-\ntional algebra. It lists the relations to be scanned in the evaluation of the ex-\npression.\n• The where clause corresponds to the selection predicate of the relational alge-\nbra. It consists of a predicate involving attributes of the relations that appear\nin the from clause.\nThat the term select has different meaning in SQL than in the relational algebra is an\nunfortunate historical fact. We emphasize the different interpretations here to mini-\nmize potential confusion.\nA typical SQL query has the form\nselect A1, A2, . . . , An\nfrom r1, r2, . . . , rm\nwhere P\nEach Ai represents an attribute, and each ri a relation. P is a predicate. The query is\nequivalent to the relational-algebra expression\nΠA1, A2,...,An(σP (r1 × r2 × · · · × rm))\nIf the where clause is omitted, the predicate P is true. However, unlike the result of a\nrelational-algebra expression, the result of the SQL query may contain multiple copies\nof some tuples; we shall return to this issue in Section 4.2.8.\nSQL forms the Cartesian product of the relations named in the from clause,\nperforms a relational-algebra selection using the where clause predicate, and then\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nII. Relational Databases\n4. SQL\n144\n© The McGraw−Hill \nCompanies, 2001\n138\nChapter 4\nSQL\nprojects the result onto the attributes of the select clause. In practice, SQL may con-\nvert the expression into an equivalent form that can be processed more efﬁciently.\nHowever, we shall defer concerns about efﬁciency to Chapters 13 and 14.\n4.2.1\nThe select Clause\nThe result of an SQL query is, of course, a relation. Let us consider a simple query\nusing our banking example, “Find the names of all branches in the loan relation”:\nselect branch-name\nfrom loan\nThe result is a relation consisting of a single attribute with the heading branch-name.\nFormal query languages are based on the mathematical notion of a relation being\na set. Thus, duplicate tuples never appear in relations. In practice, duplicate elimina-\ntion is time-consuming. Therefore, SQL (like most other commercial query languages)\nallows duplicates in relations as well as in the results of SQL expressions. Thus, the\npreceding query will list each branch-name once for every tuple in which it appears in\nthe loan relation.\nIn those cases where we want to force the elimination of duplicates, we insert the\nkeyword distinct after select. We can rewrite the preceding query as\nselect distinct branch-name\nfrom loan\nif we want duplicates removed.\nSQL allows us to use the keyword all to specify explicitly that duplicates are not\nremoved:\nselect all branch-name\nfrom loan\nSince duplicate retention is the default, we will not use all in our examples. To ensure\nthe elimination of duplicates in the results of our example queries, we will use dis-\ntinct whenever it is necessary. In most queries where distinct is not used, the exact\nnumber of duplicate copies of each tuple present in the query result is not important.\nHowever, the number is important in certain applications; we return to this issue in\nSection 4.2.8.\nThe asterisk symbol “ * ” can be used to denote “all attributes.” Thus, the use of\nloan.* in the preceding select clause would indicate that all attributes of loan are to be\nselected. A select clause of the form select * indicates that all attributes of all relations\nappearing in the from clause are selected.\nThe select clause may also contain arithmetic expressions involving the operators\n+, −, ∗, and / operating on constants or attributes of tuples. For example, the query\nselect loan-number, branch-name, amount * 100\nfrom loan\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nII. Relational Databases\n4. SQL\n145\n© The McGraw−Hill \nCompanies, 2001\n4.2\nBasic Structure\n139\nwill return a relation that is the same as the loan relation, except that the attribute\namount is multiplied by 100.\nSQL also provides special data types, such as various forms of the date type, and\nallows several arithmetic functions to operate on these types.\n4.2.2\nThe where Clause\nLet us illustrate the use of the where clause in SQL. Consider the query “Find all loan\nnumbers for loans made at the Perryridge branch with loan amounts greater that\n$1200.” This query can be written in SQL as:\nselect loan-number\nfrom loan\nwhere branch-name = ’Perryridge’ and amount > 1200\nSQL uses the logical connectives and, or, and not—rather than the mathematical\nsymbols ∧, ∨, and ¬ —in the where clause. The operands of the logical connectives\ncan be expressions involving the comparison operators <, <=, >, >=, =, and <>.\nSQL allows us to use the comparison operators to compare strings and arithmetic\nexpressions, as well as special types, such as date types.\nSQL includes a between comparison operator to simplify where clauses that spec-\nify that a value be less than or equal to some value and greater than or equal to some\nother value. If we wish to ﬁnd the loan number of those loans with loan amounts\nbetween $90,000 and $100,000, we can use the between comparison to write\nselect loan-number\nfrom loan\nwhere amount between 90000 and 100000\ninstead of\nselect loan-number\nfrom loan\nwhere amount <= 100000 and amount >= 90000\nSimilarly, we can use the not between comparison operator.\n4.2.3\nThe from Clause\nFinally, let us discuss the use of the from clause. The from clause by itself deﬁnes a\nCartesian product of the relations in the clause. Since the natural join is deﬁned in\nterms of a Cartesian product, a selection, and a projection, it is a relatively simple\nmatter to write an SQL expression for the natural join.\nWe write the relational-algebra expression\nΠcustomer-name, loan-number, amount (borrower\n\u0001 loan)\nfor the query “For all customers who have a loan from the bank, ﬁnd their names,\nloan numbers and loan amount.” In SQL, this query can be written as\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nII. Relational Databases\n4. SQL\n146\n© The McGraw−Hill \nCompanies, 2001\n140\nChapter 4\nSQL\nselect customer-name, borrower.loan-number, amount\nfrom borrower, loan\nwhere borrower.loan-number = loan.loan-number\nNotice that SQL uses the notation relation-name.attribute-name, as does the relational\nalgebra, to avoid ambiguity in cases where an attribute appears in the schema of more\nthan one relation. We could have written borrower.customer-name instead of customer-\nname in the select clause. However, since the attribute customer-name appears in only\none of the relations named in the from clause, there is no ambiguity when we write\ncustomer-name.\nWe can extend the preceding query and consider a more complicated case in which\nwe require also that the loan be from the Perryridge branch: “Find the customer\nnames, loan numbers, and loan amounts for all loans at the Perryridge branch.” To\nwrite this query, we need to state two constraints in the where clause, connected by\nthe logical connective and:\nselect customer-name, borrower.loan-number, amount\nfrom borrower, loan\nwhere borrower.loan-number = loan.loan-number and\nbranch-name = ’Perryridge’\nSQL includes extensions to perform natural joins and outer joins in the from clause.\nWe discuss these extensions in Section 4.10.\n4.2.4\nThe Rename Operation\nSQL provides a mechanism for renaming both relations and attributes. It uses the as\nclause, taking the form:\nold-name as new-name\nThe as clause can appear in both the select and from clauses.\nConsider again the query that we used earlier:\nselect customer-name, borrower.loan-number, amount\nfrom borrower, loan\nwhere borrower.loan-number = loan.loan-number\nThe result of this query is a relation with the following attributes:\ncustomer-name, loan-number, amount.\nThe names of the attributes in the result are derived from the names of the attributes\nin the relations in the from clause.\nWe cannot, however, always derive names in this way, for several reasons: First,\ntwo relations in the from clause may have attributes with the same name, in which\ncase an attribute name is duplicated in the result. Second, if we used an arithmetic\nexpression in the select clause, the resultant attribute does not have a name. Third,\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nII. Relational Databases\n4. SQL\n147\n© The McGraw−Hill \nCompanies, 2001\n4.2\nBasic Structure\n141\neven if an attribute name can be derived from the base relations as in the preced-\ning example, we may want to change the attribute name in the result. Hence, SQL\nprovides a way of renaming the attributes of a result relation.\nFor example, if we want the attribute name loan-number to be replaced with the\nname loan-id, we can rewrite the preceding query as\nselect customer-name, borrower.loan-number as loan-id, amount\nfrom borrower, loan\nwhere borrower.loan-number = loan.loan-number\n4.2.5\nTuple Variables\nThe as clause is particularly useful in deﬁning the notion of tuple variables, as is\ndone in the tuple relational calculus. A tuple variable in SQL must be associated with\na particular relation. Tuple variables are deﬁned in the from clause by way of the as\nclause. To illustrate, we rewrite the query “For all customers who have a loan from\nthe bank, ﬁnd their names, loan numbers, and loan amount” as\nselect customer-name, T.loan-number, S.amount\nfrom borrower as T, loan as S\nwhere T.loan-number = S.loan-number\nNote that we deﬁne a tuple variable in the from clause by placing it after the name of\nthe relation with which it is associated, with the keyword as in between (the keyword\nas is optional). When we write expressions of the form relation-name.attribute-name,\nthe relation name is, in effect, an implicitly deﬁned tuple variable.\nTuple variables are most useful for comparing two tuples in the same relation.\nRecall that, in such cases, we could use the rename operation in the relational algebra.\nSuppose that we want the query “Find the names of all branches that have assets\ngreater than at least one branch located in Brooklyn.” We can write the SQL expression\nselect distinct T.branch-name\nfrom branch as T, branch as S\nwhere T.assets > S.assets and S.branch-city = ’Brooklyn’\nObserve that we could not use the notation branch.asset, since it would not be clear\nwhich reference to branch is intended.\nSQL permits us to use the notation (v1, v2, . . . , vn) to denote a tuple of arity n con-\ntaining values v1, v2, . . . , vn. The comparison operators can be used on tuples, and\nthe ordering is deﬁned lexicographically. For example, (a1, a2) <= (b1, b2) is true if\na1 < b1, or (a1 = b1) ∧(a2 <= b2); similarly, the two tuples are equal if all their\nattributes are equal.\n4.2.6\nString Operations\nSQL speciﬁes strings by enclosing them in single quotes, for example, ’Perryridge’,\nas we saw earlier. A single quote character that is part of a string can be speciﬁed by\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nII. Relational Databases\n4. SQL\n148\n© The McGraw−Hill \nCompanies, 2001\n142\nChapter 4\nSQL\nusing two single quote characters; for example the string “It’s right” can be speciﬁed\nby ’It”s right’.\nThe most commonly used operation on strings is pattern matching using the op-\nerator like. We describe patterns by using two special characters:\n• Percent (%): The % character matches any substring.\n• Underscore ( ): The\ncharacter matches any character.\nPatterns are case sensitive; that is, uppercase characters do not match lowercase char-\nacters, or vice versa. To illustrate pattern matching, we consider the following exam-\nples:\n• ’Perry%’ matches any string beginning with “Perry”.\n• ’%idge%’ matches any string containing “idge” as a substring, for example,\n’Perryridge’, ’Rock Ridge’, ’Mianus Bridge’, and ’Ridgeway’.\n• ’\n’ matches any string of exactly three characters.\n• ’\n%’ matches any string of at least three characters.\nSQL expresses patterns by using the like comparison operator. Consider the query\n“Find the names of all customers whose street address includes the substring ‘Main’.”\nThis query can be written as\nselect customer-name\nfrom customer\nwhere customer-street like ’%Main%’\nFor pattern\n\ntype system of\nobject-oriented databases, combined with relations as the basis for storage of data.\nIt applies inheritance to relations, not just to types. The object-relational data model\nprovides a smooth migration path from relational databases, which is attractive to\nrelational database vendors. As a result, the SQL:1999 standard includes a number\nof object-oriented features in its type system, while continuing to use the relational\nmodel as the underlying model.\nThe XML language was initially designed as a way of adding markup informa-\ntion to text documents, but has become important because of its applications in data\nexchange. XML provides a way to represent data that have nested structure, and fur-\nthermore allows a great deal of ﬂexibility in structuring of data, which is important\nfor certain kinds of nontraditional data. Chapter 10 describes the XML language, and\nthen presents different ways of expressing queries on data represented in XML, and\ntransforming XML data from one form to another.\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nIII. Object−Based \nDatabases and XML\n8. Object−Oriented \nDatabases\n308\n© The McGraw−Hill \nCompanies, 2001\nP\nA\nR\nT\n8\nCase Studies\nThis part describes how different database systems integrate the various concepts\ndescribed earlier in the book. Speciﬁcally, three widely used database systems—IBM\nDB2, Oracle, and Microsoft SQL Server—are covered in Chapters 25, 26, and 27. These\nthree represent three of the most widely used database systems.\nEach of these chapters highlights unique features of each database system: tools,\nSQL variations and extensions, and system architecture, including storage organiza-\ntion, query processing, concurrency control and recovery, and replication.\nThe chapters cover only key aspects of the database products they describe, and\ntherefore should not be regarded as a comprehensive coverage of the product. Fur-\nthermore, since products are enhanced regularly, details of the product may change.\nWhen using a particular product version, be sure to consult the user manuals for\nspeciﬁc details.\nKeep in mind that the chapters in this part use industrial rather than academic\nterminology. For instance, they use table instead of relation, row instead of tuple,\nand column instead of attribute.\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nIII. Object−Based \nDatabases and XML\n8. Object−Oriented \nDatabases\n309\n© The McGraw−Hill \nCompanies, 2001\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nIII. Object−Based \nDatabases and XML\n8. Object−Oriented \nDatabases\n310\n© The McGraw−Hill \nCompanies, 2001\nC\nH\nA\nP\nT\nE\nR\n2\n5\nOracle\nHakan Jakobsson\nOracle Corporation\nWhen Oracle was founded in 1977 as Software Development Laboratories by Larry\nEllison, Bob Miner, and Ed Oates, there were no commercial relational database prod-\nucts. The company, which was later renamed Oracle, set out to build a relational\ndatabase management system as a commercial product, and was the ﬁrst to reach the\nmarket. Since then, Oracle has held a leading position in the relational database mar-\nket, but over the years its product and service offerings have grown beyond the rela-\ntional database server. In addition to tools directly related to database development\nand management, Oracle sells business intelligence tools, including a multidimen-\nsional database management system (Oracle Express), query and analysis tools, data-\nmining products, and an application server with close integration to the database\nserver.\nIn addition to database-related servers and tools, the company also offers appli-\ncation software for enterprise resource planning and customer-relationship manage-\nment, including areas such as ﬁnancials, human resources, manufacturing, market-\ning, sales, and supply chain management. Oracle’s Business OnLine unit offers ser-\nvices in these areas as an application service provider.\nThis chapter surveys a subset of the features, options, and functionality of Oracle\nproducts. New versions of the products are being developed continually, so all prod-\nuct descriptions are subject to change. The feature set described here is based on the\nﬁrst release of Oracle9i.\n25.1\nDatabase Design and Querying Tools\nOracle provides a variety of tools for database design, querying, report generation\nand data analysis, including OLAP.\n921\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nIII. Object−Based \nDatabases and XML\n8. Object−Oriented \nDatabases\n311\n© The McGraw−Hill \nCompanies, 2001\n922\nChapter 25\nOracle\n25.1.1\nDatabase Design Tools\nMost of Oracle’s design tools are included in the Oracle Internet Development Suite.\nThis is a suite of tools for various aspects of application development, including tools\nfor forms development, data modeling, reporting, and querying. The suite supports\nthe UML standard (see Section 2.10) for development modeling. It provides class\nmodeling to generate code for the business components for Java framework as well\nas activity modeling for general-purpose control ﬂow modeling. The suite also sup-\nports XML for data exchange with other UML tools.\nThe major database design tool in the suite is Oracle Designer, which translates\nbusiness logic and data ﬂows into a schema deﬁnitions and procedural scripts for\napplication logic. It supports such modeling techniques as E-R diagrams, information\nengineering, and object analysis and design. Oracle Designer stores the design in\nOracle Repository, which serves as a single point of metadata for the application.\nThe metadata can then be used to generate forms and reports. Oracle Repository\nprovides conﬁguration management for database objects, forms applications, Java\nclasses, XML ﬁles, and other types of ﬁles.\nThe suite also contains application development tools for generating forms, re-\nports, and tools for various aspects of Java and XML-based development. The busi-\nness intelligence component provides JavaBeans for analytic functionality such as\ndata visualization, querying, and analytic calculations.\nOracle also has an application development tool for data warehousing, Oracle\nWarehouse Builder. Warehouse Builder is a tool for design and deployment of all as-\npects of a data warehouse, including schema design, data mapping and transforma-\ntions, data load processing, and metadata management. Oracle Warehouse Builder\nsupports both 3NF and star schemas and can also import designs from Oracle De-\nsigner.\n25.1.2\nQuerying Tools\nOracle provides tools for ad-hoc querying, report generation and data analysis, in-\ncluding OLAP.\nOracle Discoverer is a Web-based, ad hoc query, reporting, analysis and Web pub-\nlishing tool for end users and data analysts. It allows users to drill up and down on\nresult sets, pivot data, and store calculations as reports that can be published in a\nvariety of formats such as spreadsheets or HTML. Discoverer has wizards to help end\nusers visualize data as graphs. Oracle9i has supports a rich set of analytical func-\ntions, such as ranking and moving aggregation in SQL. Discoverer’s ad hoc query\ninterface can generate SQL that takes advantage of this functionality and can pro-\nvide end users with rich analytical functionality. Since the processing takes place in\nthe relational database management system, Discoverer does not require a complex\nclient-side calculation engine and there is a version of Discoverer that is browser\nbased.\nOracle Express Server is a multidimensional database server. It supports a wide\nvariety of analytical queries as well as forecasting, modeling, and scenario manage-\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nIII. Object−Based \nDatabases and XML\n8. Object−Oriented \nDatabases\n312\n© The McGraw−Hill \nCompanies, 2001\n25.2\nSQL Variations and Extensions\n923\nment. It can use the relational database management system as a back end for storage\nor use its own multidimensional storage of the data.\nWith the introduction of OLAP services in Oracle9i, Oracle is moving away from\nsupporting a separate storage engine and moving most of the calculations into SQL.\nThe result is a model where all the data reside in the relational database management\nsystem and where any remaining calculations that cannot be performed in SQL are\ndone in a calculation engine running on the database server. The model also provides\na Java OLAP application programmer interface.\nThere are many reasons for moving away from a separate multidimensional stor-\nage engine:\n• A relational engine can scale to much larger data sets.\n• A common security model can be used for the analytical applications and the\ndata warehouse.\n• Multidimensional modeling can be integrated with data warehouse modeling.\n• The relational database management system has a larger set of features and\nfunctionality in many areas such as high availability, backup and recovery,\nand third-party tool support.\n• There is no need to train database administrators for two database engines.\nThe main challenge with moving away from a separate multidimensional database\nengine is to provide the same performance. A multidimensional database manage-\nment system that materializes all or large parts of a data cube can offer very fast\nresponse times for many calculations. Oracle has approached this problem in two\nways.\n• Oracle has added SQL support for a wide range of analytical functions, in-\ncluding cube, rollup, grouping sets, ranks, moving aggregation, lead and lag\nfunctions, histogram buckets, linear regression, and standard deviation, along\nwith the ability to optimize the execution of such functions in the database en-\ngine.\n• Oracle has extended materialized views to permit analytical functions, in par-\nticular grouping sets. The ability to materialize parts or all of the cube is key\nto the performance of a multidimensional database management system and\nmaterialized views give a relational database management system the ability\nto do the same thing.\n25.2\nSQL Variations and Extensions\nOracle9i supports all core SQL:1999 features fully or partially, with some minor ex-\nceptions such as distinct data types. In addition, Oracle supports a large number of\nother language constructs, some of which conform with SQL:1999, while others are\nOracle-speciﬁc in syntax or functionality. For example, Oracle supports the OLAP\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nIII. Object−Based \nDatabases and XML\n8. Object−Oriented \nDatabases\n313\n© The McGraw−Hill \nCompanies, 2001\n924\nChapter 25\nOracle\noperations described in Section 22.2, including ranking, moving aggregation, cube,\nand rollup.\nA few examples of Oracle SQL extensions are:\n• connect by, which is a form of tree traversal that allows transitive closure-\nstyle calculations in a single SQL statement. It is an Oracle-speciﬁc syntax for\na feature that Oracle has had since the 1980s.\n• Upsert and multitable inserts. The upsert operation combines update and in-\nsert, and is useful for merging new data with old data in data warehousing\napplications. If a new row has the same key value as an old row, the old row is\nupdated (for example by adding the measure values from the new row), oth-\nerwise the new row is inserted into the table. Multitable inserts allow multiple\ntables to be updated based on a single scan of new data.\n• with clause, which is described in Section 4.8.2.\n25.2.1\nObject-Relational Features\nOracle has extensive support for object-relational constructs, including:\n• Object types. A single-inheritance model is supported for type hierarchies.\n• Collection types. Oracle supports varrays which are variable length arrays,\nand nested tables.\n• Object tables. These are used to store objects while providing a relational\nview of the attributes of the objects.\n• Table functions. These are functions that produce sets of rows as output, and\ncan be used in the from clause of a query. Table functions in Oracle can be\nnested. If a table function is used to express some form of data transformation,\nnesting multiple functions allows multiple transformations to be expressed in\na single statement.\n• Object views. These provide a virtual object table view of data stored in a\nregular relational table. They allow data to be accessed or viewed in an object-\noriented style even if the data are really stored in a traditional relational for-\nmat.\n• Methods. These can be written in PL/SQL, Java, or C.\n• User-deﬁned aggregate functions. These can be used in SQL statements in the\nsame way as built-in functions such as sum and count.\n• XML data types. These can be used to store and index XML documents.\nOracle has two main procedural languages, PL/SQL and Java. PL/SQL was Oracle’s\noriginal language for stored procedures and it has syntax similar to that used in the\nAda language. Java is supported through a Java virtual machine inside the database\nengine. Oracle provides a package to encapsulate related procedures, functions, and\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nIII. Object−Based \nDatabases and XML\n8. Object−Oriented \nDatabases\n314\n© The McGraw−Hill \nCompanies, 2001\n25.3\nStorage and Indexing\n925\nvariables into single units. Oracle supports SQLJ (SQL embedded in Java) and JDBC,\nand provides a tool to generate Java class deﬁnitions corresponding to user-deﬁned\ndatabase types.\n25.2.2\nTriggers\nOracle provides several types of triggers and several options for when and how they\nare invoked. (See Section 6.4 for an introduction to triggers in SQL.) Triggers can be\nwritten in PL/SQL or Java or as C callouts.\nFor triggers that execute on DML statements such as insert, update, and delete,\nOracle supports row triggers and statement triggers. Row triggers execute once for\nevery row that is affected (updated or deleted, for example) by the DML operation.\nA statement trigger is executed just once per statement. In each case, the trigger can\nbe deﬁned as either a before or after trigger, depending on whether it is to be invoked\nbefore or after the DML operation is carried out.\nOracle allows the creation of instead of triggers for views that cannot be subject\nto DML operations. Depending on the view deﬁnition, it may not be possible for Or-\nacle to translate a DML statement on a view to modiﬁcations of the underlying base\ntables unambiguously. Hence, DML operations on views are subject to numerous re-\nstrictions. A user can create an instead of trigger on a view to specify manually what\noperations on the base tables are to occur in response to the DML operation on the\nview. Oracle executes the trigger instead of the DML operation and therefore pro-\nvides a mechanism to circumvent the restrictions on DML operations against views.\nOracle also has triggers that execute on a variety of other events, like database\nstartup or shutdown, server error messages, user logon or logoff, and DDL statements\nsuch as create, alter and drop statements.\n25.3\nStorage and Indexing\nIn Oracle parlance, a database consists of information stored in ﬁles and is accessed\nthrough an instance, which is a shared memory area and a set of processes that inter-\nact with the data in the ﬁles.\n25.3.1\nTable Spaces\nA database consists of one or more logical storage units called table spaces. Each\ntable space, in turn, consists of one or more physical structures called data ﬁles. These\nmay be either ﬁles managed by the operating system or raw devices.\nUsually, an Oracle database will have the following table spaces:\n• The system table space, which is always created. It contains the data dictio-\nnary tables and storage for triggers and stored procedures.\n• Table spaces created to store user data. While user data can be stored in the\nsystem table space, it is often desirable to separate the user data from the sys-\ntem data. Usually, the decision about what other table spaces should be cre-\nated is based on performance, availability, maintainability, and ease of admin-\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nIII. Object−Based \nDatabases and XML\n8. Object−Oriented \nDatabases\n315\n© The McGraw−Hill \nCompanies, 2001\n926\nChapter 25\nOracle\nistration. For example, having multiple table spaces can be useful for partial\nbackup and recovery operations.\n• Temporary table spaces. Many database operations require sorting the data,\nand the sort routine may have to store data temporarily on disk if the sort\ncannot be done in memory. Temporary table spaces are allocated for sorting,\nto make the space management operations involved in spilling to disk more\nefﬁcient.\nTable spaces can also be used as a means of moving data between databases. For\nexample, it is common to move data from a transactional system to a data warehouse\nat regular intervals. Oracle allows moving all the data in a table space from one sys-\ntem to the other by simply copying the ﬁles and exporting and importing a small\namount of data dictionary metadata. These operations can be much faster than un-\nloading the data from one database and then using a loader to insert it into the other.\nA requirement for this feature is that both systems use the same operating system.\n25.3.2\nSegments\nThe space in a table space is divided into units, called segments, that each contain\ndata for a speciﬁc data structure. There are four types of segments.\n• Data segments. Each table in a table space has its own data segment where\nthe table data are stored unless the table is partitioned; if so, there is one data\nsegment per partition. (Partitioning in Oracle is described in Section 25.3.10.)\n• Index segments. Each index in a table space has its own index segment, except\nfor partitioned indices, which have one index segment per partition.\n• Temporary segments. These are segments used when a sort operation needs\nto write data to disk or when data are inserted into a temporary table.\n• Rollback segments. These segments contain undo information so that an un-\ncommitted transaction can be rolled back. They also play an important roll in\nOracle’s concurrency control model and for database recovery, described in\nSections 25.5.1 and 25.5.2.\nBelow the level of segment, space is allocated at a level of granularity called extent.\nEach extent consists of a set of contiguous database blocks. A database block is the\nlowest level of granularity at which Oracle performs disk I/O. A database block does\nnot have to be the same as an operating system block in size, but should be a multiple\nthereof.\nOracle provides storage parameters that allow for detailed control of how space is\nallocated and managed, parameters such as:\n• The size of a new extent that is to be allocated to provide room for rows that\nare inserted into a table.\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nIII. Object−Based \nDatabases and XML\n8. Object−Oriented \nDatabases\n316\n© The McGraw−Hill \nCompanies, 2001\n25.3\nStorage and Indexing\n927\n• The percentage of space utilization at which a database block is considered full\nand at which no more rows will be inserted into that block. (Leaving some free\nspace in a block can allow the existing rows to grow in size through updates,\nwithout running out of space in the block.)\n25.3.3\nTables\nA standard table in Oracle is heap organized; that is, the storage location of a row in\na table is not based on the values contained in the row, and is ﬁxed when the row\nis inserted. However, if the table is partitioned, the content of the row affects the\npartition in which it is stored. There are several features and variations.\nOracle supports nested tables; that is, a table can have a column whose data type\nis another table. The nested table is not stored in line in the parent table, but is stored\nin a separate table.\nOracle supports temporary tables where the duration of the data is either the trans-\naction in which the data are inserted, or the user session. The data are private to the\nsession and are automatically removed at the end of its du",
    "precision": 0.02507192766132347,
    "recall": 0.9838709677419355,
    "iou": 0.02506162695152013,
    "f1": 0.048897795591182375,
    "gold_tokens_count": 62,
    "retrieved_tokens_count": 2433,
    "intersection_tokens": 61
  },
  {
    "config_name": "chars_rrf",
    "config": {
      "name": "chars_rrf",
      "index_prefix": "textbook_index",
      "chunking_strategy": "chars",
      "overlap": 0,
      "fusion": "rrf",
      "bm25_weight": 0.3,
      "tag_weight": 0.2,
      "top_k": 5,
      "embed_model": "sentence-transformers/all-MiniLM-L6-v2"
    },
    "question": "Define the concept of aggregation. Give two examples where this concept is useful.",
    "gold_text": "2.7.5 Aggregation One limitation of the E-R model is that it cannot express relationships among rela- tionships. To illustrate the need for such a construct, consider the ternary relationship works-on, which we saw earlier, between a employee, branch, and job (see Figure 2.13). Now, suppose we want to record managers for tasks performed by an employee at a branch; that is, we want to record managers for (employee, branch, job) combinations. Let us assume that there is an entity set manager. One alternative for representing this relationship is to create a quaternary relation- ship manages between employee, branch, job, and manager. (A quaternary relationship is required—a binary relationship between manager and employee would not permit us to represent which (branch, job) combinations of an employee are managed by which manager.) Using the basic E-R modeling constructs, we obtain the E-R diagram of Figure 2.18. (We have omitted the attributes of the entity sets, for simplicity.) It appears that the relationship sets works-on and manages can be combined into one single relationship set. Nevertheless, we should not combine them into a single relationship, since some employee, branch, job combinations many not have a manager. There is redundant information in the resultant figure, however, since every em- ployee, branch, job combination in manages is also in works-on. If the manager were a value rather than an manager entity, we could instead make manager a multivalued at- tribute of the relationship works-on. But doing so makes it more difficult (logically as well as in execution cost) to find, for example, employee-branch-job triples for which a manager is responsible. Since the manager is a manager entity, this alternative is ruled out in any case. The best way to model a situation such as the one just described is to use aggrega- tion. Aggregation is an abstraction through which relationships are treated as higher- level entities. Thus, for our example, we regard the relationship set works-on (relating the entity sets employee, branch, and job) as a higher-level entity set called works-on. Such an entity set is treated in the same manner as is any other entity set. We can then create a binary relationship manages between works-on and manager to represent who manages what tasks. Figure 2.19 shows a notation for aggregation commonly used to represent the above situation.",
    "retrieved_text": "loyees at each\nbranch of the bank separately, rather than the sum for the entire bank. To do so, we\nneed to partition the relation pt-works into groups based on the branch, and to apply\nthe aggregate function on each group.\nThe following expression using the aggregation operator G achieves the desired\nresult:\nbranch-nameGsum(salary)(pt-works)\nIn the expression, the attribute branch-name in the left-hand subscript of G indicates\nthat the input relation pt-works must be divided into groups based on the value of\nbranch-name. Figure 3.28 shows the resulting groups. The expression sum(salary) in\nthe right-hand subscript of G indicates that for each group of tuples (that is, each\nbranch), the aggregation function sum must be applied on the collection of values of\nthe salary attribute. The output relation consists of tuples with the branch name, and\nthe sum of the salaries for the branch, as shown in Figure 3.29.\nThe general form of the aggregation operation G is as follows:\nG1,G2,...,GnGF1(A1), F2(A2),..., Fm(Am)(E)\nwhere E is any relational-algebra expression; G1, G2, . . . , Gn constitute a list of at-\ntributes on which to group; each Fi is an aggregate function; and each Ai is an at-\nemployee-name\nbranch-name\nsalary\nRao\nAustin\n1500\nSato\nAustin\n1600\nJohnson\nDowntown\n1500\nLoreena\nDowntown\n1300\nPeterson\nDowntown\n2500\nAdams\nPerryridge\n1500\nBrown\nPerryridge\n1300\nGopal\nPerryridge\n5300\nFigure 3.28\nThe pt-works relation after grouping.\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nI. Data Models\n3. Relational Model\n115\n© The McGraw−Hill \nCompanies, 2001\n3.3\nExtended Relational-Algebra Operations\n107\nbranch-name\nsum of salary\nAustin\n3100\nDowntown\n5300\nPerryridge\n8100\nFigure 3.29\nResult of branch-nameGsum(salary)(pt-works).\ntribute name. The meaning of the operation is as follows. The tuples in the result of\nexpression E are partitioned into groups in such a way that\n1. All tuples in a group have the same values for G1, G2, . . . , Gn.\n2. Tuples in different groups have different values for G1, G2, . . . , Gn.\nThus, the groups can be identiﬁed by the values of attributes G1, G2, . . . , Gn. For each\ngroup (g1, g2, . . . , gn), the result has a tuple (g1, g2, . . . , gn, a1, a2, . . . , am) where, for\neach i, ai is the result of applying the aggregate function Fi on the multiset of values\nfor attribute Ai in the group.\nAs a special case of the aggregate operation, the list of attributes G1, G2, . . . , Gn can\nbe empty, in which case there is a single group containing all tuples in the relation.\nThis corresponds to aggregation without grouping.\nGoing back to our earlier example, if we want to ﬁnd the maximum salary for\npart-time employees at each branch, in addition to the sum of the salaries, we write\nthe expression\nbranch-nameGsum(salary),max(salary)(pt-works)\nAs in generalized projection, the result of an aggregation operation does not have a\nname. We can apply a rename operation to the result in order to give it a name. As\na notational convenience, attributes of an aggregation operation can be renamed as\nillustrated below:\nbranch-nameGsum(salary) as sum-salary,max(salary) as max-salary(pt-works)\nFigure 3.30 shows the result of the expression.\nbranch-name\nsum-salary\nmax-salary\nAustin\n3100\n1600\nDowntown\n5300\n2500\nPerryridge\n8100\n5300\nFigure 3.30\nResult of\nbranch-nameGsum(salary) as sum-salary,max(salary) as max-salary(pt-works).\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nI. Data Models\n3. Relational Model\n116\n© The McGraw−Hill \nCompanies, 2001\n108\nChapter 3\nRelational Model\nemployee-name\nstreet\ncity\nCoyote\nToon\nHollywood\nRabbit\nTunnel\nCarrotville\nSmith\nRevolver\nDeath Valley\nWilliams\nSeaview\nSeattle\nemployee-name\nbranch-name\nsalary\nCoyote\nMesa\n1500\nRabbit\nMesa\n1300\nGates\nRedmond\n5300\nWilliams\nRedmond\n1500\nFigure 3.31\nThe employee and ft-works relations.\n3.3.3\nOuter Join\nThe outer-join operation is an extension of the join operation to deal with missing\ninformation. Suppose that we have the relations with the following schemas, which\ncontain data on full-time employees:\nemployee (employee-name, street, city)\nft-works (employee-name, branch-name, salary)\nConsider the employee and ft-works relations in Figure 3.31. Suppose that we want\nto generate a single relation with all the information (street, city, branch name, and\nsalary) about full-time employees. A possible approach would be to use the natural-\njoin operation as follows:\nemployee\n\u0001 ft-works\nThe result of this expression appears in Figure 3.32. Notice that we have lost the street\nand city information about Smith, since the tuple describing Smith is absent from\nthe ft-works relation; similarly, we have lost the branch name and salary information\nabout Gates, since the tuple describing Gates is absent from the employee relation.\nWe can use the outer-join operation to avoid this loss of information. There are\nactually three forms of the operation: left outer join, denoted\n\u0001; right outer join, de-\nnoted\n\u0001 ; and full outer join, denoted\n\u0001 . All three forms of outer join compute the\njoin, and add extra tuples to the result of the join. The results of the expressions\nemployee-name\nstreet\ncity\nbranch-name\nsalary\nCoyote\nToon\nHollywood\nMesa\n1500\nRabbit\nTunnel\nCarrotville\nMesa\n1300\nWilliams\nSeaview\nSeattle\nRedmond\n1500\nFigure 3.32\nThe result of employee\n\u0001 ft-works.\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nI. Data Models\n3. Relational Model\n117\n© The McGraw−Hill \nCompanies, 2001\n3.3\nExtended Relational-Algebra Operations\n109\nemployee-name\nstreet\ncity\nbranch-name\nsalary\nCoyote\nToon\nHollywood\nMesa\n1500\nRabbit\nTunnel\nCarrotville\nMesa\n1300\nWilliams\nSeaview\nSeattle\nRedmond\n1500\nSmith\nRevolver\nDeath Valley\nnull\nnull\nFigure 3.33\nResult of employee\n\u0001 ft-works.\nemployee\n\u0001 ft-works,, employee\n\u0001 ft-works, and employee\n\u0001 ft-works appear in\nFigures 3.33, 3.34, and 3.35, respectively.\nThe left outer join (\n\u0001) takes all tuples in the left relation that did not match with\nany tuple in the right relation, pads the tuples with null values for all other attributes\nfrom the right relation, and adds them to the result of the natural join. In Figure 3.33,\ntuple (Smith, Revolver, Death Valley, null, null) is such a tuple. All information from\nthe left relation is present in the result of the left outer join.\nThe right outer join (\u0001 ) is symmetric with the left outer join: It pads tuples from\nthe right relation that did not match any from the left relation with nulls and adds\nthem to the result of the natural join. In Figure 3.34, tuple (Gates, null, null, Redmond,\n5300) is such a tuple. Thus, all information from the right relation is present in the\nresult of the right outer join.\nThe full outer join(\n\u0001 ) does both of those operations, padding tuples from the\nleft relation that did not match any from the right relation, as well as tuples from the\nright relation that did not match any from the left relation, and adding them to the\nresult of the join. Figure 3.35 shows the result of a full outer join.\nSince outer join operations may generate results containing null values, we need\nto specify how the different relational-algebra operations deal with null values. Sec-\ntion 3.3.4 deals with this issue.\nIt is interesting to note that the outer join operations can be expressed by the basic\nrelational-algebra operations. For instance, the left outer join operation, r\n\u0001 s, can\nbe written as\n(r\n\u0001 s) ∪(r −ΠR(r\n\u0001 s)) × {(null, . . . , null)}\nwhere the constant relation {(null, . . . , null)} is on the schema S −R.\nemployee-name\nstreet\ncity\nbranch-name\nsalary\nCoyote\nToon\nHollywood\nMesa\n1500\nRabbit\nTunnel\nCarrotville\nMesa\n1300\nWilliams\nSeaview\nSeattle\nRedmond\n1500\nGates\nnull\nnull\nRedmond\n5300\nFigure 3.34\nResult of employee\n\u0001 ft-works.\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nI. Data Models\n3. Relational Model\n118\n© The McGraw−Hill \nCompanies, 2001\n110\nChapter 3\nRelational Model\nemployee-name\nstreet\ncity\nbranch-name\nsalary\nCoyote\nToon\nHollywood\nMesa\n1500\nRabbit\nTunnel\nCarrotville\nMesa\n1300\nWilliams\nSeaview\nSeattle\nRedmond\n1500\nSmith\nRevolver\nDeath Valley\nnull\nnull\nGates\nnull\nnull\nRedmond\n5300\nFigure 3.35\nResult of employee\n\u0001 ft-works.\n3.3.4\nNull Values∗∗\nIn this section, we deﬁne how the various relational algebra operations deal with null\nvalues and complications that arise when a null value participates in an arithmetic\noperation or in a comparison. As we shall see, there is often more than one possible\nway of dealing with null values, and as a result our deﬁnitions can sometimes be\narbitrary. Operations and comparisons on null values should therefore be avoided,\nwhere possible.\nSince the special value null indicates “value unknown or nonexistent,” any arith-\nmetic operations (such as +, −, ∗, /) involving null values must return a null result.\nSimilarly, any comparisons (such as <, <=, >, >=, ̸=) involving a null value eval-\nuate to special value unknown; we cannot say for sure whether the result of the\ncomparison is true or false, so we say that the result is the new truth value unknown.\nComparisons involving nulls may occur inside Boolean expressions involving the\nand, or, and not operations. We must therefore deﬁne how the three Boolean opera-\ntions deal with the truth value unknown.\n• and: (true and unknown) = unknown; (false and unknown) = false; (unknown and\nunknown) = unknown.\n• or: (true or unknown) = true; (false or unknown) = unknown; (unknown or un-\nknown) = unknown.\n• not: (not unknown) = unknown.\nWe are now in a position to outline how the different relational operations deal\nwith null values. Our deﬁnitions follow those used in the SQL language.\n• select: The selection operation evaluates predicate P in σP (E) on each tuple t\nin E. If the predicate returns the value true, t is added to the result. Otherwise,\nif the predicate returns unknown or false, t is not added to the result.\n• join: Joins can be expressed as a cross product followed by a selection. Thus,\nthe deﬁnition of how selection handles nulls also deﬁnes how join operations\nhandle nulls.\nIn a natural join, say r\n\u0001 s, we can see from the above deﬁnition that if two\ntuples, tr ∈r and ts ∈s, both have a null value in a common attribute, then\nthe tuples do not match.\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nI. Data Models\n3. Relational Model\n119\n© The McGraw−Hill \nCompanies, 2001\n3.4\nModiﬁcation of the Database\n111\n• projection: The projection operation treats nulls just like any other value when\neliminating duplicates. Thus, if two tuples in the projection result are exactly\nthe same, and both have nulls in the same ﬁelds, they are treated as duplicates.\nThe decision is a little arbitrary since, without knowing the actual value,\nwe do not know if the two instances of null are duplicates or not.\n• union, intersection, difference: These operations treat nulls just as the projec-\ntion operation does; they treat tuples that have the same values on all ﬁelds as\nduplicates even if some of the ﬁelds have null values in both tuples.\nThe behavior is rather arbitrary, especially in the case of intersection and\ndifference, since we do not know if the actual values (if any) represented by\nthe nulls are the same.\n• generalized projection: We outlined how nulls are handled in expressions\nat the beginning of Section 3.3.4. Duplicate tuples containing null values are\nhandled as in the projection operation.\n• aggregate: When nulls occur in grouping attributes, the aggregate operation\ntreats them just as in projection: If two tuples are the same on all grouping\nattributes, the operation places them in the same group, even if some of their\nattribute values are null.\nWhen nulls occur in aggregated attributes, the operation deletes null values\nat the outset, before applying aggregation. If the resultant multiset is empty,\nthe aggregate result is null.\nNote that the treatment of nulls here is different from that in ordinary arith-\nmetic expressions; we could have deﬁned the result of an aggregate operation\nas null if even one of the aggregated values is null. However, this would mean\na single unknown value in a large group could make the aggregate result on\nthe group to be null, and we would lose a lot of useful information.\n• outer join: Outer join operations behave just like join operations, except on\ntuples that do not occur in the join result. Such tuples may be added to the\nresult (depending on whether the operation is\n\u0001,\n\u0001 , or\n\u0001 ), padded with\nnulls.\n3.4\nModiﬁcation of the Database\nWe have limited our attention until now to the extraction of information from the\ndatabase. In this section, we address how to add, remove, or change information in\nthe database.\nWe express database modiﬁcations by using the assignment operation. We make\nassignments to actual database relations by using the same notation as that described\nin Section 3.2.3 for assignment.\n3.4.1\nDeletion\nWe express a delete request in much the same way as a query. However, instead of\ndisplaying tuples to the user, we remove the selected tuples from the database. We\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nI. Data Models\n3. Relational Model\n120\n© The McGraw−Hill \nCompanies, 2001\n112\nChapter 3\nRelational Model\ncan delete only whole tuples; we cannot delete values on only particular attributes.\nIn relational algebra a deletion is expressed by\nr ←r −E\nwhere r is a relation and E is a relational-algebra query.\nHere are several examples of relational-algebra delete requests:\n• Delete all of Smith’s account records.\ndepositor ←depositor −σcustomer-name = “Smith” (depositor)\n• Delete all loans with amount in the range 0 to 50.\nloan ←loan −σamount≥0 and amount≤50 (loan)\n• Delete all accounts at branches located in Needham.\nr1 ←σbranch-city = “Needham” (account\n\u0001 branch)\nr2 ←Πbranch-name, account-number, balance (r1)\naccount ←account −r2\nNote that, in the ﬁnal example, we simpliﬁed our expression by using assign-\nment to temporary relations (r1 and r2).\n3.4.2\nInsertion\nTo insert data into a relation, we either specify a tuple to be inserted or write a query\nwhose result is a set of tuples to be inserted. Obviously, the attribute values for in-\nserted tuples must be members of the attribute’s domain. Similarly, tuples inserted\nmust be of the correct arity. The relational algebra expresses an insertion by\nr ←r ∪E\nwhere r is a relation and E is a relational-algebra expression. We express the insertion\nof a single tuple by letting E be a constant relation containing one tuple.\nSuppose that we wish to insert the fact that Smith has $1200 in account A-973 at\nthe Perryridge branch. We write\naccount ←account ∪{(A-973, “Perryridge”, 1200)}\ndepositor ←depositor ∪{(“Smith”, A-973)}\nMore generally, we might want to insert tuples on the basis of the result of a query.\nSuppose that we want to provide as a gift for all loan customers of the Perryridge\nbranch a new $200 savings account. Let the loan number serve as the account number\nfor this savings account. We write\nr1 ←(σbranch-name = “Perryridge” (borrower\n\u0001 loan))\nr2 ←Πloan-number, branch-name (r1)\naccount ←account ∪(r2 × {(200)})\ndepositor ←depositor ∪Πcustomer-name, loan-number (r1)\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nI. Data Models\n3. Relational Model\n121\n© The McGraw−Hill \nCompanies, 2001\n3.5\nViews\n113\nInstead of specifying a tuple as we did earlier, we specify a set of tuples that is in-\nserted into both the account and depositor relation. Each tuple in the account relation\nhas an account-number (which is the same as the loan number), a branch-name (Per-\nryridge), and the initial balance of the new account ($200). Each tuple in the depositor\nrelation has as customer-name the name of the loan customer who is being given the\nnew account and the same account number as the corresponding account tuple.\n3.4.3\nUpdating\nIn certain situations, we may wish to change a value in a tuple without changing all\nvalues in the tuple. We can use the generalized-projection operator to do this task:\nr ←ΠF1,F2,...,Fn(r)\nwhere each Fi is either the ith attribute of r, if the ith attribute is not updated, or, if\nthe attribute is to be updated, Fi is an expression, involving only constants and the\nattributes of r, that gives the new value for the attribute.\nIf we want to select some tuples from r and to update only them, we can use\nthe following expression; here, P denotes the selection condition that chooses which\ntuples to update:\nr ←ΠF1,F2,...,Fn(σP (r)) ∪(r −σP (r))\nTo illustrate the use of the update operation, suppose that interest payments are\nbeing made, and that all balances are to be increased by 5 percent. We write\naccount ←Πaccount-number, branch-name, balance ∗1.05 (account)\nNow suppose that accounts with balances over $10,000 receive 6 percent interest,\nwhereas all others receive 5 percent. We write\naccount ←ΠAN,BN, balance ∗1.06 (σbalance>10000 (account))\n∪ΠAN, BN balance ∗1.05 (σbalance≤10000 (account))\nwhere the abbreviations AN and BN stand for account-number and branch-name, re-\nspectively.\n3.5\nViews\nIn our examples up to this point, we have operated at the logical-model level. That\nis, we have assumed that the relations in thecollection we are given are the actual\nrelations stored in the database.\nIt is not desirable for all users to see the entire logical model. Security consider-\nations may require that certain data be hidden from users. Consider a person who\nneeds to know a customer’s loan number and branch name, but has no need to see\nthe loan amount. This person should see a relation described, in the relational alge-\nbra, by\nΠcustomer-name, loan-number, branch-name (borrower\n\u0001 loan)\nAside from security concerns, we may wish to create a personalized collection of\nrelations that is better matched to a certain user’s intuition than is the logical model.\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nI. Data Models\n3. Relational Model\n122\n© The McGraw−Hill \nCompanies, 2001\n114\nChapter 3\nRelational Model\nAn employee in the advertising department, for example, might like to see a relation\nconsisting of the customers who have either an account or a loan at the bank, and\nthe branches with which they do business. The relation that we would create for that\nemployee is\nΠbranch-name, customer-name (depositor\n\u0001 account)\n∪Πbranch-name, customer-name (borrower\n\u0001 loan)\nAny relation that is not part of the logical model, but is made visible to a user as a\nvirtual relation, is called a view. It is possible to support a large number of views on\ntop of any given set of actual relations.\n3.5.1\nView Deﬁnition\nWe deﬁne a view by using the create view statement. To deﬁne a view, we must give\nthe view a name, and must state the query that computes the view. The form of the\ncreate view statement is\ncreate view v as <query expression>\nwhere <query expression> is any legal relational-algebra query expression. The view\nname is represented by v.\nAs an example, consider the view consisting of branches and their customers. We\nwish this view to be called all-customer. We deﬁne this view as follows:\ncreate view all-customer as\nΠbranch-name, customer-name (depositor\n\u0001 account)\n∪Πbranch-name, customer-name (borrower\n\u0001 loan)\nOnce we have deﬁned a view, we can use the view name to refer to the virtual re-\nlation that the view generates. Using the view all-customer, we can ﬁnd all customers\nof the Perryridge branch by writing\nΠcustomer-name (σbranch-name = “Perryridge” (all-customer))\nRecall that we wrote the same query in Section 3.2.1 without using views.\nView names may appear in any place where a relation name may appear, so long\nas no update operations are executed on the views. We study the issue of update\noperations on views in Section 3.5.2.\nView deﬁnition differs from the relational-algebra assignment operation. Suppose\nthat we deﬁne relation r1 as follows:\nr1 ←Πbranch-name, customer-name (depositor\n\u0001 account)\n∪Πbranch-name, customer-name(borrower\n\u0001 loan\n\nration.\nA cluster is another form of organization for table data (see Section 11.7). The\nconcept, in this context, should not be confused with other meanings of the word\ncluster, such as those relating to hardware architecture. In a cluster, rows from dif-\nferent tables are stored together in the same block on the basis of some common\ncolumns. For example, a department table and an employee table could be clustered\nso that each row in the department table is stored together with all the employee\nrows for those employees who work in that department. The primary key/foreign\nkey values are used to determine the storage location. This organization gives per-\nformance beneﬁts when the two tables are joined, but without the space penalty of a\ndenormalized schema, since the values in the department table are not repeated for\neach employee. As a tradeoff, a query involving only the department table may have\nto involve a substantially larger number of blocks than if that table had been stored\non its own.\nThe cluster organization implies that a row belongs in a speciﬁc place; for example,\na new employee row must be inserted with the other rows for the same department.\nTherefore, an index on the clustering column is mandatory. An alternative organiza-\ntion is a hash cluster. Here, Oracle computes the location of a row by applying a hash\nfunction to the value for the cluster column. The hash function maps the row to a\nspeciﬁc block in the hash cluster. Since no index traversal is needed to access a row\naccording to its cluster column value, this organization can save signiﬁcant amounts\nof disk I/O. However, the number of hash buckets and other storage parameters must\nbe set carefully to avoid performance problems due to too many collisions or space\nwastage due to empty hash buckets.\nBoth the hash cluster and regular cluster organization can be applied to a single\ntable. Storing a table as a hash cluster with the primary key column as the cluster key\ncan allow an access based on a primary key value with a single disk I/O provided\nthat there is no overﬂow for that data block.\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nIII. Object−Based \nDatabases and XML\n8. Object−Oriented \nDatabases\n317\n© The McGraw−Hill \nCompanies, 2001\n928\nChapter 25\nOracle\n25.3.4\nIndex-Organized Tables\nIn an index organized table, records are stored in an Oracle B-tree index instead of in a\nheap. An index-organized table requires that a unique key be identiﬁed for use as the\nindex key. While an entry in a regular index contains the key value and row-id of the\nindexed row, an index-organized table replaces the row-id with the column values\nfor the remaining columns of the row. Compared to storing the data in a regular heap\ntable and creating an index on the key columns, index-organized table can improve\nboth performance and space utilization. Consider looking up all the column values\nof a row, given its primary key value. For a heap table, that would require an index\nprobe followed by a table access by row-id. For an index-organized table, only the\nindex probe is necessary.\nSecondary indices on nonkey columns of an index-organized table are different\nfrom indices on a regular heap table. In a heap table, each row has a ﬁxed row-id\nthat does not change. However, a B-tree is reorganized as it grows or shrinks when\nentries are inserted or deleted, and there is no guarantee that a row will stay in a\nﬁxed place inside an index-organized table. Hence, a secondary index on an index-\norganized table contains not normal row-ids, but logical row-ids instead. A logical\nrow-id consists of two parts: a physical row-id corresponding to where the row was\nwhen the index was created or last rebuilt and a value for the unique key. The phys-\nical row-id is referred to as a “guess” since it could be incorrect if the row has been\nmoved. If so, the other part of a logical row-id, the key value for the row, is used to\naccess the row; however, this access is slower than if the guess had been correct, since\nit involves a traversal of the B-tree for the index-organized table from the root all the\nway to the leaf nodes, potentially incurring several disk I/Os. However, if a table is\nhighly volatile and a large percentage of the guesses are likely to be wrong, it can be\nbetter to create the secondary index with only key values, since using an incorrect\nguess may result in a wasted disk I/O.\n25.3.5\nIndices\nOracle supports several different types of indices. The most commonly used type is a\nB-tree index, created on one or multiple columns. (Note: in the terminology of Oracle\n(as also in several other database systems) a B-tree index is what is referred to as a\nB+-tree index in Chapter 12.) Index entries have the following format: For an index\non columns col1, col2, and col3, each row in the table where at least one of the columns\nhas a nonnull value would result in the index entry\n< col1 >< col2 >< col3 >< row-id >\nwhere < coli > denotes the value for column i and < row-id > is the row-id for\nthe row. Oracle can optionally compress the preﬁx of the entry to save space. For\nexample, if there are many repeated combinations of < col1 >< col2 > values, the\nrepresentation of each distinct < col1 >< col2 > preﬁx can be shared between the\nentries that have that combination of values, rather than stored explicitly for each\nsuch entry. Preﬁx compression can lead to substantial space savings.\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nIII. Object−Based \nDatabases and XML\n8. Object−Oriented \nDatabases\n318\n© The McGraw−Hill \nCompanies, 2001\n25.3\nStorage and Indexing\n929\n25.3.6\nBitmap Indices\nBitmap indices (described in Section 12.9.4) use a bitmap representation for index\nentries, which can lead to substantial space saving (and therefore disk I/O savings),\nwhen the indexed column has a moderate number of distinct values. Bitmap indices\nin Oracle use the same kind of B-tree structure to store the entries as a regular in-\ndex. However, where a regular index on a column would have entries of the form\n< col1 >< row-id >, a bitmap index entry has the form\n< col1 >< startrow-id >< endrow-id >< compressedbitmap >\nThe bitmap conceptually represents the space of all possible rows in the table be-\ntween the start and end row-id. The number of such possible rows in a block depends\non how many rows can ﬁt into a block, which is a function of the number of columns\nin the table and their data types. Each bit in the bitmap represents one such possible\nrow in a block. If the column value of that row is that of the index entry, the bit is set\nto 1. If the row has some other value, or the row does not actually exist in the table,\nthe bit is set to 0. (It is possible that the row does not actually exist because a table\nblock may well have a smaller number of rows than the number that was calculated\nas the maximum possible.) If the difference is large, the result may be long strings\nof consecutive zeros in the bitmap, but the compression algorithm deals with such\nstrings of zeros, so the negative effect is limited.\nThe compression algorithm is a variation of a compression technique called Byte-\nAligned Bitmap Compression (BBC). Essentially, a section of the bitmap where the\ndistance between two consecutive ones is small enough is stored as verbatim bitmaps.\nIf the distance between two ones is sufﬁciently large—that is, there is a sufﬁcient\nnumber of adjacent zeros between them—a runlength of zeros, that is the number of\nzeros, is stored.\nBitmap indices allow multiple indices on the same table to be combined in the\nsame access path if there are multiple conditions on indexed columns in the where\nclause of a query. For example, for the condition\n(col1 = 1 or col1 = 2) and col2 > 5 and col3 <> 10\nOracle would be able to calculate which rows match the condition by performing\nBoolean operations on bitmaps from indices on the three columns. In this case, these\noperations would take place for each index:\n• For the index on col1, the bitmaps for key values 1 and 2 would be ored.\n• For the index on col2, all the bitmaps for key values > 5 would be merged in\nan operation that corresponds to a logical or.\n• For the index on col3, the bitmaps for key values 10 and null would be re-\ntrieved. Then, a Boolean and would be performed on the results from the ﬁrst\ntwo indices, followed by two Boolean minuses of the bitmaps for values 10\nand null for col3.\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nIII. Object−Based \nDatabases and XML\n8. Object−Oriented \nDatabases\n319\n© The McGraw−Hill \nCompanies, 2001\n930\nChapter 25\nOracle\nAll operations are performed directly on the compressed representation of the bit-\nmaps—no decompression is necessary—and the resulting (compressed) bitmap rep-\nresents those rows that match all the logical conditions.\nThe ability to use the Boolean operations to combine multiple indices is not lim-\nited to bitmap indices. Oracle can convert row-ids to the compressed bitmap repre-\nsentation, so it can use a regular B-tree index anywhere in a Boolean tree of bitmap\noperation simply by putting a row-id-to-bitmap operator on top of the index access\nin the execution plan.\nAs a rule of thumb, bitmap indices tend to be more space efﬁcient than regular\nB-tree indices if the number of distinct key values is less than half the number of\nrows in the table. For example, in a table with 1 million rows, an index on a column\nwith less than 500,000 distinct values would probably be smaller if it were created as\na bitmap index. For columns with a very small number of distinct values—for ex-\nample, columns referring to properties such as country, state, gender, marital status,\nand various status ﬂags—a bitmap index might require only a small fraction of the\nspace of a regular B-tree index. Any such space advantage can also give rise to corre-\nsponding performance advantages in the form of fewer disk I/Os when the index is\nscanned.\n25.3.7\nFunction-Based Indices\nIn addition to creating indices on one or multiple columns of a table, Oracle allows\nindices to be created on expressions that involve one or more columns, such as col1 +\ncol2 ∗5. For example, by creating an index on the expression upper(name), where upper\nis a function that returns the uppercase version of a string, and name is a column, it is\npossible to do case-insensitive searches on the name column. In order to ﬁnd all rows\nwith name “van Gogh” efﬁciently, the condition\nupper(name) = ’VAN GOGH’\nwould be used in the where clause of the query. Oracle then matches the condition\nwith the index deﬁnition and concludes that the index can be used to retrieve all the\nrows matching “van Gogh” regardless of how the name was capitalized when it was\nstored in the database. A function-based index can be created as either a bitmap or a\nB-tree index.\n25.3.8\nJoin Indices\nA join index is an index where the key columns are not in the table that is referenced\nby the row-ids in the index. Oracle supports bitmap join indices primarily for use\nwith star schemas (see Section 22.4.2). For example, if there is a column for product\nnames in a product dimension table, a bitmap join index on the fact table with this key\ncolumn could be used to retrieve the fact table rows that correspond to a product with\na speciﬁc name, although the name is not stored in the fact table. How the rows in\nthe fact and dimension tables correspond is based on a join condition that is speciﬁed\nwhen the index is created, and becomes part of the index metadata. When a query is\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nIII. Object−Based \nDatabases and XML\n8. Object−Oriented \nDatabases\n320\n© The McGraw−Hill \nCompanies, 2001\n25.3\nStorage and Indexing\n931\nprocessed, the optimizer will look for the same join condition in the where clause of\nthe query in order to determine if the join index is applicable.\nOracle allows bitmap join indices to have more than one key column and these\ncolumns can be in different tables. In all cases, the join conditions between the fact\ntable on which the index is built and the dimension tables must refer to unique keys\nin the dimension tables; that is, an indexed row in the fact table must correspond to\na unique row in each of the dimension tables.\nOracle can combine a bitmap join index on a fact table with other indices on the\nsame table—whether join indices or not—by using the operators for Boolean bitmap\noperations. For example, consider a schema with a fact table for sales, and dimension\ntables for customers, products, and time. Suppose a query requests information about\nsales to customers in a certain zip code who bought products in a certain product cat-\negory during a certain time period. If a multicolumn bitmap join index exists where\nthe key columns are the constrained dimension table columns (zip code, product cat-\negory and time), Oracle can use the join index to ﬁnd rows in the fact table that match\nthe constraining conditions. However, if individual, single-column indices exist for\nthe key columns (or a subset of them), Oracle can retrieve bitmaps for fact table rows\nthat match each individual condition, and use the Boolean and operation to generate\na fact table bitmap for those rows that satisfy all the conditions. If the query contains\nconditions on some columns of the fact table, indices on those columns could be in-\ncluded in the same access path, even if they were regular B-tree indices or domain\nindices (domain indices are described below in Section 25.3.9).\n25.3.9\nDomain Indices\nOracle allows tables to be indexed by index structures that are not native to Oracle.\nThis extensibility feature of the Oracle server allows software vendors to develop\nso-called cartridges with functionality for speciﬁc application domains, such as text,\nspatial data, and images, with indexing functionality beyond that provided by the\nstandard Oracle index types. In implementing the logic for creating, maintaining,\nand searching the index, the index designer must ensure that it adheres to a speciﬁc\nprotocol in its interaction with the Oracle server.\nA domain index must be registered in the data dictionary, together with the oper-\nators it supports. Oracle’s optimizer considers domain indices as one of the possible\naccess paths for a table. Oracle allows cost functions to be registered with the opera-\ntors so that the optimizer can compare the cost of using the domain index to those of\nother access paths.\nFor example, a domain index for advanced text searches may support an operator\ncontains. Once this operator has been registered, the domain index will be considered\nas an access path for a query like\nselect *\nfrom employees\nwhere contains(resume, ’LINUX’)\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nIII. Object−Based \nDatabases and XML\n8. Object−Oriented \nDatabases\n321\n© The McGraw−Hill \nCompanies, 2001\n932\nChapter 25\nOracle\nwhere resume is a text column in the employee table. The domain index can be stored\nin either an external data ﬁle or inside an Oracle index-organized table.\nA domain index can be combined with other (bitmap or B-tree) indices in the same\naccess path by converting between the row-id and bitmap representation and using\nBoolean bitmap operations.\n25.3.10\nPartitioning\nOracle supports various kinds of horizontal partitioning of tables and indices, and\nthis feature plays a major role in Oracle’s ability to support very large databases. The\nability to partition a table or index has advantages in many areas.\n• Backup and recovery are easier and faster, since they can be done on individ-\nual partitions rather than on the table as a whole.\n• Loading operations in a data warehousing environment are less intrusive:\ndata can be added to a partition, and then the partition added to a table, which\nis an instantaneous operation. Likewise, dropping a partition with obsolete\ndata from a table is very easy in a data warehouse that maintains a rolling\nwindow of historical data.\n• Query performance beneﬁts substantially, since the optimizer can recognize\nthat only a subset of the partitions of a table need to be accessed in order to\nresolve a query (partition pruning). Also, the optimizer can recognize that in\na join, it is not necessary to try to match all rows in one table with all rows in\nthe other, but that the joins need to be done only between matching pairs of\npartitions (partitionwise join).\nEach row in a partitioned table is associated with a speciﬁc partition. This associa-\ntion is based on the partitioning column or columns that are part of the deﬁnition of a\npartitioned table. There are several ways to map column values to partitions, giving\nrise to several types of partitioning, each with different characteristics: range, hash,\ncomposite, and list partitioning.\n25.3.10.1\nRange Partitioning\nIn range partitioning, the partitioning criteria are ranges of values. This type of par-\ntitioning is especially well suited to date columns, in which case all rows in the same\ndate range, say a day or a month, belong in the same partition. In a data warehouse\nwhere data are loaded from the transactional systems at regular intervals, range par-\ntitioning can be used to implement a rolling window of historical data efﬁciently.\nEach data load gets its own new partition, making the loading process faster and\nmore efﬁcient. The system actually loads the data into a separate table with the same\ncolumn deﬁnition as the partitioned table. It can then check the data for consistency,\ncleanse them, and index them. After that, the system can make the separate table a\nnew partition of the partitioned table, by a simple change to the metadata in the data\ndictionary—a nearly instantaneous operation.\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nIII. Object−Based \nDatabases and XML\n8. Object−Oriented \nDatabases\n322\n© The McGraw−Hill \nCompanies, 2001\n25.3\nStorage and Indexing\n933\nUp until the metadata change, the loading process does not affect the existing\ndata in the partitioned table in any way. There is no need to do any maintenance\nof existing indices as part of the loading. Old data can be removed from a table by\nsimply dropping its partition; this operation does not affect the other partitions.\nIn addition, queries in a data warehousing environment often contain conditions\nthat restrict them to a certain time period, such as a quarter or month. If date range\npartitioning is used, the query optimizer can restrict the data access to those parti-\ntions that are relevant to the query, and avoid a scan of the entire table.\n25.3.10.2\nHash Partitioning\nIn hash partitioning, a hash function maps rows to partitions according to the values\nin the partitioning columns. This type of partitioning is primarily useful when it is\nimportant to distribute the rows evenly among partitions or when partitionwise joins\nare important for query performance.\n25.3.10.3\nComposite Partitioning\nIn composite partitioning, the table is range partitioned, but each partition is subpar-\ntitioned by using hash partitioning. This type of partitioning combines the advan-\ntages of range partitioning and hash partitioning.\n25.3.10.4\nList Partitioning\nIn list partitioning, the values associated with a particular partition are stated in a\nlist. This type of partitioning is useful if the data in the partitioning column have a\nrelatively small set of discrete values. For instance, a table with a state column can be\nimplicitly partitioned by geographical region if each partition list has the states that\nbelong in the same region.\n25.3.11\nMaterialized Views\nThe materialized view feature (see Section 3.5.1) allows the result of an SQL query to\nbe stored in a table and used for later query processing. In addition, Oracle maintains\nthe materialized result, updating it when the tables that were referenced in the query\nare updated. Materialized views are used in data warehousing to speed up\n\nllection of tables, each of which is assigned a\nunique name. Each table has a structure similar to that presented in Chapter 2, where\nwe represented E-R databases by tables. A row in a table represents a relationship\namong a set of values. Since a table is a collection of such relationships, there is a\nclose correspondence between the concept of table and the mathematical concept of\n79\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nI. Data Models\n3. Relational Model\n88\n© The McGraw−Hill \nCompanies, 2001\n80\nChapter 3\nRelational Model\nrelation, from which the relational data model takes its name. In what follows, we\nintroduce the concept of relation.\nIn this chapter, we shall be using a number of different relations to illustrate the\nvarious concepts underlying the relational data model. These relations represent part\nof a banking enterprise. They differ slightly from the tables that were used in Chap-\nter 2, so that we can simplify our presentation. We shall discuss criteria for the ap-\npropriateness of relational structures in great detail in Chapter 7.\n3.1.1\nBasic Structure\nConsider the account table of Figure 3.1. It has three column headers: account-number,\nbranch-name, and balance. Following the terminology of the relational model, we refer\nto these headers as attributes (as we did for the E-R model in Chapter 2). For each\nattribute, there is a set of permitted values, called the domain of that attribute. For\nthe attribute branch-name, for example, the domain is the set of all branch names. Let\nD1 denote the set of all account numbers, D2 the set of all branch names, and D3\nthe set of all balances. As we saw in Chapter 2, any row of account must consist of\na 3-tuple (v1, v2, v3), where v1 is an account number (that is, v1 is in domain D1),\nv2 is a branch name (that is, v2 is in domain D2), and v3 is a balance (that is, v3 is in\ndomain D3). In general, account will contain only a subset of the set of all possible\nrows. Therefore, account is a subset of\nD1 × D2 × D3\nIn general, a table of n attributes must be a subset of\nD1 × D2 × · · · × Dn−1 × Dn\nMathematicians deﬁne a relation to be a subset of a Cartesian product of a list of\ndomains. This deﬁnition corresponds almost exactly with our deﬁnition of table. The\nonly difference is that we have assigned names to attributes, whereas mathematicians\nrely on numeric “names,” using the integer 1 to denote the attribute whose domain\nappears ﬁrst in the list of domains, 2 for the attribute whose domain appears second,\nand so on. Because tables are essentially relations, we shall use the mathematical\naccount-number\nbranch-name\nbalance\nA-101\nDowntown\n500\nA-102\nPerryridge\n400\nA-201\nBrighton\n900\nA-215\nMianus\n700\nA-217\nBrighton\n750\nA-222\nRedwood\n700\nA-305\nRound Hill\n350\nFigure 3.1\nThe account relation.\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nI. Data Models\n3. Relational Model\n89\n© The McGraw−Hill \nCompanies, 2001\n3.1\nStructure of Relational Databases\n81\naccount-number\nbranch-name\nbalance\nA-101\nDowntown\n500\nA-215\nMianus\n700\nA-102\nPerryridge\n400\nA-305\nRound Hill\n350\nA-201\nBrighton\n900\nA-222\nRedwood\n700\nA-217\nBrighton\n750\nFigure 3.2\nThe account relation with unordered tuples.\nterms relation and tuple in place of the terms table and row. A tuple variable is a\nvariable that stands for a tuple; in other words, a tuple variable is a variable whose\ndomain is the set of all tuples.\nIn the account relation of Figure 3.1, there are seven tuples. Let the tuple variable t\nrefer to the ﬁrst tuple of the relation. We use the notation t[account-number] to denote\nthe value of t on the account-number attribute. Thus, t[account-number] = “A-101,” and\nt[branch-name] = “Downtown”. Alternatively, we may write t[1] to denote the value\nof tuple t on the ﬁrst attribute (account-number), t[2] to denote branch-name, and so on.\nSince a relation is a set of tuples, we use the mathematical notation of t ∈r to denote\nthat tuple t is in relation r.\nThe order in which tuples appear in a relation is irrelevant, since a relation is a\nset of tuples. Thus, whether the tuples of a relation are listed in sorted order, as in\nFigure 3.1, or are unsorted, as in Figure 3.2, does not matter; the relations in the two\nﬁgures above are the same, since both contain the same set of tuples.\nWe require that, for all relations r, the domains of all attributes of r be atomic. A\ndomain is atomic if elements of the domain are considered to be indivisible units.\nFor example, the set of integers is an atomic domain, but the set of all sets of integers\nis a nonatomic domain. The distinction is that we do not normally consider inte-\ngers to have subparts, but we consider sets of integers to have subparts—namely,\nthe integers composing the set. The important issue is not what the domain itself is,\nbut rather how we use domain elements in our database. The domain of all integers\nwould be nonatomic if we considered each integer to be an ordered list of digits. In\nall our examples, we shall assume atomic domains. In Chapter 9, we shall discuss\nextensions to the relational data model to permit nonatomic domains.\nIt is possible for several attributes to have the same domain. For example, sup-\npose that we have a relation customer that has the three attributes customer-name,\ncustomer-street, and customer-city, and a relation employee that includes the attribute\nemployee-name. It is possible that the attributes customer-name and employee-name will\nhave the same domain: the set of all person names, which at the physical level is\nthe set of all character strings. The domains of balance and branch-name, on the other\nhand, certainly ought to be distinct. It is perhaps less clear whether customer-name\nand branch-name should have the same domain. At the physical level, both customer\nnames and branch names are character strings. However, at the logical level, we may\nwant customer-name and branch-name to have distinct domains.\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nI. Data Models\n3. Relational Model\n90\n© The McGraw−Hill \nCompanies, 2001\n82\nChapter 3\nRelational Model\nOne domain value that is a member of any possible domain is the null value,\nwhich signiﬁes that the value is unknown or does not exist. For example, suppose\nthat we include the attribute telephone-number in the customer relation. It may be that\na customer does not have a telephone number, or that the telephone number is un-\nlisted. We would then have to resort to null values to signify that the value is un-\nknown or does not exist. We shall see later that null values cause a number of difﬁ-\nculties when we access or update the database, and thus should be eliminated if at\nall possible. We shall assume null values are absent initially, and in Section 3.3.4, we\ndescribe the effect of nulls on different operations.\n3.1.2\nDatabase Schema\nWhen we talk about a database, we must differentiate between the database schema,\nwhich is the logical design of the database, and a database instance, which is a snap-\nshot of the data in the database at a given instant in time.\nThe concept of a relation corresponds to the programming-language notion of a\nvariable. The concept of a relation schema corresponds to the programming-language\nnotion of type deﬁnition.\nIt is convenient to give a name to a relation schema, just as we give names to type\ndeﬁnitions in programming languages. We adopt the convention of using lower-\ncase names for relations, and names beginning with an uppercase letter for rela-\ntion schemas. Following this notation, we use Account-schema to denote the relation\nschema for relation account. Thus,\nAccount-schema = (account-number, branch-name, balance)\nWe denote the fact that account is a relation on Account-schema by\naccount(Account-schema)\nIn general, a relation schema consists of a list of attributes and their corresponding\ndomains. We shall not be concerned about the precise deﬁnition of the domain of\neach attribute until we discuss the SQL language in Chapter 4.\nThe concept of a relation instance corresponds to the programming language no-\ntion of a value of a variable. The value of a given variable may change with time;\nsimilarly the contents of a relation instance may change with time as the relation is\nupdated. However, we often simply say “relation” when we actually mean “relation\ninstance.”\nAs an example of a relation instance, consider the branch relation of Figure 3.3. The\nschema for that relation is\nBranch-schema = (branch-name, branch-city, assets)\nNote that the attribute branch-name appears in both Branch-schema and Account-\nschema. This duplication is not a coincidence. Rather, using common attributes in\nrelation schemas is one way of relating tuples of distinct relations. For example, sup-\npose we wish to ﬁnd the information about all of the accounts maintained in branches\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nI. Data Models\n3. Relational Model\n91\n© The McGraw−Hill \nCompanies, 2001\n3.1\nStructure of Relational Databases\n83\nbranch-name\nbranch-city\nassets\nBrighton\nBrooklyn\n7100000\nDowntown\nBrooklyn\n9000000\nMianus\nHorseneck\n400000\nNorth Town\nRye\n3700000\nPerryridge\nHorseneck\n1700000\nPownal\nBennington\n300000\nRedwood\nPalo Alto\n2100000\nRound Hill\nHorseneck\n8000000\nFigure 3.3\nThe branch relation.\nlocated in Brooklyn. We look ﬁrst at the branch relation to ﬁnd the names of all the\nbranches located in Brooklyn. Then, for each such branch, we would look in the ac-\ncount relation to ﬁnd the information about the accounts maintained at that branch.\nThis is not surprising—recall that the primary key attributes of a strong entity set\nappear in the table created to represent the entity set, as well as in the tables created\nto represent relationships that the entity set participates in.\nLet us continue our banking example. We need a relation to describe information\nabout customers. The relation schema is\nCustomer-schema = (customer-name, customer-street, customer-city)\nFigure 3.4 shows a sample relation customer (Customer-schema). Note that we have\nomitted the customer-id attribute, which we used Chapter 2, because now we want to\nhave smaller relation schemas in our running example of a bank database. We assume\nthat the customer name uniquely identiﬁes a customer—obviously this may not be\ntrue in the real world, but the assumption makes our examples much easier to read.\ncustomer-name\ncustomer-street\ncustomer-city\nAdams\nSpring\nPittsfield\nBrooks\nSenator\nBrooklyn\nCurry\nNorth\nRye\nGlenn\nSand Hill\nWoodside\nGreen\nWalnut\nStamford\nHayes\nMain\nHarrison\nJohnson\nAlma\nPalo Alto\nJones\nMain\nHarrison\nLindsay\nPark\nPittsfield\nSmith\nNorth\nRye\nTurner\nPutnam\nStamford\nWilliams\nNassau\nPrinceton\nFigure 3.4\nThe customer relation.\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nI. Data Models\n3. Relational Model\n92\n© The McGraw−Hill \nCompanies, 2001\n84\nChapter 3\nRelational Model\nIn a real-world database, the customer-id (which could be a social-security number, or\nan identiﬁer generated by the bank) would serve to uniquely identify customers.\nWe also need a relation to describe the association between customers and ac-\ncounts. The relation schema to describe this association is\nDepositor-schema = (customer-name, account-number)\nFigure 3.5 shows a sample relation depositor (Depositor-schema).\nIt would appear that, for our banking example, we could have just one relation\nschema, rather than several. That is, it may be easier for a user to think in terms of\none relation schema, rather than in terms of several. Suppose that we used only one\nrelation for our example, with schema\n(branch-name, branch-city, assets, customer-name, customer-street\ncustomer-city, account-number, balance)\nObserve that, if a customer has several accounts, we must list her address once for\neach account. That is, we must repeat certain information several times. This repeti-\ntion is wasteful and is avoided by the use of several relations, as in our example.\nIn addition, if a branch has no accounts (a newly created branch, say, that has no\ncustomers yet), we cannot construct a complete tuple on the preceding single rela-\ntion, because no data concerning customer and account are available yet. To represent\nincomplete tuples, we must use null values that signify that the value is unknown or\ndoes not exist. Thus, in our example, the values for customer-name, customer-street, and\nso on must be null. By using several relations, we can represent the branch informa-\ntion for a bank with no customers without using null values. We simply use a tuple\non Branch-schema to represent the information about the branch, and create tuples on\nthe other schemas only when the appropriate information becomes available.\nIn Chapter 7, we shall study criteria to help us decide when one set of relation\nschemas is more appropriate than another, in terms of information repetition and\nthe existence of null values. For now, we shall assume that the relation schemas are\ngiven.\nWe include two additional relations to describe data about loans maintained in the\nvarious branches in the bank:\ncustomer-name\naccount-number\nHayes\nA-102\nJohnson\nA-101\nJohnson\nA-201\nJones\nA-217\nLindsay\nA-222\nSmith\nA-215\nTurner\nA-305\nFigure 3.5\nThe depositor relation.\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nI. Data Models\n3. Relational Model\n93\n© The McGraw−Hill \nCompanies, 2001\n3.1\nStructure of Relational Databases\n85\nloan-number\nbranch-name\namount\nL-11\nRound Hill\n900\nL-14\nDowntown\n1500\nL-15\nPerryridge\n1500\nL-16\nPerryridge\n1300\nL-17\nDowntown\n1000\nL-23\nRedwood\n2000\nL-93\nMianus\n500\nFigure 3.6\nThe loan relation.\nLoan-schema = (loan-number, branch-name, amount)\nBorrower-schema = (customer-name, loan-number)\nFigures 3.6 and 3.7, respectively, show the sample relations loan (Loan-schema) and\nborrower (Borrower-schema).\nThe E-R diagram in Figure 3.8 depicts the banking enterprise that we have just\ndescribed. The relation schemas correspond to the set of tables that we might gener-\nate by the method outlined in Section 2.9. Note that the tables for account-branch and\nloan-branch have been combined into the tables for account and loan respectively. Such\ncombining is possible since the relationships are many to one from account and loan,\nrespectively, to branch, and, further, the participation of account and loan in the corre-\nsponding relationships is total, as the double lines in the ﬁgure indicate. Finally, we\nnote that the customer relation may contain information about customers who have\nneither an account nor a loan at the bank.\nThe banking enterprise described here will serve as our primary example in this\nchapter and in subsequent ones. On occasion, we shall need to introduce additional\nrelation schemas to illustrate particular points.\n3.1.3\nKeys\nThe notions of superkey, candidate key, and primary key, as discussed in Chapter 2,\nare also applicable to the relational model. For example, in Branch-schema, {branch-\ncustomer-name\nloan-number\nAdams\nL-16\nCurry\nL-93\nHayes\nL-15\nJackson\nL-14\nJones\nL-17\nSmith\nL-11\nSmith\nL-23\nWilliams\nL-17\nFigure 3.7\nThe borrower relation.\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nI. Data Models\n3. Relational Model\n94\n© The McGraw−Hill \nCompanies, 2001\n86\nChapter 3\nRelational Model\naccount-number\nbalance\naccount\nbranch-name\nassets\nbranch\naccount-branch\ncustomer-name\ncustomer-street\ncustomer-city\ncustomer\nloan-number\namount\ndepositor\nbranch-city\nloan-branch\nloan\nborrower\nFigure 3.8\nE-R diagram for the banking enterprise.\nname} and {branch-name, branch-city} are both superkeys. {branch-name, branch-city}\nis not a candidate key, because {branch-name} is a subset of {branch-name, branch-\ncity} and {branch-name} itself is a superkey. However, {branch-name} is a candidate\nkey, and for our purpose also will serve as a primary key. The attribute branch-city is\nnot a superkey, since two branches in the same city may have different names (and\ndifferent asset ﬁgures).\nLet R be a relation schema. If we say that a subset K of R is a superkey for R, we\nare restricting consideration to relations r(R) in which no two distinct tuples have\nthe same values on all attributes in K. That is, if t1 and t2 are in r and t1 ̸= t2, then\nt1[K] ̸= t2[K].\nIf a relational database schema is based on tables derived from an E-R schema, it\nis possible to determine the primary key for a relation schema from the primary keys\nof the entity or relationship sets from which the schema is derived:\n• Strong entity set. The primary key of the entity set becomes the primary key\nof the relation.\n• Weak entity set. The table, and thus the relation, corresponding to a weak\nentity set includes\n\u0000 The attributes of the weak entity set\n\u0000 The primary key of the strong entity set on which the weak entity set\ndepends\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nI. Data Models\n3. Relational Model\n95\n© The McGraw−Hill \nCompanies, 2001\n3.1\nStructure of Relational Databases\n87\nThe primary key of the relation consists of the union of the primary key of the\nstrong entity set and the discriminator of the weak entity set.\n• Relationship set. The union of the primary keys of the related entity sets be-\ncomes a superkey of the relation. If the relationship is many-to-many, this su-\nperkey is also the primary key. Section 2.4.2 describes how to determine the\nprimary keys in other cases. Recall from Section 2.9.3 that no table is gener-\nated for relationship sets linking a weak entity set to the corresponding strong\nentity set.\n• Combined tables. Recall from Section 2.9.3 that a binary many-to-one rela-\ntionship set from A to B can be represented by a table consisting of the at-\ntributes of A and attributes (if any exist) of the relationship set. The primary\nkey of the “many” entity set becomes the primary key of the relation (that is,\nif the relationship set is many to one from A to B, the primary key of A is\nthe primary key of the relation). For one-to-one relationship sets, the relation\nis constructed like that for a many-to-one relationship set. However, we can\nchoose either entity set’s primary key as the primary key of the relation, since\nboth are candidate keys.\n• Multivalued attributes. Recall from Section 2.9.5 that a multivalued attribute\nM is represented by a table consisting of the primary key of the entity set or\nrelationship set of which M is an attribute plus a column C holding an indi-\nvidual value of M. The primary key of the entity or relationship set, together\nwith the attribute C, becomes the primary key for the relation.\nFrom the preceding list, we see that a relation schema, say r1, derived from an E-R\nschema may include among its attributes the primary key of another relation schema,\nsay r2. This attribute is called a foreign key from r1, referencing r2. The relation r1\nis also called the referencing relation of the foreign key dependency, and r2 is called\nthe referenced relation of the foreign key. For example, the attribute branch-name in\nAccount-schema is a foreign key from Account-schema referencing Branch-schema, since\nbranch-name is the primary key of Branch-schema. In any database instance, given any\ntuple, say ta, from the account relation, there must be some tuple, say tb, in the branch\nrelation such that the value of the branch-name attribute of ta is the same as the value\nof the primary key, branch-name, of tb.\nIt is customary to list the primary key attributes of a relation schema before the\nother attributes; for example, the branch-name attribute of Branch-schema is listed ﬁrst,\nsince it is the primary key.\n3.1.4\nSchema Diagram\nA database schema, along with primary key and foreign key dependencies, can be\ndepicted pictorially by schema diagrams. Figure 3.9 shows the schema diagram for\nour banking enterprise. Each relation appears as a box, with the attributes listed in-\nside it and the relation name above it. If there are primary key attributes, a horizontal\nline crosses \n\n1\nCross tabulation of sales by item-name and color.\nTo analyze the multidimensional data, a manager may want to see data laid out\nas shown in the table in Figure 22.1. The table shows total numbers for different\ncombinations of item-name and color. The value of size is speciﬁed to be all, indicating\nthat the displayed values are a summary across all values of size.\nThe table in Figure 22.1 is an example of a cross-tabulation (or cross-tab, for short),\nalso referred to as a pivot-table. In general, a cross-tab is a table where values for one\nattribute (say A) form the row headers, values for another attribute (say B) form the\ncolumn headers, and the values in an individual cell are derived as follows. Each cell\ncan be identiﬁed by (ai, bj), where ai is a value for A and bj a value for B. If there\nis at most one tuple with any (ai, bj) value, the value in the cell is derived from that\nsingle tuple (if any); for instance, it could be the value of one or more other attributes\nof the tuple. If there can be multiple tuples with an (ai, bj) value, the value in the cell\nmust be derived by aggregation on the tuples with that value. In our example, the\naggregation used is the sum of the values for attribute number. In our example, the\ncross-tab also has an extra column and an extra row storing the totals of the cells in\nthe row/column. Most cross-tabs have such summary rows and columns.\nA cross-tab is different from relational tables usually stored in databases, since the\nnumber of columns in the cross-tab depends on the actual data. A change in the data\nvalues may result in adding more columns, which is not desirable for data storage.\nHowever, a cross-tab view is desirable for display to users. It is straightforward to\nrepresent a cross-tab without summary values in a relational form with a ﬁxed num-\nber of columns. A cross-tab with summary rows/columns can be represented by in-\ntroducing a special value all to represent subtotals, as in Figure 22.2. The SQL:1999\nstandard actually uses the null value in place of all, but to avoid confusion with\nregular null values, we shall continue to use all.\nConsider the tuples (skirt, all, 53) and (dress, all, 35). We have obtained these tu-\nples by eliminating individual tuples with different values for color, and by replacing\nthe value of number by an aggregate—namely, sum. The value all can be thought of\nas representing the set of all values for an attribute. Tuples with the value all only for\nthe color dimension can be obtained by an SQL query performing a group by on the\ncolumn item-name. Similarly, a group by on color can be used to get the tuples with\nthe value all for item-name, and a group by with no attributes (which can simply be\nomitted in SQL) can be used to get the tuple with value all for item-name and color.\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nVII. Other Topics\n22. Advanced Querying and \nInformation Retrieval\n814\n© The McGraw−Hill \nCompanies, 2001\n22.2\nData Analysis and OLAP\n821\nitem-name\ncolor\nnumber\nskirt\ndark\n8\nskirt\npastel\n35\nskirt\nwhite\n10\nskirt\nall\n53\ndress\ndark\n20\ndress\npastel\n10\ndress\nwhite\n5\ndress\nall\n35\nshirt\ndark\n14\nshirt\npastel\n7\nshirt\nwhite\n28\nshirt\nall\n49\npant\ndark\n20\npant\npastel\n2\npant\nwhite\n5\npant\nall\n27\nall\ndark\n62\nall\npastel\n54\nall\nwhite\n48\nall\nall\n164\nFigure 22.2\nRelational representation of the data in Figure 22.1.\nThe generalization of a cross-tab, which is 2-dimensional, to n dimensions can be\nvisualized as an n-dimensional cube, called the data cube. Figure 22.3 shows a data\ncube on the sales relation. The data cube has three dimensions, namely item-name,\ncolor, and size, and the measure attribute is number. Each cell is identiﬁed by values\nfor these three dimensions. Each cell in the data cube contains a value, just as in a\ncross-tab. In Figure 22.3, the value contained in a cell is shown on one of the faces of\nthe cell; other faces of the cell are shown blank if they are visible.\nThe value for a dimension may be all, in which case the cell contains a summary\nover all values of that dimension, as in the case of cross-tabs. The number of different\nways in which the tuples can be grouped for aggregation can be large. In fact, for a\ntable with n dimensions, aggregation can be performed with grouping on each of the\n2n subsets of the n dimensions.1\nAn online analytical processing or OLAP system is an interactive system that per-\nmits an analyst to view different summaries of multidimensional data. The word\nonline indicates that the an analyst must be able to request new summaries and get\nresponses online, within a few seconds, and should not be forced to wait for a long\ntime to see the result of a query.\nWith an OLAP system, a data analyst can look at different cross-tabs on the same\ndata by interactively selecting the attributes in the cross-tab. Each cross-tab is a\n1.\nGrouping on the set of all n dimensions is useful only if the table may have duplicates.\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nVII. Other Topics\n22. Advanced Querying and \nInformation Retrieval\n815\n© The McGraw−Hill \nCompanies, 2001\n822\nChapter 22\nAdvanced Querying and Information Retrieval\n8\n35\n10\n53\n20\n10\n8\n35\n14\n7\n28\n49\n20\n2\n5\n27\n62\n2\n8\n5\n7\n22\n4\n7\n6\n12\n29\n2\n5\n3\n1\n11\n54\n48\n164\n34\n21\n77\n4\n9\n42\n16\n18\n45\ndark\npastel\nwhite\nskirt dress\nshirts\nitem name\npant\nall\nall\nsmall\nlarge\nmedium\nall\ncolor\nsize\nFigure 22.3\nThree-dimensional data cube.\ntwo-dimensional view on a multidimensional data cube. For instance the analyst may\nselect a cross-tab on item-name and size, or a cross-tab on color and size. The operation\nof changing the dimensions used in a cross-tab is called pivoting.\nAn OLAP system provides other functionality as well. For instance, the analyst\nmay wish to see a cross-tab on item-name and color for a ﬁxed value of size, for ex-\nample, large, instead of the sum across all sizes. Such an operation is referred to as\nslicing, since it can be thought of as viewing a slice of the data cube. The operation is\nsometimes called dicing, particularly when values for multiple dimensions are ﬁxed.\nWhen a cross-tab is used to view a multidimensional cube, the values of dimension\nattributes that are not part of the cross-tab are shown above the cross-tab. The value of\nsuch an attribute can be all, as shown in Figure 22.1, indicating that data in the cross-\ntab are a summary over all values for the attribute. Slicing/dicing simply consists of\nselecting speciﬁc values for these attributes, which are then displayed on top of the\ncross-tab.\nOLAP systems permit users to view data at any desired level of granularity. The\noperation of moving from ﬁner-granularity data to a coarser granularity (by means\nof aggregation) is called a rollup. In our example, starting from the data cube on the\nsales table, we got our example cross-tab by rolling up on the attribute size. The op-\nposite operation—that of moving from coarser-granularity data to ﬁner-granularity\ndata—is called a drill down. Clearly, ﬁner-granularity data cannot be generated from\ncoarse-granularity data; they must be generated either from the original data, or from\neven ﬁner-granularity summary data.\nAnalysts may wish to view a dimension at different levels of detail. For instance,\nan attribute of type datetime contains a date and a time of day. Using time precise to\na second (or less) may not be meaningful: An analyst who is interested in rough time\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nVII. Other Topics\n22. Advanced Querying and \nInformation Retrieval\n816\n© The McGraw−Hill \nCompanies, 2001\n22.2\nData Analysis and OLAP\n823\nHour of day\nDate\nDateTime\nDay of week\nMonth\nQuarter\nYear\nState\nCountry\nRegion\nCity\na) Time Hierarchy\nb) Location Hierarchy\nFigure 22.4\nHierarchies on dimensions.\nof day may look at only the hour value. An analyst who is interested in sales by day\nof the week may map the date to a day-of-the-week and look only at that. Another\nanalyst may be interested in aggregates over a month, or a quarter, or for an entire\nyear.\nThe different levels of detail for an attribute can be organized into a hierarchy.\nFigure 22.4(a) shows a hierarchy on the datetime attribute. As another example, Fig-\nure 22.4(b) shows a hierarchy on location, with the city being at the bottom of the\nhierarchy, state above it, country at the next level, and region being the top level. In\nour earlier example, clothes can be grouped by category (for instance, menswear or\nwomenswear); category would then lie above item-name in our hierarchy on clothes.\nAt the level of actual values, skirts and dresses would fall under the womenswear\ncategory and pants and shirts under the menswear category.\nAn analyst may be interested in viewing sales of clothes divided as menswear and\nwomenswear, and not interested in individual values. After viewing the aggregates\nat the level of womenswear and menswear, an analyst may drill down the hierarchy\nto look at individual values. An analyst looking at the detailed level may drill up the\nhierarchy, and look at coarser-level aggregates. Both levels can be displayed on the\nsame cross-tab, as in Figure 22.5.\n22.2.2\nOLAP Implementation\nThe earliest OLAP systems used multidimensional arrays in memory to store data\ncubes, and are referred to as multidimensional OLAP (MOLAP) systems. Later, OLAP\nfacilities were integrated into relational systems, with data stored in a relational data-\nbase. Such systems are referred to as relational OLAP (ROLAP) systems. Hybrid\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nVII. Other Topics\n22. Advanced Querying and \nInformation Retrieval\n817\n© The McGraw−Hill \nCompanies, 2001\n824\nChapter 22\nAdvanced Querying and Information Retrieval\ncategory\ndark\nitem-name\nwomenswear\nmenswear\ntotal\nskirt\ndress\nsubtotal\nskirt\ndress\nsubtotal\n8\n20\n28\n14\n20\n34\n62\npastel\n8\n20\n28\n14\n20\n34\n62\nwhite\ntotal\n10\n5\n15\n28\n5\n33\n48\n53\n35\n49\n27\n88\n76\n164\nFigure 22.5\nCross tabulation of sales with hierarchy on item-name.\nsystems, which store some summaries in memory and store the base data and other\nsummaries in a relational database, are called hybrid OLAP (HOLAP) systems.\nMany OLAP systems are implemented as client–server systems. The server con-\ntains the relational database as well as any MOLAP data cubes. Client systems obtain\nviews of the data by communicating with the server.\nA na¨ıve way of computing the entire data cube (all groupings) on a relation is to\nuse any standard algorithm for computing aggregate operations, one grouping at a\ntime. The na¨ıve algorithm would require a large number of scans of the relation. A\nsimple optimization is to compute an aggregation on, say, (item-name, color) from an\naggregation (item-name, color, size), instead of from the original relation. For the stan-\ndard SQL aggregate functions, we can compute an aggregate with grouping on a set\nof attributes A from an aggregate with grouping on a set of attributes B if A ⊆B; you\ncan do so as an exercise (see Exercise 22.1), but note that to compute avg, we addi-\ntionally need the count value. (For some non-standard aggregate functions, such as\nmedian, aggregates cannot be computed as above; the optimization described here\ndo not apply to such “non-decomposable” aggregate functions.) The amount of data\nread drops signiﬁcantly by computing an aggregate from another aggregate, instead\nof from the original relation. Further improvements are possible; for instance, multi-\nple groupings can be computed on a single scan of the data. See the bibliographical\nnotes for references to algorithms for efﬁciently computing data cubes.\nEarly OLAP implementations precomputed and stored entire data cubes, that is,\ngroupings on all subsets of the dimension attributes. Precomputation allows OLAP\nqueries to be answered within a few seconds, even on datasets that may contain\nmillions of tuples adding up to gigabytes of data. However, there are 2n groupings\nwith n dimension attributes; hierarchies on attributes increase the number further.\nAs a result, the entire data cube is often larger than the original relation that formed\nthe data cube and in many cases it is not feasible to store the entire data cube.\nInstead of precomputing and storing all possible groupings, it makes sense to pre-\ncompute and store some of the groupings, and to compute others on demand. In-\nstead of computing queries from the original relation, which may take a very long\ntime, we can compute them from other precomputed queries. For instance, suppose\na query requires summaries by (item-name, color), which has not been precomputed.\nThe query result can be computed from summaries by (item-name, color, size), if that\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nVII. Other Topics\n22. Advanced Querying and \nInformation Retrieval\n818\n© The McGraw−Hill \nCompanies, 2001\n22.2\nData Analysis and OLAP\n825\nhas been precomputed. See the bibliographical notes for references on how to select\na good set of groupings for precomputation, given limits on the storage available for\nprecomputed results.\nThe data in a data cube cannot be generated by a single SQL query using the basic\ngroup by constructs, since aggregates are computed for several different groupings\nof the dimension attributes. Section 22.2.3 discusses SQL extensions to support OLAP\nfunctionality.\n22.2.3\nExtended Aggregation\nThe SQL-92 aggregation functionality is limited, so several extensions were imple-\nmented by different databases. The SQL:1999 standard, however, deﬁnes a rich set of\naggregate functions, which we outline in this section and in the next two sections. The\nOracle and IBM DB2 databases support most of these features, and other databases\nwill no doubt support these features in the near future.\nThe new aggregate functions on single attributes are standard deviation and vari-\nance (stddev and variance). Standard deviation is the square root of variance.2 Some\ndatabase systems support other aggregate functions such as median and mode. Some\ndatabase systems even allow users to add new aggregate functions.\nSQL:1999 also supports a new class of binary aggregate functions, which can com-\npute statistical results on pairs of attributes; they include correlations, covariances,\nand regression curves, which give a line approximating the relation between the val-\nues of the pair of attributes. Deﬁnitions of these functions may be found in any stan-\ndard textbook on statistics, such as those referenced in the bibliographical notes.\nSQL:1999 also supports generalizations of the group by construct, using the cube\nand rollup constructs. A representative use of the cube construct is:\nselect item-name, color, size, sum(number)\nfrom sales\ngroup by cube(item-name, color, size)\nThis query computes the union of eight different groupings of the sales relation:\n{ (item-name, color, size), (item-name, color), (item-name, size),\n(color, size), (item-name), (color), (size), () }\nwhere () denotes an empty group by list.\nFor each grouping, the result contains the null value for attributes not present in\nthe grouping. For instance, the table in Figure 22.2, with occurrences of all replaced\nby null, can be computed by the query\nselect item-name, color, sum(number)\nfrom sales\ngroup by cube(item-name, color)\n2.\nThe SQL:1999 standard actually supports two types of variance, called population variance and sam-\nple variance, and correspondingly two types of standard deviation. The deﬁnitions of the two types differ\nslightly; see a statistics textbook for details.\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nVII. Other Topics\n22. Advanced Querying and \nInformation Retrieval\n819\n© The McGraw−Hill \nCompanies, 2001\n826\nChapter 22\nAdvanced Querying and Information Retrieval\nA representative rollup construct is\nselect item-name, color, size, sum(number)\nfrom sales\ngroup by rollup(item-name, color, size)\nHere, only four groupings are generated:\n{ (item-name, color, size), (item-name, color), (item-name), () }\nRollup can be used to generate aggregates at multiple levels of a hierarchy on a\ncolumn. For instance, suppose we have a table itemcategory(item-name, category) giv-\ning the category of each item. Then the query\nselect category, item-name, sum(number)\nfrom sales, category\nwhere sales.item-name = itemcategory.item-name\ngroup by rollup(category, item-name)\nwould give a hierarchical summary by item-name and by category.\nMultiple rollups and cubes can be used in a single group by clause. For instance,\nthe following query\nselect item-name, color, size, sum(number)\nfrom sales\ngroup by rollup(item-name), rollup(color, size)\ngenerates the groupings\n{ (item-name, color, size), (item-name, color), (item-name),\n(color, size), (color), () }\nTo understand why, note that rollup(item-name) generates two groupings, {(item-\nname), ()}, and rollup(color, size) generates three groupings, {(color, size), (color), () }.\nThe cross product of the two gives us the six groupings shown.\nAs we mentioned in Section 22.2.1, SQL:1999 uses the value null to indicate the\nusual sense of null as well as all. This dual use of null can cause ambiguity if the\nattributes used in a rollup or cube clause contain null values. The function grouping\ncan be applied on an attribute; it returns 1 if the value is a null value representing all,\nand returns 0 in all other cases. Consider the following query:\nselect item-name, color, size, sum(number),\ngrouping(item-name) as item-name-ﬂag,\ngrouping(color) as color-ﬂag,\ngrouping(size) as size-ﬂag\nfrom sales\ngroup by cube(item-name, color, size)\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nVII. Other Topics\n22. Advanced Querying and \nInformation Retrieval\n820\n© The McGraw−Hill \nCompanies, 2001\n22.2\nData Analysis and OLAP\n827\nThe output is the same as in the version of the query without grouping, but with\nthree extra columns called item-name-ﬂag, color-ﬂag, and size-ﬂag. In each tuple, the\nvalue of a ﬂag ﬁeld is 1 if the corresponding ﬁeld is a null representing all.\nInstead of using tags to indicate nulls that represent all, we can replace the null\nvalue by a value of our choice:\ndecode(grouping(item-name), 1, ’all’, item-name)\nThis expression returns the value “all” if the value of item-name is a null correspond-\ning to all, and returns the actual value of item-name otherwise. This expression can be\nused in place of item-name in the select clause to get “all” in the output of the query,\nin place of nulls representing all.\nNeither the rollup nor the cube clause gives complete control on the groupings\nthat are generated. For instance, we cannot use them to specify that we want only\ngroupings {(color, size), (size, item-name)}. Such restricted groupings can be generated\nby using the grouping construct in the having clause; we leave the details as an\nexercise for you.\n22.2.4\nRanking\nFinding the position of a value in a larger set is a common operation. For instance,\nwe may wish to assign students a rank in class based on their total marks, with the\nrank 1 going to the student with the highest marks, the rank 2 to the student with\nthe next highest marks, and so on. While such queries can be expressed in SQL-92,\nthey are difﬁcult to express and inefﬁcient to evaluate. Programmers often resort to\nwriting the query partly in SQL and partly in a programming language. A related\ntype of query is to ﬁnd the percentile in which a value in a (multi)set belongs, for\nexample, the bottom third, middle third, or top third. We study SQL:1999 support for\nthese types of queries here.\nRanking is done in conjunction with an order by speciﬁcation. Suppose we are\ngiven a relation student-marks(student-id, marks) which stores the marks obtained by\neach student. The following query gives the rank of each student.\nselect student-id, rank() over (order by (marks) desc) as s-rank\nfrom student-marks\nNote that the order of tuples in the output is not deﬁned, so they may not be sorted\nby rank. An extra order\n\n, 2001\n22.3\nData Mining\n841\n22.3.5\nClustering\nIntuitively, clustering refers to the problem of ﬁnding clusters of points in the given\ndata. The problem of clustering can be formalized from distance metrics in several\nways. One way is to phrase it as the problem of grouping points into k sets (for a\ngiven k) so that the average distance of points from the centroid of their assigned\ncluster is minimized.5 Another way is to group points so that the average distance\nbetween every pair of points in each cluster is minimized. There are other deﬁni-\ntions too; see the bibliographical notes for details. But the intuition behind all these\ndeﬁnitions is to group similar points together in a single set.\nAnother type of clustering appears in classiﬁcation systems in biology. (Such clas-\nsiﬁcation systems do not attempt to predict classes, rather they attempt to cluster re-\nlated items together.) For instance, leopards and humans are clustered under the class\nmammalia, while crocodiles and snakes are clustered under reptilia. Both mammalia\nand reptilia come under the common class chordata. The clustering of mammalia has\nfurther subclusters, such as carnivora and primates. We thus have hierarchical clus-\ntering. Given characteristics of different species, biologists have created a complex\nhierarchical clustering scheme grouping related species together at different levels of\nthe hierarchy.\nHierarchical clustering is also useful in other domains—for clustering documents,\nfor example. Internet directory systems (such as Yahoo’s) cluster related documents\nin a hierarchical fashion (see Section 22.5.5). Hierarchical clustering algorithms can\nbe classiﬁed as agglomerative clustering algorithms, which start by building small\nclusters and then creater higher levels, or divisive clustering algorithms, which ﬁrst\ncreate higher levels of the hierarchical clustering, then reﬁne each resulting cluster\ninto lower level clusters.\nThe statistics community has studied clustering extensively. Database research has\nprovided scalable clustering algorithms that can cluster very large data sets (that may\nnot ﬁt in memory). The Birch clustering algorithm is one such algorithm. Intuitively,\ndata points are inserted into a multidimensional tree structure (based on R-trees, de-\nscribed in Section 23.3.5.3), and guided to appropriate leaf nodes based on nearness\nto representative points in the internal nodes of the tree. Nearby points are thus clus-\ntered together in leaf nodes, and summarized if there are more points than ﬁt in\nmemory. Some postprocessing after insertion of all points gives the desired overall\nclustering. See the bibliographical notes for references to the Birch algorithm, and\nother techniques for clustering, including algorithms for hierarchical clustering.\nAn interesting application of clustering is to predict what new movies (or books,\nor music) a person is likely to be interested in, on the basis of:\n1. The person’s past preferences in movies\n2. Other people with similar past preferences\n3. The preferences of such people for new movies\n5.\nThe centroid of a set of points is deﬁned as a point whose coordinate on each dimension is the average\nof the coordinates of all the points of that set on that dimension. For example in two dimensions, the\ncentroid of a set of points { (x1, y1), (x2, y2), . . ., (xn, yn) } is given by (\nPn\ni=1 xi\nn\n,\nPn\ni=1 yi\nn\n)\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nVII. Other Topics\n22. Advanced Querying and \nInformation Retrieval\n835\n© The McGraw−Hill \nCompanies, 2001\n842\nChapter 22\nAdvanced Querying and Information Retrieval\nOne approach to this problem is as follows. To ﬁnd people with similar past prefer-\nences we create clusters of people based on their preferences for movies. The accuracy\nof clustering can be improved by previously clustering movies by their similarity, so\neven if people have not seen the same movies, if they have seen similar movies they\nwould be clustered together. We can repeat the clustering, alternately clustering peo-\nple, then movies, then people, and so on till we reache an equilibrium. Given a new\nuser, we ﬁnd a cluster of users most similar to that user, on the basis of the user’s\npreferences for movies already seen. We then predict movies in movie clusters that\nare popular with that user’s cluster as likely to be interesting to the new user. In fact,\nthis problem is an instance of collaborative ﬁltering, where users collaborate in the task\nof ﬁltering information to ﬁnd information of interest.\n22.3.6\nOther Types of Mining\nText mining applies data mining techniques to textual documents. For instance, there\nare tools that form clusters on pages that a user has visited; this helps users when\nthey browse the history of their browsing to ﬁnd pages they have visited earlier. The\ndistance between pages can be based, for instance, on common words in the pages\n(see Section 22.5.1.3). Another application is to classify pages into a Web directory\nautomatically, according to their similarity with other pages (see Section 22.5.5).\nData-visualization systems help users to examine large volumes of data, and to\ndetect patterns visually. Visual displays of data—such as maps, charts, and other\ngraphical representations—allow data to be presented compactly to users. A sin-\ngle graphical screen can encode as much information as a far larger number of text\nscreens. For example, if the user wants to ﬁnd out whether production problems at\nplants are correlated to the locations of the plants, the problem locations can be en-\ncoded in a special color—say, red—on a map. The user can then quickly discover\nlocations where problems are occurring. The user may then form hypotheses about\nwhy problems are occurring in those locations, and may verify the hypotheses quan-\ntitatively against the database.\nAs another example, information about values can be encoded as a color, and can\nbe displayed with as little as one pixel of screen area. To detect associations between\npairs of items, we can use a two-dimensional pixel matrix, with each row and each\ncolumn representing an item. The percentage of transactions that buy both items can\nbe encoded by the color intensity of the pixel. Items with high association will show\nup as bright pixels in the screen—easy to detect against the darker background.\nData visualization systems do not automatically detect patterns, but provide sys-\ntem support for users to detect patterns. Since humans are very good at detecting\nvisual patterns, data visualization is an important component of data mining.\n22.4\nData Warehousing\nLarge companies have presences in many places, each of which may generate a large\nvolume of data. For instance, large retail chains have hundreds or thousands of stores,\nwhereas insurance companies may have data from thousands of local branches. Fur-\nther, large organizations have a complex internal organization structure, and there-\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nVII. Other Topics\n22. Advanced Querying and \nInformation Retrieval\n836\n© The McGraw−Hill \nCompanies, 2001\n22.4\nData Warehousing\n843\ndata\nloaders\nDBMS\ndata warehouse\nquery and\nanalysis tools\ndata source n\ndata source 2\ndata source 1\n…\nFigure 22.8\nData-warehouse architecture.\nfore different data may be present in different locations, or on different operational\nsystems, or under different schemas. For instance, manufacturing-problem data and\ncustomer-complaint data may be stored on different database systems. Corporate de-\ncision makers require access to information from all such sources. Setting up queries\non individual sources is both cumbersome and inefﬁcient. Moreover, the sources of\ndata may store only current data, whereas decision makers may need access to past\ndata as well; for instance, information about how purchase patterns have changed in\nthe past year could be of great importance. Data warehouses provide a solution to\nthese problems.\nA data warehouse is a repository (or archive) of information gathered from mul-\ntiple sources, stored under a uniﬁed schema, at a single site. Once gathered, the data\nare stored for a long time, permitting access to historical data. Thus, data warehouses\nprovide the user a single consolidated interface to data, making decision-support\nqueries easier to write. Moreover, by accessing information for decision support from\na data warehouse, the decision maker ensures that online transaction-processing sys-\ntems are not affected by the decision-support workload.\n22.4.1\nComponents of a Data Warehouse\nFigure 22.8 shows the architecture of a typical data warehouse, and illustrates the\ngathering of data, the storage of data, and the querying and data-analysis support.\nAmong the issues to be addressed in building a warehouse are the following:\n• When and how to gather data. In a source-driven architecture for gather-\ning data, the data sources transmit new information, either continually (as\ntransaction processing takes place), or periodically (nightly, for example). In\na destination-driven architecture, the data warehouse periodically sends re-\nquests for new data to the sources.\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nVII. Other Topics\n22. Advanced Querying and \nInformation Retrieval\n837\n© The McGraw−Hill \nCompanies, 2001\n844\nChapter 22\nAdvanced Querying and Information Retrieval\nUnless updates at the sources are replicated at the warehouse via two-phase\ncommit, the warehouse will never be quite up to date with the sources. Two-\nphase commit is usually far too expensive to be an option, so data warehouses\ntypically have slightly out-of-date data. That, however, is usually not a prob-\nlem for decision-support systems.\n• What schema to use. Data sources that have been constructed independently\nare likely to have different schemas. In fact, they may even use different data\nmodels. Part of the task of a warehouse is to perform schema integration, and\nto convert data to the integrated schema before they are stored. As a result, the\ndata stored in the warehouse are not just a copy of the data at the sources. In-\nstead, they can be thought of as a materialized view of the data at the sources.\n• Data cleansing. The task of correcting and preprocessing data is called data\ncleansing. Data sources often deliver data with numerous minor inconsisten-\ncies, that can be corrected. For example, names are often misspelled, and ad-\ndresses may have street/area/city names misspelled, or zip codes entered in-\ncorrectly. These can be corrected to a reasonable extent by consulting a data-\nbase of street names and zip codes in each city. Address lists collected from\nmultiple sources may have duplicates that need to be eliminated in a merge–\npurge operation. Records for multiple individuals in a house may be grouped\ntogether so only one mailing is sent to each house; this operation is called\nhouseholding.\n• How to propagate updates. Updates on relations at the data sources must\nbe propagated to the data warehouse. If the relations at the data warehouse\nare exactly the same as those at the data source, the propagation is straight-\nforward. If they are not, the problem of propagating updates is basically the\nview-maintenance problem, which was discussed in Section 14.5.\n• What data to summarize. The raw data generated by a transaction-processing\nsystem may be too large to store online. However, we can answer many queries\nby maintaining just summary data obtained by aggregation on a relation,\nrather than maintaining the entire relation. For example, instead of storing\ndata about every sale of clothing, we can store total sales of clothing by item-\nname and category.\nSuppose that a relation r has been replaced by a summary relation s. Users\nmay still be permitted to pose queries as though the relation r were available\nonline. If the query requires only summary data, it may be possible to trans-\nform it into an equivalent one using s instead; see Section 14.5.\n22.4.2\nWarehouse Schemas\nData warehouses typically have schemas that are designed for data analysis, using\ntools such as OLAP tools. Thus, the data are usually multidimensional data, with di-\nmension attributes and measure attributes. Tables containing multidimensional data\nare called fact tables and are usually very large. A table recording sales information\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nVII. Other Topics\n22. Advanced Querying and \nInformation Retrieval\n838\n© The McGraw−Hill \nCompanies, 2001\n22.4\nData Warehousing\n845\nfor a retail store, with one tuple for each item that is sold, is a typical example of a fact\ntable. The dimensions of the sales table would include what the item is (usually an\nitem identiﬁer such as that used in bar codes), the date when the item is sold, which\nlocation (store) the item was sold from, which customer bought the item, and so on.\nThe measure attributes may include the number of items sold and the price of the\nitems.\nTo minimize storage requirements, dimension attributes are usually short identi-\nﬁers that are foreign keys into other other tables called dimension tables. For\ninstance, a fact table sales would have attributes item-id, store-id, customer-id, and date,\nand measure attributes number and price. The attribute store-id is a foreign key into\na dimension table store, which has other attributes such as store location (city, state,\ncountry). The item-id attribute of the sales table would be a foreign key into a di-\nmension table item-info, which would contain information such as the name of the\nitem, the category to which the item belongs, and other item details such as color and\nsize. The customer-id attribute would be a foreign key into a customer table containing\nattributes such as name and address of the customer. We can also view the date at-\ntribute as a foreign key into a date-info table giving the month, quarter, and year of\neach date.\nThe resultant schema appears in Figure 22.9. Such a schema, with a fact table,\nmultiple dimension tables, and foreign keys from the fact table to the dimension ta-\nbles, is called a star schema. More complex data warehouse designs may have multi-\nple levels of dimension tables; for instance, the item-info table may have an attribute\nmanufacturer-id that is a foreign key into another table giving details of the manufac-\nturer. Such schemas are called snowﬂake schemas. Complex data warehouse designs\nmay also have more than one fact table.\nitem-id\nstore-id\nstore-id\nitem-id\nitemname\ncolor\nsize\nitem-info\nsales\nstore\ncity\nstate\ncountry\ndate\nmonth\nquarter\nyear\ndate-info\nnumber\ndate\ncustomer-id\ncustomer\ncustomer-id\nname\nstreet\ncity\nstate\nzipcode\ncountry\ncategory\nprice\nFigure 22.9\nStar schema for a data warehouse.\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nVII. Other Topics\n22. Advanced Querying and \nInformation Retrieval\n839\n© The McGraw−Hill \nCompanies, 2001\n846\nChapter 22\nAdvanced Querying and Information Retrieval\n22.5\nInformation-Retrieval Systems\nThe ﬁeld of information retrieval has developed in parallel with the ﬁeld of databases.\nIn the traditional model used in the ﬁeld of information retrieval, information is orga-\nnized into documents, and it is assumed that there is a large number of documents.\nData contained in documents is unstructured, without any associated schema. The\nprocess of information retrieval consists of locating relevant documents, on the basis\nof user input, such as keywords or example documents.\nThe Web provides a convenient way to get to, and to interact with, information\nsources across the Internet. However, a persistent problem facing the Web is the ex-\nplosion of stored information, with little guidance to help the user to locate what\nis interesting. Information retrieval has played a critical role in making the Web a\nproductive and useful tool, especially for researchers.\nTraditional examples of information-retrieval systems are online library catalogs\nand online document-management systems such as those that store newspaper arti-\ncles. The data in such systems are organized as a collection of documents; a newspaper\narticle or a catalog entry (in a library catalog) are examples of documents. In the con-\ntext of the Web, usually each HTML page is considered to be a document.\nA user of such a system may want to retrieve a particular document or a particular\nclass of documents. The intended documents are typically described by a set of key-\nwords—for example, the keywords “database system” may be used to locate books\non database systems, and the keywords “stock” and “scandal” may be used to locate\narticles about stock-market scandals. Documents have associated with them a set of\nkeywords, and documents whose keywords contain those supplied by the user are\nretrieved.\nKeyword-based information retrieval can be used not only for retrieving textual\ndata, but also for retrieving other types of data, such as video or audio data, that\nhave descriptive keywords associated with them. For instance, a video movie may\nhave associated with it keywords such as its title, director, actors, type, and so on.\nThere are several differences between this model and the models used in tradi-\ntional database systems.\n• Database systems deal with several operations that are not addressed in infor-\nmation-retrieval systems. For instance, database systems deal with updates\nand with the associated transactional requirements of concurrency control\nand durability. These matters are viewed as less important in information sys-\ntems. Similarly, database systems deal with structured information organized\nwith relatively complex data models (such as the relational model or object-\noriented data models), whereas information-retrieval systems traditionally\nhave used a much simpler model, where the information in the database is\norganized simply as a collection of unstructured documents.\n• Information-retrieval systems deal with several issues that have not been ad-\ndressed adequately in database systems. For instance, the ﬁeld of information\nretrieval has dealt with the problems of managing unstructured documents,\nsuch as approximate searching by keywords, and of ranking of documents on\nestimated degree of relevance of the documents to the query.\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nVII. Other Topics\n22. Advanced Querying and \nInformation Retrieval\n840\n© The McGraw−Hill \nCompanies, 2001\n22.5\nInformation-Retrieval Systems\n847\n22.5.1\nKeyword Search\nInformation-retrieval systems typically allow query expressions formed using key-\nwords and the logical connectives and, or, and not. For example, a user could ask\nfor all documents that contain the keywords “motorcycle and maintenance,” or docu-\nments that contain the keywords “computer or microprocessor,” or even documents\nthat contain the keyword “computer but not database.” A query containing keywords\nwithout any of the above connectives is assumed to have ands implicitly connecting\nthe keywords.\nIn full text retrieval, all the words in each document are considered to be key-\nwords. For unstructured documents, full text retrieval is essential since there may be\nno information about what words in the document are keywords. We shall use the\nword term to refer to the words in a document, since all words are keywords.\nIn its simplest form an information retrieval system locates and returns all doc-\numents that contain all the keywords in the query, if the query has no connectives;\nconnectives are handled as you would expect. More sophisticated systems estimate\nrelevance of documents to a query so that the documents can be shown in order of\nestimated relevance. They use information about term occurrences, as well as hyper-\nlink information, to estimate relevance; Section 22.5.1.1 and 22.5.1.2 outline how to do\nso. Section 22.5.1.3 outlines how to deﬁne similarity of documents, and use similarity\nfor searching. Some systems a",
    "precision": 0.06430568499534017,
    "recall": 0.7976878612716763,
    "iou": 0.06327372764786796,
    "f1": 0.11901681759379044,
    "gold_tokens_count": 173,
    "retrieved_tokens_count": 2146,
    "intersection_tokens": 138
  },
  {
    "config_name": "chars_rrf",
    "config": {
      "name": "chars_rrf",
      "index_prefix": "textbook_index",
      "chunking_strategy": "chars",
      "overlap": 0,
      "fusion": "rrf",
      "bm25_weight": 0.3,
      "tag_weight": 0.2,
      "top_k": 5,
      "embed_model": "sentence-transformers/all-MiniLM-L6-v2"
    },
    "question": "A weak entity set can always be made into a strong entity set by adding to its attributes the primary key attributes of its identifying entity set. Outline what sort of redundancy will result if we do so.",
    "gold_text": "2.9.3.1 Redundancy of Tables A relationship set linking a weak entity set to the corresponding strong entity set is treated specially. As we noted in Section 2.6, these relationships are many-to-one and have no descriptive attributes. Furthermore, the primary key of a weak entity set in- cludes the primary key of the strong entity set. In the E-R diagram of Figure 2.16, the weak entity set payment is dependent on the strong entity set loan via the relation- ship set loan-payment. The primary key of payment is {loan-number, payment-number}, and the primary key of loan is {loan-number}. Since loan-payment has no descriptive attributes, the loan-payment table would have two columns, loan-number and payment- number. The table for the entity set payment has four columns, loan-number, payment- number, payment-date, and payment-amount. Every (loan-number, payment-number) com- bination in loan-payment would also be present in the payment table, and vice versa. Thus, the loan-payment table is redundant. In general, the table for the relationship set linking a weak entity set to its corresponding strong entity set is redundant and does not need to be present in a tabular representation of an E-R diagram.",
    "retrieved_text": "eatures are the multival-\nued attribute dependent-name, the base attribute start-date, and the derived at-\ntribute employment-length.\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nI. Data Models\n2. Entity−Relationship \nModel\n70\n© The McGraw−Hill \nCompanies, 2001\n2.8\nDesign of an E-R Database Schema\n61\n• Two account entity sets—savings-account and checking-account—with the com-\nmon attributes of account-number and balance; in addition, savings-account has\nthe attribute interest-rate and checking-account has the attribute overdraft-amount.\n• The loan entity set, with the attributes loan-number, amount, and originating-\nbranch.\n• The weak entity set loan-payment, with attributes payment-number, payment-\ndate, and payment-amount.\n2.8.2.3\nRelationship Sets Designation\nWe now return to the rudimentary design scheme of Section 2.8.2.2 and specify the\nfollowing relationship sets and mapping cardinalities. In the process, we also reﬁne\nsome of the decisions we made earlier regarding attributes of entity sets.\n• borrower, a many-to-many relationship set between customer and loan.\n• loan-branch, a many-to-one relationship set that indicates in which branch a\nloan originated. Note that this relationship set replaces the attribute originating-\nbranch of the entity set loan.\n• loan-payment, a one-to-many relationship from loan to payment, which docu-\nments that a payment is made on a loan.\n• depositor, with relationship attribute access-date, a many-to-many relationship\nset between customer and account, indicating that a customer owns an account.\n• cust-banker, with relationship attribute type, a many-to-one relationship set ex-\npressing that a customer can be advised by a bank employee, and that a bank\nemployee can advise one or more customers. Note that this relationship set\nhas replaced the attribute banker-name of the entity set customer.\n• works-for, a relationship set between employee entities with role indicators man-\nager and worker; the mapping cardinalities express that an employee works\nfor only one manager and that a manager supervises one or more employees.\nNote that this relationship set has replaced the manager attribute of employee.\n2.8.2.4\nE-R Diagram\nDrawing on the discussions in Section 2.8.2.3, we now present the completed E-R di-\nagram for our example banking enterprise. Figure 2.22 depicts the full representation\nof a conceptual model of a bank, expressed in terms of E-R concepts. The diagram in-\ncludes the entity sets, attributes, relationship sets, and mapping cardinalities arrived\nat through the design processes of Sections 2.8.2.1 and 2.8.2.2, and reﬁned in Section\n2.8.2.3.\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nI. Data Models\n2. Entity−Relationship \nModel\n71\n© The McGraw−Hill \nCompanies, 2001\n62\nChapter 2\nEntity-Relationship Model\ninterest-rate\noverdraft-amount\naccount-number\nbalance\nISA\ncustomer-name\ncustomer-street\ncustomer-id\ncustomer-city\ncustomer\nbranch-city\nbranch-name\nassets\nloan-number\namount\npayment-number\nloan-\npayment\npayment-date\ntype\ndependent-name\nemployee-id\nemployment-\nlength\n access-date\nborrower\nloan-branch\ncust-banker\ndepositor\nworks-for\nmanager\nworker\nemployee-name\ntelephone-number\nstart-date\nbranch\nloan\npayment\npayment-amount\naccount\nchecking-account\nsavings-account\nemployee\nFigure 2.22\nE-R diagram for a banking enterprise.\n2.9\nReduction of an E-R Schema to Tables\nWe can represent a database that conforms to an E-R database schema by a collection\nof tables. For each entity set and for each relationship set in the database, there is a\nunique table to which we assign the name of the corresponding entity set or relation-\nship set. Each table has multiple columns, each of which has a unique name.\nBoth the E-R model and the relational-database model are abstract, logical rep-\nresentations of real-world enterprises. Because the two models employ similar de-\nsign principles, we can convert an E-R design into a relational design. Converting a\ndatabase representation from an E-R diagram to a table format is the way we arrive\nat a relational-database design from an E-R diagram. Although important differences\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nI. Data Models\n2. Entity−Relationship \nModel\n72\n© The McGraw−Hill \nCompanies, 2001\n2.9\nReduction of an E-R Schema to Tables\n63\nexist between a relation and a table, informally, a relation can be considered to be a\ntable of values.\nIn this section, we describe how an E-R schema can be represented by tables; and\nin Chapter 3, we show how to generate a relational-database schema from an E-R\nschema.\nThe constraints speciﬁed in an E-R diagram, such as primary keys and cardinality\nconstraints, are mapped to constraints on the tables generated from the E-R diagram.\nWe provide more details about this mapping in Chapter 6 after describing how to\nspecify constraints on tables.\n2.9.1\nTabular Representation of Strong Entity Sets\nLet E be a strong entity set with descriptive attributes a1, a2, . . . , an. We represent\nthis entity by a table called E with n distinct columns, each of which corresponds to\none of the attributes of E. Each row in this table corresponds to one entity of the entity\nset E.\nAs an illustration, consider the entity set loan of the E-R diagram in Figure 2.8. This\nentity set has two attributes: loan-number and amount. We represent this entity set by\na table called loan, with two columns, as in Figure 2.23. The row\n(L-17, 1000)\nin the loan table means that loan number L-17 has a loan amount of $1000. We can\nadd a new entity to the database by inserting a row into a table. We can also delete or\nmodify rows.\nLet D1 denote the set of all loan numbers, and let D2 denote the set of all balances.\nAny row of the loan table must consist of a 2-tuple (v1, v2), where v1 is a loan (that\nis, v1 is in set D1) and v2 is an amount (that is, v2 is in set D2). In general, the loan\ntable will contain only a subset of the set of all possible rows. We refer to the set of all\npossible rows of loan as the Cartesian product of D1 and D2, denoted by\nD1 × D2\nIn general, if we have a table of n columns, we denote the Cartesian product of\nD1, D2, · · · , Dn by\nD1 × D2 × · · · × Dn−1 × Dn\nloan-number\namount\nL-11\n900\nL-14\n1500\nL-15\n1500\nL-16\n1300\nL-17\n1000\nL-23\n2000\nL-93\n500\nFigure 2.23\nThe loan table.\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nI. Data Models\n2. Entity−Relationship \nModel\n73\n© The McGraw−Hill \nCompanies, 2001\n64\nChapter 2\nEntity-Relationship Model\ncustomer-id\ncustomer-name\ncustomer-street\ncustomer-city\n019-28-3746\n182-73-6091\n192-83-7465\n244-66-8800\n321-12-3123\n335-57-7991\n336-66-9999\n677-89-9011\n963-96-3963\nSmith\nTurner\nJohnson\nCurry\nJones\nAdams\nLindsay\nHayes\nWilliams\nNorth\nPutnam\nAlma\nNorth\nMain\nSpring\nPark\nMain\nNassau\nRye\nStamford\nPalo Alto\nRye\nHarrison\nPittsfield\nPittsfield\nHarrison\nPrinceton\nFigure 2.24\nThe customer table.\nAs another example, consider the entity set customer of the E-R diagram in Fig-\nure 2.8. This entity set has the attributes customer-id, customer-name, customer-street,\nand customer-city. The table corresponding to customer has four columns, as in Fig-\nure 2.24.\n2.9.2\nTabular Representation of Weak Entity Sets\nLet A be a weak entity set with attributes a1, a2, . . . , am. Let B be the strong entity set\non which A depends. Let the primary key of B consist of attributes b1, b2, . . . , bn. We\nrepresent the entity set A by a table called A with one column for each attribute of\nthe set:\n{a1, a2, . . . , am} ∪{b1, b2, . . . , bn}\nAs an illustration, consider the entity set payment in the E-R diagram of Figure 2.16.\nThis entity set has three attributes: payment-number, payment-date, and payment-amount.\nThe primary key of the loan entity set, on which payment depends, is loan-number.\nThus, we represent payment by a table with four columns labeled loan-number, payment-\nnumber, payment-date, and payment-amount, as in Figure 2.25.\n2.9.3\nTabular Representation of Relationship Sets\nLet R be a relationship set, let a1, a2, . . . , am be the set of attributes formed by the\nunion of the primary keys of each of the entity sets participating in R, and let the\ndescriptive attributes (if any) of R be b1, b2, . . . , bn. We represent this relationship set\nby a table called R with one column for each attribute of the set:\n{a1, a2, . . . , am} ∪{b1, b2, . . . , bn}\nAs an illustration, consider the relationship set borrower in the E-R diagram of Fig-\nure 2.8. This relationship set involves the following two entity sets:\n• customer, with the primary key customer-id\n• loan, with the primary key loan-number\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nI. Data Models\n2. Entity−Relationship \nModel\n74\n© The McGraw−Hill \nCompanies, 2001\n2.9\nReduction of an E-R Schema to Tables\n65\nloan-number\npayment-number\npayment-date\npayment-amount\nL-11\n53\n7 June 2001\n125\nL-14\n69\n28 May 2001\n500\nL-15\n22\n23 May 2001\n300\nL-16\n58\n18 June 2001\n135\nL-17\n5\n10 May 2001\n50\nL-17\n6\n7 June 2001\n50\nL-17\n7\n17 June 2001\n100\nL-23\n11\n17 May 2001\n75\nL-93\n103\n3 June 2001\n900\nL-93\n104\n13 June 2001\n200\nFigure 2.25\nThe payment table.\nSince the relationship set has no attributes, the borrower table has two columns, la-\nbeled customer-id and loan-number, as shown in Figure 2.26.\n2.9.3.1\nRedundancy of Tables\nA relationship set linking a weak entity set to the corresponding strong entity set is\ntreated specially. As we noted in Section 2.6, these relationships are many-to-one and\nhave no descriptive attributes. Furthermore, the primary key of a weak entity set in-\ncludes the primary key of the strong entity set. In the E-R diagram of Figure 2.16, the\nweak entity set payment is dependent on the strong entity set loan via the relation-\nship set loan-payment. The primary key of payment is {loan-number, payment-number},\nand the primary key of loan is {loan-number}. Since loan-payment has no descriptive\nattributes, the loan-payment table would have two columns, loan-number and payment-\nnumber. The table for the entity set payment has four columns, loan-number, payment-\nnumber, payment-date, and payment-amount. Every (loan-number, payment-number) com-\nbination in loan-payment would also be present in the payment table, and vice versa.\nThus, the loan-payment table is redundant. In general, the table for the relationship set\ncustomer-id\nloan-number\n019-28-3746\nL-11\n019-28-3746\nL-23\n244-66-8800\nL-93\n321-12-3123\nL-17\n335-57-7991\nL-16\n555-55-5555\nL-14\n677-89-9011\nL-15\n963-96-3963\nL-17\nFigure 2.26\nThe borrower table.\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nI. Data Models\n2. Entity−Relationship \nModel\n75\n© The McGraw−Hill \nCompanies, 2001\n66\nChapter 2\nEntity-Relationship Model\nlinking a weak entity set to its corresponding strong entity set is redundant and does\nnot need to be present in a tabular representation of an E-R diagram.\n2.9.3.2\nCombination of Tables\nConsider a many-to-one relationship set AB from entity set A to entity set B. Using\nour table-construction scheme outlined previously, we get three tables: A, B, and AB.\nSuppose further that the participation of A in the relationship is total; that is, every\nentity a in the entity set A must participate in the relationship AB. Then we can\ncombine the tables A and AB to form a single table consisting of the union of columns\nof both tables.\nAs an illustration, consider the E-R diagram of Figure 2.27. The double line in the\nE-R diagram indicates that the participation of account in the account-branch is total.\nHence, an account cannot exist without being associated with a particular branch.\nFurther, the relationship set account-branch is many to one from account to branch.\nTherefore, we can combine the table for account-branch with the table for account and\nrequire only the following two tables:\n• account, with attributes account-number, balance, and branch-name\n• branch, with attributes branch-name, branch-city, and assets\n2.9.4\nComposite Attributes\nWe handle composite attributes by creating a separate attribute for each of the com-\nponent attributes; we do not create a separate column for the composite attribute\nitself. Suppose address is a composite attribute of entity set customer, and the com-\nponents of address are street and city. The table generated from customer would then\ncontain columns address-street and address-city; there is no separate column for address.\n2.9.5\nMultivalued Attributes\nWe have seen that attributes in an E-R diagram generally map directly into columns\nfor the appropriate tables. Multivalued attributes, however, are an exception; new\ntables are created for these attributes.\naccount-number\nbalance\naccount\nbranch-name\nbranch-city\nbranch\naccount-\nbranch\nassets\nFigure 2.27\nE-R diagram.\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nI. Data Models\n2. Entity−Relationship \nModel\n76\n© The McGraw−Hill \nCompanies, 2001\n2.9\nReduction of an E-R Schema to Tables\n67\nFor a multivalued attribute M, we create a table T with a column C that corre-\nsponds to M and columns corresponding to the primary key of the entity set or rela-\ntionship set of which M is an attribute. As an illustration, consider the E-R diagram\nin Figure 2.22. The diagram includes the multivalued attribute dependent-name. For\nthis multivalued attribute, we create a table dependent-name, with columns dname, re-\nferring to the dependent-name attribute of employee, and employee-id, representing the\nprimary key of the entity set employee. Each dependent of an employee is represented\nas a unique row in the table.\n2.9.6\nTabular Representation of Generalization\nThere are two different methods for transforming to a tabular form an E-R diagram\nthat includes generalization. Although we refer to the generalization in Figure 2.17\nin this discussion, we simplify it by including only the ﬁrst tier of lower-level entity\nsets—that is, savings-account and checking-account.\n1. Create a table for the higher-level entity set. For each lower-level entity set,\ncreate a table that includes a column for each of the attributes of that entity set\nplus a column for each attribute of the primary key of the higher-level entity\nset. Thus, for the E-R diagram of Figure 2.17, we have three tables:\n• account, with attributes account-number and balance\n• savings-account, with attributes account-number and interest-rate\n• checking-account, with attributes account-number and overdraft-amount\n2. An alternative representation is possible, if the generalization is disjoint and\ncomplete—that is, if no entity is a member of two lower-level entity sets di-\nrectly below a higher-level entity set, and if every entity in the higher level\nentity set is also a member of one of the lower-level entity sets. Here, do not\ncreate a table for the higher-level entity set. Instead, for each lower-level en-\ntity set, create a table that includes a column for each of the attributes of that\nentity set plus a column for each attribute of the higher-level entity set. Then,\nfor the E-R diagram of Figure 2.17, we have two tables.\n• savings-account, with attributes account-number, balance, and interest-rate\n• checking-account, with attributes account-number, balance, and overdraft-\namount\nThe savings-account and checking-account relations corresponding to these\ntables both have account-number as the primary key.\nIf the second method were used for an overlapping generalization, some values\nsuch as balance would be stored twice unnecessarily. Similarly, if the generalization\nwere not complete—that is, if some accounts were neither savings nor checking\naccounts—then such accounts could not be represented with the second method.\n2.9.7\nTabular Representation of Aggregation\nTransforming an E-R diagram containing aggregation to a tabular form is straight-\nforward. Consider the diagram of Figure 2.19. The table for the relationship set\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nI. Data Models\n2. Entity−Relationship \nModel\n77\n© The McGraw−Hill \nCompanies, 2001\n68\nChapter 2\nEntity-Relationship Model\nmanages between the aggregation of works-on and the entity set manager includes a\ncolumn for each attribute in the primary keys of the entity set manager and the rela-\ntionship set works-on. It would also include a column for any descriptive attributes,\nif they exist, of the relationship set manages. We then transform the relationship sets\nand entity sets within the aggregated entity.\n2.10\nThe Uniﬁed Modeling Language UML∗∗\nEntity-relationship diagrams help model the data representation component of a soft-\nware system. Data representation, however, forms only one part of an overall system\ndesign. Other components include models of user interactions with the system, spec-\niﬁcation of functional modules of the system and their interaction, etc. The Uniﬁed\nModeling Language (UML), is a proposed standard for creating speciﬁcations of var-\nious components of a software system. Some of the parts of UML are:\n• Class diagram. A class diagram is similar to an E-R diagram. Later in this\nsection we illustrate a few features of class diagrams and how they relate to\nE-R diagrams.\n• Use case diagram. Use case diagrams show the interaction between users and\nthe system, in particular the steps of tasks that users perform (such as with-\ndrawing money or registering for a course).\n• Activity diagram. Activity diagrams depict the ﬂow of tasks between various\ncomponents of a system.\n• Implementation diagram. Implementation diagrams show the system com-\nponents and their interconnections, both at the software component level and\nthe hardware component level.\nWe do not attempt to provide detailed coverage of the different parts of UML here.\nSee the bibliographic notes for references on UML. Instead we illustrate some features\nof UML through examples.\nFigure 2.28 shows several E-R diagram constructs and their equivalent UML class\ndiagram constructs. We describe these constructs below. UML shows entity sets as\nboxes and, unlike E-R, shows attributes within the box rather than as separate el-\nlipses. UML actually models objects, whereas E-R models entities. Objects are like\nentities, and have attributes, but additionally provide a set of functions (called meth-\nods) that can be invoked to compute values on the basis of attributes of the objects,\nor to update the object itself. Class diagrams can depict methods in addition to at-\ntributes. We cover objects in Chapter 8.\nWe represent binary relationship sets in UML by just drawing a line connecting\nthe entity sets. We write the relationship set name adjacent to the line. We may also\nspecify the role played by an entity set in a relationship set by writing the role name\non the line, adjacent to the entity set. Alternatively, we may write the relationship set\nname in a box, along with attributes of the relationship set, and connect the box by a\ndotted line to the line depicting the relationship set. This box can then be treated as\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nI. Data Models\n2. Entity−Relationship \nModel\n78\n© The McGraw−Hill \nCompanies, 2001\n2.10\nThe Uniﬁed Modeling Language UML∗∗\n69\ndisjoint\nE1\nE2\nR\nrole1\nrole2\nE2\nE2\nperson\ncustomer\nemployee\nperson\ncustomer\nemployee\nperson\nISA\ncustomer\nemployee\nperson\nISA\ncustomer\nemployee\n1. entity sets\n    and attributes\ncustomer-id\ncustomer-name\ncustomer-street\ncustomer-city\ncustomer\ncustomer-city\ncustomer-street\ncustomer\n2. relationships\na1\nrole1\nrole2\na1\na2\nR\nR\n3. cardinality\n    constraints \nR\n0..1\nR\nE1\nE2\nE1\nE1\nE2\nE2\nrole1\nrole2\nrole1\nrole2\n4. generalization and\n    specialization\nclass diagram in UML\nE-R diagram\nR\n0..*\n0..*\n0..1\ncustomer-name\ncustomer-id\n(overlapping \ngeneralization)\n(disjoint \ngeneralization)\na2\nE1\nE1\nFigure 2.28\nSymbols used in the UML class diagram notation.\nan entity set, in the same way as an aggregation in E-R diagrams and can participate\nin relationships with other entity sets.\nNonbinar\n\nity set in\nonly one ISA relationship; that is, entity sets in this diagram have only single inher-\nitance. If an entity set is a lower-level entity set in more than one ISA relationship,\nthen the entity set has multiple inheritance, and the resulting structure is said to be\na lattice.\n2.7.4\nConstraints on Generalizations\nTo model an enterprise more accurately, the database designer may choose to place\ncertain constraints on a particular generalization. One type of constraint involves\ndetermining which entities can be members of a given lower-level entity set. Such\nmembership may be one of the following:\n• Condition-deﬁned. In condition-deﬁned lower-level entity sets, membership\nis evaluated on the basis of whether or not an entity satisﬁes an explicit con-\ndition or predicate. For example, assume that the higher-level entity set ac-\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nI. Data Models\n2. Entity−Relationship \nModel\n62\n© The McGraw−Hill \nCompanies, 2001\n2.7\nExtended E-R Features\n53\ncount has the attribute account-type. All account entities are evaluated on the\ndeﬁning account-type attribute. Only those entities that satisfy the condition\naccount-type = “savings account” are allowed to belong to the lower-level en-\ntity set person. All entities that satisfy the condition account-type = “checking\naccount” are included in checking account. Since all the lower-level entities are\nevaluated on the basis of the same attribute (in this case, on account-type), this\ntype of generalization is said to be attribute-deﬁned.\n• User-deﬁned. User-deﬁned lower-level entity sets are not constrained by a\nmembership condition; rather, the database user assigns entities to a given en-\ntity set. For instance, let us assume that, after 3 months of employment, bank\nemployees are assigned to one of four work teams. We therefore represent the\nteams as four lower-level entity sets of the higher-level employee entity set. A\ngiven employee is not assigned to a speciﬁc team entity automatically on the\nbasis of an explicit deﬁning condition. Instead, the user in charge of this de-\ncision makes the team assignment on an individual basis. The assignment is\nimplemented by an operation that adds an entity to an entity set.\nA second type of constraint relates to whether or not entities may belong to more\nthan one lower-level entity set within a single generalization. The lower-level entity\nsets may be one of the following:\n• Disjoint. A disjointness constraint requires that an entity belong to no more\nthan one lower-level entity set. In our example, an account entity can satisfy\nonly one condition for the account-type attribute; an entity can be either a sav-\nings account or a checking account, but cannot be both.\n• Overlapping. In overlapping generalizations, the same entity may belong to\nmore than one lower-level entity set within a single generalization. For an\nillustration, consider the employee work team example, and assume that cer-\ntain managers participate in more than one work team. A given employee may\ntherefore appear in more than one of the team entity sets that are lower-level\nentity sets of employee. Thus, the generalization is overlapping.\nAs another example, suppose generalization applied to entity sets customer\nand employee leads to a higher-level entity set person. The generalization is\noverlapping if an employee can also be a customer.\nLower-level entity overlap is the default case; a disjointness constraint must be placed\nexplicitly on a generalization (or specialization). We can note a disjointedness con-\nstraint in an E-R diagram by adding the word disjoint next to the triangle symbol.\nA ﬁnal constraint, the completeness constraint on a generalization or specializa-\ntion, speciﬁes whether or not an entity in the higher-level entity set must belong to at\nleast one of the lower-level entity sets within the generalization/specialization. This\nconstraint may be one of the following:\n• Total generalization or specialization. Each higher-level entity must belong\nto a lower-level entity set.\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nI. Data Models\n2. Entity−Relationship \nModel\n63\n© The McGraw−Hill \nCompanies, 2001\n54\nChapter 2\nEntity-Relationship Model\n• Partial generalization or specialization. Some higher-level entities may not\nbelong to any lower-level entity set.\nPartial generalization is the default. We can specify total generalization in an E-R dia-\ngram by using a double line to connect the box representing the higher-level entity set\nto the triangle symbol. (This notation is similar to the notation for total participation\nin a relationship.)\nThe account generalization is total: All account entities must be either a savings\naccount or a checking account. Because the higher-level entity set arrived at through\ngeneralization is generally composed of only those entities in the lower-level entity\nsets, the completeness constraint for a generalized higher-level entity set is usually\ntotal. When the generalization is partial, a higher-level entity is not constrained to\nappear in a lower-level entity set. The work team entity sets illustrate a partial spe-\ncialization. Since employees are assigned to a team only after 3 months on the job,\nsome employee entities may not be members of any of the lower-level team entity sets.\nWe may characterize the team entity sets more fully as a partial, overlapping spe-\ncialization of employee. The generalization of checking-account and savings-account into\naccount is a total, disjoint generalization. The completeness and disjointness con-\nstraints, however, do not depend on each other. Constraint patterns may also be\npartial-disjoint and total-overlapping.\nWe can see that certain insertion and deletion requirements follow from the con-\nstraints that apply to a given generalization or specialization. For instance, when a\ntotal completeness constraint is in place, an entity inserted into a higher-level en-\ntity set must also be inserted into at least one of the lower-level entity sets. With a\ncondition-deﬁned constraint, all higher-level entities that satisfy the condition must\nbe inserted into that lower-level entity set. Finally, an entity that is deleted from a\nhigher-level entity set also is deleted from all the associated lower-level entity sets to\nwhich it belongs.\n2.7.5\nAggregation\nOne limitation of the E-R model is that it cannot express relationships among rela-\ntionships. To illustrate the need for such a construct, consider the ternary relationship\nworks-on, which we saw earlier, between a employee, branch, and job (see Figure 2.13).\nNow, suppose we want to record managers for tasks performed by an employee at a\nbranch; that is, we want to record managers for (employee, branch, job) combinations.\nLet us assume that there is an entity set manager.\nOne alternative for representing this relationship is to create a quaternary relation-\nship manages between employee, branch, job, and manager. (A quaternary relationship is\nrequired—a binary relationship between manager and employee would not permit us\nto represent which (branch, job) combinations of an employee are managed by which\nmanager.) Using the basic E-R modeling constructs, we obtain the E-R diagram of\nFigure 2.18. (We have omitted the attributes of the entity sets, for simplicity.)\nIt appears that the relationship sets works-on and manages can be combined into\none single relationship set. Nevertheless, we should not combine them into a single\nrelationship, since some employee, branch, job combinations many not have a manager.\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nI. Data Models\n2. Entity−Relationship \nModel\n64\n© The McGraw−Hill \nCompanies, 2001\n2.7\nExtended E-R Features\n55\nemployee\nbranch\nmanages\nmanager\njob\nworks-on\nFigure 2.18\nE-R diagram with redundant relationships.\nThere is redundant information in the resultant ﬁgure, however, since every em-\nployee, branch, job combination in manages is also in works-on. If the manager were a\nvalue rather than an manager entity, we could instead make manager a multivalued at-\ntribute of the relationship works-on. But doing so makes it more difﬁcult (logically as\nwell as in execution cost) to ﬁnd, for example, employee-branch-job triples for which\na manager is responsible. Since the manager is a manager entity, this alternative is\nruled out in any case.\nThe best way to model a situation such as the one just described is to use aggrega-\ntion. Aggregation is an abstraction through which relationships are treated as higher-\nlevel entities. Thus, for our example, we regard the relationship set works-on (relating\nthe entity sets employee, branch, and job) as a higher-level entity set called works-on.\nSuch an entity set is treated in the same manner as is any other entity set. We can\nthen create a binary relationship manages between works-on and manager to represent\nwho manages what tasks. Figure 2.19 shows a notation for aggregation commonly\nused to represent the above situation.\n2.7.6\nAlternative E-R Notations\nFigure 2.20 summarizes the set of symbols we have used in E-R diagrams. There is\nno universal standard for E-R diagram notation, and different books and E-R diagram\nsoftware use different notations; Figure 2.21 indicates some of the alternative nota-\ntions that are widely used. An entity set may be represented as a box with the name\noutside, and the attributes listed one below the other within the box. The primary\nkey attributes are indicated by listing them at the top, with a line separating them\nfrom the other attributes.\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nI. Data Models\n2. Entity−Relationship \nModel\n65\n© The McGraw−Hill \nCompanies, 2001\n56\nChapter 2\nEntity-Relationship Model\nbranch\nemployee\nmanages\nmanager\nworks-on\njob\nFigure 2.19\nE-R diagram with aggregation.\nCardinality constraints can be indicated in several different ways, as Figure 2.21\nshows. The labels ∗and 1 on the edges out of the relationship are sometimes used for\ndepicting many-to-many, one-to-one, and many-to-one relationships, as the ﬁgure\nshows. The case of one-to-many is symmetric to many-to-one, and is not shown. In\nanother alternative notation in the ﬁgure, relationship sets are represented by lines\nbetween entity sets, without diamonds; only binary relationships can be modeled\nthus. Cardinality constraints in such a notation are shown by “crow’s foot” notation,\nas in the ﬁgure.\n2.8\nDesign of an E-R Database Schema\nThe E-R data model gives us much ﬂexibility in designing a database schema to\nmodel a given enterprise. In this section, we consider how a database designer may\nselect from the wide range of alternatives. Among the designer’s decisions are:\n• Whether to use an attribute or an entity set to represent an object (discussed\nearlier in Section 2.2.1)\n• Whether a real-world concept is expressed more accurately by an entity set or\nby a relationship set (Section 2.2.2)\n• Whether to use a ternary relationship or a pair of binary relationships (Sec-\ntion 2.2.3)\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nI. Data Models\n2. Entity−Relationship \nModel\n66\n© The McGraw−Hill \nCompanies, 2001\n2.8\nDesign of an E-R Database Schema\n57\ntotal \nparticipation \nof entity set \nin relationship\nA\nR\nE\nmany-to-many\nrelationship\nR\nR\nE\nrole-\nname\nISA\n(specialization or\ngeneralization) \nmany-to-one\nrelationship\nR\nE\nR\nl..h\ncardinality \nlimits\ndiscriminating \nattribute of \nweak entity set\nISA\nISA\ntotal\ngeneralization\nISA\nattribute\nmultivalued\nattribute\nderived attribute\nweak entity set\nentity set\nE\nR\nA\nA\nR\nE\nA\nA\nprimary key\nrelationship set\nidentifying\nrelationship \nset for weak \nentity set\nrole indicator\none-to-one\nrelationship\nR\ndisjoint\ndisjoint\ngeneralization\nFigure 2.20\nSymbols used in the E-R notation.\n• Whether to use a strong or a weak entity set (Section 2.6); a strong entity set\nand its dependent weak entity sets may be regarded as a single “object” in the\ndatabase, since weak entities are existence dependent on a strong entity\n• Whether using generalization (Section 2.7.2) is appropriate; generalization, or\na hierarchy of ISA relationships, contributes to modularity by allowing com-\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nI. Data Models\n2. Entity−Relationship \nModel\n67\n© The McGraw−Hill \nCompanies, 2001\n58\nChapter 2\nEntity-Relationship Model\nR\nR\nmany-to-many\nrelationship\nentity set E with\nattributes A1, A2, A3\nand primary key A1\n*\n*\nR\n1\n1\nR\n*\n1\nR\nR\none-to-one\nrelationship\nmany-to-one\nrelationship\nE\nA1\nA2\nA3\nFigure 2.21\nAlternative E-R notations.\nmon attributes of similar entity sets to be represented in one place in an E-R\ndiagram\n• Whether using aggregation (Section 2.7.5) is appropriate; aggregation groups\na part of an E-R diagram into a single entity set, allowing us to treat the ag-\ngregate entity set as a single unit without concern for the details of its internal\nstructure.\nWe shall see that the database designer needs a good understanding of the enterprise\nbeing modeled to make these decisions.\n2.8.1\nDesign Phases\nA high-level data model serves the database designer by providing a conceptual\nframework in which to specify, in a systematic fashion, what the data requirements\nof the database users are, and how the database will be structured to fulﬁll these\nrequirements. The initial phase of database design, then, is to characterize fully the\ndata needs of the prospective database users. The database designer needs to interact\nextensively with domain experts and users to carry out this task. The outcome of this\nphase is a speciﬁcation of user requirements.\nNext, the designer chooses a data model, and by applying the concepts of the\nchosen data model, translates these requirements into a conceptual schema of the\ndatabase. The schema developed at this conceptual-design phase provides a detailed\noverview of the enterprise. Since we have studied only the E-R model so far, we shall\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nI. Data Models\n2. Entity−Relationship \nModel\n68\n© The McGraw−Hill \nCompanies, 2001\n2.8\nDesign of an E-R Database Schema\n59\nuse it to develop the conceptual schema. Stated in terms of the E-R model, the schema\nspeciﬁes all entity sets, relationship sets, attributes, and mapping constraints. The de-\nsigner reviews the schema to conﬁrm that all data requirements are indeed satisﬁed\nand are not in conﬂict with one another. She can also examine the design to remove\nany redundant features. Her focus at this point is describing the data and their rela-\ntionships, rather than on specifying physical storage details.\nA fully developed conceptual schema will also indicate the functional require-\nments of the enterprise. In a speciﬁcation of functional requirements, users describe\nthe kinds of operations (or transactions) that will be performed on the data. Example\noperations include modifying or updating data, searching for and retrieving speciﬁc\ndata, and deleting data. At this stage of conceptual design, the designer can review\nthe schema to ensure it meets functional requirements.\nThe process of moving from an abstract data model to the implementation of the\ndatabase proceeds in two ﬁnal design phases. In the logical-design phase, the de-\nsigner maps the high-level conceptual schema onto the implementation data model\nof the database system that will be used. The designer uses the resulting system-\nspeciﬁc database schema in the subsequent physical-design phase, in which the\nphysical features of the database are speciﬁed. These features include the form of ﬁle\norganization and the internal storage structures; they are discussed in Chapter 11.\nIn this chapter, we cover only the concepts of the E-R model as used in the concep-\ntual-schema-design phase. We have presented a brief overview of the database-design\nprocess to provide a context for the discussion of the E-R data model. Database design\nreceives a full treatment in Chapter 7.\nIn Section 2.8.2, we apply the two initial database-design phases to our banking-\nenterprise example. We employ the E-R data model to translate user requirements\ninto a conceptual design schema that is depicted as an E-R diagram.\n2.8.2\nDatabase Design for Banking Enterprise\nWe now look at the database-design requirements of a banking enterprise in more\ndetail, and develop a more realistic, but also more complicated, design than what\nwe have seen in our earlier examples. However, we do not attempt to model every\naspect of the database-design for a bank; we consider only a few aspects, in order to\nillustrate the process of database design.\n2.8.2.1\nData Requirements\nThe initial speciﬁcation of user requirements may be based on interviews with the\ndatabase users, and on the designer’s own analysis of the enterprise. The description\nthat arises from this design phase serves as the basis for specifying the conceptual\nstructure of the database. Here are the major characteristics of the banking enterprise.\n• The bank is organized into branches. Each branch is located in a particular\ncity and is identiﬁed by a unique name. The bank monitors the assets of each\nbranch.\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nI. Data Models\n2. Entity−Relationship \nModel\n69\n© The McGraw−Hill \nCompanies, 2001\n60\nChapter 2\nEntity-Relationship Model\n• Bank customers are identiﬁed by their customer-id values. The bank stores each\ncustomer’s name, and the street and city where the customer lives. Customers\nmay have accounts and can take out loans. A customer may be associated with\na particular banker, who may act as a loan ofﬁcer or personal banker for that\ncustomer.\n• Bank employees are identiﬁed by their employee-id values. The bank adminis-\ntration stores the name and telephone number of each employee, the names\nof the employee’s dependents, and the employee-id number of the employee’s\nmanager. The bank also keeps track of the employee’s start date and, thus,\nlength of employment.\n• The bank offers two types of accounts—savings and checking accounts. Ac-\ncounts can be held by more than one customer, and a customer can have more\nthan one account. Each account is assigned a unique account number. The\nbank maintains a record of each account’s balance, and the most recent date on\nwhich the account was accessed by each customer holding the account. In ad-\ndition, each savings account has an interest rate, and overdrafts are recorded\nfor each checking account.\n• A loan originates at a particular branch and can be held by one or more cus-\ntomers. A loan is identiﬁed by a unique loan number. For each loan, the bank\nkeeps track of the loan amount and the loan payments. Although a loan-\npayment number does not uniquely identify a particular payment among\nthose for all the bank’s loans, a payment number does identify a particular\npayment for a speciﬁc loan. The date and amount are recorded for each pay-\nment.\nIn a real banking enterprise, the bank would keep track of deposits and with-\ndrawals from savings and checking accounts, just as it keeps track of payments to\nloan accounts. Since the modeling requirements for that tracking are similar, and we\nwould like to keep our example application small, we do not keep track of such de-\nposits and withdrawals in our model.\n2.8.2.2\nEntity Sets Designation\nOur speciﬁcation of data requirements serves as the starting point for constructing a\nconceptual schema for the database. From the characteristics listed in Section 2.8.2.1,\nwe begin to identify entity sets and their attributes:\n• The branch entity set, with attributes branch-name, branch-city, and assets.\n• The customer entity set, with attributes customer-id, customer-name, customer-\nstreet; and customer-city. A possible additional attribute is banker-name.\n• The employee entity set, with attributes employee-id, employee-name, telephone-\nnumber, salary, and manager. Additional descriptive f\n\n can be speciﬁed easily in an E-R diagram. Figure 2.13\nconsists of the three entity sets employee, job, and branch, related through the relation-\nship set works-on.\nWe can specify some types of many-to-one relationships in the case of nonbinary\nrelationship sets. Suppose an employee can have at most one job in each branch (for\nexample, Jones cannot be a manager and an auditor at the same branch). This con-\nstraint can be speciﬁed by an arrow pointing to job on the edge from works-on.\nWe permit at most one arrow out of a relationship set, since an E-R diagram with\ntwo or more arrows out of a nonbinary relationship set can be interpreted in two\nways. Suppose there is a relationship set R between entity sets A1, A2, . . . , An, and the\nonly arrows are on the edges to entity sets Ai+1, Ai+2, . . . , An. Then, the two possible\ninterpretations are:\n1. A particular combination of entities from A1, A2, . . . , Ai can be associated with\nat most one combination of entities from Ai+1, Ai+2, . . . , An. Thus, the pri-\nmary key for the relationship R can be constructed by the union of the primary\nkeys of A1, A2, . . . , Ai.\nbranch\nbranch-city\nbranch-name\nassets\nemployee-id\ntitle\nlevel\nstreet\ncity\nemployee-name\nemployee\njob\nworks-on\nFigure 2.13\nE-R diagram with a ternary relationship.\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nI. Data Models\n2. Entity−Relationship \nModel\n55\n© The McGraw−Hill \nCompanies, 2001\n46\nChapter 2\nEntity-Relationship Model\nborrower\namount\nloan-number\nloan\ncustomer-city\ncustomer-id\ncustomer-name\ncustomer-street\ncustomer\nFigure 2.14\nTotal participation of an entity set in a relationship set.\n2. For each entity set Ak, i < k ≤n, each combination of the entities from the\nother entity sets can be associated with at most one entity from Ak. Each set\n{A1, A2, . . . , Ak−1, Ak+1, . . . , An}, for i < k ≤n, then forms a candidate key.\nEach of these interpretations has been used in different books and systems. To avoid\nconfusion, we permit only one arrow out of a relationship set, in which case the two\ninterpretations are equivalent. In Chapter 7 (Section 7.3) we study the notion of func-\ntional dependencies, which allow either of these interpretations to be speciﬁed in an\nunambiguous manner.\nDouble lines are used in an E-R diagram to indicate that the participation of an\nentity set in a relationship set is total; that is, each entity in the entity set occurs in at\nleast one relationship in that relationship set. For instance, consider the relationship\nborrower between customers and loans. A double line from loan to borrower, as in\nFigure 2.14, indicates that each loan must have at least one associated customer.\nE-R diagrams also provide a way to indicate more complex constraints on the num-\nber of times each entity participates in relationships in a relationship set. An edge\nbetween an entity set and a binary relationship set can have an associated minimum\nand maximum cardinality, shown in the form l..h, where l is the minimum and h\nthe maximum cardinality. A minimum value of 1 indicates total participation of the\nentity set in the relationship set. A maximum value of 1 indicates that the entity par-\nticipates in at most one relationship, while a maximum value ∗indicates no limit.\nNote that a label 1..∗on an edge is equivalent to a double line.\nFor example, consider Figure 2.15. The edge between loan and borrower has a car-\ndinality constraint of 1..1, meaning the minimum and the maximum cardinality are\nboth 1. That is, each loan must have exactly one associated customer. The limit 0..∗\non the edge from customer to borrower indicates that a customer can have zero or\nmore loans. Thus, the relationship borrower is one to many from customer to loan, and\nfurther the participation of loan in borrower is total.\nIt is easy to misinterpret the 0..∗on the edge between customer and borrower, and\nthink that the relationship borrower is many to one from customer to loan—this is\nexactly the reverse of the correct interpretation.\nIf both edges from a binary relationship have a maximum value of 1, the relation-\nship is one to one. If we had speciﬁed a cardinality limit of 1..∗on the edge between\ncustomer and borrower, we would be saying that each customer must have at least one\nloan.\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nI. Data Models\n2. Entity−Relationship \nModel\n56\n© The McGraw−Hill \nCompanies, 2001\n2.6\nWeak Entity Sets\n47\nborrower\namount\nloan-number\nloan\ncustomer-city\ncustomer-street\ncustomer\n0..*\n1..1\ncustomer-name\ncustomer-id\nFigure 2.15\nCardinality limits on relationship sets.\n2.6\nWeak Entity Sets\nAn entity set may not have sufﬁcient attributes to form a primary key. Such an entity\nset is termed a weak entity set. An entity set that has a primary key is termed a strong\nentity set.\nAs an illustration, consider the entity set payment, which has the three attributes:\npayment-number, payment-date, and payment-amount. Payment numbers are typically\nsequential numbers, starting from 1, generated separately for each loan. Thus, al-\nthough each payment entity is distinct, payments for different loans may share the\nsame payment number. Thus, this entity set does not have a primary key; it is a weak\nentity set.\nFor a weak entity set to be meaningful, it must be associated with another entity\nset, called the identifying or owner entity set. Every weak entity must be associated\nwith an identifying entity; that is, the weak entity set is said to be existence depen-\ndent on the identifying entity set. The identifying entity set is said to own the weak\nentity set that it identiﬁes. The relationship associating the weak entity set with the\nidentifying entity set is called the identifying relationship. The identifying relation-\nship is many to one from the weak entity set to the identifying entity set, and the\nparticipation of the weak entity set in the relationship is total.\nIn our example, the identifying entity set for payment is loan, and a relationship\nloan-payment that associates payment entities with their corresponding loan entities is\nthe identifying relationship.\nAlthough a weak entity set does not have a primary key, we nevertheless need a\nmeans of distinguishing among all those entities in the weak entity set that depend\non one particular strong entity. The discriminator of a weak entity set is a set of at-\ntributes that allows this distinction to be made. For example, the discriminator of the\nweak entity set payment is the attribute payment-number, since, for each loan, a pay-\nment number uniquely identiﬁes one single payment for that loan. The discriminator\nof a weak entity set is also called the partial key of the entity set.\nThe primary key of a weak entity set is formed by the primary key of the iden-\ntifying entity set, plus the weak entity set’s discriminator. In the case of the entity\nset payment, its primary key is {loan-number, payment-number}, where loan-number is\nthe primary key of the identifying entity set, namely loan, and payment-number dis-\ntinguishes payment entities within the same loan.\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nI. Data Models\n2. Entity−Relationship \nModel\n57\n© The McGraw−Hill \nCompanies, 2001\n48\nChapter 2\nEntity-Relationship Model\nThe identifying relationship set should have no descriptive attributes, since any\nrequired attributes can be associated with the weak entity set (see the discussion of\nmoving relationship-set attributes to participating entity sets in Section 2.2.1).\nA weak entity set can participate in relationships other than the identifying re-\nlationship. For instance, the payment entity could participate in a relationship with\nthe account entity set, identifying the account from which the payment was made. A\nweak entity set may participate as owner in an identifying relationship with another\nweak entity set. It is also possible to have a weak entity set with more than one iden-\ntifying entity set. A particular weak entity would then be identiﬁed by a combination\nof entities, one from each identifying entity set. The primary key of the weak entity\nset would consist of the union of the primary keys of the identifying entity sets, plus\nthe discriminator of the weak entity set.\nIn E-R diagrams, a doubly outlined box indicates a weak entity set, and a dou-\nbly outlined diamond indicates the corresponding identifying relationship. In Fig-\nure 2.16, the weak entity set payment depends on the strong entity set loan via the\nrelationship set loan-payment.\nThe ﬁgure also illustrates the use of double lines to indicate total participation—the\nparticipation of the (weak) entity set payment in the relationship loan-payment is total,\nmeaning that every payment must be related via loan-payment to some loan. Finally,\nthe arrow from loan-payment to loan indicates that each payment is for a single loan.\nThe discriminator of a weak entity set also is underlined, but with a dashed, rather\nthan a solid, line.\nIn some cases, the database designer may choose to express a weak entity set as\na multivalued composite attribute of the owner entity set. In our example, this alter-\nnative would require that the entity set loan have a multivalued, composite attribute\npayment, consisting of payment-number, payment-date, and payment-amount. A weak\nentity set may be more appropriately modeled as an attribute if it participates in only\nthe identifying relationship, and if it has few attributes. Conversely, a weak-entity-\nset representation will more aptly model a situation where the set participates in\nrelationships other than the identifying relationship, and where the weak entity set\nhas several attributes.\nloan-number\namount\nloan\npayment-number\npayment-amount\npayment\nloan-payment\npayment-date\nFigure 2.16\nE-R diagram with a weak entity set.\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nI. Data Models\n2. Entity−Relationship \nModel\n58\n© The McGraw−Hill \nCompanies, 2001\n2.7\nExtended E-R Features\n49\nAs another example of an entity set that can be modeled as a weak entity set,\nconsider offerings of a course at a university. The same course may be offered in\ndifferent semesters, and within a semester there may be several sections for the same\ncourse. Thus we can create a weak entity set course-offering, existence dependent on\ncourse; different offerings of the same course are identiﬁed by a semester and a section-\nnumber, which form a discriminator but not a primary key.\n2.7\nExtended E-R Features\nAlthough the basic E-R concepts can model most database features, some aspects of a\ndatabase may be more aptly expressed by certain extensions to the basic E-R model.\nIn this section, we discuss the extended E-R features of specialization, generalization,\nhigher- and lower-level entity sets, attribute inheritance, and aggregation.\n2.7.1\nSpecialization\nAn entity set may include subgroupings of entities that are distinct in some way\nfrom other entities in the set. For instance, a subset of entities within an entity set\nmay have attributes that are not shared by all the entities in the entity set. The E-R\nmodel provides a means for representing these distinctive entity groupings.\nConsider an entity set person, with attributes name, street, and city. A person may\nbe further classiﬁed as one of the following:\n• customer\n• employee\nEach of these person types is described by a set of attributes that includes all the at-\ntributes of entity set person plus possibly additional attributes. For example, customer\nentities may be described further by the attribute customer-id, whereas employee enti-\nties may be described further by the attributes employee-id and salary. The process of\ndesignating subgroupings within an entity set is called specialization. The special-\nization of person allows us to distinguish among persons according to whether they\nare employees or customers.\nAs another example, suppose the bank wishes to divide accounts into two cat-\negories, checking account and savings account. Savings accounts need a minimum\nbalance, but the bank may set interest rates differently for different customers, offer-\ning better rates to favored customers. Checking accounts have a ﬁxed interest rate,\nbut offer an overdraft facility; the overdraft amount on a checking account must be\nrecorded.\nThe bank could then create two specializations of account, namely savings-account\nand checking-account. As we saw earlier, account entities are described by the at-\ntributes account-number and balance. The entity set savings-account would have all the\nattributes of account and an additional attribute interest-rate. The entity set checking-\naccount would have all the attributes of account, and an additional attribute overdraft-\namount.\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nI. Data Models\n2. Entity−Relationship \nModel\n59\n© The McGraw−Hill \nCompanies, 2001\n50\nChapter 2\nEntity-Relationship Model\nWe can apply specialization repeatedly to reﬁne a design scheme. For instance,\nbank employees may be further classiﬁed as one of the following:\n• ofﬁcer\n• teller\n• secretary\nEach of these employee types is described by a set of attributes that includes all the\nattributes of entity set employee plus additional attributes. For example, ofﬁcer entities\nmay be described further by the attribute ofﬁce-number, teller entities by the attributes\nstation-number and hours-per-week, and secretary entities by the attribute hours-per-\nweek. Further, secretary entities may participate in a relationship secretary-for, which\nidentiﬁes which employees are assisted by a secretary.\nAn entity set may be specialized by more than one distinguishing feature. In our\nexample, the distinguishing feature among employee entities is the job the employee\nperforms. Another, coexistent, specialization could be based on whether the person\nis a temporary (limited-term) employee or a permanent employee, resulting in the\nentity sets temporary-employee and permanent-employee. When more than one special-\nization is formed on an entity set, a particular entity may belong to multiple spe-\ncializations. For instance, a given employee may be a temporary employee who is a\nsecretary.\nIn terms of an E-R diagram, specialization is depicted by a triangle component\nlabeled ISA, as Figure 2.17 shows. The label ISA stands for “is a” and represents, for\nexample, that a customer “is a” person. The ISA relationship may also be referred to as\na superclass-subclass relationship. Higher- and lower-level entity sets are depicted\nas regular entity sets—that is, as rectangles containing the name of the entity set.\n2.7.2\nGeneralization\nThe reﬁnement from an initial entity set into successive levels of entity subgroupings\nrepresents a top-down design process in which distinctions are made explicit. The\ndesign process may also proceed in a bottom-up manner, in which multiple entity\nsets are synthesized into a higher-level entity set on the basis of common features. The\ndatabase designer may have ﬁrst identiﬁed a customer entity set with the attributes\nname, street, city, and customer-id, and an employee entity set with the attributes name,\nstreet, city, employee-id, and salary.\nThere are similarities between the customer entity set and the employee entity set\nin the sense that they have several attributes in common. This commonality can be\nexpressed by generalization, which is a containment relationship that exists between\na higher-level entity set and one or more lower-level entity sets. In our example, person\nis the higher-level entity set and customer and employee are lower-level entity sets.\nHigher- and lower-level entity sets also may be designated by the terms superclass\nand subclass, respectively. The person entity set is the superclass of the customer and\nemployee subclasses.\nFor all practical purposes, generalization is a simple inversion of specialization.\nWe will apply both processes, in combination, in the course of designing the E-R\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nI. Data Models\n2. Entity−Relationship \nModel\n60\n© The McGraw−Hill \nCompanies, 2001\n2.7\nExtended E-R Features\n51\nstreet\nemployee\ncustomer\nofficer\nteller\nsecretary\nISA\nISA\nperson\nname\ncity\nhours-worked\noffice-number\nhours-worked\nstation-number\ncredit-rating\nsalary\nFigure 2.17\nSpecialization and generalization.\nschema for an enterprise. In terms of the E-R diagram itself, we do not distinguish be-\ntween specialization and generalization. New levels of entity representation will be\ndistinguished (specialization) or synthesized (generalization) as the design schema\ncomes to express fully the database application and the user requirements of the\ndatabase. Differences in the two approaches may be characterized by their starting\npoint and overall goal.\nSpecialization stems from a single entity set; it emphasizes differences among enti-\nties within the set by creating distinct lower-level entity sets. These lower-level entity\nsets may have attributes, or may participate in relationships, that do not apply to all\nthe entities in the higher-level entity set. Indeed, the reason a designer applies special-\nization is to represent such distinctive features. If customer and employee neither have\nattributes that person entities do not have nor participate in different relationships\nthan those in which person entities participate, there would be no need to specialize\nthe person entity set.\nGeneralization proceeds from the recognition that a number of entity sets share\nsome common features (namely, they are described by the same attributes and par-\nticipate in the same relationship sets). On the basis of their commonalities, generaliza-\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nI. Data Models\n2. Entity−Relationship \nModel\n61\n© The McGraw−Hill \nCompanies, 2001\n52\nChapter 2\nEntity-Relationship Model\ntion synthesizes these entity sets into a single, higher-level entity set. Generalization\nis used to emphasize the similarities among lower-level entity sets and to hide the\ndifferences; it also permits an economy of representation in that shared attributes are\nnot repeated.\n2.7.3\nAttribute Inheritance\nA crucial property of the higher- and lower-level entities created by specialization\nand generalization is attribute inheritance. The attributes of the higher-level entity\nsets are said to be inherited by the lower-level entity sets. For example, customer and\nemployee inherit the attributes of person. Thus, customer is described by its name, street,\nand city attributes, and additionally a customer-id attribute; employee is described by\nits name, street, and city attributes, and additionally employee-id and salary attributes.\nA lower-level entity set (or subclass) also inherits participation in the relationship\nsets in which its higher-level entity (or superclass) participates. The ofﬁcer, teller, and\nsecretary entity sets can participate in the works-for relationship set, since the super-\nclass employee participates in the works-for relationship. Attribute inheritance applies\nthrough all tiers of lower-level entity sets. The above entity sets can participate in any\nrelationships in which the person entity set participates.\nWhether a given portion of an E-R model was arrived at by specialization or gen-\neralization, the outcome is basically the same:\n• A higher-level entity set with attributes and relationships that apply to all of\nits lower-level entity sets\n• Lower-level entity sets with distinctive features that apply only within a par-\nticular lower-level entity set\nIn what follows, although we often refer to only generalization, the properties that\nwe discuss belong fully to both processes.\nFigure 2.17 depicts a hierarchy of entity sets. In the ﬁgure, employee is a lower-level\nentity set of person and a higher-level entity set of the ofﬁcer, teller, and secretary entity\nsets. In a hierarchy, a given entity set may be involved as a lower-level ent\n\nn 2.1.2),\nthe role name is used instead of the name of the entity set, to form a unique attribute\nname.\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nI. Data Models\n2. Entity−Relationship \nModel\n46\n© The McGraw−Hill \nCompanies, 2001\n2.4\nDesign Issues\n37\nThe structure of the primary key for the relationship set depends on the map-\nping cardinality of the relationship set. As an illustration, consider the entity sets\ncustomer and account, and the relationship set depositor, with attribute access-date, in\nSection 2.1.2. Suppose that the relationship set is many to many. Then the primary\nkey of depositor consists of the union of the primary keys of customer and account.\nHowever, if a customer can have only one account—that is, if the depositor relation-\nship is many to one from customer to account—then the primary key of depositor is\nsimply the primary key of customer. Similarly, if the relationship is many to one from\naccount to customer—that is, each account is owned by at most one customer—then\nthe primary key of depositor is simply the primary key of account. For one-to-one re-\nlationships either primary key can be used.\nFor nonbinary relationships, if no cardinality constraints are present then the su-\nperkey formed as described earlier in this section is the only candidate key, and it\nis chosen as the primary key. The choice of the primary key is more complicated if\ncardinality constraints are present. Since we have not discussed how to specify cardi-\nnality constraints on nonbinary relations, we do not discuss this issue further in this\nchapter. We consider the issue in more detail in Section 7.3.\n2.4\nDesign Issues\nThe notions of an entity set and a relationship set are not precise, and it is possible\nto deﬁne a set of entities and the relationships among them in a number of differ-\nent ways. In this section, we examine basic issues in the design of an E-R database\nschema. Section 2.7.4 covers the design process in further detail.\n2.4.1\nUse of Entity Sets versus Attributes\nConsider the entity set employee with attributes employee-name and telephone-number.\nIt can easily be argued that a telephone is an entity in its own right with attributes\ntelephone-number and location (the ofﬁce where the telephone is located). If we take\nthis point of view, we must redeﬁne the employee entity set as:\n• The employee entity set with attribute employee-name\n• The telephone entity set with attributes telephone-number and location\n• The relationship set emp-telephone, which denotes the association between em-\nployees and the telephones that they have\nWhat, then, is the main difference between these two deﬁnitions of an employee?\nTreating a telephone as an attribute telephone-number implies that employees have\nprecisely one telephone number each. Treating a telephone as an entity telephone per-\nmits employees to have several telephone numbers (including zero) associated with\nthem. However, we could instead easily deﬁne telephone-number as a multivalued at-\ntribute to allow multiple telephones per employee.\nThe main difference then is that treating a telephone as an entity better models a\nsituation where one may want to keep extra information about a telephone, such as\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nI. Data Models\n2. Entity−Relationship \nModel\n47\n© The McGraw−Hill \nCompanies, 2001\n38\nChapter 2\nEntity-Relationship Model\nits location, or its type (mobile, video phone, or plain old telephone), or who all share\nthe telephone. Thus, treating telephone as an entity is more general than treating it\nas an attribute and is appropriate when the generality may be useful.\nIn contrast, it would not be appropriate to treat the attribute employee-name as an\nentity; it is difﬁcult to argue that employee-name is an entity in its own right (in contrast\nto the telephone). Thus, it is appropriate to have employee-name as an attribute of the\nemployee entity set.\nTwo natural questions thus arise: What constitutes an attribute, and what con-\nstitutes an entity set? Unfortunately, there are no simple answers. The distinctions\nmainly depend on the structure of the real-world enterprise being modeled, and on\nthe semantics associated with the attribute in question.\nA common mistake is to use the primary key of an entity set as an attribute of an-\nother entity set, instead of using a relationship. For example, it is incorrect to model\ncustomer-id as an attribute of loan even if each loan had only one customer. The re-\nlationship borrower is the correct way to represent the connection between loans and\ncustomers, since it makes their connection explicit, rather than implicit via an at-\ntribute.\nAnother related mistake that people sometimes make is to designate the primary\nkey attributes of the related entity sets as attributes of the relationship set. This should\nnot be done, since the primary key attributes are already implicit in the relationship.\n2.4.2\nUse of Entity Sets versus Relationship Sets\nIt is not always clear whether an object is best expressed by an entity set or a rela-\ntionship set. In Section 2.1.1, we assumed that a bank loan is modeled as an entity.\nAn alternative is to model a loan not as an entity, but rather as a relationship between\ncustomers and branches, with loan-number and amount as descriptive attributes. Each\nloan is represented by a relationship between a customer and a branch.\nIf every loan is held by exactly one customer and is associated with exactly one\nbranch, we may ﬁnd satisfactory the design where a loan is represented as a rela-\ntionship. However, with this design, we cannot represent conveniently a situation in\nwhich several customers hold a loan jointly. To handle such a situation, we must de-\nﬁne a separate relationship for each holder of the joint loan. Then, we must replicate\nthe values for the descriptive attributes loan-number and amount in each such relation-\nship. Each such relationship must, of course, have the same value for the descriptive\nattributes loan-number and amount.\nTwo problems arise as a result of the replication: (1) the data are stored multiple\ntimes, wasting storage space, and (2) updates potentially leave the data in an incon-\nsistent state, where the values differ in two relationships for attributes that are sup-\nposed to have the same value. The issue of how to avoid such replication is treated\nformally by normalization theory, discussed in Chapter 7.\nThe problem of replication of the attributes loan-number and amount is absent in\nthe original design of Section 2.1.1, because there loan is an entity set.\nOne possible guideline in determining whether to use an entity set or a relation-\nship set is to designate a relationship set to describe an action that occurs between\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nI. Data Models\n2. Entity−Relationship \nModel\n48\n© The McGraw−Hill \nCompanies, 2001\n2.4\nDesign Issues\n39\nentities. This approach can also be useful in deciding whether certain attributes may\nbe more appropriately expressed as relationships.\n2.4.3\nBinary versus n-ary Relationship Sets\nRelationships in databases are often binary. Some relationships that appear to be\nnonbinary could actually be better represented by several binary relationships. For\ninstance, one could create a ternary relationship parent, relating a child to his/her\nmother and father. However, such a relationship could also be represented by two\nbinary relationships, mother and father, relating a child to his/her mother and father\nseparately. Using the two relationships mother and father allows us record a child’s\nmother, even if we are not aware of the father’s identity; a null value would be\nrequired if the ternary relationship parent is used. Using binary relationship sets is\npreferable in this case.\nIn fact, it is always possible to replace a nonbinary (n-ary, for n > 2) relationship\nset by a number of distinct binary relationship sets. For simplicity, consider the ab-\nstract ternary (n = 3) relationship set R, relating entity sets A, B, and C. We replace\nthe relationship set R by an entity set E, and create three relationship sets:\n• RA, relating E and A\n• RB, relating E and B\n• RC, relating E and C\nIf the relationship set R had any attributes, these are assigned to entity set E; further,\na special identifying attribute is created for E (since it must be possible to distinguish\ndifferent entities in an entity set on the basis of their attribute values). For each rela-\ntionship (ai, bi, ci) in the relationship set R, we create a new entity ei in the entity set\nE. Then, in each of the three new relationship sets, we insert a relationship as follows:\n• (ei, ai) in RA\n• (ei, bi) in RB\n• (ei, ci) in RC\nWe can generalize this process in a straightforward manner to n-ary relationship\nsets. Thus, conceptually, we can restrict the E-R model to include only binary rela-\ntionship sets. However, this restriction is not always desirable.\n• An identifying attribute may have to be created for the entity set created to\nrepresent the relationship set. This attribute, along with the extra relationship\nsets required, increases the complexity of the design and (as we shall see in\nSection 2.9) overall storage requirements.\n• A n-ary relationship set shows more clearly that several entities participate in\na single relationship.\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nI. Data Models\n2. Entity−Relationship \nModel\n49\n© The McGraw−Hill \nCompanies, 2001\n40\nChapter 2\nEntity-Relationship Model\n• There may not be a way to translate constraints on the ternary relationship\ninto constraints on the binary relationships. For example, consider a constraint\nthat says that R is many-to-one from A, B to C; that is, each pair of entities\nfrom A and B is associated with at most one C entity. This constraint cannot\nbe expressed by using cardinality constraints on the relationship sets RA, RB,\nand RC.\nConsider the relationship set works-on in Section 2.1.2, relating employee, branch,\nand job. We cannot directly split works-on into binary relationships between employee\nand branch and between employee and job. If we did so, we would be able to record\nthat Jones is a manager and an auditor and that Jones works at Perryridge and Down-\ntown; however, we would not be able to record that Jones is a manager at Perryridge\nand an auditor at Downtown, but is not an auditor at Perryridge or a manager at\nDowntown.\nThe relationship set works-on can be split into binary relationships by creating a\nnew entity set as described above. However, doing so would not be very natural.\n2.4.4\nPlacement of Relationship Attributes\nThe cardinality ratio of a relationship can affect the placement of relationship at-\ntributes. Thus, attributes of one-to-one or one-to-many relationship sets can be as-\nsociated with one of the participating entity sets, rather than with the relationship\nset. For instance, let us specify that depositor is a one-to-many relationship set such\nthat one customer may have several accounts, but each account is held by only one\ncustomer. In this case, the attribute access-date, which speciﬁes when the customer last\naccessed that account, could be associated with the account entity set, as Figure 2.6 de-\npicts; to keep the ﬁgure simple, only some of the attributes of the two entity sets are\nshown. Since each account entity participates in a relationship with at most one in-\nstance of customer, making this attribute designation would have the same meaning\nA-101    24 May 1996\nA-215     3 June 1996\nA-102    10 June 1996\nA-305    28 May 1996\nA-201    17 June 1996\nA-222    24 June 1996\nA-217    23 May 1996\ncustomer (customer-name)\naccount (account-number, access-date)\ndepositor\nJohnson\nSmith\nHayes\nTurner\nJones\nLindsay\nFigure 2.6\nAccess-date as attribute of the account entity set.\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nI. Data Models\n2. Entity−Relationship \nModel\n50\n© The McGraw−Hill \nCompanies, 2001\n2.4\nDesign Issues\n41\nas would placing access-date with the depositor relationship set. Attributes of a one-to-\nmany relationship set can be repositioned to only the entity set on the “many” side of\nthe relationship. For one-to-one relationship sets, on the other hand, the relationship\nattribute can be associated with either one of the participating entities.\nThe design decision of where to place descriptive attributes in such cases—as a\nrelationship or entity attribute—should reﬂect the characteristics of the enterprise\nbeing modeled. The designer may choose to retain access-date as an attribute of depos-\nitor to express explicitly that an access occurs at the point of interaction between the\ncustomer and account entity sets.\nThe choice of attribute placement is more clear-cut for many-to-many relationship\nsets. Returning to our example, let us specify the perhaps more realistic case that\ndepositor is a many-to-many relationship set expressing that a customer may have\none or more accounts, and that an account can be held by one or more customers.\nIf we are to express the date on which a speciﬁc customer last accessed a speciﬁc\naccount, access-date must be an attribute of the depositor relationship set, rather than\neither one of the participating entities. If access-date were an attribute of account, for\ninstance, we could not determine which customer made the most recent access to a\njoint account. When an attribute is determined by the combination of participating\nentity sets, rather than by either entity separately, that attribute must be associated\nwith the many-to-many relationship set. Figure 2.7 depicts the placement of access-\ndate as a relationship attribute; again, to keep the ﬁgure simple, only some of the\nattributes of the two entity sets are shown.\nJohnson\nSmith\nHayes\nTurner\nJones\nLindsay\nA-101    \nA-215  \nA-102   \nA-305    \nA-201    \nA-222    \nA-217   \ncustomer(customer-name)\naccount(account-number)\ndepositor(access-date)\n24 May 1996\n  3 June 1996\n21 June 1996\n10 June 1996\n17 June 1996\n28 May 1996\n28 May 1996\n24 June 1996\n23 May 1996\nFigure 2.7\nAccess-date as attribute of the depositor relationship set.\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nI. Data Models\n2. Entity−Relationship \nModel\n51\n© The McGraw−Hill \nCompanies, 2001\n42\nChapter 2\nEntity-Relationship Model\n2.5\nEntity-Relationship Diagram\nAs we saw brieﬂy in Section 1.4, an E-R diagram can express the overall logical struc-\nture of a database graphically. E-R diagrams are simple and clear—qualities that may\nwell account in large part for the widespread use of the E-R model. Such a diagram\nconsists of the following major components:\n• Rectangles, which represent entity sets\n• Ellipses, which represent attributes\n• Diamonds, which represent relationship sets\n• Lines, which link attributes to entity sets and entity sets to relationship sets\n• Double ellipses, which represent multivalued attributes\n• Dashed ellipses, which denote derived attributes\n• Double lines, which indicate total participation of an entity in a relation-\nship set\n• Double rectangles, which represent weak entity sets (described later, in Sec-\ntion 2.6.)\nConsider the entity-relationship diagram in Figure 2.8, which consists of two en-\ntity sets, customer and loan, related through a binary relationship set borrower. The at-\ntributes associated with customer are customer-id, customer-name, customer-street, and\ncustomer-city. The attributes associated with loan are loan-number and amount. In Fig-\nure 2.8, attributes of an entity set that are members of the primary key are underlined.\nThe relationship set borrower may be many-to-many, one-to-many, many-to-one,\nor one-to-one. To distinguish among these types, we draw either a directed line (→)\nor an undirected line (—) between the relationship set and the entity set in question.\n• A directed line from the relationship set borrower to the entity set loan speci-\nﬁes that borrower is either a one-to-one or many-to-one relationship set, from\ncustomer to loan; borrower cannot be a many-to-many or a one-to-many rela-\ntionship set from customer to loan.\ncustomer-name\ncustomer-street\ncustomer-id\ncustomer-city\ncustomer\nloan-number\namount\nloan\nborrower\nFigure 2.8\nE-R diagram corresponding to customers and loans.\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nI. Data Models\n2. Entity−Relationship \nModel\n52\n© The McGraw−Hill \nCompanies, 2001\n2.5\nEntity-Relationship Diagram\n43\n• An undirected line from the relationship set borrower to the entity set loan spec-\niﬁes that borrower is either a many-to-many or one-to-many relationship set\nfrom customer to loan.\nReturning to the E-R diagram of Figure 2.8, we see that the relationship set borrower\nis many-to-many. If the relationship set borrower were one-to-many, from customer to\nloan, then the line from borrower to customer would be directed, with an arrow point-\ning to the customer entity set (Figure 2.9a). Similarly, if the relationship set borrower\nwere many-to-one from customer to loan, then the line from borrower to loan would\nhave an arrow pointing to the loan entity set (Figure 2.9b). Finally, if the relation-\nship set borrower were one-to-one, then both lines from borrower would have arrows:\ncustomer-name\ncustomer-street\ncustomer-id\ncustomer-city\ncustomer\nloan-number\namount\nloan\nborrower\n(a)\ncustomer-name\ncustomer-street\ncustomer-id\ncustomer-city\ncustomer\nloan-number\namount\nloan\nborrower\n(b)\ncustomer-name\ncustomer-street\ncustomer-id\ncustomer-city\ncustomer\nloan-number\namount\nloan\nborrower\n(c)\nFigure 2.9\nRelationships. (a) one to many. (b) many to one. (c) one-to-one.\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nI. Data Models\n2. Entity−Relationship \nModel\n53\n© The McGraw−Hill \nCompanies, 2001\n44\nChapter 2\nEntity-Relationship Model\ncustomer-name\ncustomer-street\ncustomer-id\ncustomer-city\ncustomer\nbalance\naccount\ndepositor\naccess-date\naccount-number\nFigure 2.10\nE-R diagram with an attribute attached to a relationship set.\none pointing to the loan entity set and one pointing to the customer entity set (Fig-\nure 2.9c).\nIf a relationship set has also some attributes associated with it, then we link these\nattributes to that relationship set. For example, in Figure 2.10, we have the access-\ndate descriptive attribute attached to the relationship set depositor to specify the most\nrecent date on which a customer accessed that account.\nFigure 2.11 shows how composite attributes can be represented in the E-R notation.\nHere, a composite attribute name, with component attributes ﬁrst-name, middle-initial,\nand last-name replaces the simple attribute customer-name of customer. Also, a compos-\nite attribute address, whose component attributes are street, city, state, and zip-code re-\nplaces the attributes customer-street and customer-city of customer. The attribute street is\nitself a composite attribute whose component attributes are street-number, street-name,\nand apartment number.\nFigure 2.11 also illustrates a multivalued attribute phone-number, depicted by a\ndouble ellipse, and a derived attribute age, depicted by a dashed ellipse.\ncity\nzip-code\nstreet\nstate\nname\ncustomer\ncustomer-id\nmiddle-initial\nlast-name\nfirst-name\nstreet-number\nstreet-name\napartment-number\naddress\nphone-number\ndate-of-birth\nage\nFigure 2.11\nE-R diagram with composite, multivalued, and derived attributes.\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nI. Data Models\n2. Entity−Relationship \nModel\n54\n© The McGraw−Hill \nCompanies, 2001\n2.5\nEntity-Relationship Diagram\n45\nemployee-id\nemployee-name\ntelephone-number\nemployee\nworks-for\nmanager\nworker\nFigure 2.12\nE-R diagram with role indicators.\nWe indicate roles in E-R diagrams by labeling the lines that connect diamonds\nto rectangles. Figure 2.12 shows the role indicators manager and worker between the\nemployee entity set and the works-for relationship set.\nNonbinary relationship sets\n\ne set-valued attributes, such as keyword-set, by enumerating their elements\nwithin parentheses following the keyword set. We can create multiset values just like\nset values, by replacing set by multiset.3\nThus, we can create a tuple of the type deﬁned by the books relation as:\n(’Compilers’, array[’Smith’, ’Jones’], Publisher(’McGraw-Hill’, ’New York’),\nset(’parsing’, ’analysis’))\n3.\nAlthough sets and multisets are not part of the SQL:1999 standard, the other constructs shown in this\nsection are part of the standard. Future versions of SQL are likely to support sets and multisets.\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nIII. Object−Based \nDatabases and XML\n9. Object−Relational \nDatabases\n344\n© The McGraw−Hill \nCompanies, 2001\n342\nChapter 9\nObject-Relational Databases\nHere we have created a value for the attribute Publisher by invoking a constructor\nfunction for Publisher with appropriate arguments.\nIf we want to insert the preceding tuple into the relation books, we could execute\nthe statement\ninsert into books\nvalues\n(’Compilers’, array[’Smith’, ’Jones’], Publisher(’McGraw-Hill’, ’New York’),\nset(’parsing’, ’analysis’))\n9.3\nInheritance\nInheritance can be at the level of types, or at the level of tables. We ﬁrst consider\ninheritance of types, then inheritance at the level of tables.\n9.3.1\nType Inheritance\nSuppose that we have the following type deﬁnition for people:\ncreate type Person\n(name varchar(20),\naddress varchar(20))\nWe may want to store extra information in the database about people who are stu-\ndents, and about people who are teachers. Since students and teachers are also peo-\nple, we can use inheritance to deﬁne the student and teacher types in SQL:1999:\ncreate type Student\nunder Person\n(degree varchar(20),\ndepartment varchar(20))\ncreate type Teacher\nunder Person\n(salary integer,\ndepartment varchar(20))\nBoth Student and Teacher inherit the attributes of Person—namely, name and address.\nStudent and Teacher are said to be subtypes of Person, and Person is a supertype of\nStudent, as well as of Teacher.\nMethods of a structured type are inherited by its subtypes, just as attributes are.\nHowever, a subtype can redeﬁne the effect of a method by declaring the method\nagain, using overriding method in place of method in the method declaration.\nNow suppose that we want to store information about teaching assistants, who\nare simultaneously students and teachers, perhaps even in different departments.\nWe can do this by using multiple inheritance, which we studied in Chapter 8. The\nSQL:1999 standard does not support multiple inheritance. However, draft versions\nof the SQL:1999 standard provided for multiple inheritance, and although the ﬁnal\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nIII. Object−Based \nDatabases and XML\n9. Object−Relational \nDatabases\n345\n© The McGraw−Hill \nCompanies, 2001\n9.3\nInheritance\n343\nSQL:1999 omitted it, future versions of the SQL standard may introduce it. We base\nour discussion on the draft versions of the SQL:1999 standard.\nFor instance, if our type system supports multiple inheritance, we can deﬁne a\ntype for teaching assistant as follows:\ncreate type TeachingAssistant\nunder Student, Teacher\nTeachingAssistant would inherit all the attributes of Student and Teacher. There is a\nproblem, however, since the attributes name, address, and department are present in\nStudent, as well as in Teacher.\nThe attributes name and address are actually inherited from a common source, Per-\nson. So there is no conﬂict caused by inheriting them from Student as well as Teacher.\nHowever, the attribute department is deﬁned separately in Student and Teacher. In fact,\na teaching assistant may be a student of one department and a teacher in another\ndepartment. To avoid a conﬂict between the two occurrences of department, we can\nrename them by using an as clause, as in this deﬁnition of the type TeachingAssistant:\ncreate type TeachingAssistant\nunder Student with (department as student-dept),\nTeacher with (department as teacher-dept)\nWe note that SQL:1999 supports only single inheritance— that is, a type can inherit\nfrom only a single type; the syntax used is as in our earlier examples. Multiple inher-\nitance as in the TeachingAssistant example is not supported in SQL:1999. The SQL:1999\nstandard also requires an extra ﬁeld at the end of the type deﬁnition, whose value\nis either ﬁnal or not ﬁnal. The keyword ﬁnal says that subtypes may not be created\nfrom the given type, while not ﬁnal says that subtypes may be created.\nIn SQL as in most other languages, a value of a structured type must have exactly\none “most-speciﬁc type.” That is, each value must be associated with one speciﬁc\ntype, called its most-speciﬁc type, when it is created. By means of inheritance, it\nis also associated with each of the supertypes of its most speciﬁc type. For example,\nsuppose that an entity has the type Person, as well as the type Student. Then, the most-\nspeciﬁc type of the entity is Student, since Student is a subtype of Person. However, an\nentity cannot have the type Student, as well as the type Teacher, unless it has a type,\nsuch as TeachingAssistant, that is a subtype of Teacher, as well as of Student.\n9.3.2\nTable Inheritance\nSubtables in SQL:1999 correspond to the E-R notion of specialization/generalization.\nFor instance, suppose we deﬁne the people table as follows:\ncreate table people of Person\nWe can then deﬁne tables students and teachers as subtables of people, as follows:\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nIII. Object−Based \nDatabases and XML\n9. Object−Relational \nDatabases\n346\n© The McGraw−Hill \nCompanies, 2001\n344\nChapter 9\nObject-Relational Databases\ncreate table students of Student\nunder people\ncreate table teachers of Teacher\nunder people\nThe types of the subtables must be subtypes of the type of the parent table. Thereby,\nevery attribute present in people is also present in the subtables.\nFurther, when we declare students and teachers as subtables of people, every tuple\npresent in students or teachers becomes also implicitly present in people. Thus, if a\nquery uses the table people, it will ﬁnd not only tuples directly inserted into that table,\nbut also tuples inserted into its subtables, namely students and teachers. However,\nonly those attributes that are present in people can be accessed.\nMultiple inheritance is possible with tables, just as it is possible with types. (We\nnote, however, that multiple inheritance of tables is not supported by SQL:1999.) For\nexample, we can create a table of type TeachingAssistant:\ncreate table teaching-assistants\nof TeachingAssistant\nunder students, teachers\nAs a result of the declaration, every tuple present in the teaching-assistants table is\nalso implicitly present in the teachers and in the students table, and in turn in the\npeople table.\nSQL:1999 permits us to ﬁnd tuples that are in people but not in its subtables by using\n“only people” in place of people in a query.\nThere are some consistency requirements for subtables. Before we state the con-\nstraints, we need a deﬁnition: We say that tuples in a subtable corresponds to tuples\nin a parent table if they have the same values for all inherited attributes. Thus, corre-\nsponding tuples represent the same entity.\nThe consistency requirements for subtables are:\n1. Each tuple of the supertable can correspond to at most one tuple in each of its\nimmediate subtables.\n2. SQL:1999 has an additional constraint that all the tuples corresponding to each\nother must be derived from one tuple (inserted into one table).\nFor example, without the ﬁrst condition, we could have two tuples in students (or\nteachers) that correspond to the same person.\nThe second condition rules out a tuple in people corresponding to both a tuple in\nstudents and a tuple in teachers, unless all these tuples are implicitly present because\na tuple was inserted in a table teaching-assistants, which is a subtable of both teachers\nand students.\nSince SQL:1999 does not support multiple inheritance, the second condition actu-\nally prevents a person from being both a teacher and a student. The same problem\nwould arise if the subtable teaching-assistants is absent, even if multiple inheritance\nwere supported. Obviously it would be useful to model a situation where a person\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nIII. Object−Based \nDatabases and XML\n9. Object−Relational \nDatabases\n347\n© The McGraw−Hill \nCompanies, 2001\n9.3\nInheritance\n345\ncan be a teacher and a student, even if a common subtable teaching-assistants is not\npresent. Thus, it can be useful to remove the second consistency constraint. We return\nto this issue in Section 9.3.3.\nSubtables can be stored in an efﬁcient manner without replication of all inherited\nﬁelds, in one of two ways:\n• Each table stores the primary key (which may be inherited from a parent table)\nand the attributes deﬁned locally. Inherited attributes (other than the primary\nkey) do not need to be stored, and can be derived by means of a join with the\nsupertable, based on the primary key.\n• Each table stores all inherited and locally deﬁned attributes. When a tuple is\ninserted, it is stored only in the table in which it is inserted, and its presence is\ninferred in each of the supertables. Access to all attributes of a tuple is faster,\nsince a join is not required. However, in case the second consistency constraint\nis absent—that is, an entity can be represented in two subtables without be-\ning present in a common subtable of both—this representation can result in\nreplication of information.\n9.3.3\nOverlapping Subtables\nInheritance of types should be used with care. A university database may have many\nsubtypes of Person, such as Student, Teacher, FootballPlayer, ForeignCitizen, and so on.\nStudent may itself have subtypes such as UndergraduateStudent, GraduateStudent, and\nPartTimeStudent. Clearly, a person can belong to several of these categories at once.\nAs Chapter 8 mentions, each of these categories is sometimes called a role.\nFor each entity to have exactly one most-speciﬁc type, we would have to create\na subtype for every possible combination of the supertypes. In the preceding exam-\nple, we would have subtypes such as ForeignUndergraduateStudent, ForeignGraduate-\nStudentFootballPlayer, and so on. Unfortunately, we would end up with an enormous\nnumber of subtypes of Person.\nA better approach in the context of database systems is to allow an object to have\nmultiple types, without having a most-speciﬁc type. Object-relational systems can\nmodel such a feature by using inheritance at the level of tables, rather than of types,\nand allowing an entity to exist in more than one table at once.\nFor example, suppose we again have the type Person, with subtypes Student and\nTeacher, and the corresponding table people, with subtables teachers and students. We\ncan then have a tuple in teachers and a tuple in students corresponding to the same\ntuple in people.\nThere is no need to have a type TeachingAssistant that is a subtype of both Student\nand Teacher. We need not create a type TeachingAssistant unless we wish to store extra\nattributes or redeﬁne methods in a manner speciﬁc to people who are both students\nand teachers.\nWe note, however, that SQL:1999 prohibits such a situation, because of consistency\nrequirement 2 from Section 9.3.2. Since SQL:1999 also does not support multiple in-\nheritance, we cannot use inheritance to model a situation where a person can be\nboth a student and a teacher. We can of course create separate tables to represent the\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nIII. Object−Based \nDatabases and XML\n9. Object−Relational \nDatabases\n348\n© The McGraw−Hill \nCompanies, 2001\n346\nChapter 9\nObject-Relational Databases\ninformation without using inheritance. We would have to add appropriate referen-\ntial integrity constraints to ensure that students and teachers are also represented in\nthe people table.\n9.4\nReference Types\nObject-oriented languages provide the ability to refer to objects. An attribute of a type\ncan be a reference to an object of a speciﬁed type. For example, in SQL:1999 we can\ndeﬁne a type Department with a ﬁeld name and a ﬁeld head which is a reference to the\ntype Person, and a table departments of type Department, as follows:\ncreate type Department (\nname varchar(20),\nhead ref(Person) scope people\n)\ncreate table departments of Department\nHere, the reference is restricted to tuples of the table people. The restriction of the\nscope of a reference to tuples of a table is mandatory in SQL:1999, and it makes refer-\nences behave like foreign keys.\nWe can omit the declaration scope people from the type declaration and instead\nmake an addition to the create table statement:\ncreate table departments of Department\n(head with options scope people)\nIn order to initialize a reference attribute, we need to get the identiﬁer of the tuple\nthat is to be referenced. We can get the identiﬁer value of a tuple by means of a query.\nThus, to create a tuple with the reference value, we may ﬁrst create the tuple with a\nnull reference and then set the reference separately:\ninsert into departments\nvalues (’CS’, null)\nupdate departments\nset head = (select ref(p)\nfrom people as p\nwhere name = ’John’)\nwhere name = ’CS’\nThis syntax for accessing the identiﬁer of a tuple is based on the Oracle syntax.\nSQL:1999 adopts a different approach, one where the referenced table must have an\nattribute that stores the identiﬁer of the tuple. We declare this attribute, called the\nself-referential attribute, by adding a ref is clause to the create table statement:\ncreate table people of Person\nref is oid system generated\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nIII. Object−Based \nDatabases and XML\n9. Object−Relational \nDatabases\n349\n© The McGraw−Hill \nCompanies, 2001\n9.4\nReference Types\n347\nHere, oid is an attribute name, not a keyword. The subquery above would then use\nselect p.oid\ninstead of select ref(p).\nAn alternative to system-generated identiﬁers is to allow users to generate iden-\ntiﬁers. The type of the self-referential attribute must be speciﬁed as part of the type\ndeﬁnition of the referenced table, and the table deﬁnition must specify that the refer-\nence is user generated:\ncreate type Person\n(name varchar(20),\naddress varchar(20))\nref using varchar(20)\ncreate table people of Person\nref is oid user generated\nWhen inserting a tuple in people, we must provide a value for the identiﬁer:\ninsert into people values\n(’01284567’, ’John’, ’23 Coyote Run’)\nNo other tuple for people or its supertables or subtables can have the same identiﬁer.\nWe can then use the identiﬁer value when inserting a tuple into departments, without\nthe need for a separate query to retrieve the identiﬁer:\ninsert into departments\nvalues (’CS’, ’01284567’)\nIt is even possible to use an existing primary key value as the identiﬁer, by includ-\ning the ref from clause in the type deﬁnition:\ncreate type Person\n(name varchar(20) primary key,\naddress varchar(20))\nref from(name)\ncreate table people of Person\nref is oid derived\nNote that the table deﬁnition must specify that the reference is derived, and must still\nspecify a self-referential attribute name. When inserting a tuple for departments, we\ncan then use\ninsert into departments\nvalues (’CS’, ’John’)\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nIII. Object−Based \nDatabases and XML\n9. Object−Relational \nDatabases\n350\n© The McGraw−Hill \nCompanies, 2001\n348\nChapter 9\nObject-Relational Databases\n9.5\nQuerying with Complex Types\nIn this section, we present extensions of the SQL query language to deal with complex\ntypes. Let us start with a simple example: Find the title and the name of the publisher\nof each book. This query carries out the task:\nselect title, publisher.name\nfrom books\nNotice that the ﬁeld name of the composite attribute publisher is referred to by a dot\nnotation.\n9.5.1\nPath Expressions\nReferences are dereferenced in SQL:1999 by the −> symbol. Consider the departments\ntable deﬁned earlier. We can use this query to ﬁnd the names and addresses of the\nheads of all departments:\nselect head−>name, head−>address\nfrom departments\nAn expression such as “head−>name” is called a path expression.\nSince head is a reference to a tuple in the people table, the attribute name in the\npreceding query is the name attribute of the tuple from the people table. References can\nbe used to hide join operations; in the preceding example, without the references, the\nhead ﬁeld of department would be declared a foreign key of the table people. To ﬁnd\nthe name and address of the head of a department, we would require an explicit\njoin of the relations departments and people. The use of references simpliﬁes the query\nconsiderably.\n9.5.2\nCollection-Valued Attributes\nWe now consider how to handle collection-valued attributes. Arrays are the only\ncollection type supported by SQL:1999, but we use the same syntax for relation-valued\nattributes also. An expression evaluating to a collection can appear anywhere that a\nrelation name may appear, such as in a from clause, as the following paragraphs\nillustrate. We use the table books which we deﬁned earlier.\nIf we want to ﬁnd all books that have the word “database” as one of their key-\nwords, we can use this query:\nselect title\nfrom books\nwhere ’database’ in (unnest(keyword-set))\nNote that we have used unnest(keyword-set) in a position where SQL without nested\nrelations would have required a select-from-where subexpression.\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nIII. Object−Based \nDatabases and XML\n9. Object−Relational \nDatabases\n351\n© The McGraw−Hill \nCompanies, 2001\n9.5\nQuerying with Complex Types\n349\nIf we know that a particular book has three authors, we could write:\nselect author-array[1], author-array[2], author-array[3]\nfrom books\nwhere title = ’Database System Concepts’\nNow, suppose that we want a relation containing pairs of the form “title, author-\nname” for each book and each author of the book. We can use this query:\nselect B.title, A.name\nfrom books as B, unnest(B.author-array) as A\nSince the author-array attribute of books is a collection-valued ﬁeld, it can be used in a\nfrom clause, where a relation is expected.\n9.5.3\nNesting and Unnesting\nThe transformation of a nested relation into a form with fewer (or no) relation-valued\nattributes is called unnesting. The books relation has two attributes, author-array and\nkeyword-set, that are collections, and two attributes, title and publisher, that are not.\nSuppose that we want to convert the relation into a single ﬂat relation, with no nested\nrelations or structured types as attributes. We can use the following query to carry out\nthe task:\nselect title, A as author, publisher.name as pub-name, publisher.branch\nas pub-branch, K as keyword\nfrom books as B, unnest(B.author-array) as A, unnest (B.keyword-set) as K\nThe variable B in the from clause is declared to range over books. The variable A is\ndeclared to range over the authors in author-array for the book B, and K is declared to\nrange over the keywords in the keyword-set of the book B. Figure 9.1 (in Section 9.1)\nshows an instance books relation, and Figure 9.2 shows the 1NF relation that is the\nresult of the preceding query.\nThe reverse process of transforming a 1NF relation into a nested relation is called\nnesting. Nesting can be carried out by an extension of grouping in SQL. In the normal\nuse of grouping in SQL, a temporary multiset relation is (logically) created for each\ngroup, and an aggregate function is applied on the temporary relation. By return-\ning the multiset instead of applying the aggregate function, we can create a nested\nrelation. Suppose that we are given a 1NF relation ﬂat-books, as in Figure 9.2. The\nfollowing query nests the relation on the attrib",
    "precision": 0.04859028194361128,
    "recall": 1.0,
    "iou": 0.04859028194361128,
    "f1": 0.09267734553775744,
    "gold_tokens_count": 81,
    "retrieved_tokens_count": 1667,
    "intersection_tokens": 81
  },
  {
    "config_name": "chars_rrf",
    "config": {
      "name": "chars_rrf",
      "index_prefix": "textbook_index",
      "chunking_strategy": "chars",
      "overlap": 0,
      "fusion": "rrf",
      "bm25_weight": 0.3,
      "tag_weight": 0.2,
      "top_k": 5,
      "embed_model": "sentence-transformers/all-MiniLM-L6-v2"
    },
    "question": "Describe the circumstances in which you would choose to use embedded SQL rather than SQL alone or only a general-purpose programming language.",
    "gold_text": "4.12 Embedded SQL SQL provides a powerful declarative query language. Writing queries in SQL is usu- ally much easier than coding the same queries in a general-purpose programming language. However, a programmer must have access to a database from a general- purpose programming language for at least two reasons: 1. Not all queries can be expressed in SQL, since SQL does not provide the full expressive power of a general-purpose language. That is, there exist queries that can be expressed in a language such as C, Java, or Cobol that cannot be expressed in SQL. To write such queries, we can embed SQL within a more powerful language. SQL is designed so that queries written in it can be optimized automatically and executed efficiently—and providing the full power of a programming language makes automatic optimization exceedingly difficult. 2. Nondeclarative actions—such as printing a report, interacting with a user, or sending the results of a query to a graphical user interface—cannot be done from within SQL. Applications usually have several components, and query- ing or updating data is only one component; other components are written in general-purpose programming languages. For an integrated application, the programs written in the programming language must be able to access the database. The SQL standard defines embeddings of SQL in a variety of programming lan- guages, such as C, Cobol, Pascal, Java, PL/I, and Fortran. A language in which SQL queries are embedded is referred to as a host language, and the SQL structures per- mitted in the host language constitute embedded SQL. Programs written in the host language can use the embedded SQL syntax to ac- cess and update data stored in a database. This embedded form of SQL extends the programmer’s ability to manipulate the database even further. In embedded SQL, all query processing is performed by the database system, which then makes the result of the query available to the program one tuple (record) at a time. An embedded SQL program must be processed by a special preprocessor prior to compilation. The preprocessor replaces embedded SQL requests with host-language declarations and procedure calls that allow run-time execution of the database ac- cesses. Then, the resulting program is compiled by the host-language compiler. To identify embedded SQL requests to the preprocessor, we use the EXEC SQL statement; it has the form EXEC SQL <embedded SQL statement > END-EXEC The exact syntax for embedded SQL requests depends on the language in which SQL is embedded. For instance, a semicolon is used instead of END-EXEC when SQL is embedded in C. The Java embedding of SQL (called SQLJ) uses the syntax # SQL { <embedded SQL statement > }; We place the statement SQL INCLUDE in the program to identify the place where the preprocessor should insert the special variables used for communication between the program and the database system. Variables of the host language can be used within embedded SQL statements, but they must be preceded by a colon (:) to distin- guish them from SQL variables. Embedded SQL statements are similar in form to the SQL statements that we de- scribed in this chapter. There are, however, several important differences, as we note here. To write a relational query, we use the declare cursor statement. The result of the query is not yet computed. Rather, the program must use the open and fetch com- mands (discussed later in this section) to obtain the result tuples. Consider the banking schema that we have used in this chapter. Assume that we have a host-language variable amount, and that we wish to find the names and cities of residence of customers who have more than amount dollars in any account. We can write this query as follows: EXEC SQL declare c cursor for select customer-name, customer-city from depositor, customer, account where depositor.customer-name= customer.customer-name and account.account-number= depositor.account-number and account.balance > :amount END-EXEC The variable c in the preceding expression is called a cursor for the query. We use this variable to identify the query in the open statement, which causes the query to be evaluated, and in the fetch statement, which causes the values of one tuple to be placed in host-language variables. The open statement for our sample query is as follows: EXEC SQL open c END-EXEC This statement causes the database system to execute the query and to save the results within a temporary relation. The query has a host-language variable (:amount); the query uses the value of the variable at the time the open statement was executed. If the SQL query results in an error, the database system stores an error diagnostic in the SQL communication-area (SQLCA) variables, whose declarations are inserted by the SQL INCLUDE statement. An embedded SQL program executes a series of fetch statements to retrieve tuples of the result. The fetch statement requires one host-language variable for each at- tribute of the result relation. For our example query, we need one variable to hold the customer-name value and another to hold the customer-city value. Suppose that those variables are cn and cc, respectively. Then the statement: EXEC SQL fetch c into :cn, :cc END-EXEC produces a tuple of the result relation. The program can then manipulate the vari- ables cn and cc by using the features of the host programming language. A single fetch request returns only one tuple. To obtain all tuples of the result, the program must contain a loop to iterate over all tuples. Embedded SQL assists the programmer in managing this iteration. Although a relation is conceptually a set, the tuples of the result of a query are in some fixed physical order. When the program executes an open statement on a cursor, the cursor is set to point to the first tuple of the result. Each time it executes a fetch statement, the cursor is updated to point to the next tuple of the result. When no further tuples remain to be processed, the variable SQLSTATE in the SQLCA is set to ’02000’ (meaning“no data”). Thus, we can use a while loop (or equivalent loop) to process each tuple of the result. We must use the close statement to tell the database system to delete the tempo- rary relation that held the result of the query. For our example, this statement takes the form EXEC SQL close c END-EXEC SQLJ, the Java embedding of SQL, provides a variation of the above scheme, where Java iterators are used in place of cursors. SQLJ associates the results of a query with an iterator, and the next() method of the Java iterator interface can be used to step through the result tuples, just as the preceding examples use fetch on the cursor. Embedded SQL expressions for database modification (update, insert, and delete) do not return a result. Thus, they are somewhat simpler to express. A database- modification request takes the form EXEC SQL < any valid update, insert, or delete> END-EXEC Host-language variables, preceded by a colon, may appear in the SQL database- modification expression. If an error condition arises in the execution of the statement, a diagnostic is set in the SQLCA. Database relations can also be updated through cursors. For example, if we want to add 100 to the balance attribute of every account where the branch name is“Per- ryridge”, we could declare a cursor as follows. declare c cursor for select * from account where branch-name = ‘Perryridge‘ for update We then iterate through the tuples by performing fetch operations on the cursor (as illustrated earlier), and after fetching each tuple we execute the following code update account set balance= balance + 100 where current of c Embedded SQL allows a host-language program to access the database, but it pro- vides no assistance in presenting results to the user or in generating reports. Most commercial database products include tools to assist application programmers in creating user interfaces and formatted reports. ",
    "retrieved_text": "found at the end of that chapter.\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nII. Relational Databases\nIntroduction\n140\n© The McGraw−Hill \nCompanies, 2001\nP\nA\nR\nT\n2\nRelational Databases\nA relational database is a shared repository of data. To make data from a relational\ndatabase available to users, we have to address several issues. One is how users spec-\nify requests for data: Which of the various query languages do they use? Chapter 4\ncovers the SQL language, which is the most widely used query language today. Chap-\nter 5 covers two other query languages, QBE and Datalog, which offer alternative\napproaches to querying relational data.\nAnother issue is data integrity and security; databases need to protect data from\ndamage by user actions, whether unintentional or intentional. The integrity main-\ntenance component of a database ensures that updates do not violate integrity con-\nstraints that have been speciﬁed on the data. The security component of a database\nincludes authentication of users, and access control, to restrict the permissible actions\nfor each user. Chapter 6 covers integrity and security issues. Security and integrity\nissues are present regardless of the data model, but for concreteness we study them\nin the context of the relational model. Integrity constraints form the basis of relational\ndatabase design, which we study in Chapter 7.\nRelational database design—the design of the relational schema—is the ﬁrst step\nin building a database application. Schema design was covered informally in ear-\nlier chapters. There are, however, principles that can be used to distinguish good\ndatabase designs from bad ones. These are formalized by means of several “normal\nforms,” which offer different tradeoffs between the possibility of inconsistencies and\nthe efﬁciency of certain queries. Chapter 7 describes the formal design of relational\nschemas.\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nII. Relational Databases\n4. SQL\n141\n© The McGraw−Hill \nCompanies, 2001\nC\nH\nA\nP\nT\nE\nR\n4\nSQL\nThe formal languages described in Chapter 3 provide a concise notation for repre-\nsenting queries. However, commercial database systems require a query language\nthat is more user friendly. In this chapter, we study SQL, the most inﬂuential commer-\ncially marketed query language, SQL. SQL uses a combination of relational-algebra\nand relational-calculus constructs.\nAlthough we refer to the SQL language as a “query language,” it can do much\nmore than just query a database. It can deﬁne the structure of the data, modify data\nin the database, and specify security constraints.\nIt is not our intention to provide a complete users’ guide for SQL. Rather, we\npresent SQL’s fundamental constructs and concepts. Individual implementations of\nSQL may differ in details, or may support only a subset of the full language.\n4.1\nBackground\nIBM developed the original version of SQL at its San Jose Research Laboratory (now\nthe Almaden Research Center). IBM implemented the language, originally called Se-\nquel, as part of the System R project in the early 1970s. The Sequel language has\nevolved since then, and its name has changed to SQL (Structured Query Language).\nMany products now support the SQL language. SQL has clearly established itself as\nthe standard relational-database language.\nIn 1986, the American National Standards Institute (ANSI) and the International\nOrganization for Standardization (ISO) published an SQL standard, called SQL-86.\nIBM published its own corporate SQL standard, the Systems Application Architec-\nture Database Interface (SAA-SQL) in 1987. ANSI published an extended standard for\nSQL, SQL-89, in 1989. The next version of the standard was SQL-92 standard, and the\nmost recent version is SQL:1999. The bibliographic notes provide references to these\nstandards.\n135\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nII. Relational Databases\n4. SQL\n142\n© The McGraw−Hill \nCompanies, 2001\n136\nChapter 4\nSQL\nIn this chapter, we present a survey of SQL, based mainly on the widely imple-\nmented SQL-92 standard. The SQL:1999 standard is a superset of the SQL-92 standard;\nwe cover some features of SQL:1999 in this chapter, and provide more detailed cov-\nerage in Chapter 9. Many database systems support some of the new constructs in\nSQL:1999, although currently no database system supports all the new constructs. You\nshould also be aware that some database systems do not even support all the fea-\ntures of SQL-92, and that many databases provide nonstandard features that we do\nnot cover here.\nThe SQL language has several parts:\n• Data-deﬁnition language (DDL). The SQL DDL provides commands for deﬁn-\ning relation schemas, deleting relations, and modifying relation schemas.\n• Interactive data-manipulation language (DML). The SQL DML includes a\nquery language based on both the relational algebra and the tuple relational\ncalculus. It includes also commands to insert tuples into, delete tuples from,\nand modify tuples in the database.\n• View deﬁnition. The SQL DDL includes commands for deﬁning views.\n• Transaction control. SQL includes commands for specifying the beginning\nand ending of transactions.\n• Embedded SQL and dynamic SQL. Embedded and dynamic SQL deﬁne how\nSQL statements can be embedded within general-purpose programming lan-\nguages, such as C, C++, Java, PL/I, Cobol, Pascal, and Fortran.\n• Integrity. The SQL DDL includes commands for specifying integrity constraints\nthat the data stored in the database must satisfy. Updates that violate integrity\nconstraints are disallowed.\n• Authorization. The SQL DDL includes commands for specifying access rights\nto relations and views.\nIn this chapter, we cover the DML and the basic DDL features of SQL. We also\nbrieﬂy outline embedded and dynamic SQL, including the ODBC and JDBC standards\nfor interacting with a database from programs written in the C and Java languages.\nSQL features supporting integrity and authorization are described in Chapter 6, while\nChapter 9 outlines object-oriented extensions to SQL.\nThe enterprise that we use in the examples in this chapter, and later chapters, is a\nbanking enterprise with the following relation schemas:\nBranch-schema = (branch-name, branch-city, assets)\nCustomer-schema = (customer-name, customer-street, customer-city)\nLoan-schema = (loan-number, branch-name, amount)\nBorrower-schema = (customer-name, loan-number)\nAccount-schema = (account-number, branch-name, balance)\nDepositor-schema = (customer-name, account-number)\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nII. Relational Databases\n4. SQL\n143\n© The McGraw−Hill \nCompanies, 2001\n4.2\nBasic Structure\n137\nNote that in this chapter, as elsewhere in the text, we use hyphenated names for\nschema, relations, and attributes for ease of reading. In actual SQL systems, however,\nhyphens are not valid parts of a name (they are treated as the minus operator). A\nsimple way of translating the names we use to valid SQL names is to replace all hy-\nphens by the underscore symbol (“ ”). For example, we use branch name in place of\nbranch-name.\n4.2\nBasic Structure\nA relational database consists of a collection of relations, each of which is assigned\na unique name. Each relation has a structure similar to that presented in Chapter 3.\nSQL allows the use of null values to indicate that the value either is unknown or does\nnot exist. It allows a user to specify which attributes cannot be assigned null values,\nas we shall discuss in Section 4.11.\nThe basic structure of an SQL expression consists of three clauses: select, from, and\nwhere.\n• The select clause corresponds to the projection operation of the relational al-\ngebra. It is used to list the attributes desired in the result of a query.\n• The from clause corresponds to the Cartesian-product operation of the rela-\ntional algebra. It lists the relations to be scanned in the evaluation of the ex-\npression.\n• The where clause corresponds to the selection predicate of the relational alge-\nbra. It consists of a predicate involving attributes of the relations that appear\nin the from clause.\nThat the term select has different meaning in SQL than in the relational algebra is an\nunfortunate historical fact. We emphasize the different interpretations here to mini-\nmize potential confusion.\nA typical SQL query has the form\nselect A1, A2, . . . , An\nfrom r1, r2, . . . , rm\nwhere P\nEach Ai represents an attribute, and each ri a relation. P is a predicate. The query is\nequivalent to the relational-algebra expression\nΠA1, A2,...,An(σP (r1 × r2 × · · · × rm))\nIf the where clause is omitted, the predicate P is true. However, unlike the result of a\nrelational-algebra expression, the result of the SQL query may contain multiple copies\nof some tuples; we shall return to this issue in Section 4.2.8.\nSQL forms the Cartesian product of the relations named in the from clause,\nperforms a relational-algebra selection using the where clause predicate, and then\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nII. Relational Databases\n4. SQL\n144\n© The McGraw−Hill \nCompanies, 2001\n138\nChapter 4\nSQL\nprojects the result onto the attributes of the select clause. In practice, SQL may con-\nvert the expression into an equivalent form that can be processed more efﬁciently.\nHowever, we shall defer concerns about efﬁciency to Chapters 13 and 14.\n4.2.1\nThe select Clause\nThe result of an SQL query is, of course, a relation. Let us consider a simple query\nusing our banking example, “Find the names of all branches in the loan relation”:\nselect branch-name\nfrom loan\nThe result is a relation consisting of a single attribute with the heading branch-name.\nFormal query languages are based on the mathematical notion of a relation being\na set. Thus, duplicate tuples never appear in relations. In practice, duplicate elimina-\ntion is time-consuming. Therefore, SQL (like most other commercial query languages)\nallows duplicates in relations as well as in the results of SQL expressions. Thus, the\npreceding query will list each branch-name once for every tuple in which it appears in\nthe loan relation.\nIn those cases where we want to force the elimination of duplicates, we insert the\nkeyword distinct after select. We can rewrite the preceding query as\nselect distinct branch-name\nfrom loan\nif we want duplicates removed.\nSQL allows us to use the keyword all to specify explicitly that duplicates are not\nremoved:\nselect all branch-name\nfrom loan\nSince duplicate retention is the default, we will not use all in our examples. To ensure\nthe elimination of duplicates in the results of our example queries, we will use dis-\ntinct whenever it is necessary. In most queries where distinct is not used, the exact\nnumber of duplicate copies of each tuple present in the query result is not important.\nHowever, the number is important in certain applications; we return to this issue in\nSection 4.2.8.\nThe asterisk symbol “ * ” can be used to denote “all attributes.” Thus, the use of\nloan.* in the preceding select clause would indicate that all attributes of loan are to be\nselected. A select clause of the form select * indicates that all attributes of all relations\nappearing in the from clause are selected.\nThe select clause may also contain arithmetic expressions involving the operators\n+, −, ∗, and / operating on constants or attributes of tuples. For example, the query\nselect loan-number, branch-name, amount * 100\nfrom loan\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nII. Relational Databases\n4. SQL\n145\n© The McGraw−Hill \nCompanies, 2001\n4.2\nBasic Structure\n139\nwill return a relation that is the same as the loan relation, except that the attribute\namount is multiplied by 100.\nSQL also provides special data types, such as various forms of the date type, and\nallows several arithmetic functions to operate on these types.\n4.2.2\nThe where Clause\nLet us illustrate the use of the where clause in SQL. Consider the query “Find all loan\nnumbers for loans made at the Perryridge branch with loan amounts greater that\n$1200.” This query can be written in SQL as:\nselect loan-number\nfrom loan\nwhere branch-name = ’Perryridge’ and amount > 1200\nSQL uses the logical connectives and, or, and not—rather than the mathematical\nsymbols ∧, ∨, and ¬ —in the where clause. The operands of the logical connectives\ncan be expressions involving the comparison operators <, <=, >, >=, =, and <>.\nSQL allows us to use the comparison operators to compare strings and arithmetic\nexpressions, as well as special types, such as date types.\nSQL includes a between comparison operator to simplify where clauses that spec-\nify that a value be less than or equal to some value and greater than or equal to some\nother value. If we wish to ﬁnd the loan number of those loans with loan amounts\nbetween $90,000 and $100,000, we can use the between comparison to write\nselect loan-number\nfrom loan\nwhere amount between 90000 and 100000\ninstead of\nselect loan-number\nfrom loan\nwhere amount <= 100000 and amount >= 90000\nSimilarly, we can use the not between comparison operator.\n4.2.3\nThe from Clause\nFinally, let us discuss the use of the from clause. The from clause by itself deﬁnes a\nCartesian product of the relations in the clause. Since the natural join is deﬁned in\nterms of a Cartesian product, a selection, and a projection, it is a relatively simple\nmatter to write an SQL expression for the natural join.\nWe write the relational-algebra expression\nΠcustomer-name, loan-number, amount (borrower\n\u0001 loan)\nfor the query “For all customers who have a loan from the bank, ﬁnd their names,\nloan numbers and loan amount.” In SQL, this query can be written as\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nII. Relational Databases\n4. SQL\n146\n© The McGraw−Hill \nCompanies, 2001\n140\nChapter 4\nSQL\nselect customer-name, borrower.loan-number, amount\nfrom borrower, loan\nwhere borrower.loan-number = loan.loan-number\nNotice that SQL uses the notation relation-name.attribute-name, as does the relational\nalgebra, to avoid ambiguity in cases where an attribute appears in the schema of more\nthan one relation. We could have written borrower.customer-name instead of customer-\nname in the select clause. However, since the attribute customer-name appears in only\none of the relations named in the from clause, there is no ambiguity when we write\ncustomer-name.\nWe can extend the preceding query and consider a more complicated case in which\nwe require also that the loan be from the Perryridge branch: “Find the customer\nnames, loan numbers, and loan amounts for all loans at the Perryridge branch.” To\nwrite this query, we need to state two constraints in the where clause, connected by\nthe logical connective and:\nselect customer-name, borrower.loan-number, amount\nfrom borrower, loan\nwhere borrower.loan-number = loan.loan-number and\nbranch-name = ’Perryridge’\nSQL includes extensions to perform natural joins and outer joins in the from clause.\nWe discuss these extensions in Section 4.10.\n4.2.4\nThe Rename Operation\nSQL provides a mechanism for renaming both relations and attributes. It uses the as\nclause, taking the form:\nold-name as new-name\nThe as clause can appear in both the select and from clauses.\nConsider again the query that we used earlier:\nselect customer-name, borrower.loan-number, amount\nfrom borrower, loan\nwhere borrower.loan-number = loan.loan-number\nThe result of this query is a relation with the following attributes:\ncustomer-name, loan-number, amount.\nThe names of the attributes in the result are derived from the names of the attributes\nin the relations in the from clause.\nWe cannot, however, always derive names in this way, for several reasons: First,\ntwo relations in the from clause may have attributes with the same name, in which\ncase an attribute name is duplicated in the result. Second, if we used an arithmetic\nexpression in the select clause, the resultant attribute does not have a name. Third,\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nII. Relational Databases\n4. SQL\n147\n© The McGraw−Hill \nCompanies, 2001\n4.2\nBasic Structure\n141\neven if an attribute name can be derived from the base relations as in the preced-\ning example, we may want to change the attribute name in the result. Hence, SQL\nprovides a way of renaming the attributes of a result relation.\nFor example, if we want the attribute name loan-number to be replaced with the\nname loan-id, we can rewrite the preceding query as\nselect customer-name, borrower.loan-number as loan-id, amount\nfrom borrower, loan\nwhere borrower.loan-number = loan.loan-number\n4.2.5\nTuple Variables\nThe as clause is particularly useful in deﬁning the notion of tuple variables, as is\ndone in the tuple relational calculus. A tuple variable in SQL must be associated with\na particular relation. Tuple variables are deﬁned in the from clause by way of the as\nclause. To illustrate, we rewrite the query “For all customers who have a loan from\nthe bank, ﬁnd their names, loan numbers, and loan amount” as\nselect customer-name, T.loan-number, S.amount\nfrom borrower as T, loan as S\nwhere T.loan-number = S.loan-number\nNote that we deﬁne a tuple variable in the from clause by placing it after the name of\nthe relation with which it is associated, with the keyword as in between (the keyword\nas is optional). When we write expressions of the form relation-name.attribute-name,\nthe relation name is, in effect, an implicitly deﬁned tuple variable.\nTuple variables are most useful for comparing two tuples in the same relation.\nRecall that, in such cases, we could use the rename operation in the relational algebra.\nSuppose that we want the query “Find the names of all branches that have assets\ngreater than at least one branch located in Brooklyn.” We can write the SQL expression\nselect distinct T.branch-name\nfrom branch as T, branch as S\nwhere T.assets > S.assets and S.branch-city = ’Brooklyn’\nObserve that we could not use the notation branch.asset, since it would not be clear\nwhich reference to branch is intended.\nSQL permits us to use the notation (v1, v2, . . . , vn) to denote a tuple of arity n con-\ntaining values v1, v2, . . . , vn. The comparison operators can be used on tuples, and\nthe ordering is deﬁned lexicographically. For example, (a1, a2) <= (b1, b2) is true if\na1 < b1, or (a1 = b1) ∧(a2 <= b2); similarly, the two tuples are equal if all their\nattributes are equal.\n4.2.6\nString Operations\nSQL speciﬁes strings by enclosing them in single quotes, for example, ’Perryridge’,\nas we saw earlier. A single quote character that is part of a string can be speciﬁed by\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nII. Relational Databases\n4. SQL\n148\n© The McGraw−Hill \nCompanies, 2001\n142\nChapter 4\nSQL\nusing two single quote characters; for example the string “It’s right” can be speciﬁed\nby ’It”s right’.\nThe most commonly used operation on strings is pattern matching using the op-\nerator like. We describe patterns by using two special characters:\n• Percent (%): The % character matches any substring.\n• Underscore ( ): The\ncharacter matches any character.\nPatterns are case sensitive; that is, uppercase characters do not match lowercase char-\nacters, or vice versa. To illustrate pattern matching, we consider the following exam-\nples:\n• ’Perry%’ matches any string beginning with “Perry”.\n• ’%idge%’ matches any string containing “idge” as a substring, for example,\n’Perryridge’, ’Rock Ridge’, ’Mianus Bridge’, and ’Ridgeway’.\n• ’\n’ matches any string of exactly three characters.\n• ’\n%’ matches any string of at least three characters.\nSQL expresses patterns by using the like comparison operator. Consider the query\n“Find the names of all customers whose street address includes the substring ‘Main’.”\nThis query can be written as\nselect customer-name\nfrom customer\nwhere customer-street like ’%Main%’\nFor pattern\n\ne value d, we can use extract (ﬁeld from\nd), where ﬁeld can be one of year, month, day, hour, minute, or second.\nSQL allows comparison operations on all the domains listed here, and it allows\nboth arithmetic and comparison operations on the various numeric domains. SQL\nalso provides a data type called interval, and it allows computations based on dates\nand times and on intervals. For example, if x and y are of type date, then x −y is an\ninterval whose value is the number of days from date x to date y. Similarly, adding\nor subtracting an interval to a date or time gives back a date or time, respectively.\nIt is often useful to compare values from compatible domains. For example, since\nevery small integer is an integer, a comparison x < y, where x is a small integer and\ny is an integer (or vice versa), makes sense. We make such a comparison by casting\nsmall integer x as an integer. A transformation of this sort is called a type coercion.\nType coercion is used routinely in common programming languages, as well as in\ndatabase systems.\nAs an illustration, suppose that the domain of customer-name is a character string\nof length 20, and the domain of branch-name is a character string of length 15. Al-\nthough the string lengths might differ, standard SQL will consider the two domains\ncompatible.\nAs we discussed in Chapter 3, the null value is a member of all domains. For cer-\ntain attributes, however, null values may be inappropriate. Consider a tuple in the\ncustomer relation where customer-name is null. Such a tuple gives a street and city for\nan anonymous customer; thus, it does not contain useful information. In cases such\nas this, we wish to forbid null values, and we do so by restricting the domain of\ncustomer-name to exclude null values.\nSQL allows the domain declaration of an attribute to include the speciﬁcation not\nnull and thus prohibits the insertion of a null value for this attribute. Any database\nmodiﬁcation that would cause a null to be inserted in a not null domain generates\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nII. Relational Databases\n4. SQL\n176\n© The McGraw−Hill \nCompanies, 2001\n170\nChapter 4\nSQL\nan error diagnostic. There are many situations where we want to avoid null values.\nIn particular, it is essential to prohibit null values in the primary key of a relation\nschema. Thus, in our bank example, in the customer relation, we must prohibit a null\nvalue for the attribute customer-name, which is the primary key for customer.\n4.11.2\nSchema Deﬁnition in SQL\nWe deﬁne an SQL relation by using the create table command:\ncreate table r(A1D1, A2D2, . . . , AnDn,\n⟨integrity-constraint1⟩,\n. . . ,\n⟨integrity-constraintk⟩)\nwhere r is the name of the relation, each Ai is the name of an attribute in the schema\nof relation r, and Di is the domain type of values in the domain of attribute Ai. The\nallowed integrity constraints include\n• primary key (Aj1, Aj2, . . . , Ajm): The primary key speciﬁcation says that at-\ntributes Aj1, Aj2, . . . , Ajm form the primary key for the relation. The primary\nkey attributes are required to be non-null and unique; that is, no tuple can have\na null value for a primary key attribute, and no two tuples in the relation can\nbe equal on all the primary-key attributes.1 Although the primary key speciﬁ-\ncation is optional, it is generally a good idea to specify a primary key for each\nrelation.\n• check(P): The check clause speciﬁes a predicate P that must be satisﬁed by\nevery tuple in the relation.\nThe create table command also includes other integrity constraints, which we shall\ndiscuss in Chapter 6.\nFigure 4.8 presents a partial SQL DDL deﬁnition of our bank database. Note that,\nas in earlier chapters, we do not attempt to model precisely the real world in the\nbank-database example. In the real world, multiple people may have the same name,\nso customer-name would not be a primary key customer; a customer-id would more\nlikely be used as a primary key. We use customer-name as a primary key to keep our\ndatabase schema simple and short.\nIf a newly inserted or modiﬁed tuple in a relation has null values for any primary-\nkey attribute, or if the tuple has the same value on the primary-key attributes as does\nanother tuple in the relation, SQL ﬂags an error and prevents the update. Similarly, it\nﬂags an error and prevents the update if the check condition on the tuple fails.\nBy default null is a legal value for every attribute in SQL, unless the attribute is\nspeciﬁcally stated to be not null. An attribute can be declared to be not null in the\nfollowing way:\naccount-number char(10) not null\n1.\nIn SQL-89, primary-key attributes were not implicitly declared to be not null; an explicit not null\ndeclaration was required.\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nII. Relational Databases\n4. SQL\n177\n© The McGraw−Hill \nCompanies, 2001\n4.11\nData-Deﬁnition Language\n171\ncreate table customer\n(customer-name\nchar(20),\ncustomer-street\nchar(30),\ncustomer-city\nchar(30),\nprimary key (customer-name))\ncreate table branch\n(branch-name\nchar(15),\nbranch-city\nchar(30),\nassets\ninteger,\nprimary key (branch-name),\ncheck (assets >= 0))\ncreate table account\n(account-number char(10),\nbranch-name\nchar(15),\nbalance\ninteger,\nprimary key (account-number),\ncheck (balance >= 0))\ncreate table depositor\n(customer-name\nchar(20),\naccount-number\nchar(10),\nprimary key (customer-name, account-number))\nFigure 4.8\nSQL data deﬁnition for part of the bank database.\nSQL also supports an integrity constraint\nunique (Aj1, Aj2, . . . , Ajm)\nThe unique speciﬁcation says that attributes Aj1, Aj2, . . . , Ajm form a candidate key;\nthat is, no two tuples in the relation can be equal on all the primary-key attributes.\nHowever, candidate key attributes are permitted to be null unless they have explicitly\nbeen declared to be not null. Recall that a null value does not equal any other value.\nThe treatment of nulls here is the same as that of the unique construct deﬁned in\nSection 4.6.4.\nA common use of the check clause is to ensure that attribute values satisfy spec-\niﬁed conditions, in effect creating a powerful type system. For instance, the check\nclause in the create table command for relation branch checks that the value of assets\nis nonnegative. As another example, consider the following:\ncreate table student\n(name\nchar(15) not null,\nstudent-id\nchar(10),\ndegree-level\nchar(15),\nprimary key (student-id),\ncheck (degree-level in (’Bachelors’, ’Masters’, ’Doctorate’)))\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nII. Relational Databases\n4. SQL\n178\n© The McGraw−Hill \nCompanies, 2001\n172\nChapter 4\nSQL\nHere, we use the check clause to simulate an enumerated type, by specifying that\ndegree-level must be one of ’Bachelors’, ’Masters’, or ’Doctorate’. We consider more\ngeneral forms of check conditions, as well as a class of constraints called referential\nintegrity constraints, in Chapter 6.\nA newly created relation is empty initially. We can use the insert command to load\ndata into the relation. Many relational-database products have special bulk loader\nutilities to load an initial set of tuples into a relation.\nTo remove a relation from an SQL database, we use the drop table command. The\ndrop table command deletes all information about the dropped relation from the\ndatabase. The command\ndrop table r\nis a more drastic action than\ndelete from r\nThe latter retains relation r, but deletes all tuples in r. The former deletes not only all\ntuples of r, but also the schema for r. After r is dropped, no tuples can be inserted\ninto r unless it is re-created with the create table command.\nWe use the alter table command to add attributes to an existing relation. All tuples\nin the relation are assigned null as the value for the new attribute. The form of the\nalter table command is\nalter table r add A D\nwhere r is the name of an existing relation, A is the name of the attribute to be added,\nand D is the domain of the added attribute. We can drop attributes from a relation by\nthe command\nalter table r drop A\nwhere r is the name of an existing relation, and A is the name of an attribute of the\nrelation. Many database systems do not support dropping of attributes, although\nthey will allow an entire table to be dropped.\n4.12\nEmbedded SQL\nSQL provides a powerful declarative query language. Writing queries in SQL is usu-\nally much easier than coding the same queries in a general-purpose programming\nlanguage. However, a programmer must have access to a database from a general-\npurpose programming language for at least two reasons:\n1. Not all queries can be expressed in SQL, since SQL does not provide the full\nexpressive power of a general-purpose language. That is, there exist queries\nthat can be expressed in a language such as C, Java, or Cobol that cannot be\nexpressed in SQL. To write such queries, we can embed SQL within a more\npowerful language.\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nII. Relational Databases\n4. SQL\n179\n© The McGraw−Hill \nCompanies, 2001\n4.12\nEmbedded SQL\n173\nSQL is designed so that queries written in it can be optimized automatically\nand executed efﬁciently—and providing the full power of a programming\nlanguage makes automatic optimization exceedingly difﬁcult.\n2. Nondeclarative actions—such as printing a report, interacting with a user, or\nsending the results of a query to a graphical user interface—cannot be done\nfrom within SQL. Applications usually have several components, and query-\ning or updating data is only one component; other components are written in\ngeneral-purpose programming languages. For an integrated application, the\nprograms written in the programming language must be able to access the\ndatabase.\nThe SQL standard deﬁnes embeddings of SQL in a variety of programming lan-\nguages, such as C, Cobol, Pascal, Java, PL/I, and Fortran. A language in which SQL\nqueries are embedded is referred to as a host language, and the SQL structures per-\nmitted in the host language constitute embedded SQL.\nPrograms written in the host language can use the embedded SQL syntax to ac-\ncess and update data stored in a database. This embedded form of SQL extends the\nprogrammer’s ability to manipulate the database even further. In embedded SQL, all\nquery processing is performed by the database system, which then makes the result\nof the query available to the program one tuple (record) at a time.\nAn embedded SQL program must be processed by a special preprocessor prior to\ncompilation. The preprocessor replaces embedded SQL requests with host-language\ndeclarations and procedure calls that allow run-time execution of the database ac-\ncesses. Then, the resulting program is compiled by the host-language compiler. To\nidentify embedded SQL requests to the preprocessor, we use the EXEC SQL statement;\nit has the form\nEXEC SQL <embedded SQL statement > END-EXEC\nThe exact syntax for embedded SQL requests depends on the language in which\nSQL is embedded. For instance, a semicolon is used instead of END-EXEC when SQL\nis embedded in C. The Java embedding of SQL (called SQLJ) uses the syntax\n# SQL { <embedded SQL statement > };\nWe place the statement SQL INCLUDE in the program to identify the place where\nthe preprocessor should insert the special variables used for communication between\nthe program and the database system. Variables of the host language can be used\nwithin embedded SQL statements, but they must be preceded by a colon (:) to distin-\nguish them from SQL variables.\nEmbedded SQL statements are similar in form to the SQL statements that we de-\nscribed in this chapter. There are, however, several important differences, as we note\nhere.\nTo write a relational query, we use the declare cursor statement. The result of the\nquery is not yet computed. Rather, the program must use the open and fetch com-\nmands (discussed later in this section) to obtain the result tuples.\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nII. Relational Databases\n4. SQL\n180\n© The McGraw−Hill \nCompanies, 2001\n174\nChapter 4\nSQL\nConsider the banking schema that we have used in this chapter. Assume that we\nhave a host-language variable amount, and that we wish to ﬁnd the names and cities\nof residence of customers who have more than amount dollars in any account. We can\nwrite this query as follows:\nEXEC SQL\ndeclare c cursor for\nselect customer-name, customer-city\nfrom depositor, customer, account\nwhere depositor.customer-name = customer.customer-name and\naccount.account-number = depositor.account-number and\naccount.balance > :amount\nEND-EXEC\nThe variable c in the preceding expression is called a cursor for the query. We use\nthis variable to identify the query in the open statement, which causes the query to\nbe evaluated, and in the fetch statement, which causes the values of one tuple to be\nplaced in host-language variables.\nThe open statement for our sample query is as follows:\nEXEC SQL open c END-EXEC\nThis statement causes the database system to execute the query and to save the results\nwithin a temporary relation. The query has a host-language variable (:amount); the\nquery uses the value of the variable at the time the open statement was executed.\nIf the SQL query results in an error, the database system stores an error diagnostic\nin the SQL communication-area (SQLCA) variables, whose declarations are inserted\nby the SQL INCLUDE statement.\nAn embedded SQL program executes a series of fetch statements to retrieve tuples\nof the result. The fetch statement requires one host-language variable for each at-\ntribute of the result relation. For our example query, we need one variable to hold the\ncustomer-name value and another to hold the customer-city value. Suppose that those\nvariables are cn and cc, respectively. Then the statement:\nEXEC SQL fetch c into :cn, :cc END-EXEC\nproduces a tuple of the result relation. The program can then manipulate the vari-\nables cn and cc by using the features of the host programming language.\nA single fetch request returns only one tuple. To obtain all tuples of the result,\nthe program must contain a loop to iterate over all tuples. Embedded SQL assists the\nprogrammer in managing this iteration. Although a relation is conceptually a set, the\ntuples of the result of a query are in some ﬁxed physical order. When the program\nexecutes an open statement on a cursor, the cursor is set to point to the ﬁrst tuple\nof the result. Each time it executes a fetch statement, the cursor is updated to point\nto the next tuple of the result. When no further tuples remain to be processed, the\nvariable SQLSTATE in the SQLCA is set to ’02000’ (meaning “no data”). Thus, we can\nuse a while loop (or equivalent loop) to process each tuple of the result.\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nII. Relational Databases\n4. SQL\n181\n© The McGraw−Hill \nCompanies, 2001\n4.13\nDynamic SQL\n175\nWe must use the close statement to tell the database system to delete the tempo-\nrary relation that held the result of the query. For our example, this statement takes\nthe form\nEXEC SQL close c END-EXEC\nSQLJ, the Java embedding of SQL, provides a variation of the above scheme, where\nJava iterators are used in place of cursors. SQLJ associates the results of a query with\nan iterator, and the next() method of the Java iterator interface can be used to step\nthrough the result tuples, just as the preceding examples use fetch on the cursor.\nEmbedded SQL expressions for database modiﬁcation (update, insert, and delete)\ndo not return a result. Thus, they are somewhat simpler to express. A database-\nmodiﬁcation request takes the form\nEXEC SQL < any valid update, insert, or delete> END-EXEC\nHost-language variables, preceded by a colon, may appear in the SQL database-\nmodiﬁcation expression. If an error condition arises in the execution of the statement,\na diagnostic is set in the SQLCA.\nDatabase relations can also be updated through cursors. For example, if we want\nto add 100 to the balance attribute of every account where the branch name is “Per-\nryridge”, we could declare a cursor as follows.\ndeclare c cursor for\nselect *\nfrom account\nwhere branch-name = ‘Perryridge‘\nfor update\nWe then iterate through the tuples by performing fetch operations on the cursor (as\nillustrated earlier), and after fetching each tuple we execute the following code\nupdate account\nset balance = balance + 100\nwhere current of c\nEmbedded SQL allows a host-language program to access the database, but it pro-\nvides no assistance in presenting results to the user or in generating reports. Most\ncommercial database products include tools to assist application programmers in\ncreating user interfaces and formatted reports. We discuss such tools in Chapter 5\n(Section 5.3).\n4.13\nDynamic SQL\nThe dynamic SQL component of SQL allows programs to construct and submit SQL\nqueries at run time. In contrast, embedded SQL statements must be completely present\nat compile time; they are compiled by the embedded SQL preprocessor. Using dy-\nnamic SQL, programs can create SQL queries as strings at run time (perhaps based on\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nII. Relational Databases\n4. SQL\n182\n© The McGraw−Hill \nCompanies, 2001\n176\nChapter 4\nSQL\ninput from the user) and can either have them executed immediately or have them\nprepared for subsequent use. Preparing a dynamic SQL statement compiles it, and\nsubsequent uses of the prepared statement use the compiled version.\nSQL deﬁnes standards for embedding dynamic SQL calls in a host language, such\nas C, as in the following example.\nchar * sqlprog = ”update account set balance = balance ∗1.05\nwhere account-number = ?”\nEXEC SQL prepare dynprog from :sqlprog;\nchar account[10] = ”A-101”;\nEXEC SQL execute dynprog using :account;\nThe dynamic SQL program contains a ?, which is a place holder for a value that is\nprovided when the SQL program is executed.\nHowever, the syntax above requires extensions to the language or a preprocessor\nfor the extended language. An alternative that is very widely used is to use an appli-\ncation program interface to send SQL queries or updates to a database system, and\nnot make any changes in the programming language itself.\nIn the rest of this section, we look at two standards for connecting to an SQL\ndatabase and performing queries and updates. One, ODBC, is an application pro-\ngram interface for the C language, while the other, JDBC, is an application program\ninterface for the Java language.\nTo understand these standards, we need to understand the concept of SQL ses-\nsions. The user or application connects to an SQL server, establishing a session; exe-\ncutes a series of statements; and ﬁnally disconnects the session. Thus, all activities of\nthe user or application are in the context of an SQL session. In addition to the normal\nSQL commands, a session can also contain commands to commit the work carried out\nin the session, or to rollback the work carried out in the session.\n4.13.1\nODBC∗∗\nThe Open DataBase Connectivity (ODBC) standard deﬁnes a way for an application\nprogram to communicate with a database server. ODBC deﬁnes an application pro-\ngram interface (API) that applications can use to open a connection with a database,\nsend queries and updates, and get back results. Applications such as graphical user\ninterfaces, statistics packages, and spreadsheets can make use of the same ODBC API\nto connect to any database server that supports ODBC.\nEach database system supporting ODBC provides a library that must be linked\nwith the client program. When the client program makes an ODBC API call, the code\nin the library communicates with the server to carry out the requested action, and\nfetch results.\nFigure 4.9 shows an example of C code using the ODBC API. The ﬁrst step in using\nODBC to communicate with a server is to set up a connection with the server. To do\nso, the program ﬁrst allocates an SQ\n\ntype system of\nobject-oriented databases, combined with relations as the basis for storage of data.\nIt applies inheritance to relations, not just to types. The object-relational data model\nprovides a smooth migration path from relational databases, which is attractive to\nrelational database vendors. As a result, the SQL:1999 standard includes a number\nof object-oriented features in its type system, while continuing to use the relational\nmodel as the underlying model.\nThe XML language was initially designed as a way of adding markup informa-\ntion to text documents, but has become important because of its applications in data\nexchange. XML provides a way to represent data that have nested structure, and fur-\nthermore allows a great deal of ﬂexibility in structuring of data, which is important\nfor certain kinds of nontraditional data. Chapter 10 describes the XML language, and\nthen presents different ways of expressing queries on data represented in XML, and\ntransforming XML data from one form to another.\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nIII. Object−Based \nDatabases and XML\n8. Object−Oriented \nDatabases\n308\n© The McGraw−Hill \nCompanies, 2001\nP\nA\nR\nT\n8\nCase Studies\nThis part describes how different database systems integrate the various concepts\ndescribed earlier in the book. Speciﬁcally, three widely used database systems—IBM\nDB2, Oracle, and Microsoft SQL Server—are covered in Chapters 25, 26, and 27. These\nthree represent three of the most widely used database systems.\nEach of these chapters highlights unique features of each database system: tools,\nSQL variations and extensions, and system architecture, including storage organiza-\ntion, query processing, concurrency control and recovery, and replication.\nThe chapters cover only key aspects of the database products they describe, and\ntherefore should not be regarded as a comprehensive coverage of the product. Fur-\nthermore, since products are enhanced regularly, details of the product may change.\nWhen using a particular product version, be sure to consult the user manuals for\nspeciﬁc details.\nKeep in mind that the chapters in this part use industrial rather than academic\nterminology. For instance, they use table instead of relation, row instead of tuple,\nand column instead of attribute.\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nIII. Object−Based \nDatabases and XML\n8. Object−Oriented \nDatabases\n309\n© The McGraw−Hill \nCompanies, 2001\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nIII. Object−Based \nDatabases and XML\n8. Object−Oriented \nDatabases\n310\n© The McGraw−Hill \nCompanies, 2001\nC\nH\nA\nP\nT\nE\nR\n2\n5\nOracle\nHakan Jakobsson\nOracle Corporation\nWhen Oracle was founded in 1977 as Software Development Laboratories by Larry\nEllison, Bob Miner, and Ed Oates, there were no commercial relational database prod-\nucts. The company, which was later renamed Oracle, set out to build a relational\ndatabase management system as a commercial product, and was the ﬁrst to reach the\nmarket. Since then, Oracle has held a leading position in the relational database mar-\nket, but over the years its product and service offerings have grown beyond the rela-\ntional database server. In addition to tools directly related to database development\nand management, Oracle sells business intelligence tools, including a multidimen-\nsional database management system (Oracle Express), query and analysis tools, data-\nmining products, and an application server with close integration to the database\nserver.\nIn addition to database-related servers and tools, the company also offers appli-\ncation software for enterprise resource planning and customer-relationship manage-\nment, including areas such as ﬁnancials, human resources, manufacturing, market-\ning, sales, and supply chain management. Oracle’s Business OnLine unit offers ser-\nvices in these areas as an application service provider.\nThis chapter surveys a subset of the features, options, and functionality of Oracle\nproducts. New versions of the products are being developed continually, so all prod-\nuct descriptions are subject to change. The feature set described here is based on the\nﬁrst release of Oracle9i.\n25.1\nDatabase Design and Querying Tools\nOracle provides a variety of tools for database design, querying, report generation\nand data analysis, including OLAP.\n921\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nIII. Object−Based \nDatabases and XML\n8. Object−Oriented \nDatabases\n311\n© The McGraw−Hill \nCompanies, 2001\n922\nChapter 25\nOracle\n25.1.1\nDatabase Design Tools\nMost of Oracle’s design tools are included in the Oracle Internet Development Suite.\nThis is a suite of tools for various aspects of application development, including tools\nfor forms development, data modeling, reporting, and querying. The suite supports\nthe UML standard (see Section 2.10) for development modeling. It provides class\nmodeling to generate code for the business components for Java framework as well\nas activity modeling for general-purpose control ﬂow modeling. The suite also sup-\nports XML for data exchange with other UML tools.\nThe major database design tool in the suite is Oracle Designer, which translates\nbusiness logic and data ﬂows into a schema deﬁnitions and procedural scripts for\napplication logic. It supports such modeling techniques as E-R diagrams, information\nengineering, and object analysis and design. Oracle Designer stores the design in\nOracle Repository, which serves as a single point of metadata for the application.\nThe metadata can then be used to generate forms and reports. Oracle Repository\nprovides conﬁguration management for database objects, forms applications, Java\nclasses, XML ﬁles, and other types of ﬁles.\nThe suite also contains application development tools for generating forms, re-\nports, and tools for various aspects of Java and XML-based development. The busi-\nness intelligence component provides JavaBeans for analytic functionality such as\ndata visualization, querying, and analytic calculations.\nOracle also has an application development tool for data warehousing, Oracle\nWarehouse Builder. Warehouse Builder is a tool for design and deployment of all as-\npects of a data warehouse, including schema design, data mapping and transforma-\ntions, data load processing, and metadata management. Oracle Warehouse Builder\nsupports both 3NF and star schemas and can also import designs from Oracle De-\nsigner.\n25.1.2\nQuerying Tools\nOracle provides tools for ad-hoc querying, report generation and data analysis, in-\ncluding OLAP.\nOracle Discoverer is a Web-based, ad hoc query, reporting, analysis and Web pub-\nlishing tool for end users and data analysts. It allows users to drill up and down on\nresult sets, pivot data, and store calculations as reports that can be published in a\nvariety of formats such as spreadsheets or HTML. Discoverer has wizards to help end\nusers visualize data as graphs. Oracle9i has supports a rich set of analytical func-\ntions, such as ranking and moving aggregation in SQL. Discoverer’s ad hoc query\ninterface can generate SQL that takes advantage of this functionality and can pro-\nvide end users with rich analytical functionality. Since the processing takes place in\nthe relational database management system, Discoverer does not require a complex\nclient-side calculation engine and there is a version of Discoverer that is browser\nbased.\nOracle Express Server is a multidimensional database server. It supports a wide\nvariety of analytical queries as well as forecasting, modeling, and scenario manage-\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nIII. Object−Based \nDatabases and XML\n8. Object−Oriented \nDatabases\n312\n© The McGraw−Hill \nCompanies, 2001\n25.2\nSQL Variations and Extensions\n923\nment. It can use the relational database management system as a back end for storage\nor use its own multidimensional storage of the data.\nWith the introduction of OLAP services in Oracle9i, Oracle is moving away from\nsupporting a separate storage engine and moving most of the calculations into SQL.\nThe result is a model where all the data reside in the relational database management\nsystem and where any remaining calculations that cannot be performed in SQL are\ndone in a calculation engine running on the database server. The model also provides\na Java OLAP application programmer interface.\nThere are many reasons for moving away from a separate multidimensional stor-\nage engine:\n• A relational engine can scale to much larger data sets.\n• A common security model can be used for the analytical applications and the\ndata warehouse.\n• Multidimensional modeling can be integrated with data warehouse modeling.\n• The relational database management system has a larger set of features and\nfunctionality in many areas such as high availability, backup and recovery,\nand third-party tool support.\n• There is no need to train database administrators for two database engines.\nThe main challenge with moving away from a separate multidimensional database\nengine is to provide the same performance. A multidimensional database manage-\nment system that materializes all or large parts of a data cube can offer very fast\nresponse times for many calculations. Oracle has approached this problem in two\nways.\n• Oracle has added SQL support for a wide range of analytical functions, in-\ncluding cube, rollup, grouping sets, ranks, moving aggregation, lead and lag\nfunctions, histogram buckets, linear regression, and standard deviation, along\nwith the ability to optimize the execution of such functions in the database en-\ngine.\n• Oracle has extended materialized views to permit analytical functions, in par-\nticular grouping sets. The ability to materialize parts or all of the cube is key\nto the performance of a multidimensional database management system and\nmaterialized views give a relational database management system the ability\nto do the same thing.\n25.2\nSQL Variations and Extensions\nOracle9i supports all core SQL:1999 features fully or partially, with some minor ex-\nceptions such as distinct data types. In addition, Oracle supports a large number of\nother language constructs, some of which conform with SQL:1999, while others are\nOracle-speciﬁc in syntax or functionality. For example, Oracle supports the OLAP\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nIII. Object−Based \nDatabases and XML\n8. Object−Oriented \nDatabases\n313\n© The McGraw−Hill \nCompanies, 2001\n924\nChapter 25\nOracle\noperations described in Section 22.2, including ranking, moving aggregation, cube,\nand rollup.\nA few examples of Oracle SQL extensions are:\n• connect by, which is a form of tree traversal that allows transitive closure-\nstyle calculations in a single SQL statement. It is an Oracle-speciﬁc syntax for\na feature that Oracle has had since the 1980s.\n• Upsert and multitable inserts. The upsert operation combines update and in-\nsert, and is useful for merging new data with old data in data warehousing\napplications. If a new row has the same key value as an old row, the old row is\nupdated (for example by adding the measure values from the new row), oth-\nerwise the new row is inserted into the table. Multitable inserts allow multiple\ntables to be updated based on a single scan of new data.\n• with clause, which is described in Section 4.8.2.\n25.2.1\nObject-Relational Features\nOracle has extensive support for object-relational constructs, including:\n• Object types. A single-inheritance model is supported for type hierarchies.\n• Collection types. Oracle supports varrays which are variable length arrays,\nand nested tables.\n• Object tables. These are used to store objects while providing a relational\nview of the attributes of the objects.\n• Table functions. These are functions that produce sets of rows as output, and\ncan be used in the from clause of a query. Table functions in Oracle can be\nnested. If a table function is used to express some form of data transformation,\nnesting multiple functions allows multiple transformations to be expressed in\na single statement.\n• Object views. These provide a virtual object table view of data stored in a\nregular relational table. They allow data to be accessed or viewed in an object-\noriented style even if the data are really stored in a traditional relational for-\nmat.\n• Methods. These can be written in PL/SQL, Java, or C.\n• User-deﬁned aggregate functions. These can be used in SQL statements in the\nsame way as built-in functions such as sum and count.\n• XML data types. These can be used to store and index XML documents.\nOracle has two main procedural languages, PL/SQL and Java. PL/SQL was Oracle’s\noriginal language for stored procedures and it has syntax similar to that used in the\nAda language. Java is supported through a Java virtual machine inside the database\nengine. Oracle provides a package to encapsulate related procedures, functions, and\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nIII. Object−Based \nDatabases and XML\n8. Object−Oriented \nDatabases\n314\n© The McGraw−Hill \nCompanies, 2001\n25.3\nStorage and Indexing\n925\nvariables into single units. Oracle supports SQLJ (SQL embedded in Java) and JDBC,\nand provides a tool to generate Java class deﬁnitions corresponding to user-deﬁned\ndatabase types.\n25.2.2\nTriggers\nOracle provides several types of triggers and several options for when and how they\nare invoked. (See Section 6.4 for an introduction to triggers in SQL.) Triggers can be\nwritten in PL/SQL or Java or as C callouts.\nFor triggers that execute on DML statements such as insert, update, and delete,\nOracle supports row triggers and statement triggers. Row triggers execute once for\nevery row that is affected (updated or deleted, for example) by the DML operation.\nA statement trigger is executed just once per statement. In each case, the trigger can\nbe deﬁned as either a before or after trigger, depending on whether it is to be invoked\nbefore or after the DML operation is carried out.\nOracle allows the creation of instead of triggers for views that cannot be subject\nto DML operations. Depending on the view deﬁnition, it may not be possible for Or-\nacle to translate a DML statement on a view to modiﬁcations of the underlying base\ntables unambiguously. Hence, DML operations on views are subject to numerous re-\nstrictions. A user can create an instead of trigger on a view to specify manually what\noperations on the base tables are to occur in response to the DML operation on the\nview. Oracle executes the trigger instead of the DML operation and therefore pro-\nvides a mechanism to circumvent the restrictions on DML operations against views.\nOracle also has triggers that execute on a variety of other events, like database\nstartup or shutdown, server error messages, user logon or logoff, and DDL statements\nsuch as create, alter and drop statements.\n25.3\nStorage and Indexing\nIn Oracle parlance, a database consists of information stored in ﬁles and is accessed\nthrough an instance, which is a shared memory area and a set of processes that inter-\nact with the data in the ﬁles.\n25.3.1\nTable Spaces\nA database consists of one or more logical storage units called table spaces. Each\ntable space, in turn, consists of one or more physical structures called data ﬁles. These\nmay be either ﬁles managed by the operating system or raw devices.\nUsually, an Oracle database will have the following table spaces:\n• The system table space, which is always created. It contains the data dictio-\nnary tables and storage for triggers and stored procedures.\n• Table spaces created to store user data. While user data can be stored in the\nsystem table space, it is often desirable to separate the user data from the sys-\ntem data. Usually, the decision about what other table spaces should be cre-\nated is based on performance, availability, maintainability, and ease of admin-\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nIII. Object−Based \nDatabases and XML\n8. Object−Oriented \nDatabases\n315\n© The McGraw−Hill \nCompanies, 2001\n926\nChapter 25\nOracle\nistration. For example, having multiple table spaces can be useful for partial\nbackup and recovery operations.\n• Temporary table spaces. Many database operations require sorting the data,\nand the sort routine may have to store data temporarily on disk if the sort\ncannot be done in memory. Temporary table spaces are allocated for sorting,\nto make the space management operations involved in spilling to disk more\nefﬁcient.\nTable spaces can also be used as a means of moving data between databases. For\nexample, it is common to move data from a transactional system to a data warehouse\nat regular intervals. Oracle allows moving all the data in a table space from one sys-\ntem to the other by simply copying the ﬁles and exporting and importing a small\namount of data dictionary metadata. These operations can be much faster than un-\nloading the data from one database and then using a loader to insert it into the other.\nA requirement for this feature is that both systems use the same operating system.\n25.3.2\nSegments\nThe space in a table space is divided into units, called segments, that each contain\ndata for a speciﬁc data structure. There are four types of segments.\n• Data segments. Each table in a table space has its own data segment where\nthe table data are stored unless the table is partitioned; if so, there is one data\nsegment per partition. (Partitioning in Oracle is described in Section 25.3.10.)\n• Index segments. Each index in a table space has its own index segment, except\nfor partitioned indices, which have one index segment per partition.\n• Temporary segments. These are segments used when a sort operation needs\nto write data to disk or when data are inserted into a temporary table.\n• Rollback segments. These segments contain undo information so that an un-\ncommitted transaction can be rolled back. They also play an important roll in\nOracle’s concurrency control model and for database recovery, described in\nSections 25.5.1 and 25.5.2.\nBelow the level of segment, space is allocated at a level of granularity called extent.\nEach extent consists of a set of contiguous database blocks. A database block is the\nlowest level of granularity at which Oracle performs disk I/O. A database block does\nnot have to be the same as an operating system block in size, but should be a multiple\nthereof.\nOracle provides storage parameters that allow for detailed control of how space is\nallocated and managed, parameters such as:\n• The size of a new extent that is to be allocated to provide room for rows that\nare inserted into a table.\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nIII. Object−Based \nDatabases and XML\n8. Object−Oriented \nDatabases\n316\n© The McGraw−Hill \nCompanies, 2001\n25.3\nStorage and Indexing\n927\n• The percentage of space utilization at which a database block is considered full\nand at which no more rows will be inserted into that block. (Leaving some free\nspace in a block can allow the existing rows to grow in size through updates,\nwithout running out of space in the block.)\n25.3.3\nTables\nA standard table in Oracle is heap organized; that is, the storage location of a row in\na table is not based on the values contained in the row, and is ﬁxed when the row\nis inserted. However, if the table is partitioned, the content of the row affects the\npartition in which it is stored. There are several features and variations.\nOracle supports nested tables; that is, a table can have a column whose data type\nis another table. The nested table is not stored in line in the parent table, but is stored\nin a separate table.\nOracle supports temporary tables where the duration of the data is either the trans-\naction in which the data are inserted, or the user session. The data are private to the\nsession and are automatically removed at the end of its du\n\n3\nIf the above query were run on the tables in Figure 1.3, the system would ﬁnd that\nthe two accounts numbered A-101 and A-201 are owned by customer 192-83-7465\nand would print out the balances of the two accounts, namely 500 and 900.\nThere are a number of database query languages in use, either commercially or\nexperimentally. We study the most widely used query language, SQL, in Chapter 4.\nWe also study some other query languages in Chapter 5.\nThe levels of abstraction that we discussed in Section 1.3 apply not only to deﬁning\nor structuring data, but also to manipulating data. At the physical level, we must\ndeﬁne algorithms that allow efﬁcient access to data. At higher levels of abstraction,\nwe emphasize ease of use. The goal is to allow humans to interact efﬁciently with the\nsystem. The query processor component of the database system (which we study in\nChapters 13 and 14) translates DML queries into sequences of actions at the physical\nlevel of the database system.\n1.5.3\nDatabase Access from Application Programs\nApplication programs are programs that are used to interact with the database. Ap-\nplication programs are usually written in a host language, such as Cobol, C, C++, or\nJava. Examples in a banking system are programs that generate payroll checks, debit\naccounts, credit accounts, or transfer funds between accounts.\nTo access the database, DML statements need to be executed from the host lan-\nguage. There are two ways to do this:\n• By providing an application program interface (set of procedures) that can\nbe used to send DML and DDL statements to the database, and retrieve the\nresults.\nThe Open Database Connectivity (ODBC) standard deﬁned by Microsoft\nfor use with the C language is a commonly used application program inter-\nface standard. The Java Database Connectivity (JDBC) standard provides cor-\nresponding features to the Java language.\n• By extending the host language syntax to embed DML calls within the host\nlanguage program. Usually, a special character prefaces DML calls, and a pre-\nprocessor, called the DML precompiler, converts the DML statements to nor-\nmal procedure calls in the host language.\n1.6\nDatabase Users and Administrators\nA primary goal of a database system is to retrieve information from and store new\ninformation in the database. People who work with a database can be categorized as\ndatabase users or database administrators.\n1.6.1\nDatabase Users and User Interfaces\nThere are four different types of database-system users, differentiated by the way\nthey expect to interact with the system. Different types of user interfaces have been\ndesigned for the different types of users.\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\n1. Introduction\nText\n24\n© The McGraw−Hill \nCompanies, 2001\n14\nChapter 1\nIntroduction\n• Naive users are unsophisticated users who interact with the system by invok-\ning one of the application programs that have been written previously. For\nexample, a bank teller who needs to transfer $50 from account A to account B\ninvokes a program called transfer. This program asks the teller for the amount\nof money to be transferred, the account from which the money is to be trans-\nferred, and the account to which the money is to be transferred.\nAs another example, consider a user who wishes to ﬁnd her account bal-\nance over the World Wide Web. Such a user may access a form, where she\nenters her account number. An application program at the Web server then\nretrieves the account balance, using the given account number, and passes\nthis information back to the user.\nThe typical user interface for naive users is a forms interface, where the\nuser can ﬁll in appropriate ﬁelds of the form. Naive users may also simply\nread reports generated from the database.\n• Application programmers are computer professionals who write application\nprograms. Application programmers can choose from many tools to develop\nuser interfaces. Rapid application development (RAD) tools are tools that en-\nable an application programmer to construct forms and reports without writ-\ning a program. There are also special types of programming languages that\ncombine imperative control structures (for example, for loops, while loops\nand if-then-else statements) with statements of the data manipulation lan-\nguage. These languages, sometimes called fourth-generation languages, often\ninclude special features to facilitate the generation of forms and the display of\ndata on the screen. Most major commercial database systems include a fourth-\ngeneration language.\n• Sophisticated users interact with the system without writing programs. In-\nstead, they form their requests in a database query language. They submit\neach such query to a query processor, whose function is to break down DML\nstatements into instructions that the storage manager understands. Analysts\nwho submit queries to explore data in the database fall in this category.\nOnline analytical processing (OLAP) tools simplify analysts’ tasks by let-\nting them view summaries of data in different ways. For instance, an analyst\ncan see total sales by region (for example, North, South, East, and West), or by\nproduct, or by a combination of region and product (that is, total sales of each\nproduct in each region). The tools also permit the analyst to select speciﬁc re-\ngions, look at data in more detail (for example, sales by city within a region)\nor look at the data in less detail (for example, aggregate products together by\ncategory).\nAnother class of tools for analysts is data mining tools, which help them\nﬁnd certain kinds of patterns in data.\nWe study OLAP tools and data mining in Chapter 22.\n• Specialized users are sophisticated users who write specialized database\napplications that do not ﬁt into the traditional data-processing framework.\nAmong these applications are computer-aided design systems, knowledge-\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\n1. Introduction\nText\n25\n© The McGraw−Hill \nCompanies, 2001\n1.7\nTransaction Management\n15\nbase and expert systems, systems that store data with complex data types (for\nexample, graphics data and audio data), and environment-modeling systems.\nChapters 8 and 9 cover several of these applications.\n1.6.2\nDatabase Administrator\nOne of the main reasons for using DBMSs is to have central control of both the data\nand the programs that access those data. A person who has such central control over\nthe system is called a database administrator (DBA). The functions of a DBA include:\n• Schema deﬁnition. The DBA creates the original database schema by execut-\ning a set of data deﬁnition statements in the DDL.\n• Storage structure and access-method deﬁnition.\n• Schema and physical-organization modiﬁcation. The DBA carries out chang-\nes to the schema and physical organization to reﬂect the changing needs of the\norganization, or to alter the physical organization to improve performance.\n• Granting of authorization for data access. By granting different types of\nauthorization, the database administrator can regulate which parts of the data-\nbase various users can access. The authorization information is kept in a\nspecial system structure that the database system consults whenever some-\none attempts to access the data in the system.\n• Routine maintenance. Examples of the database administrator’s routine\nmaintenance activities are:\n\u0000 Periodically backing up the database, either onto tapes or onto remote\nservers, to prevent loss of data in case of disasters such as ﬂooding.\n\u0000 Ensuring that enough free disk space is available for normal operations,\nand upgrading disk space as required.\n\u0000 Monitoring jobs running on the database and ensuring that performance\nis not degraded by very expensive tasks submitted by some users.\n1.7\nTransaction Management\nOften, several operations on the database form a single logical unit of work. An ex-\nample is a funds transfer, as in Section 1.2, in which one account (say A) is debited and\nanother account (say B) is credited. Clearly, it is essential that either both the credit\nand debit occur, or that neither occur. That is, the funds transfer must happen in its\nentirety or not at all. This all-or-none requirement is called atomicity. In addition, it\nis essential that the execution of the funds transfer preserve the consistency of the\ndatabase. That is, the value of the sum A + B must be preserved. This correctness\nrequirement is called consistency. Finally, after the successful execution of a funds\ntransfer, the new values of accounts A and B must persist, despite the possibility of\nsystem failure. This persistence requirement is called durability.\nA transaction is a collection of operations that performs a single logical function\nin a database application. Each transaction is a unit of both atomicity and consis-\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\n1. Introduction\nText\n26\n© The McGraw−Hill \nCompanies, 2001\n16\nChapter 1\nIntroduction\ntency. Thus, we require that transactions do not violate any database-consistency\nconstraints. That is, if the database was consistent when a transaction started, the\ndatabase must be consistent when the transaction successfully terminates. However,\nduring the execution of a transaction, it may be necessary temporarily to allow incon-\nsistency, since either the debit of A or the credit of B must be done before the other.\nThis temporary inconsistency, although necessary, may lead to difﬁculty if a failure\noccurs.\nIt is the programmer’s responsibility to deﬁne properly the various transactions,\nso that each preserves the consistency of the database. For example, the transaction to\ntransfer funds from account A to account B could be deﬁned to be composed of two\nseparate programs: one that debits account A, and another that credits account B. The\nexecution of these two programs one after the other will indeed preserve consistency.\nHowever, each program by itself does not transform the database from a consistent\nstate to a new consistent state. Thus, those programs are not transactions.\nEnsuring the atomicity and durability properties is the responsibility of the data-\nbase system itself—speciﬁcally, of the transaction-management component. In the\nabsence of failures, all transactions complete successfully, and atomicity is achieved\neasily. However, because of various types of failure, a transaction may not always\ncomplete its execution successfully. If we are to ensure the atomicity property, a failed\ntransaction must have no effect on the state of the database. Thus, the database must\nbe restored to the state in which it was before the transaction in question started exe-\ncuting. The database system must therefore perform failure recovery, that is, detect\nsystem failures and restore the database to the state that existed prior to the occur-\nrence of the failure.\nFinally, when several transactions update the database concurrently, the consis-\ntency of data may no longer be preserved, even though each individual transac-\ntion is correct. It is the responsibility of the concurrency-control manager to control\nthe interaction among the concurrent transactions, to ensure the consistency of the\ndatabase.\nDatabase systems designed for use on small personal computers may not have\nall these features. For example, many small systems allow only one user to access\nthe database at a time. Others do not offer backup and recovery, leaving that to the\nuser. These restrictions allow for a smaller data manager, with fewer requirements for\nphysical resources—especially main memory. Although such a low-cost, low-feature\napproach is adequate for small personal databases, it is inadequate for a medium- to\nlarge-scale enterprise.\n1.8\nDatabase System Structure\nA database system is partitioned into modules that deal with each of the responsi-\nbilites of the overall system. The functional components of a database system can be\nbroadly divided into the storage manager and the query processor components.\nThe storage manager is important because databases typically require a large\namount of storage space. Corporate databases range in size from hundreds of gi-\ngabytes to, for the largest databases, terabytes of data. A gigabyte is 1000 megabytes\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\n1. Introduction\nText\n27\n© The McGraw−Hill \nCompanies, 2001\n1.8\nDatabase System Structure\n17\n(1 billion bytes), and a terabyte is 1 million megabytes (1 trillion bytes). Since the\nmain memory of computers cannot store this much information, the information is\nstored on disks. Data are moved between disk storage and main memory as needed.\nSince the movement of data to and from disk is slow relative to the speed of the cen-\ntral processing unit, it is imperative that the database system structure the data so as\nto minimize the need to move data between disk and main memory.\nThe query processor is important because it helps the database system simplify\nand facilitate access to data. High-level views help to achieve this goal; with them,\nusers of the system are not be burdened unnecessarily with the physical details of the\nimplementation of the system. However, quick processing of updates and queries\nis important. It is the job of the database system to translate updates and queries\nwritten in a nonprocedural language, at the logical level, into an efﬁcient sequence of\noperations at the physical level.\n1.8.1\nStorage Manager\nA storage manager is a program module that provides the interface between the low-\nlevel data stored in the database and the application programs and queries submit-\nted to the system. The storage manager is responsible for the interaction with the ﬁle\nmanager. The raw data are stored on the disk using the ﬁle system, which is usu-\nally provided by a conventional operating system. The storage manager translates\nthe various DML statements into low-level ﬁle-system commands. Thus, the storage\nmanager is responsible for storing, retrieving, and updating data in the database.\nThe storage manager components include:\n• Authorization and integrity manager, which tests for the satisfaction of in-\ntegrity constraints and checks the authority of users to access data.\n• Transaction manager, which ensures that the database remains in a consistent\n(correct) state despite system failures, and that concurrent transaction execu-\ntions proceed without conﬂicting.\n• File manager, which manages the allocation of space on disk storage and the\ndata structures used to represent information stored on disk.\n• Buffer manager, which is responsible for fetching data from disk storage into\nmain memory, and deciding what data to cache in main memory. The buffer\nmanager is a critical part of the database system, since it enables the database\nto handle data sizes that are much larger than the size of main memory.\nThe storage manager implements several data structures as part of the physical\nsystem implementation:\n• Data ﬁles, which store the database itself.\n• Data dictionary, which stores metadata about the structure of the database, in\nparticular the schema of the database.\n• Indices, which provide fast access to data items that hold particular values.\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\n1. Introduction\nText\n28\n© The McGraw−Hill \nCompanies, 2001\n18\nChapter 1\nIntroduction\n1.8.2\nThe Query Processor\nThe query processor components include\n• DDL interpreter, which interprets DDL statements and records the deﬁnitions\nin the data dictionary.\n• DML compiler, which translates DML statements in a query language into an\nevaluation plan consisting of low-level instructions that the query evaluation\nengine understands.\nA query can usually be translated into any of a number of alternative eval-\nuation plans that all give the same result. The DML compiler also performs\nquery optimization, that is, it picks the lowest cost evaluation plan from amo-\nng the alternatives.\n• Query evaluation engine, which executes low-level instructions generated by\nthe DML compiler.\nFigure 1.4 shows these components and the connections among them.\n1.9\nApplication Architectures\nMost users of a database system today are not present at the site of the database\nsystem, but connect to it through a network. We can therefore differentiate between\nclient machines, on which remote database users work, and server machines, on\nwhich the database system runs.\nDatabase applications are usually partitioned into two or three parts, as in Fig-\nure 1.5. In a two-tier architecture, the application is partitioned into a component\nthat resides at the client machine, which invokes database system functionality at the\nserver machine through query language statements. Application program interface\nstandards like ODBC and JDBC are used for interaction between the client and the\nserver.\nIn contrast, in a three-tier architecture, the client machine acts as merely a front\nend and does not contain any direct database calls. Instead, the client end communi-\ncates with an application server, usually through a forms interface. The application\nserver in turn communicates with a database system to access data. The business\nlogic of the application, which says what actions to carry out under what conditions,\nis embedded in the application server, instead of being distributed across multiple\nclients. Three-tier applications are more appropriate for large applications, and for\napplications that run on the World Wide Web.\n1.10\nHistory of Database Systems\nData processing drives the growth of computers, as it has from the earliest days of\ncommercial computers. In fact, automation of data processing tasks predates com-\nputers. Punched cards, invented by Hollerith, were used at the very beginning of the\ntwentieth century to record U.S. census data, and mechanical systems were used to\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\n1. Introduction\nText\n29\n© The McGraw−Hill \nCompanies, 2001\n1.10\nHistory of Database Systems\n19\nnaive users\n(tellers, agents, \nweb-users) \nquery processor\nstorage manager\ndisk storage\nindices\nstatistical data\ndata\ndata dictionary\napplication\nprogrammers\napplication\ninterfaces\napplication\nprogram\nobject code\ncompiler and\nlinker\nbuffer manager\nfile manager\nauthorization\nand integrity\n manager\ntransaction\nmanager\nDML compiler \nand organizer\nquery evaluation\nengine\nDML queries\nDDL interpreter\napplication\nprograms\nquery\ntools\nadministration\ntools\nsophisticated\nusers\n(analysts)\ndatabase\nadministrator\nuse\nwrite\nuse\nuse\nFigure 1.4\nSystem structure.\nprocess the cards and tabulate results. Punched cards were later widely used as a\nmeans of entering data into computers.\nTechniques for data storage and processing have evolved over the years:\n• 1950s and early 1960s: Magnetic tapes were developed for data storage. Data\nprocessing tasks such as payroll were automated, with data stored on tapes.\nProcessing of data consisted of reading data from one or more tapes and\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\n1. Introduction\nText\n30\n© The McGraw−Hill \nCompanies, 2001\n20\nChapter 1\nIntroduction\nuser\napplication server\ndatabase system\nuser\ndatabase system\nb.  three-tier architecture\na.  two-tier architecture\nnetwork\nserver\nclient\napplication\nnetwork\napplication client\nFigure 1.5\nTwo-tier and three-tier architectures.\nwriting data to a new tape. Data could also be input from punched card decks,\nand output to printers. For example, salary raises were processed by entering\nthe raises on punched cards and reading the punched card deck in synchro-\nnization with a tape containing the master salary details. The records had to\nbe in the same sorted order. The salary raises would be added to the salary\nread from the master tape, and written to a new tape; the new tape would\nbecome the new master tape.\nTapes (and card decks) could be read only sequentially, and data sizes were\nmuch larger than main memory; thus, data process\n\nferent\nsystems. With the use of gateways, the remote systems can include non-Oracle data-\nbases. Oracle has built-in capability to optimize a query that includes tables at differ-\nent sites, retrieve the relevant data, and return the result as if it had been a normal,\nlocal query. Oracle also transparently supports transactions spanning multiple sites\nby a built-in two-phase-commit protocol.\n25.7.3\nExternal Data Sources\nOracle has several mechanisms for supporting external data sources. The most com-\nmon usage is in data warehousing when large amounts of data are regularly loaded\nfrom a transactional system.\n25.7.3.1\nSQL*Loader\nOracle has a direct load utility, SQL*Loader, that supports fast parallel loads of large\namounts of data from external ﬁles. It supports a variety of data formats and it can\nperform various ﬁltering operations on the data being loaded.\n25.7.3.2\nExternal Tables\nOracle allows external data sources, such as ﬂat ﬁles, to be referenced in the from\nclause of a query as if they were regular tables. An external table is deﬁned by meta-\ndata that describe the Oracle column types and the mapping of the external data into\nthose columns. An access driver is also needed to access the external data. Oracle\nprovides a default driver for ﬂat ﬁles.\nThe external table feature is primarily intended for extraction, transformation, and\nloading (ETL) operations in a data warehousing environment. Data can be loaded into\nthe data warehouse from a ﬂat ﬁle using\ncreate table table as\nselect ... from < external table >\nwhere ...\nBy adding operations on the data in either the select list or where clause, trans-\nformations and ﬁltering can be done as part of the same SQL statement. Since these\noperations can be expressed either in native SQL or in functions written in PL/SQL or\nJava, the external table feature provides a very powerful mechanism for expressing\nall kinds of data transformation and ﬁltering operations. For scalability, the access to\nthe external table can be parallelized by Oracle’s parallel execution feature.\n25.8\nDatabase Administration Tools\nOracle provides users a number of tools for system management and application\ndevelopment.\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nIII. Object−Based \nDatabases and XML\n8. Object−Oriented \nDatabases\n336\n© The McGraw−Hill \nCompanies, 2001\nBibliographical Notes\n947\n25.8.1\nOracle Enterprise Manager\nOracle Enterprise Manager is Oracle’s main tool for database systems management.\nIt provides an easy-to-use graphical user interface (GUI) and a variety of wizards for\nschema management, security management, instance management, storage manage-\nment, and job scheduling. It also provides performance monitoring and tools to help\nan administrator tune application SQL, access paths, and instance and data storage\nparameters. For example, it includes a wizard that can suggest what indices are the\nmost cost-effective to create under a given workload.\n25.8.2\nDatabase Resource Management\nA database administrator needs to be able to control how the processing power of\nthe hardware is divided among users or groups of users. Some groups may execute\ninteractive queries where response time is critical; others may execute long-running\nreports that can be run as batch jobs in the background when the system load is low.\nIt is also important to be able to prevent a user from inadvertently submitting an\nextremely expensive ad hoc query that will unduly delay other users.\nOracle’s Database Resource Management feature allows the database administra-\ntor to divide users into resource consumer groups with different priorities and prop-\nerties. For example, a group of high-priority, interactive users may be guaranteed at\nleast 60 percent of the CPU. The remainder, plus any part of the 60 percent not used\nup by the high-priority group, would be allocated among resource consumer groups\nwith lower priority. A really low-priority group could get assigned 0 percent, which\nwould mean that queries issued by this group would run only when there are spare\nCPU cycles available. Limits for the degree of parallelism for parallel execution can\nbe set for each group. The database administrator can also set time limits for how\nlong an SQL statement is allowed to run for each group. When a users submits a\nstatement, the Resource Manager estimates how long it would take to execute it and\nreturns an error if the statement violates the limit. The resource manager can also\nlimit the number of user sessions that can be active concurrently for each resource\nconsumer group.\nBibliographical Notes\nUp-to-date product information, including documentation, on Oracle products can\nbe found at the Web sites http://www.oracle.com and http://technet.oracle.com.\nExtensible indexing in Oracle8i is described by Srinivasan et al. [2000b], while\nSrinivasan et al. [2000a] describe index organized tables in Oracle8i. Banerjee et al.\n[2000] describe XML support in Oracle8i. Bello et al. [1998] describe materialized\nviews in Oracle. Antoshenkov [1995] describes the byte-aligned bitmap compression\ntechnique used in Oracle; see also Johnson [1999b].\nThe Oracle Parallel Server is described by Bamford et al. [1998]. Recovery in Oracle\nis described by Joshi et al. [1998] and Lahiri et al. [2001]. Messaging and queuing in\nOracle are described by Gawlick [1998].\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nIII. Object−Based \nDatabases and XML\n9. Object−Relational \nDatabases\n337\n© The McGraw−Hill \nCompanies, 2001\nC\nH\nA\nP\nT\nE\nR\n9\nObject-Relational Databases\nPersistent programming languages add persistence and other database features to ex-\nisting programming languages by using an existing object-oriented type system. In\ncontrast, object-relational data models extend the relational data model by providing a\nricher type system including complex data types and object orientation. Relational\nquery languages, in particular SQL, need to be correspondingly extended to deal\nwith the richer type system. Such extensions attempt to preserve the relational foun-\ndations—in particular, the declarative access to data—while extending the model-\ning power. Object-relational database systems (that is, database systems based on\nthe object-relation model) provide a convenient migration path for users of relational\ndatabases who wish to use object-oriented features.\nWe ﬁrst present the motivation for the nested relational model, which allows rela-\ntions that are not in ﬁrst normal form, and allows direct representation of hierarchical\nstructures. We then show how to extend SQL by adding a variety of object-relational\nfeatures. Our discussion is based on the SQL:1999 standard.\nFinally, we discuss differences between persistent programming languages and\nobject-relational systems, and mention criteria for choosing between them.\n9.1\nNested Relations\nIn Chapter 7, we deﬁned ﬁrst normal form (1NF), which requires that all attributes\nhave atomic domains. Recall that a domain is atomic if elements of the domain are\nconsidered to be indivisible units.\nThe assumption of 1NF is a natural one in the bank examples we have considered.\nHowever, not all applications are best modeled by 1NF relations. For example, rather\nthan view a database as a set of records, users of certain applications view it as a set of\nobjects (or entities). These objects may require several records for their representation.\nWe shall see that a simple, easy-to-use interface requires a one-to-one correspondence\n335\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nIII. Object−Based \nDatabases and XML\n9. Object−Relational \nDatabases\n338\n© The McGraw−Hill \nCompanies, 2001\n336\nChapter 9\nObject-Relational Databases\ntitle\nauthor-set\npublisher\nkeyword-set\n(name, branch)\nCompilers\n{Smith, Jones}\n(McGraw-Hill, New York)\n{parsing, analysis}\nNetworks\n{Jones, Frick}\n(Oxford, London)\n{Internet, Web}\nFigure 9.1\nNon-1NF books relation, books.\nbetween the user’s intuitive notion of an object and the database system’s notion of\na data item.\nThe nested relational model is an extension of the relational model in which do-\nmains may be either atomic or relation valued. Thus, the value of a tuple on an at-\ntribute may be a relation, and relations may be contained within relations. A complex\nobject thus can be represented by a single tuple of a nested relation. If we view a tu-\nple of a nested relation as a data item, we have a one-to-one correspondence between\ndata items and objects in the user’s view of the database.\nWe illustrate nested relations by an example from a library. Suppose we store for\neach book the following information:\n• Book title\n• Set of authors\n• Publisher\n• Set of keywords\nWe can see that, if we deﬁne a relation for the preceding information, several domains\nwill be nonatomic.\n• Authors. A book may have a set of authors. Nevertheless, we may want to\nﬁnd all books of which Jones was one of the authors. Thus, we are interested\nin a subpart of the domain element “set of authors.”\n• Keywords. If we store a set of keywords for a book, we expect to be able to\nretrieve all books whose keywords include one or more keywords. Thus, we\nview the domain of the set of keywords as nonatomic.\n• Publisher. Unlike keywords and authors, publisher does not have a set-valued\ndomain. However, we may view publisher as consisting of the subﬁelds name\nand branch. This view makes the domain of publisher nonatomic.\nFigure 9.1 shows an example relation, books. The books relation can be represented\nin 1NF, as in Figure 9.2. Since we must have atomic domains in 1NF, yet want ac-\ncess to individual authors and to individual keywords, we need one tuple for each\n(keyword, author) pair. The publisher attribute is replaced in the 1NF version by two\nattributes: one for each subﬁeld of publisher.\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nIII. Object−Based \nDatabases and XML\n9. Object−Relational \nDatabases\n339\n© The McGraw−Hill \nCompanies, 2001\n9.2\nComplex Types\n337\ntitle\nauthor\npub-name\npub-branch\nkeyword\nCompilers\nSmith\nMcGraw-Hill\nNew York\nparsing\nCompilers\nJones\nMcGraw-Hill\nNew York\nparsing\nCompilers\nSmith\nMcGraw-Hill\nNew York\nanalysis\nCompilers\nJones\nMcGraw-Hill\nNew York\nanalysis\nNetworks\nJones\nOxford\nLondon\nInternet\nNetworks\nFrick\nOxford\nLondon\nInternet\nNetworks\nJones\nOxford\nLondon\nWeb\nNetworks\nFrick\nOxford\nLondon\nWeb\nFigure 9.2\nﬂat-books, a 1NF version of non-1NF relation books.\nMuch of the awkwardness of the ﬂat-books relation in Figure 9.2 disappears if we\nassume that the following multivalued dependencies hold:\n• title →→author\n• title →→keyword\n• title →pub-name, pub-branch\nThen, we can decompose the relation into 4NF using the schemas:\n• authors(title, author)\n• keywords(title, keyword)\n• books4(title, pub-name, pub-branch)\nFigure 9.3 shows the projection of the relation ﬂat-books of Figure 9.2 onto the preced-\ning decomposition.\nAlthough our example book database can be adequately expressed without using\nnested relations, the use of nested relations leads to an easier-to-understand model:\nThe typical user of an information-retrieval system thinks of the database in terms of\nbooks having sets of authors, as the non-1NF design models. The 4NF design would\nrequire users to include joins in their queries, thereby complicating interaction with\nthe system.\nWe could deﬁne a non-nested relational view (whose contents are identical to ﬂat-\nbooks) that eliminates the need for users to write joins in their query. In such a view,\nhowever, we lose the one-to-one correspondence between tuples and books.\n9.2\nComplex Types\nNested relations are just one example of extensions to the basic relational model;\nother nonatomic data types, such as nested records, have also proved useful. The\nobject-oriented data model has caused a need for features such as inheritance and\nreferences to objects. With complex type systems and object orientation, we can rep-\nresent E-R model concepts, such as identity of entities, multivalued attributes, and\ngeneralization and specialization directly, without a complex translation to the rela-\ntional model.\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nIII. Object−Based \nDatabases and XML\n9. Object−Relational \nDatabases\n340\n© The McGraw−Hill \nCompanies, 2001\n338\nChapter 9\nObject-Relational Databases\ntitle\nauthor\nCompilers\nSmith\nCompilers\nJones\nNetworks\nJones\nNetworks\nFrick\nauthors\ntitle\nkeyword\nCompilers\nparsing\nCompilers\nanalysis\nNetworks\nInternet\nNetworks\nWeb\nkeywords\ntitle\npub-name\npub-branch\nCompilers\nMcGraw-Hill\nNew York\nNetworks\nOxford\nLondon\nbooks4\nFigure 9.3\n4NF version of the relation ﬂat-books of Figure 9.2.\nIn this section, we describe extensions to SQL to allow complex types, includ-\ning nested relations, and object-oriented features. Our presentation is based on the\nSQL:1999 standard, but we also outline features that are not currently in the standard\nbut may be introduced in future versions of SQL standards.\n9.2.1\nCollection and Large Object Types\nConsider this fragment of code.\ncreate table books (\n. . .\nkeyword-set setof(varchar(20))\n. . .\n)\nThis table deﬁnition differs from table deﬁnitions in ordinary relational databases,\nsince it allows attributes that are sets, thereby permitting multivalued attributes of\nE-R diagrams to be represented directly.\nSets are an instance of collection types. Other instances of collection types include\narrays and multisets (that is, unordered collections, where an element may occur\nmultiple times). The following attribute deﬁnitions illustrate the declaration of an\narray:\nauthor-array varchar(20) array [10]\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nIII. Object−Based \nDatabases and XML\n9. Object−Relational \nDatabases\n341\n© The McGraw−Hill \nCompanies, 2001\n9.2\nComplex Types\n339\nHere, author-array is an array of up to 10 author names. We can access elements of an\narray by specifying the array index, for example author-array[1].\nArrays are the only collection type supported by SQL:1999; the syntax used is as\nin the preceding declaration. SQL:1999 does not support unordered sets or multisets,\nalthough they may appear in future versions of SQL.1\nMany current-generation database applications need to store attributes that can\nbe large (of the order of many kilobytes), such as a photograph of a person, or very\nlarge (of the order of many megabytes or even gigabytes), such as a high-resolution\nmedical image or video clip. SQL:1999 therefore provides new large-object data types\nfor character data (clob) and binary data (blob). The letters “lob” in these data types\nstand for “Large OBject”. For example, we may declare attributes\nbook-review clob(10KB)\nimage blob(10MB)\nmovie blob(2GB))\nLarge objects are typically used in external applications, and it makes little sense to\nretrieve them in their entirety by SQL. Instead, an application would usually retrieve\na “locator” for a large object and then use the locator to manipulate the object from\nthe host language. For instance, JDBC permits the programmer to fetch a large object\nin small pieces, rather than all at once, much like fetching data from an operating\nsystem ﬁle.\n9.2.2\nStructured Types\nStructured types can be declared and used in SQL:1999 as in the following example:\ncreate type Publisher as\n(name varchar(20),\nbranch varchar(20))\ncreate type Book as\n(title varchar(20),\nauthor-array varchar(20) array [10],\npub-date date,\npublisher Publisher,\nkeyword-set setof(varchar(20)))\ncreate table books of Book\nThe ﬁrst statement deﬁnes a type called Publisher, which has two components: a name\nand a branch. The second statement deﬁnes a structured type Book, which contains\na title, an author-array, which is an array of authors, a publication date, a publisher\n(of type Publisher), and a set of keywords. (The declaration of keyword-set as a set\nuses our extended syntax, and is not supported by the SQL:1999 standard.) The types\nillustrated above are called structured types in SQL:1999.\n1.\nThe Oracle 8 database system supports nested relations, but uses a syntax different from that in this\nchapter.\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nIII. Object−Based \nDatabases and XML\n9. Object−Relational \nDatabases\n342\n© The McGraw−Hill \nCompanies, 2001\n340\nChapter 9\nObject-Relational Databases\nFinally, a table books containing tuples of type Book is created. The table is similar\nto the nested relation books in Figure 9.1, except we have decided to create an array\nof author names instead of a set of author names. The array permits us to record the\norder of author names.\nStructured types allow composite attributes of E-R diagrams to be represented\ndirectly. Unnamed row types can also be used in SQL:1999 to deﬁne composite at-\ntributes. For instance, we could have deﬁned an attribute publisher1 as\npublisher1 row (name varchar(20),\nbranch varchar(20))\ninstead of creating a named type Publisher.\nWe can of course create tables without creating an intermediate type for the table.\nFor example, the table books could also be deﬁned as follows:\ncreate table books\n(title varchar(20),\nauthor-array varchar(20) array[10],\npub-date date,\npublisher Publisher,\nkeyword-set setof(varchar(20)))\nWith the above declaration, there is no explicit type for rows of the table. 2\nA structured type can have methods deﬁned on it. We declare methods as part of\nthe type deﬁnition of a structured type:\ncreate type Employee as (\nname varchar(20),\nsalary integer )\nmethod giveraise (percent integer)\nWe create the method body separately:\ncreate method giveraise (percent integer) for Employee\nbegin\nset self.salary = self.salary + (self.salary * percent) / 100;\nend\nThe variable self refers to the structured type instance on which the method is in-\nvoked. The body of the method can contain procedural statements, which we shall\nstudy in Section 9.6.\n2.\nIn Oracle PL/SQL, given a table t, t%rowtype denotes the type of the rows of the table. Similarly,\nt.a%type denotes the type of attribute a of table t.\nSilberschatz−Korth−Sudarshan: \nDatabase System \nConcepts, Fourth Edition\nIII. Object−Based \nDatabases and XML\n9. Object−Relational \nDatabases\n343\n© The McGraw−Hill \nCompanies, 2001\n9.2\nComplex Types\n341\n9.2.3\nCreation of Values of Complex Types\nIn SQL:1999 constructor functions are used to create values of structured types. A\nfunction with the same name as a structured type is a constructor function for the\nstructured type. For instance, we could declare a constructor for the type Publisher\nlike this:\ncreate function Publisher (n varchar(20), b varchar(20))\nreturns Publisher\nbegin\nset name = n;\nset branch = b;\nend\nWe can then use Publisher(’McGraw-Hill’, ’New York’) to create a value of the type\nPublisher.\nSQL:1999 also supports functions other than constructors, as we shall see in Sec-\ntion 9.6; the names of such functions must be different from the name of any struc-\ntured type.\nNote that in SQL:1999, unlike in object-oriented databases, a constructor creates a\nvalue of the type, not an object of the type. That is, the value the constructor creates\nhas no object identity. In SQL:1999 objects correspond to tuples of a relation, and are\ncreated by inserting a tuple in a relation.\nBy default every structured type has a constructor with no arguments, which sets\nthe attributes to their default values. Any other constructors have to be created explic-\nitly. There can be more than one constructor for the same structured type; although\nthey have the same name, they must be distinguishable by the number of arguments\nand types of their arguments.\nAn array of values can be created in SQL:1999 in this way:\narray[’Silberschatz’, ’Korth’, ’Sudarshan’]\nWe can construct a row value by listing its attributes within parentheses. For instance,\nif we declare an attribute publisher1 as a row type (as in Section 9.2.2), we can con-\nstruct this value for it:\n(’McGraw-Hill’, ’New York’)\nwithout using a constructor.\nWe creat",
    "precision": 0.1708720672864099,
    "recall": 0.9872122762148338,
    "iou": 0.1704946996466431,
    "f1": 0.29132075471698116,
    "gold_tokens_count": 391,
    "retrieved_tokens_count": 2259,
    "intersection_tokens": 386
  }
]